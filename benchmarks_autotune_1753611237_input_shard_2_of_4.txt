Running all 4 kernels...


============================================================
Kernel: gemm
============================================================

Running gemm benchmark with 2 Helion implementations...

Running input shard 2/4: inputs 8 to 15 (of 31 total)
  0%|          | 0/8 [00:00<?, ?it/s]INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for aten_matmul
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_tutorial_matmul
INFO:tritonbench.utils.triton_op:Took 0.07ms to get benchmark function for matmul_partition_k
/home/willfeng/local/pytorch-nightly/torch/backends/cuda/__init__.py:131: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:78.)
  return torch._C._get_cublas_allow_tf32()
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_ops_matmul
INFO:tritonbench.utils.triton_op:Took 4.48ms to get benchmark function for aten_tunableop_matmul
INFO:tritonbench.utils.triton_op:Took 47.30ms to get benchmark function for pt2_triton_matmul
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for streamk_matmul
INFO:tritonbench.utils.triton_op:Took 33.35ms to get benchmark function for pt2_cutlass_matmul
INFO:tritonbench.utils.triton_op:Took 0.05ms to get benchmark function for helion_matmul_tritonbench
[0s] Starting DifferentialEvolutionSearch with population=40, generations=20, crossover_rate=0.8
Removed 8 outliers from 747 samples
Removed 1 outliers from 102 samples
Removed 8 outliers from 737 samples
Removed 2 outliers from 777 samples
[11s] Initial population: failed=3 min=0.0283 mid=0.2837 max=1.2571 best=Config(block_sizes=[64, 32, 32], loop_orders=[[0, 1]], l2_groupings=[1], range_unroll_factors=[0, 1], range_num_stages=[0, 4], range_multi_buffers=[None, False], range_flattens=[None, None], num_warps=2, num_stages=3, indexing='block_ptr', pid_type='flat', range_warp_specializes=[])
[41s] Generation 2: replaced=18 min=0.0207 mid=0.0699 max=0.2941 best=Config(block_sizes=[64, 32, 64], loop_orders=[[1, 0]], l2_groupings=[1], range_unroll_factors=[0, 1], range_num_stages=[0, 4], range_multi_buffers=[None, False], range_flattens=[None, True], num_warps=4, num_stages=8, indexing='pointer', pid_type='flat', range_warp_specializes=[])
[71s] Generation 3: replaced=14 min=0.0207 mid=0.0609 max=0.2448 best=Config(block_sizes=[64, 32, 64], loop_orders=[[1, 0]], l2_groupings=[1], range_unroll_factors=[0, 1], range_num_stages=[0, 4], range_multi_buffers=[None, False], range_flattens=[None, True], num_warps=4, num_stages=8, indexing='pointer', pid_type='flat', range_warp_specializes=[])
[101s] Generation 4: replaced=16 min=0.0207 mid=0.0500 max=0.1465 best=Config(block_sizes=[64, 32, 64], loop_orders=[[1, 0]], l2_groupings=[1], range_unroll_factors=[0, 1], range_num_stages=[0, 4], range_multi_buffers=[None, False], range_flattens=[None, True], num_warps=4, num_stages=8, indexing='pointer', pid_type='flat', range_warp_specializes=[])
[147s] Generation 5: replaced=12 min=0.0207 mid=0.0470 max=0.1342 best=Config(block_sizes=[64, 32, 64], loop_orders=[[1, 0]], l2_groupings=[1], range_unroll_factors=[0, 1], range_num_stages=[0, 4], range_multi_buffers=[None, False], range_flattens=[None, True], num_warps=4, num_stages=8, indexing='pointer', pid_type='flat', range_warp_specializes=[])
[184s] Generation 6: replaced=16 min=0.0181 mid=0.0387 max=0.1342 best=Config(block_sizes=[64, 128, 128], loop_orders=[[1, 0]], l2_groupings=[4], range_unroll_factors=[1, 1], range_num_stages=[1, 4], range_multi_buffers=[False, False], range_flattens=[False, True], num_warps=4, num_stages=3, indexing='pointer', pid_type='persistent_interleaved')
[214s] Generation 7: replaced=6 min=0.0181 mid=0.0387 max=0.0907 best=Config(block_sizes=[64, 128, 128], loop_orders=[[1, 0]], l2_groupings=[4], range_unroll_factors=[1, 1], range_num_stages=[1, 4], range_multi_buffers=[False, False], range_flattens=[False, True], num_warps=4, num_stages=3, indexing='pointer', pid_type='persistent_interleaved')
[248s] Generation 8: replaced=11 min=0.0181 mid=0.0353 max=0.0907 best=Config(block_sizes=[64, 128, 128], loop_orders=[[1, 0]], l2_groupings=[4], range_unroll_factors=[1, 1], range_num_stages=[1, 4], range_multi_buffers=[False, False], range_flattens=[False, True], num_warps=4, num_stages=3, indexing='pointer', pid_type='persistent_interleaved')
[289s] Generation 9: replaced=10 min=0.0181 mid=0.0330 max=0.0907 best=Config(block_sizes=[64, 128, 128], loop_orders=[[1, 0]], l2_groupings=[4], range_unroll_factors=[1, 1], range_num_stages=[1, 4], range_multi_buffers=[False, False], range_flattens=[False, True], num_warps=4, num_stages=3, indexing='pointer', pid_type='persistent_interleaved')
[328s] Generation 10: replaced=10 min=0.0181 mid=0.0298 max=0.0609 best=Config(block_sizes=[64, 128, 128], loop_orders=[[1, 0]], l2_groupings=[4], range_unroll_factors=[1, 1], range_num_stages=[1, 4], range_multi_buffers=[False, False], range_flattens=[False, True], num_warps=4, num_stages=3, indexing='pointer', pid_type='persistent_interleaved')
[368s] Generation 11: replaced=10 min=0.0164 mid=0.0291 max=0.0609 best=Config(block_sizes=[64, 64, 64], loop_orders=[[0, 1]], l2_groupings=[1], range_unroll_factors=[0, 1], range_num_stages=[0, 3], range_multi_buffers=[None, True], range_flattens=[None, True], num_warps=4, num_stages=8, indexing='block_ptr', pid_type='flat', range_warp_specializes=[])
[397s] Generation 12: replaced=6 min=0.0164 mid=0.0268 max=0.0360 best=Config(block_sizes=[64, 64, 64], loop_orders=[[0, 1]], l2_groupings=[1], range_unroll_factors=[0, 1], range_num_stages=[0, 3], range_multi_buffers=[None, True], range_flattens=[None, True], num_warps=4, num_stages=8, indexing='block_ptr', pid_type='flat', range_warp_specializes=[])
[435s] Generation 13: replaced=7 min=0.0164 mid=0.0268 max=0.0360 best=Config(block_sizes=[64, 64, 64], loop_orders=[[0, 1]], l2_groupings=[1], range_unroll_factors=[0, 1], range_num_stages=[0, 3], range_multi_buffers=[None, True], range_flattens=[None, True], num_warps=4, num_stages=8, indexing='block_ptr', pid_type='flat', range_warp_specializes=[])
[462s] Generation 14: replaced=9 min=0.0155 mid=0.0248 max=0.0359 best=Config(block_sizes=[128, 128, 64], loop_orders=[[0, 1]], l2_groupings=[16], range_unroll_factors=[0, 1], range_num_stages=[0, 4], range_multi_buffers=[None, False], range_flattens=[None, True], num_warps=8, num_stages=8, indexing='block_ptr', pid_type='flat', range_warp_specializes=[])
[501s] Generation 15: replaced=5 min=0.0155 mid=0.0240 max=0.0359 best=Config(block_sizes=[128, 128, 64], loop_orders=[[0, 1]], l2_groupings=[16], range_unroll_factors=[0, 1], range_num_stages=[0, 4], range_multi_buffers=[None, False], range_flattens=[None, True], num_warps=8, num_stages=8, indexing='block_ptr', pid_type='flat', range_warp_specializes=[])
[530s] PTXASError compiling config: Config(block_sizes=[128, 512, 32], loop_orders=[[1, 0]], l2_groupings=[16], range_unroll_factors=[0, 2], range_num_stages=[0, 2], range_multi_buffers=[None, True], range_flattens=[None, None], num_warps=16, num_stages=3, indexing='block_ptr', pid_type='flat', range_warp_specializes=[])
[545s] Generation 16: replaced=10 min=0.0155 mid=0.0234 max=0.0344 best=Config(block_sizes=[128, 128, 64], loop_orders=[[0, 1]], l2_groupings=[16], range_unroll_factors=[0, 1], range_num_stages=[0, 4], range_multi_buffers=[None, False], range_flattens=[None, True], num_warps=8, num_stages=8, indexing='block_ptr', pid_type='flat', range_warp_specializes=[])
[582s] Generation 17: replaced=4 min=0.0148 mid=0.0234 max=0.0344 best=Config(block_sizes=[128, 128, 64], loop_orders=[[0, 1]], l2_groupings=[16], range_unroll_factors=[1, 1], range_num_stages=[0, 4], range_multi_buffers=[True, True], range_flattens=[None, True], num_warps=8, num_stages=1, indexing='block_ptr', pid_type='persistent_interleaved')
[610s] Generation 18: replaced=8 min=0.0148 mid=0.0227 max=0.0344 best=Config(block_sizes=[128, 128, 64], loop_orders=[[0, 1]], l2_groupings=[16], range_unroll_factors=[1, 1], range_num_stages=[0, 4], range_multi_buffers=[True, True], range_flattens=[None, True], num_warps=8, num_stages=1, indexing='block_ptr', pid_type='persistent_interleaved')
[637s] Generation 19: replaced=13 min=0.0148 mid=0.0199 max=0.0291 best=Config(block_sizes=[128, 128, 64], loop_orders=[[0, 1]], l2_groupings=[16], range_unroll_factors=[1, 1], range_num_stages=[0, 4], range_multi_buffers=[True, True], range_flattens=[None, True], num_warps=8, num_stages=1, indexing='block_ptr', pid_type='persistent_interleaved')
[637s] Autotuning complete in 637.6s after searching 1520 configs.
One can hardcode the best config and skip autotuning with:
    @helion.kernel(config=helion.Config(block_sizes=[128, 128, 64], loop_orders=[[0, 1]], l2_groupings=[16], range_unroll_factors=[1, 1], range_num_stages=[0, 4], range_multi_buffers=[True, True], range_flattens=[None, True], num_warps=8, num_stages=1, indexing='block_ptr', pid_type='persistent_interleaved'))

INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import helion
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _matmul_kernel(x, y, out, _NUM_SM: tl.constexpr, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    total_pids = tl.cdiv(1280, _BLOCK_SIZE_0) * tl.cdiv(1280, _BLOCK_SIZE_1)
    for virtual_pid in tl.range(tl.program_id(0), total_pids, _NUM_SM, loop_unroll_factor=1, disallow_acc_multi_buffer=False):
        num_pid_m = tl.cdiv(1280, _BLOCK_SIZE_0)
        num_pid_n = tl.cdiv(1280, _BLOCK_SIZE_1)
        inner_2d_pid = virtual_pid
        num_pid_in_group = 16 * num_pid_n
        group_id = inner_2d_pid // num_pid_in_group
        first_pid_m = group_id * 16
        group_size_m = min(num_pid_m - first_pid_m, 16)
        pid_0 = first_pid_m + inner_2d_pid % num_pid_in_group % group_size_m
        pid_1 = inner_2d_pid % num_pid_in_group // group_size_m
        offset_0 = pid_0 * _BLOCK_SIZE_0
        offset_1 = pid_1 * _BLOCK_SIZE_1
        acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, 1280, _BLOCK_SIZE_2, loop_unroll_factor=1, num_stages=4, disallow_acc_multi_buffer=False, flatten=True):
            acc_copy = acc
            acc_copy_0 = acc_copy
            load = tl.load(tl.make_block_ptr(x, [1280, 1280], [1280, 1], [offset_0, offset_2], [_BLOCK_SIZE_0, _BLOCK_SIZE_2], [1, 0]), boundary_check=[0, 1], padding_option='zero')
            load_1 = tl.load(tl.make_block_ptr(y, [1280, 1280], [1, 1280], [offset_2, offset_1], [_BLOCK_SIZE_2, _BLOCK_SIZE_1], [0, 1]), boundary_check=[0, 1], padding_option='zero')
            acc = tl.dot(load, load_1, acc=acc_copy_0, input_precision='tf32')
        v_0 = acc.to(tl.float16)
        tl.store(tl.make_block_ptr(out, [1280, 1280], [1280, 1], [offset_0, offset_1], [_BLOCK_SIZE_0, _BLOCK_SIZE_1], [1, 0]), v_0, boundary_check=[0, 1])

def matmul(x: torch.Tensor, y: torch.Tensor, epilogue: Callable[[torch.Tensor, list[torch.Tensor]], torch.Tensor]=lambda acc, tile: acc, *, _launcher=_default_launcher):
    m, k = x.size()
    k2, n = y.size()
    assert k == k2, f'size mismatch {k} != {k2}'
    out = torch.empty([m, n], dtype=torch.promote_types(x.dtype, y.dtype), device=x.device)
    _NUM_SM = helion.runtime.get_num_sm(x.device)
    _BLOCK_SIZE_0 = 128
    _BLOCK_SIZE_1 = 128
    _BLOCK_SIZE_2 = 64
    _launcher(_matmul_kernel, (_NUM_SM,), x, y, out, _NUM_SM, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=8, num_stages=1)
    return out
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import helion
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _matmul_kernel(x, y, out, _NUM_SM: tl.constexpr, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    total_pids = tl.cdiv(1280, _BLOCK_SIZE_0) * tl.cdiv(1280, _BLOCK_SIZE_1)
    for virtual_pid in tl.range(tl.program_id(0), total_pids, _NUM_SM, loop_unroll_factor=1, disallow_acc_multi_buffer=False):
        num_pid_m = tl.cdiv(1280, _BLOCK_SIZE_0)
        num_pid_n = tl.cdiv(1280, _BLOCK_SIZE_1)
        inner_2d_pid = virtual_pid
        num_pid_in_group = 16 * num_pid_n
        group_id = inner_2d_pid // num_pid_in_group
        first_pid_m = group_id * 16
        group_size_m = min(num_pid_m - first_pid_m, 16)
        pid_0 = first_pid_m + inner_2d_pid % num_pid_in_group % group_size_m
        pid_1 = inner_2d_pid % num_pid_in_group // group_size_m
        offset_0 = pid_0 * _BLOCK_SIZE_0
        offset_1 = pid_1 * _BLOCK_SIZE_1
        acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, 1280, _BLOCK_SIZE_2, loop_unroll_factor=1, num_stages=4, disallow_acc_multi_buffer=False, flatten=True):
            acc_copy = acc
            acc_copy_0 = acc_copy
            load = tl.load(tl.make_block_ptr(x, [1280, 1280], [1280, 1], [offset_0, offset_2], [_BLOCK_SIZE_0, _BLOCK_SIZE_2], [1, 0]), boundary_check=[0, 1], padding_option='zero')
            load_1 = tl.load(tl.make_block_ptr(y, [1280, 1280], [1, 1280], [offset_2, offset_1], [_BLOCK_SIZE_2, _BLOCK_SIZE_1], [0, 1]), boundary_check=[0, 1], padding_option='zero')
            acc = tl.dot(load, load_1, acc=acc_copy_0, input_precision='tf32')
        v_0 = acc.to(tl.float16)
        tl.store(tl.make_block_ptr(out, [1280, 1280], [1280, 1], [offset_0, offset_1], [_BLOCK_SIZE_0, _BLOCK_SIZE_1], [1, 0]), v_0, boundary_check=[0, 1])

def matmul(x: torch.Tensor, y: torch.Tensor, epilogue: Callable[[torch.Tensor, list[torch.Tensor]], torch.Tensor]=lambda acc, tile: acc, *, _launcher=_default_launcher):
    m, k = x.size()
    k2, n = y.size()
    assert k == k2, f'size mismatch {k} != {k2}'
    out = torch.empty([m, n], dtype=torch.promote_types(x.dtype, y.dtype), device=x.device)
    _NUM_SM = helion.runtime.get_num_sm(x.device)
    _BLOCK_SIZE_0 = 128
    _BLOCK_SIZE_1 = 128
    _BLOCK_SIZE_2 = 64
    _launcher(_matmul_kernel, (_NUM_SM,), x, y, out, _NUM_SM, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=8, num_stages=1)
    return out
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import helion
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _matmul_kernel(x, y, out, _NUM_SM: tl.constexpr, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    total_pids = tl.cdiv(1280, _BLOCK_SIZE_0) * tl.cdiv(1280, _BLOCK_SIZE_1)
    for virtual_pid in tl.range(tl.program_id(0), total_pids, _NUM_SM, loop_unroll_factor=1, disallow_acc_multi_buffer=False):
        num_pid_m = tl.cdiv(1280, _BLOCK_SIZE_0)
        num_pid_n = tl.cdiv(1280, _BLOCK_SIZE_1)
        inner_2d_pid = virtual_pid
        num_pid_in_group = 16 * num_pid_n
        group_id = inner_2d_pid // num_pid_in_group
        first_pid_m = group_id * 16
        group_size_m = min(num_pid_m - first_pid_m, 16)
        pid_0 = first_pid_m + inner_2d_pid % num_pid_in_group % group_size_m
        pid_1 = inner_2d_pid % num_pid_in_group // group_size_m
        offset_0 = pid_0 * _BLOCK_SIZE_0
        offset_1 = pid_1 * _BLOCK_SIZE_1
        acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, 1280, _BLOCK_SIZE_2, loop_unroll_factor=1, num_stages=4, disallow_acc_multi_buffer=False, flatten=True):
            acc_copy = acc
            acc_copy_0 = acc_copy
            load = tl.load(tl.make_block_ptr(x, [1280, 1280], [1280, 1], [offset_0, offset_2], [_BLOCK_SIZE_0, _BLOCK_SIZE_2], [1, 0]), boundary_check=[0, 1], padding_option='zero')
            load_1 = tl.load(tl.make_block_ptr(y, [1280, 1280], [1, 1280], [offset_2, offset_1], [_BLOCK_SIZE_2, _BLOCK_SIZE_1], [0, 1]), boundary_check=[0, 1], padding_option='zero')
            acc = tl.dot(load, load_1, acc=acc_copy_0, input_precision='tf32')
        v_0 = acc.to(tl.float16)
        tl.store(tl.make_block_ptr(out, [1280, 1280], [1280, 1], [offset_0, offset_1], [_BLOCK_SIZE_0, _BLOCK_SIZE_1], [1, 0]), v_0, boundary_check=[0, 1])

def matmul(x: torch.Tensor, y: torch.Tensor, epilogue: Callable[[torch.Tensor, list[torch.Tensor]], torch.Tensor]=lambda acc, tile: acc, *, _launcher=_default_launcher):
    m, k = x.size()
    k2, n = y.size()
    assert k == k2, f'size mismatch {k} != {k2}'
    out = torch.empty([m, n], dtype=torch.promote_types(x.dtype, y.dtype), device=x.device)
    _NUM_SM = helion.runtime.get_num_sm(x.device)
    _BLOCK_SIZE_0 = 128
    _BLOCK_SIZE_1 = 128
    _BLOCK_SIZE_2 = 64
    _launcher(_matmul_kernel, (_NUM_SM,), x, y, out, _NUM_SM, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=8, num_stages=1)
    return out
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import helion
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _matmul_kernel(x, y, out, _NUM_SM: tl.constexpr, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    total_pids = tl.cdiv(1280, _BLOCK_SIZE_0) * tl.cdiv(1280, _BLOCK_SIZE_1)
    for virtual_pid in tl.range(tl.program_id(0), total_pids, _NUM_SM, loop_unroll_factor=1, disallow_acc_multi_buffer=False):
        num_pid_m = tl.cdiv(1280, _BLOCK_SIZE_0)
        num_pid_n = tl.cdiv(1280, _BLOCK_SIZE_1)
        inner_2d_pid = virtual_pid
        num_pid_in_group = 16 * num_pid_n
        group_id = inner_2d_pid // num_pid_in_group
        first_pid_m = group_id * 16
        group_size_m = min(num_pid_m - first_pid_m, 16)
        pid_0 = first_pid_m + inner_2d_pid % num_pid_in_group % group_size_m
        pid_1 = inner_2d_pid % num_pid_in_group // group_size_m
        offset_0 = pid_0 * _BLOCK_SIZE_0
        offset_1 = pid_1 * _BLOCK_SIZE_1
        acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, 1280, _BLOCK_SIZE_2, loop_unroll_factor=1, num_stages=4, disallow_acc_multi_buffer=False, flatten=True):
            acc_copy = acc
            acc_copy_0 = acc_copy
            load = tl.load(tl.make_block_ptr(x, [1280, 1280], [1280, 1], [offset_0, offset_2], [_BLOCK_SIZE_0, _BLOCK_SIZE_2], [1, 0]), boundary_check=[0, 1], padding_option='zero')
            load_1 = tl.load(tl.make_block_ptr(y, [1280, 1280], [1, 1280], [offset_2, offset_1], [_BLOCK_SIZE_2, _BLOCK_SIZE_1], [0, 1]), boundary_check=[0, 1], padding_option='zero')
            acc = tl.dot(load, load_1, acc=acc_copy_0, input_precision='tf32')
        v_0 = acc.to(tl.float16)
        tl.store(tl.make_block_ptr(out, [1280, 1280], [1280, 1], [offset_0, offset_1], [_BLOCK_SIZE_0, _BLOCK_SIZE_1], [1, 0]), v_0, boundary_check=[0, 1])

def matmul(x: torch.Tensor, y: torch.Tensor, epilogue: Callable[[torch.Tensor, list[torch.Tensor]], torch.Tensor]=lambda acc, tile: acc, *, _launcher=_default_launcher):
    m, k = x.size()
    k2, n = y.size()
    assert k == k2, f'size mismatch {k} != {k2}'
    out = torch.empty([m, n], dtype=torch.promote_types(x.dtype, y.dtype), device=x.device)
    _NUM_SM = helion.runtime.get_num_sm(x.device)
    _BLOCK_SIZE_0 = 128
    _BLOCK_SIZE_1 = 128
    _BLOCK_SIZE_2 = 64
    _launcher(_matmul_kernel, (_NUM_SM,), x, y, out, _NUM_SM, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=8, num_stages=1)
    return out
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import helion
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _matmul_kernel(x, y, out, _NUM_SM: tl.constexpr, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    total_pids = tl.cdiv(1280, _BLOCK_SIZE_0) * tl.cdiv(1280, _BLOCK_SIZE_1)
    for virtual_pid in tl.range(tl.program_id(0), total_pids, _NUM_SM, loop_unroll_factor=1, disallow_acc_multi_buffer=False):
        num_pid_m = tl.cdiv(1280, _BLOCK_SIZE_0)
        num_pid_n = tl.cdiv(1280, _BLOCK_SIZE_1)
        inner_2d_pid = virtual_pid
        num_pid_in_group = 16 * num_pid_n
        group_id = inner_2d_pid // num_pid_in_group
        first_pid_m = group_id * 16
        group_size_m = min(num_pid_m - first_pid_m, 16)
        pid_0 = first_pid_m + inner_2d_pid % num_pid_in_group % group_size_m
        pid_1 = inner_2d_pid % num_pid_in_group // group_size_m
        offset_0 = pid_0 * _BLOCK_SIZE_0
        offset_1 = pid_1 * _BLOCK_SIZE_1
        acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, 1280, _BLOCK_SIZE_2, loop_unroll_factor=1, num_stages=4, disallow_acc_multi_buffer=False, flatten=True):
            acc_copy = acc
            acc_copy_0 = acc_copy
            load = tl.load(tl.make_block_ptr(x, [1280, 1280], [1280, 1], [offset_0, offset_2], [_BLOCK_SIZE_0, _BLOCK_SIZE_2], [1, 0]), boundary_check=[0, 1], padding_option='zero')
            load_1 = tl.load(tl.make_block_ptr(y, [1280, 1280], [1, 1280], [offset_2, offset_1], [_BLOCK_SIZE_2, _BLOCK_SIZE_1], [0, 1]), boundary_check=[0, 1], padding_option='zero')
            acc = tl.dot(load, load_1, acc=acc_copy_0, input_precision='tf32')
        v_0 = acc.to(tl.float16)
        tl.store(tl.make_block_ptr(out, [1280, 1280], [1280, 1], [offset_0, offset_1], [_BLOCK_SIZE_0, _BLOCK_SIZE_1], [1, 0]), v_0, boundary_check=[0, 1])

def matmul(x: torch.Tensor, y: torch.Tensor, epilogue: Callable[[torch.Tensor, list[torch.Tensor]], torch.Tensor]=lambda acc, tile: acc, *, _launcher=_default_launcher):
    m, k = x.size()
    k2, n = y.size()
    assert k == k2, f'size mismatch {k} != {k2}'
    out = torch.empty([m, n], dtype=torch.promote_types(x.dtype, y.dtype), device=x.device)
    _NUM_SM = helion.runtime.get_num_sm(x.device)
    _BLOCK_SIZE_0 = 128
    _BLOCK_SIZE_1 = 128
    _BLOCK_SIZE_2 = 64
    _launcher(_matmul_kernel, (_NUM_SM,), x, y, out, _NUM_SM, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=8, num_stages=1)
    return out
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import helion
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _matmul_kernel(x, y, out, _NUM_SM: tl.constexpr, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    total_pids = tl.cdiv(1280, _BLOCK_SIZE_0) * tl.cdiv(1280, _BLOCK_SIZE_1)
    for virtual_pid in tl.range(tl.program_id(0), total_pids, _NUM_SM, loop_unroll_factor=1, disallow_acc_multi_buffer=False):
        num_pid_m = tl.cdiv(1280, _BLOCK_SIZE_0)
        num_pid_n = tl.cdiv(1280, _BLOCK_SIZE_1)
        inner_2d_pid = virtual_pid
        num_pid_in_group = 16 * num_pid_n
        group_id = inner_2d_pid // num_pid_in_group
        first_pid_m = group_id * 16
        group_size_m = min(num_pid_m - first_pid_m, 16)
        pid_0 = first_pid_m + inner_2d_pid % num_pid_in_group % group_size_m
        pid_1 = inner_2d_pid % num_pid_in_group // group_size_m
        offset_0 = pid_0 * _BLOCK_SIZE_0
        offset_1 = pid_1 * _BLOCK_SIZE_1
        acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, 1280, _BLOCK_SIZE_2, loop_unroll_factor=1, num_stages=4, disallow_acc_multi_buffer=False, flatten=True):
            acc_copy = acc
            acc_copy_0 = acc_copy
            load = tl.load(tl.make_block_ptr(x, [1280, 1280], [1280, 1], [offset_0, offset_2], [_BLOCK_SIZE_0, _BLOCK_SIZE_2], [1, 0]), boundary_check=[0, 1], padding_option='zero')
            load_1 = tl.load(tl.make_block_ptr(y, [1280, 1280], [1, 1280], [offset_2, offset_1], [_BLOCK_SIZE_2, _BLOCK_SIZE_1], [0, 1]), boundary_check=[0, 1], padding_option='zero')
            acc = tl.dot(load, load_1, acc=acc_copy_0, input_precision='tf32')
        v_0 = acc.to(tl.float16)
        tl.store(tl.make_block_ptr(out, [1280, 1280], [1280, 1], [offset_0, offset_1], [_BLOCK_SIZE_0, _BLOCK_SIZE_1], [1, 0]), v_0, boundary_check=[0, 1])

def matmul(x: torch.Tensor, y: torch.Tensor, epilogue: Callable[[torch.Tensor, list[torch.Tensor]], torch.Tensor]=lambda acc, tile: acc, *, _launcher=_default_launcher):
    m, k = x.size()
    k2, n = y.size()
    assert k == k2, f'size mismatch {k} != {k2}'
    out = torch.empty([m, n], dtype=torch.promote_types(x.dtype, y.dtype), device=x.device)
    _NUM_SM = helion.runtime.get_num_sm(x.device)
    _BLOCK_SIZE_0 = 128
    _BLOCK_SIZE_1 = 128
    _BLOCK_SIZE_2 = 64
    _launcher(_matmul_kernel, (_NUM_SM,), x, y, out, _NUM_SM, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=8, num_stages=1)
    return out
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import helion
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _matmul_kernel(x, y, out, _NUM_SM: tl.constexpr, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    total_pids = tl.cdiv(1280, _BLOCK_SIZE_0) * tl.cdiv(1280, _BLOCK_SIZE_1)
    for virtual_pid in tl.range(tl.program_id(0), total_pids, _NUM_SM, loop_unroll_factor=1, disallow_acc_multi_buffer=False):
        num_pid_m = tl.cdiv(1280, _BLOCK_SIZE_0)
        num_pid_n = tl.cdiv(1280, _BLOCK_SIZE_1)
        inner_2d_pid = virtual_pid
        num_pid_in_group = 16 * num_pid_n
        group_id = inner_2d_pid // num_pid_in_group
        first_pid_m = group_id * 16
        group_size_m = min(num_pid_m - first_pid_m, 16)
        pid_0 = first_pid_m + inner_2d_pid % num_pid_in_group % group_size_m
        pid_1 = inner_2d_pid % num_pid_in_group // group_size_m
        offset_0 = pid_0 * _BLOCK_SIZE_0
        offset_1 = pid_1 * _BLOCK_SIZE_1
        acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, 1280, _BLOCK_SIZE_2, loop_unroll_factor=1, num_stages=4, disallow_acc_multi_buffer=False, flatten=True):
            acc_copy = acc
            acc_copy_0 = acc_copy
            load = tl.load(tl.make_block_ptr(x, [1280, 1280], [1280, 1], [offset_0, offset_2], [_BLOCK_SIZE_0, _BLOCK_SIZE_2], [1, 0]), boundary_check=[0, 1], padding_option='zero')
            load_1 = tl.load(tl.make_block_ptr(y, [1280, 1280], [1, 1280], [offset_2, offset_1], [_BLOCK_SIZE_2, _BLOCK_SIZE_1], [0, 1]), boundary_check=[0, 1], padding_option='zero')
            acc = tl.dot(load, load_1, acc=acc_copy_0, input_precision='tf32')
        v_0 = acc.to(tl.float16)
        tl.store(tl.make_block_ptr(out, [1280, 1280], [1280, 1], [offset_0, offset_1], [_BLOCK_SIZE_0, _BLOCK_SIZE_1], [1, 0]), v_0, boundary_check=[0, 1])

def matmul(x: torch.Tensor, y: torch.Tensor, epilogue: Callable[[torch.Tensor, list[torch.Tensor]], torch.Tensor]=lambda acc, tile: acc, *, _launcher=_default_launcher):
    m, k = x.size()
    k2, n = y.size()
    assert k == k2, f'size mismatch {k} != {k2}'
    out = torch.empty([m, n], dtype=torch.promote_types(x.dtype, y.dtype), device=x.device)
    _NUM_SM = helion.runtime.get_num_sm(x.device)
    _BLOCK_SIZE_0 = 128
    _BLOCK_SIZE_1 = 128
    _BLOCK_SIZE_2 = 64
    _launcher(_matmul_kernel, (_NUM_SM,), x, y, out, _NUM_SM, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=8, num_stages=1)
    return out
INFO:tritonbench.utils.triton_op:Took 0.02ms to get benchmark function for helion_matmul_split_k_tritonbench
[0s] Starting DifferentialEvolutionSearch with population=40, generations=20, crossover_rate=0.8
[15s] Initial population: failed=6 min=0.0503 mid=0.3193 max=11.3656 best=Config(block_sizes=[16, 16, 128], loop_orders=[[1, 0, 2]], l2_groupings=[2], range_unroll_factors=[0, 0], range_num_stages=[0, 2], range_multi_buffers=[None, False], range_flattens=[None, None], num_warps=1, num_stages=6, indexing='tensor_descriptor', pid_type='flat', split_k=4, range_warp_specializes=[])
[57s] Generation 2: replaced=17 min=0.0503 mid=0.1343 max=0.3508 best=Config(block_sizes=[16, 16, 128], loop_orders=[[1, 0, 2]], l2_groupings=[2], range_unroll_factors=[0, 0], range_num_stages=[0, 2], range_multi_buffers=[None, False], range_flattens=[None, None], num_warps=1, num_stages=6, indexing='tensor_descriptor', pid_type='flat', split_k=4, range_warp_specializes=[])
[99s] Generation 3: replaced=19 min=0.0297 mid=0.1018 max=0.3046 best=Config(block_sizes=[128, 128, 32], loop_orders=[[0, 2, 1]], l2_groupings=[4], range_unroll_factors=[1, 1], range_num_stages=[4, 3], range_multi_buffers=[False, None], range_flattens=[False, None], num_warps=16, num_stages=1, indexing='block_ptr', pid_type='persistent_interleaved', split_k=4)
[140s] Generation 4: replaced=16 min=0.0240 mid=0.0676 max=0.1656 best=Config(block_sizes=[128, 64, 128], loop_orders=[[1, 2, 0]], l2_groupings=[2], range_unroll_factors=[0, 1], range_num_stages=[0, 3], range_multi_buffers=[None, None], range_flattens=[None, False], num_warps=4, num_stages=2, indexing='tensor_descriptor', pid_type='flat', split_k=1, range_warp_specializes=[])
[192s] Generation 5: replaced=12 min=0.0240 mid=0.0625 max=0.1646 best=Config(block_sizes=[128, 64, 128], loop_orders=[[1, 2, 0]], l2_groupings=[2], range_unroll_factors=[0, 1], range_num_stages=[0, 3], range_multi_buffers=[None, None], range_flattens=[None, False], num_warps=4, num_stages=2, indexing='tensor_descriptor', pid_type='flat', split_k=1, range_warp_specializes=[])
[239s] Generation 6: replaced=16 min=0.0240 mid=0.0429 max=0.1204 best=Config(block_sizes=[128, 64, 128], loop_orders=[[1, 2, 0]], l2_groupings=[2], range_unroll_factors=[0, 1], range_num_stages=[0, 3], range_multi_buffers=[None, None], range_flattens=[None, False], num_warps=4, num_stages=2, indexing='tensor_descriptor', pid_type='flat', split_k=1, range_warp_specializes=[])
[317s] Generation 7: replaced=13 min=0.0240 mid=0.0419 max=0.1013 best=Config(block_sizes=[128, 64, 128], loop_orders=[[1, 2, 0]], l2_groupings=[2], range_unroll_factors=[0, 1], range_num_stages=[0, 3], range_multi_buffers=[None, None], range_flattens=[None, False], num_warps=4, num_stages=2, indexing='tensor_descriptor', pid_type='flat', split_k=1, range_warp_specializes=[])
[362s] PTXASError compiling config: Config(block_sizes=[512, 128, 32], loop_orders=[[2, 0, 1]], l2_groupings=[1], range_unroll_factors=[0, 3], range_num_stages=[0, 4], range_multi_buffers=[None, True], range_flattens=[None, True], num_warps=32, num_stages=1, indexing='block_ptr', pid_type='flat', split_k=2, range_warp_specializes=[])
[366s] Generation 8: replaced=10 min=0.0226 mid=0.0388 max=0.1013 best=Config(block_sizes=[128, 128, 32], loop_orders=[[1, 2, 0]], l2_groupings=[2], range_unroll_factors=[0, 0], range_num_stages=[0, 4], range_multi_buffers=[None, True], range_flattens=[None, False], num_warps=4, num_stages=2, indexing='block_ptr', pid_type='flat', split_k=4, range_warp_specializes=[])
[417s] Generation 9: replaced=14 min=0.0226 mid=0.0329 max=0.0563 best=Config(block_sizes=[128, 128, 32], loop_orders=[[1, 2, 0]], l2_groupings=[2], range_unroll_factors=[0, 0], range_num_stages=[0, 4], range_multi_buffers=[None, True], range_flattens=[None, False], num_warps=4, num_stages=2, indexing='block_ptr', pid_type='flat', split_k=4, range_warp_specializes=[])
[468s] PTXASError compiling config: Config(block_sizes=[512, 256, 32], loop_orders=[[1, 2, 0]], l2_groupings=[2], range_unroll_factors=[0, 1], range_num_stages=[0, 0], range_multi_buffers=[None, True], range_flattens=[None, True], num_warps=16, num_stages=3, indexing='pointer', pid_type='flat', split_k=2, range_warp_specializes=[])
[468s] Generation 10: replaced=11 min=0.0226 mid=0.0309 max=0.0539 best=Config(block_sizes=[128, 128, 32], loop_orders=[[1, 2, 0]], l2_groupings=[2], range_unroll_factors=[0, 0], range_num_stages=[0, 4], range_multi_buffers=[None, True], range_flattens=[None, False], num_warps=4, num_stages=2, indexing='block_ptr', pid_type='flat', split_k=4, range_warp_specializes=[])
[475s] PTXASError compiling config: Config(block_sizes=[256, 256, 32], loop_orders=[[1, 0, 2]], l2_groupings=[2], range_unroll_factors=[0, 1], range_num_stages=[0, 3], range_multi_buffers=[None, True], range_flattens=[None, True], num_warps=16, num_stages=2, indexing='tensor_descriptor', pid_type='flat', split_k=1, range_warp_specializes=[])
[477s] PTXASError compiling config: Config(block_sizes=[256, 256, 64], loop_orders=[[0, 1, 2]], l2_groupings=[2], range_unroll_factors=[0, 0], range_num_stages=[0, 3], range_multi_buffers=[None, True], range_flattens=[None, None], num_warps=16, num_stages=6, indexing='tensor_descriptor', pid_type='flat', split_k=1, range_warp_specializes=[])
[509s] PTXASError compiling config: Config(block_sizes=[512, 256, 16], loop_orders=[[1, 0, 2]], l2_groupings=[16], range_unroll_factors=[0, 0], range_num_stages=[0, 4], range_multi_buffers=[None, True], range_flattens=[None, True], num_warps=16, num_stages=3, indexing='block_ptr', pid_type='flat', split_k=2, range_warp_specializes=[])
[516s] Generation 11: replaced=10 min=0.0211 mid=0.0299 max=0.0494 best=Config(block_sizes=[128, 64, 64], loop_orders=[[1, 0, 2]], l2_groupings=[1], range_unroll_factors=[0, 0], range_num_stages=[0, 4], range_multi_buffers=[None, True], range_flattens=[None, False], num_warps=4, num_stages=7, indexing='block_ptr', pid_type='flat', split_k=1, range_warp_specializes=[])
[552s] Generation 12: replaced=8 min=0.0211 mid=0.0284 max=0.0494 best=Config(block_sizes=[128, 128, 64], loop_orders=[[1, 0, 2]], l2_groupings=[1], range_unroll_factors=[0, 0], range_num_stages=[0, 3], range_multi_buffers=[None, False], range_flattens=[None, True], num_warps=8, num_stages=7, indexing='tensor_descriptor', pid_type='flat', split_k=1, range_warp_specializes=[])
[570s] PTXASError compiling config: Config(block_sizes=[256, 256, 32], loop_orders=[[0, 1, 2]], l2_groupings=[2], range_unroll_factors=[0, 1], range_num_stages=[0, 3], range_multi_buffers=[None, None], range_flattens=[None, True], num_warps=16, num_stages=6, indexing='pointer', pid_type='flat', split_k=4, range_warp_specializes=[])
[630s] Timeout after 60s compiling Config(block_sizes=[512, 16, 16], loop_orders=[[0, 1, 2]], l2_groupings=[8], range_unroll_factors=[0, 1], range_num_stages=[0, 4], range_multi_buffers=[None, False], range_flattens=[None, True], num_warps=4, num_stages=3, indexing='pointer', pid_type='flat', split_k=4, range_warp_specializes=[])
[641s] PTXASError compiling config: Config(block_sizes=[512, 256, 32], loop_orders=[[1, 2, 0]], l2_groupings=[8], range_unroll_factors=[3, 1], range_num_stages=[2, 3], range_multi_buffers=[True, False], range_flattens=[False, None], num_warps=16, num_stages=1, indexing='tensor_descriptor', pid_type='persistent_blocked', split_k=4)
[650s] PTXASError compiling config: Config(block_sizes=[256, 256, 16], loop_orders=[[2, 1, 0]], l2_groupings=[32], range_unroll_factors=[0, 2], range_num_stages=[0, 4], range_multi_buffers=[None, None], range_flattens=[None, False], num_warps=16, num_stages=4, indexing='tensor_descriptor', pid_type='flat', split_k=2, range_warp_specializes=[])
[656s] Generation 13: replaced=8 min=0.0211 mid=0.0279 max=0.0494 best=Config(block_sizes=[128, 128, 64], loop_orders=[[1, 0, 2]], l2_groupings=[1], range_unroll_factors=[0, 0], range_num_stages=[0, 3], range_multi_buffers=[None, False], range_flattens=[None, True], num_warps=8, num_stages=7, indexing='tensor_descriptor', pid_type='flat', split_k=1, range_warp_specializes=[])
[712s] Generation 14: replaced=8 min=0.0211 mid=0.0254 max=0.0494 best=Config(block_sizes=[128, 128, 64], loop_orders=[[1, 0, 2]], l2_groupings=[1], range_unroll_factors=[0, 0], range_num_stages=[0, 3], range_multi_buffers=[None, False], range_flattens=[None, True], num_warps=8, num_stages=7, indexing='tensor_descriptor', pid_type='flat', split_k=1, range_warp_specializes=[])
[750s] Generation 15: replaced=8 min=0.0192 mid=0.0252 max=0.0383 best=Config(block_sizes=[128, 64, 64], loop_orders=[[2, 0, 1]], l2_groupings=[4], range_unroll_factors=[0, 0], range_num_stages=[0, 4], range_multi_buffers=[None, False], range_flattens=[None, True], num_warps=8, num_stages=5, indexing='block_ptr', pid_type='flat', split_k=1, range_warp_specializes=[])
[783s] Generation 16: replaced=10 min=0.0192 mid=0.0238 max=0.0383 best=Config(block_sizes=[128, 64, 64], loop_orders=[[2, 0, 1]], l2_groupings=[4], range_unroll_factors=[0, 0], range_num_stages=[0, 4], range_multi_buffers=[None, False], range_flattens=[None, True], num_warps=8, num_stages=5, indexing='block_ptr', pid_type='flat', split_k=1, range_warp_specializes=[])
[821s] Generation 17: replaced=4 min=0.0192 mid=0.0236 max=0.0383 best=Config(block_sizes=[128, 64, 64], loop_orders=[[2, 0, 1]], l2_groupings=[4], range_unroll_factors=[0, 0], range_num_stages=[0, 4], range_multi_buffers=[None, False], range_flattens=[None, True], num_warps=8, num_stages=5, indexing='block_ptr', pid_type='flat', split_k=1, range_warp_specializes=[])
[853s] Generation 18: replaced=10 min=0.0192 mid=0.0234 max=0.0358 best=Config(block_sizes=[128, 64, 64], loop_orders=[[2, 0, 1]], l2_groupings=[4], range_unroll_factors=[0, 0], range_num_stages=[0, 4], range_multi_buffers=[None, False], range_flattens=[None, True], num_warps=8, num_stages=5, indexing='block_ptr', pid_type='flat', split_k=1, range_warp_specializes=[])
[884s] Generation 19: replaced=8 min=0.0192 mid=0.0230 max=0.0293 best=Config(block_sizes=[128, 64, 64], loop_orders=[[2, 0, 1]], l2_groupings=[4], range_unroll_factors=[0, 0], range_num_stages=[0, 4], range_multi_buffers=[None, False], range_flattens=[None, True], num_warps=8, num_stages=5, indexing='block_ptr', pid_type='flat', split_k=1, range_warp_specializes=[])
[884s] Autotuning complete in 884.4s after searching 1519 configs.
One can hardcode the best config and skip autotuning with:
    @helion.kernel(config=helion.Config(block_sizes=[128, 64, 64], loop_orders=[[2, 0, 1]], l2_groupings=[4], range_unroll_factors=[0, 0], range_num_stages=[0, 4], range_multi_buffers=[None, False], range_flattens=[None, True], num_warps=8, num_stages=5, indexing='block_ptr', pid_type='flat', split_k=1, range_warp_specializes=[]))

INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import helion
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

import examples.matmul_split_k as _source_module

@triton.jit
def _matmul_split_k_kernel(x, y, out, _BLOCK_SIZE_2: tl.constexpr, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_3: tl.constexpr):
    num_blocks_0 = tl.cdiv(1280, _BLOCK_SIZE_2)
    num_blocks_1 = tl.cdiv(1280, _BLOCK_SIZE_0)
    num_pid_m = tl.cdiv(1280, _BLOCK_SIZE_2)
    num_pid_n = tl.cdiv(1280, _BLOCK_SIZE_0)
    inner_2d_size = num_pid_m * num_pid_n
    inner_2d_pid = tl.program_id(0) % inner_2d_size
    num_pid_in_group = 4 * num_pid_n
    group_id = inner_2d_pid // num_pid_in_group
    first_pid_m = group_id * 4
    group_size_m = min(num_pid_m - first_pid_m, 4)
    pid_0 = first_pid_m + inner_2d_pid % num_pid_in_group % group_size_m
    pid_1 = inner_2d_pid % num_pid_in_group // group_size_m
    pid_2 = tl.program_id(0) // (num_blocks_0 * num_blocks_1)
    offset_2 = pid_0 * _BLOCK_SIZE_2
    offset_0 = pid_1 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_2 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
    tile_end = tl.minimum(offset_2 + _BLOCK_SIZE_2, 1280)
    for offset_3 in tl.range(offset_2.to(tl.int32), tile_end.to(tl.int32), _BLOCK_SIZE_3, num_stages=4, disallow_acc_multi_buffer=True, flatten=True):
        indices_3 = offset_3 + tl.arange(0, _BLOCK_SIZE_3).to(tl.int32)
        mask_3 = indices_3 < tile_end
        acc_copy = acc
        acc_copy_0 = acc_copy
        load = tl.load(x + (indices_0[:, None] * 1280 + indices_3[None, :] * 1), mask_3[None, :], other=0)
        load_1 = tl.load(y + (indices_3[:, None] * 1 + indices_1[None, :] * 1280), mask_3[:, None], other=0)
        acc = tl.dot(load, load_1, acc=acc_copy_0, input_precision='tf32')
    eq = offset_2 == 0
    if eq:
        acc_copy_1 = acc
        acc = acc_copy_1
    tl.atomic_add(out + (indices_0[:, None] * 1280 + indices_1[None, :] * 1), acc, mask=None, sem='relaxed')

def matmul_split_k(x: torch.Tensor, y: torch.Tensor, epilogue: Callable[[torch.Tensor, list[torch.Tensor]], torch.Tensor]=lambda acc, tile: acc, *, _launcher=_default_launcher):
    m, k = x.size()
    k2, n = y.size()
    assert k == k2, f'size mismatch {k} != {k2}'
    out = torch.zeros([m, n], dtype=torch.promote_types(x.dtype, y.dtype), device=x.device)
    split_k = 1
    k_block = helion.next_power_of_2(helion.cdiv(k, split_k))
    _BLOCK_SIZE_2 = k_block
    _BLOCK_SIZE_0 = 128
    _BLOCK_SIZE_1 = 64
    _BLOCK_SIZE_3 = 64
    _launcher(_matmul_split_k_kernel, (triton.cdiv(1280, _BLOCK_SIZE_2) * triton.cdiv(1280, _BLOCK_SIZE_0) * triton.cdiv(1280, _BLOCK_SIZE_1),), x, y, out, _BLOCK_SIZE_2, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_3, num_warps=8, num_stages=5)
    return out
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import helion
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

import examples.matmul_split_k as _source_module

@triton.jit
def _matmul_split_k_kernel(x, y, out, _BLOCK_SIZE_2: tl.constexpr, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_3: tl.constexpr):
    num_blocks_0 = tl.cdiv(1280, _BLOCK_SIZE_2)
    num_blocks_1 = tl.cdiv(1280, _BLOCK_SIZE_0)
    num_pid_m = tl.cdiv(1280, _BLOCK_SIZE_2)
    num_pid_n = tl.cdiv(1280, _BLOCK_SIZE_0)
    inner_2d_size = num_pid_m * num_pid_n
    inner_2d_pid = tl.program_id(0) % inner_2d_size
    num_pid_in_group = 4 * num_pid_n
    group_id = inner_2d_pid // num_pid_in_group
    first_pid_m = group_id * 4
    group_size_m = min(num_pid_m - first_pid_m, 4)
    pid_0 = first_pid_m + inner_2d_pid % num_pid_in_group % group_size_m
    pid_1 = inner_2d_pid % num_pid_in_group // group_size_m
    pid_2 = tl.program_id(0) // (num_blocks_0 * num_blocks_1)
    offset_2 = pid_0 * _BLOCK_SIZE_2
    offset_0 = pid_1 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_2 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
    tile_end = tl.minimum(offset_2 + _BLOCK_SIZE_2, 1280)
    for offset_3 in tl.range(offset_2.to(tl.int32), tile_end.to(tl.int32), _BLOCK_SIZE_3, num_stages=4, disallow_acc_multi_buffer=True, flatten=True):
        indices_3 = offset_3 + tl.arange(0, _BLOCK_SIZE_3).to(tl.int32)
        mask_3 = indices_3 < tile_end
        acc_copy = acc
        acc_copy_0 = acc_copy
        load = tl.load(x + (indices_0[:, None] * 1280 + indices_3[None, :] * 1), mask_3[None, :], other=0)
        load_1 = tl.load(y + (indices_3[:, None] * 1 + indices_1[None, :] * 1280), mask_3[:, None], other=0)
        acc = tl.dot(load, load_1, acc=acc_copy_0, input_precision='tf32')
    eq = offset_2 == 0
    if eq:
        acc_copy_1 = acc
        acc = acc_copy_1
    tl.atomic_add(out + (indices_0[:, None] * 1280 + indices_1[None, :] * 1), acc, mask=None, sem='relaxed')

def matmul_split_k(x: torch.Tensor, y: torch.Tensor, epilogue: Callable[[torch.Tensor, list[torch.Tensor]], torch.Tensor]=lambda acc, tile: acc, *, _launcher=_default_launcher):
    m, k = x.size()
    k2, n = y.size()
    assert k == k2, f'size mismatch {k} != {k2}'
    out = torch.zeros([m, n], dtype=torch.promote_types(x.dtype, y.dtype), device=x.device)
    split_k = 1
    k_block = helion.next_power_of_2(helion.cdiv(k, split_k))
    _BLOCK_SIZE_2 = k_block
    _BLOCK_SIZE_0 = 128
    _BLOCK_SIZE_1 = 64
    _BLOCK_SIZE_3 = 64
    _launcher(_matmul_split_k_kernel, (triton.cdiv(1280, _BLOCK_SIZE_2) * triton.cdiv(1280, _BLOCK_SIZE_0) * triton.cdiv(1280, _BLOCK_SIZE_1),), x, y, out, _BLOCK_SIZE_2, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_3, num_warps=8, num_stages=5)
    return out
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import helion
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

import examples.matmul_split_k as _source_module

@triton.jit
def _matmul_split_k_kernel(x, y, out, _BLOCK_SIZE_2: tl.constexpr, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_3: tl.constexpr):
    num_blocks_0 = tl.cdiv(1280, _BLOCK_SIZE_2)
    num_blocks_1 = tl.cdiv(1280, _BLOCK_SIZE_0)
    num_pid_m = tl.cdiv(1280, _BLOCK_SIZE_2)
    num_pid_n = tl.cdiv(1280, _BLOCK_SIZE_0)
    inner_2d_size = num_pid_m * num_pid_n
    inner_2d_pid = tl.program_id(0) % inner_2d_size
    num_pid_in_group = 4 * num_pid_n
    group_id = inner_2d_pid // num_pid_in_group
    first_pid_m = group_id * 4
    group_size_m = min(num_pid_m - first_pid_m, 4)
    pid_0 = first_pid_m + inner_2d_pid % num_pid_in_group % group_size_m
    pid_1 = inner_2d_pid % num_pid_in_group // group_size_m
    pid_2 = tl.program_id(0) // (num_blocks_0 * num_blocks_1)
    offset_2 = pid_0 * _BLOCK_SIZE_2
    offset_0 = pid_1 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_2 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
    tile_end = tl.minimum(offset_2 + _BLOCK_SIZE_2, 1280)
    for offset_3 in tl.range(offset_2.to(tl.int32), tile_end.to(tl.int32), _BLOCK_SIZE_3, num_stages=4, disallow_acc_multi_buffer=True, flatten=True):
        indices_3 = offset_3 + tl.arange(0, _BLOCK_SIZE_3).to(tl.int32)
        mask_3 = indices_3 < tile_end
        acc_copy = acc
        acc_copy_0 = acc_copy
        load = tl.load(x + (indices_0[:, None] * 1280 + indices_3[None, :] * 1), mask_3[None, :], other=0)
        load_1 = tl.load(y + (indices_3[:, None] * 1 + indices_1[None, :] * 1280), mask_3[:, None], other=0)
        acc = tl.dot(load, load_1, acc=acc_copy_0, input_precision='tf32')
    eq = offset_2 == 0
    if eq:
        acc_copy_1 = acc
        acc = acc_copy_1
    tl.atomic_add(out + (indices_0[:, None] * 1280 + indices_1[None, :] * 1), acc, mask=None, sem='relaxed')

def matmul_split_k(x: torch.Tensor, y: torch.Tensor, epilogue: Callable[[torch.Tensor, list[torch.Tensor]], torch.Tensor]=lambda acc, tile: acc, *, _launcher=_default_launcher):
    m, k = x.size()
    k2, n = y.size()
    assert k == k2, f'size mismatch {k} != {k2}'
    out = torch.zeros([m, n], dtype=torch.promote_types(x.dtype, y.dtype), device=x.device)
    split_k = 1
    k_block = helion.next_power_of_2(helion.cdiv(k, split_k))
    _BLOCK_SIZE_2 = k_block
    _BLOCK_SIZE_0 = 128
    _BLOCK_SIZE_1 = 64
    _BLOCK_SIZE_3 = 64
    _launcher(_matmul_split_k_kernel, (triton.cdiv(1280, _BLOCK_SIZE_2) * triton.cdiv(1280, _BLOCK_SIZE_0) * triton.cdiv(1280, _BLOCK_SIZE_1),), x, y, out, _BLOCK_SIZE_2, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_3, num_warps=8, num_stages=5)
    return out
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import helion
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

import examples.matmul_split_k as _source_module

@triton.jit
def _matmul_split_k_kernel(x, y, out, _BLOCK_SIZE_2: tl.constexpr, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_3: tl.constexpr):
    num_blocks_0 = tl.cdiv(1280, _BLOCK_SIZE_2)
    num_blocks_1 = tl.cdiv(1280, _BLOCK_SIZE_0)
    num_pid_m = tl.cdiv(1280, _BLOCK_SIZE_2)
    num_pid_n = tl.cdiv(1280, _BLOCK_SIZE_0)
    inner_2d_size = num_pid_m * num_pid_n
    inner_2d_pid = tl.program_id(0) % inner_2d_size
    num_pid_in_group = 4 * num_pid_n
    group_id = inner_2d_pid // num_pid_in_group
    first_pid_m = group_id * 4
    group_size_m = min(num_pid_m - first_pid_m, 4)
    pid_0 = first_pid_m + inner_2d_pid % num_pid_in_group % group_size_m
    pid_1 = inner_2d_pid % num_pid_in_group // group_size_m
    pid_2 = tl.program_id(0) // (num_blocks_0 * num_blocks_1)
    offset_2 = pid_0 * _BLOCK_SIZE_2
    offset_0 = pid_1 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_2 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
    tile_end = tl.minimum(offset_2 + _BLOCK_SIZE_2, 1280)
    for offset_3 in tl.range(offset_2.to(tl.int32), tile_end.to(tl.int32), _BLOCK_SIZE_3, num_stages=4, disallow_acc_multi_buffer=True, flatten=True):
        indices_3 = offset_3 + tl.arange(0, _BLOCK_SIZE_3).to(tl.int32)
        mask_3 = indices_3 < tile_end
        acc_copy = acc
        acc_copy_0 = acc_copy
        load = tl.load(x + (indices_0[:, None] * 1280 + indices_3[None, :] * 1), mask_3[None, :], other=0)
        load_1 = tl.load(y + (indices_3[:, None] * 1 + indices_1[None, :] * 1280), mask_3[:, None], other=0)
        acc = tl.dot(load, load_1, acc=acc_copy_0, input_precision='tf32')
    eq = offset_2 == 0
    if eq:
        acc_copy_1 = acc
        acc = acc_copy_1
    tl.atomic_add(out + (indices_0[:, None] * 1280 + indices_1[None, :] * 1), acc, mask=None, sem='relaxed')

def matmul_split_k(x: torch.Tensor, y: torch.Tensor, epilogue: Callable[[torch.Tensor, list[torch.Tensor]], torch.Tensor]=lambda acc, tile: acc, *, _launcher=_default_launcher):
    m, k = x.size()
    k2, n = y.size()
    assert k == k2, f'size mismatch {k} != {k2}'
    out = torch.zeros([m, n], dtype=torch.promote_types(x.dtype, y.dtype), device=x.device)
    split_k = 1
    k_block = helion.next_power_of_2(helion.cdiv(k, split_k))
    _BLOCK_SIZE_2 = k_block
    _BLOCK_SIZE_0 = 128
    _BLOCK_SIZE_1 = 64
    _BLOCK_SIZE_3 = 64
    _launcher(_matmul_split_k_kernel, (triton.cdiv(1280, _BLOCK_SIZE_2) * triton.cdiv(1280, _BLOCK_SIZE_0) * triton.cdiv(1280, _BLOCK_SIZE_1),), x, y, out, _BLOCK_SIZE_2, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_3, num_warps=8, num_stages=5)
    return out
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import helion
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

import examples.matmul_split_k as _source_module

@triton.jit
def _matmul_split_k_kernel(x, y, out, _BLOCK_SIZE_2: tl.constexpr, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_3: tl.constexpr):
    num_blocks_0 = tl.cdiv(1280, _BLOCK_SIZE_2)
    num_blocks_1 = tl.cdiv(1280, _BLOCK_SIZE_0)
    num_pid_m = tl.cdiv(1280, _BLOCK_SIZE_2)
    num_pid_n = tl.cdiv(1280, _BLOCK_SIZE_0)
    inner_2d_size = num_pid_m * num_pid_n
    inner_2d_pid = tl.program_id(0) % inner_2d_size
    num_pid_in_group = 4 * num_pid_n
    group_id = inner_2d_pid // num_pid_in_group
    first_pid_m = group_id * 4
    group_size_m = min(num_pid_m - first_pid_m, 4)
    pid_0 = first_pid_m + inner_2d_pid % num_pid_in_group % group_size_m
    pid_1 = inner_2d_pid % num_pid_in_group // group_size_m
    pid_2 = tl.program_id(0) // (num_blocks_0 * num_blocks_1)
    offset_2 = pid_0 * _BLOCK_SIZE_2
    offset_0 = pid_1 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_2 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
    tile_end = tl.minimum(offset_2 + _BLOCK_SIZE_2, 1280)
    for offset_3 in tl.range(offset_2.to(tl.int32), tile_end.to(tl.int32), _BLOCK_SIZE_3, num_stages=4, disallow_acc_multi_buffer=True, flatten=True):
        indices_3 = offset_3 + tl.arange(0, _BLOCK_SIZE_3).to(tl.int32)
        mask_3 = indices_3 < tile_end
        acc_copy = acc
        acc_copy_0 = acc_copy
        load = tl.load(x + (indices_0[:, None] * 1280 + indices_3[None, :] * 1), mask_3[None, :], other=0)
        load_1 = tl.load(y + (indices_3[:, None] * 1 + indices_1[None, :] * 1280), mask_3[:, None], other=0)
        acc = tl.dot(load, load_1, acc=acc_copy_0, input_precision='tf32')
    eq = offset_2 == 0
    if eq:
        acc_copy_1 = acc
        acc = acc_copy_1
    tl.atomic_add(out + (indices_0[:, None] * 1280 + indices_1[None, :] * 1), acc, mask=None, sem='relaxed')

def matmul_split_k(x: torch.Tensor, y: torch.Tensor, epilogue: Callable[[torch.Tensor, list[torch.Tensor]], torch.Tensor]=lambda acc, tile: acc, *, _launcher=_default_launcher):
    m, k = x.size()
    k2, n = y.size()
    assert k == k2, f'size mismatch {k} != {k2}'
    out = torch.zeros([m, n], dtype=torch.promote_types(x.dtype, y.dtype), device=x.device)
    split_k = 1
    k_block = helion.next_power_of_2(helion.cdiv(k, split_k))
    _BLOCK_SIZE_2 = k_block
    _BLOCK_SIZE_0 = 128
    _BLOCK_SIZE_1 = 64
    _BLOCK_SIZE_3 = 64
    _launcher(_matmul_split_k_kernel, (triton.cdiv(1280, _BLOCK_SIZE_2) * triton.cdiv(1280, _BLOCK_SIZE_0) * triton.cdiv(1280, _BLOCK_SIZE_1),), x, y, out, _BLOCK_SIZE_2, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_3, num_warps=8, num_stages=5)
    return out
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import helion
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

import examples.matmul_split_k as _source_module

@triton.jit
def _matmul_split_k_kernel(x, y, out, _BLOCK_SIZE_2: tl.constexpr, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_3: tl.constexpr):
    num_blocks_0 = tl.cdiv(1280, _BLOCK_SIZE_2)
    num_blocks_1 = tl.cdiv(1280, _BLOCK_SIZE_0)
    num_pid_m = tl.cdiv(1280, _BLOCK_SIZE_2)
    num_pid_n = tl.cdiv(1280, _BLOCK_SIZE_0)
    inner_2d_size = num_pid_m * num_pid_n
    inner_2d_pid = tl.program_id(0) % inner_2d_size
    num_pid_in_group = 4 * num_pid_n
    group_id = inner_2d_pid // num_pid_in_group
    first_pid_m = group_id * 4
    group_size_m = min(num_pid_m - first_pid_m, 4)
    pid_0 = first_pid_m + inner_2d_pid % num_pid_in_group % group_size_m
    pid_1 = inner_2d_pid % num_pid_in_group // group_size_m
    pid_2 = tl.program_id(0) // (num_blocks_0 * num_blocks_1)
    offset_2 = pid_0 * _BLOCK_SIZE_2
    offset_0 = pid_1 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_2 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
    tile_end = tl.minimum(offset_2 + _BLOCK_SIZE_2, 1280)
    for offset_3 in tl.range(offset_2.to(tl.int32), tile_end.to(tl.int32), _BLOCK_SIZE_3, num_stages=4, disallow_acc_multi_buffer=True, flatten=True):
        indices_3 = offset_3 + tl.arange(0, _BLOCK_SIZE_3).to(tl.int32)
        mask_3 = indices_3 < tile_end
        acc_copy = acc
        acc_copy_0 = acc_copy
        load = tl.load(x + (indices_0[:, None] * 1280 + indices_3[None, :] * 1), mask_3[None, :], other=0)
        load_1 = tl.load(y + (indices_3[:, None] * 1 + indices_1[None, :] * 1280), mask_3[:, None], other=0)
        acc = tl.dot(load, load_1, acc=acc_copy_0, input_precision='tf32')
    eq = offset_2 == 0
    if eq:
        acc_copy_1 = acc
        acc = acc_copy_1
    tl.atomic_add(out + (indices_0[:, None] * 1280 + indices_1[None, :] * 1), acc, mask=None, sem='relaxed')

def matmul_split_k(x: torch.Tensor, y: torch.Tensor, epilogue: Callable[[torch.Tensor, list[torch.Tensor]], torch.Tensor]=lambda acc, tile: acc, *, _launcher=_default_launcher):
    m, k = x.size()
    k2, n = y.size()
    assert k == k2, f'size mismatch {k} != {k2}'
    out = torch.zeros([m, n], dtype=torch.promote_types(x.dtype, y.dtype), device=x.device)
    split_k = 1
    k_block = helion.next_power_of_2(helion.cdiv(k, split_k))
    _BLOCK_SIZE_2 = k_block
    _BLOCK_SIZE_0 = 128
    _BLOCK_SIZE_1 = 64
    _BLOCK_SIZE_3 = 64
    _launcher(_matmul_split_k_kernel, (triton.cdiv(1280, _BLOCK_SIZE_2) * triton.cdiv(1280, _BLOCK_SIZE_0) * triton.cdiv(1280, _BLOCK_SIZE_1),), x, y, out, _BLOCK_SIZE_2, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_3, num_warps=8, num_stages=5)
    return out
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import helion
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

import examples.matmul_split_k as _source_module

@triton.jit
def _matmul_split_k_kernel(x, y, out, _BLOCK_SIZE_2: tl.constexpr, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_3: tl.constexpr):
    num_blocks_0 = tl.cdiv(1280, _BLOCK_SIZE_2)
    num_blocks_1 = tl.cdiv(1280, _BLOCK_SIZE_0)
    num_pid_m = tl.cdiv(1280, _BLOCK_SIZE_2)
    num_pid_n = tl.cdiv(1280, _BLOCK_SIZE_0)
    inner_2d_size = num_pid_m * num_pid_n
    inner_2d_pid = tl.program_id(0) % inner_2d_size
    num_pid_in_group = 4 * num_pid_n
    group_id = inner_2d_pid // num_pid_in_group
    first_pid_m = group_id * 4
    group_size_m = min(num_pid_m - first_pid_m, 4)
    pid_0 = first_pid_m + inner_2d_pid % num_pid_in_group % group_size_m
    pid_1 = inner_2d_pid % num_pid_in_group // group_size_m
    pid_2 = tl.program_id(0) // (num_blocks_0 * num_blocks_1)
    offset_2 = pid_0 * _BLOCK_SIZE_2
    offset_0 = pid_1 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_2 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
    tile_end = tl.minimum(offset_2 + _BLOCK_SIZE_2, 1280)
    for offset_3 in tl.range(offset_2.to(tl.int32), tile_end.to(tl.int32), _BLOCK_SIZE_3, num_stages=4, disallow_acc_multi_buffer=True, flatten=True):
        indices_3 = offset_3 + tl.arange(0, _BLOCK_SIZE_3).to(tl.int32)
        mask_3 = indices_3 < tile_end
        acc_copy = acc
        acc_copy_0 = acc_copy
        load = tl.load(x + (indices_0[:, None] * 1280 + indices_3[None, :] * 1), mask_3[None, :], other=0)
        load_1 = tl.load(y + (indices_3[:, None] * 1 + indices_1[None, :] * 1280), mask_3[:, None], other=0)
        acc = tl.dot(load, load_1, acc=acc_copy_0, input_precision='tf32')
    eq = offset_2 == 0
    if eq:
        acc_copy_1 = acc
        acc = acc_copy_1
    tl.atomic_add(out + (indices_0[:, None] * 1280 + indices_1[None, :] * 1), acc, mask=None, sem='relaxed')

def matmul_split_k(x: torch.Tensor, y: torch.Tensor, epilogue: Callable[[torch.Tensor, list[torch.Tensor]], torch.Tensor]=lambda acc, tile: acc, *, _launcher=_default_launcher):
    m, k = x.size()
    k2, n = y.size()
    assert k == k2, f'size mismatch {k} != {k2}'
    out = torch.zeros([m, n], dtype=torch.promote_types(x.dtype, y.dtype), device=x.device)
    split_k = 1
    k_block = helion.next_power_of_2(helion.cdiv(k, split_k))
    _BLOCK_SIZE_2 = k_block
    _BLOCK_SIZE_0 = 128
    _BLOCK_SIZE_1 = 64
    _BLOCK_SIZE_3 = 64
    _launcher(_matmul_split_k_kernel, (triton.cdiv(1280, _BLOCK_SIZE_2) * triton.cdiv(1280, _BLOCK_SIZE_0) * triton.cdiv(1280, _BLOCK_SIZE_1),), x, y, out, _BLOCK_SIZE_2, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_3, num_warps=8, num_stages=5)
    return out
 12%|        | 1/8 [25:31<2:58:40, 1531.45s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for aten_matmul
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_tutorial_matmul
INFO:tritonbench.utils.triton_op:Took 0.06ms to get benchmark function for matmul_partition_k
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_ops_matmul
INFO:tritonbench.utils.triton_op:Took 4.01ms to get benchmark function for aten_tunableop_matmul
AUTOTUNE mm(1408x1408, 1408x1408)
strides: [1408, 1], [1, 1408]
dtypes: torch.float16, torch.float16
  mm 0.0140 ms 100.0% 
  triton_mm_18 0.0140 ms 99.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_11 0.0164 ms 85.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_7 0.0174 ms 80.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
  triton_mm_17 0.0177 ms 79.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_12 0.0188 ms 74.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
  triton_mm_10 0.0190 ms 73.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
  triton_mm_14 0.0196 ms 71.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
  triton_mm_9 0.0234 ms 59.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_8 0.0238 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.2455 seconds and 2.5747 seconds precompiling for 20 choices
INFO:tritonbench.utils.triton_op:Took 3016.98ms to get benchmark function for pt2_triton_matmul
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for streamk_matmul
 12%|        | 1/8 [25:42<2:59:58, 1542.59s/it]
WARNING:tritonbench.utils.triton_op:Caught exception, terminating early with partial results
Traceback (most recent call last):
  File "/data/users/willfeng/helion/benchmarks/tritonbench/tritonbench/utils/triton_op.py", line 902, in run
    y_vals: Dict[str, BenchmarkOperatorMetrics] = functools.reduce(
                                                  ^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/benchmarks/tritonbench/tritonbench/utils/triton_op.py", line 890, in _reduce_benchmarks
    acc[bm_name] = self._do_bench(
                   ^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/benchmarks/tritonbench/tritonbench/utils/triton_op.py", line 1149, in _do_bench
    fn = self._get_bm_func(fn_name)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/benchmarks/tritonbench/tritonbench/utils/triton_op.py", line 786, in _get_bm_func
    fwd_fn = fwd_fn_lambda(*self.example_inputs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/benchmarks/tritonbench/tritonbench/utils/triton_op.py", line 612, in _inner
    return function(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/benchmarks/tritonbench/tritonbench/operators/gemm/operator.py", line 335, in pt2_cutlass_matmul
    compiled(a, b)
  File "/home/willfeng/local/pytorch-nightly/torch/_dynamo/eval_frame.py", line 816, in compile_wrapper
    raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/torch/_inductor/compile_fx.py", line 952, in _compile_fx_inner
    raise InductorError(e, currentframe()).with_traceback(
  File "/home/willfeng/local/pytorch-nightly/torch/_inductor/compile_fx.py", line 936, in _compile_fx_inner
    mb_compiled_graph = fx_codegen_and_compile(
                        ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/torch/_inductor/compile_fx.py", line 1616, in fx_codegen_and_compile
    return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/torch/_inductor/compile_fx.py", line 1400, in codegen_and_compile
    graph.run(*example_inputs)
  File "/home/willfeng/local/pytorch-nightly/torch/_inductor/graph.py", line 915, in run
    return super().run(*args)
           ^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/torch/fx/interpreter.py", line 174, in run
    self.env[node] = self.run_node(node)
                     ^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/torch/_inductor/graph.py", line 1595, in run_node
    result = super().run_node(n)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/torch/fx/interpreter.py", line 256, in run_node
    return getattr(self, n.op)(n.target, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/torch/_inductor/graph.py", line 1262, in call_function
    raise LoweringException(e, target, args, kwargs).with_traceback(
  File "/home/willfeng/local/pytorch-nightly/torch/_inductor/graph.py", line 1252, in call_function
    out = lowerings[target](*args, **kwargs)  # type: ignore[index]
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/torch/_inductor/lowering.py", line 449, in wrapped
    out = decomp_fn(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/torch/_inductor/kernel/mm.py", line 780, in tuned_mm
    and use_cutlass_template(layout, m, n, k)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/torch/_inductor/utils.py", line 1649, in use_cutlass_template
    if not try_import_cutlass():
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/torch/_inductor/codegen/cuda/cutlass_utils.py", line 146, in try_import_cutlass
    link_and_append(
  File "/home/willfeng/local/pytorch-nightly/torch/_inductor/codegen/cuda/cutlass_utils.py", line 140, in link_and_append
    os.remove(dst_link)  # remove if exists
    ^^^^^^^^^^^^^^^^^^^
torch._inductor.exc.InductorError: LoweringException: FileNotFoundError: [Errno 2] No such file or directory: '/tmp/torchinductor_willfeng/torch_cutlass/cutlass_library'
  target: aten.mm.default
  args[0]: TensorBox(StorageBox(
    InputBuffer(name='arg0_1', layout=FixedLayout('cuda:0', torch.float16, size=[1408, 1408], stride=[1408, 1]))
  ))
  args[1]: TensorBox(StorageBox(
    InputBuffer(name='arg1_1', layout=FixedLayout('cuda:0', torch.float16, size=[1408, 1408], stride=[1, 1408]))
  ))

Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"

Removed 1 outliers from 780 samples
Removed 5 outliers from 742 samples
Removed 2 outliers from 85 samples
Removed 10 outliers from 776 samples
Removed 6 outliers from 768 samples
Removed 1 outliers from 666 samples
Traceback (most recent call last):
  File "/data/users/willfeng/helion/benchmarks/run.py", line 537, in <module>
    main()
  File "/data/users/willfeng/helion/benchmarks/run.py", line 533, in main
    run_kernel(kernel_name, tritonbench_args.copy(), input_shard_info)
  File "/data/users/willfeng/helion/benchmarks/run.py", line 245, in run_kernel
    run_kernel_variants(
  File "/data/users/willfeng/helion/benchmarks/run.py", line 442, in run_kernel_variants
    op.run(warmup=warmup, rep=rep)
  File "/data/users/willfeng/helion/benchmarks/tritonbench/tritonbench/utils/triton_op.py", line 902, in run
    y_vals: Dict[str, BenchmarkOperatorMetrics] = functools.reduce(
                                                  ^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/benchmarks/tritonbench/tritonbench/utils/triton_op.py", line 890, in _reduce_benchmarks
    acc[bm_name] = self._do_bench(
                   ^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/benchmarks/tritonbench/tritonbench/utils/triton_op.py", line 1149, in _do_bench
    fn = self._get_bm_func(fn_name)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/benchmarks/tritonbench/tritonbench/utils/triton_op.py", line 786, in _get_bm_func
    fwd_fn = fwd_fn_lambda(*self.example_inputs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/benchmarks/tritonbench/tritonbench/utils/triton_op.py", line 612, in _inner
    return function(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/benchmarks/tritonbench/tritonbench/operators/gemm/operator.py", line 335, in pt2_cutlass_matmul
    compiled(a, b)
  File "/home/willfeng/local/pytorch-nightly/torch/_dynamo/eval_frame.py", line 816, in compile_wrapper
    raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/torch/_inductor/compile_fx.py", line 952, in _compile_fx_inner
    raise InductorError(e, currentframe()).with_traceback(
  File "/home/willfeng/local/pytorch-nightly/torch/_inductor/compile_fx.py", line 936, in _compile_fx_inner
    mb_compiled_graph = fx_codegen_and_compile(
                        ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/torch/_inductor/compile_fx.py", line 1616, in fx_codegen_and_compile
    return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/torch/_inductor/compile_fx.py", line 1400, in codegen_and_compile
    graph.run(*example_inputs)
  File "/home/willfeng/local/pytorch-nightly/torch/_inductor/graph.py", line 915, in run
    return super().run(*args)
           ^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/torch/fx/interpreter.py", line 174, in run
    self.env[node] = self.run_node(node)
                     ^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/torch/_inductor/graph.py", line 1595, in run_node
    result = super().run_node(n)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/torch/fx/interpreter.py", line 256, in run_node
    return getattr(self, n.op)(n.target, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/torch/_inductor/graph.py", line 1262, in call_function
    raise LoweringException(e, target, args, kwargs).with_traceback(
  File "/home/willfeng/local/pytorch-nightly/torch/_inductor/graph.py", line 1252, in call_function
    out = lowerings[target](*args, **kwargs)  # type: ignore[index]
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/torch/_inductor/lowering.py", line 449, in wrapped
    out = decomp_fn(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/torch/_inductor/kernel/mm.py", line 780, in tuned_mm
    and use_cutlass_template(layout, m, n, k)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/torch/_inductor/utils.py", line 1649, in use_cutlass_template
    if not try_import_cutlass():
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/torch/_inductor/codegen/cuda/cutlass_utils.py", line 146, in try_import_cutlass
    link_and_append(
  File "/home/willfeng/local/pytorch-nightly/torch/_inductor/codegen/cuda/cutlass_utils.py", line 140, in link_and_append
    os.remove(dst_link)  # remove if exists
    ^^^^^^^^^^^^^^^^^^^
torch._inductor.exc.InductorError: LoweringException: FileNotFoundError: [Errno 2] No such file or directory: '/tmp/torchinductor_willfeng/torch_cutlass/cutlass_library'
  target: aten.mm.default
  args[0]: TensorBox(StorageBox(
    InputBuffer(name='arg0_1', layout=FixedLayout('cuda:0', torch.float16, size=[1408, 1408], stride=[1408, 1]))
  ))
  args[1]: TensorBox(StorageBox(
    InputBuffer(name='arg1_1', layout=FixedLayout('cuda:0', torch.float16, size=[1408, 1408], stride=[1, 1408]))
  ))

Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"

