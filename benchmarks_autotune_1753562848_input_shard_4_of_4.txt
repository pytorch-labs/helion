Running 4 kernels...


============================================================
Kernel: gemm
============================================================

Running gemm benchmark with Helion implementation (variant: matmul_split_k)...

Running input shard 4/4: inputs 24 to 30 (of 31 total)
  0%|          | 0/7 [00:00<?, ?it/s]INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for aten_matmul
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_tutorial_matmul
INFO:tritonbench.utils.triton_op:Took 0.09ms to get benchmark function for matmul_partition_k
/home/willfeng/local/pytorch-nightly/torch/backends/cuda/__init__.py:131: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:78.)
  return torch._C._get_cublas_allow_tf32()
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_ops_matmul
INFO:tritonbench.utils.triton_op:Took 23.69ms to get benchmark function for aten_tunableop_matmul
INFO:tritonbench.utils.triton_op:Took 47.25ms to get benchmark function for pt2_triton_matmul
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for streamk_matmul
INFO:tritonbench.utils.triton_op:Took 32.99ms to get benchmark function for pt2_cutlass_matmul
INFO:tritonbench.utils.triton_op:Took 0.08ms to get benchmark function for helion_gemm_matmul_split_k
[0s] Starting DifferentialEvolutionSearch with population=40, generations=20, crossover_rate=0.8
Removed 1 outliers from 126 samples
Removed 1 outliers from 411 samples
Removed 15 outliers from 372 samples
Removed 2 outliers from 452 samples
Removed 2 outliers from 453 samples
[15s] PTXASError compiling config: Config(block_sizes=[128, 512, 16], loop_orders=[[1, 2, 0]], l2_groupings=[8], range_unroll_factors=[2, 2], range_num_stages=[1, 1], range_multi_buffers=[False, False], range_flattens=[True, False], num_warps=16, num_stages=1, indexing='block_ptr', pid_type='persistent_blocked', split_k=64)
  0%|          | 0/7 [00:32<?, ?it/s]
WARNING:tritonbench.utils.triton_op:Caught exception, terminating early with partial results
Traceback (most recent call last):
  File "/data/users/willfeng/helion/benchmarks/tritonbench/tritonbench/utils/triton_op.py", line 902, in run
    y_vals: Dict[str, BenchmarkOperatorMetrics] = functools.reduce(
                                                  ^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/benchmarks/tritonbench/tritonbench/utils/triton_op.py", line 890, in _reduce_benchmarks
    acc[bm_name] = self._do_bench(
                   ^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/benchmarks/tritonbench/tritonbench/utils/triton_op.py", line 1164, in _do_bench
    metrics.latency = do_bench_wrapper(
                      ^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/benchmarks/tritonbench/tritonbench/components/do_bench/run.py", line 202, in do_bench_wrapper
    times=triton.testing.do_bench(
          ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/testing.py", line 149, in do_bench
    fn()
  File "/data/users/willfeng/helion/benchmarks/run.py", line 335, in _inner
    result = kernel_func(*args)
             ^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/examples/matmul_split_k.py", line 54, in matmul_split_k
    return matmul_split_k_no_bias(x, y)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/helion/runtime/kernel.py", line 252, in __call__
    return self.bind(args)(*args)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/helion/runtime/kernel.py", line 535, in __call__
    self.autotune(args)
  File "/data/users/willfeng/helion/helion/runtime/kernel.py", line 447, in autotune
    ).autotune()
      ^^^^^^^^^^
  File "/data/users/willfeng/helion/helion/autotuner/base_search.py", line 242, in autotune
    best = self._autotune()
           ^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/helion/autotuner/differential_evolution.py", line 97, in _autotune
    self.initial_two_generations()
  File "/data/users/willfeng/helion/helion/autotuner/differential_evolution.py", line 59, in initial_two_generations
    self.parallel_benchmark_flat(
  File "/data/users/willfeng/helion/helion/autotuner/base_search.py", line 358, in parallel_benchmark_flat
    to_check, configs, self.parallel_benchmark(configs), strict=True
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/helion/autotuner/base_search.py", line 226, in parallel_benchmark
    results.append((config, self.benchmark_function(config, fn)))
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/helion/autotuner/base_search.py", line 124, in benchmark_function
    res = do_bench(
          ^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/testing.py", line 188, in do_bench
    di.synchronize()
  File "/home/willfeng/local/pytorch-nightly/torch/cuda/__init__.py", line 1083, in synchronize
    return torch._C._cuda_synchronize()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
Traceback (most recent call last):
  File "/data/users/willfeng/helion/benchmarks/run.py", line 510, in <module>
    main()
  File "/data/users/willfeng/helion/benchmarks/run.py", line 506, in main
    run_kernel(kernel_name, tritonbench_args.copy(), input_shard_info)
  File "/data/users/willfeng/helion/benchmarks/run.py", line 235, in run_kernel
    run_single_kernel_variant(kernel_name, tritonbench_module, module_path, func_name, tritonbench_args.copy(), variant_name, input_shard_info)
  File "/data/users/willfeng/helion/benchmarks/run.py", line 413, in run_single_kernel_variant
    op.run(warmup=warmup, rep=rep)
  File "/data/users/willfeng/helion/benchmarks/tritonbench/tritonbench/utils/triton_op.py", line 902, in run
    y_vals: Dict[str, BenchmarkOperatorMetrics] = functools.reduce(
                                                  ^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/benchmarks/tritonbench/tritonbench/utils/triton_op.py", line 890, in _reduce_benchmarks
    acc[bm_name] = self._do_bench(
                   ^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/benchmarks/tritonbench/tritonbench/utils/triton_op.py", line 1164, in _do_bench
    metrics.latency = do_bench_wrapper(
                      ^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/benchmarks/tritonbench/tritonbench/components/do_bench/run.py", line 202, in do_bench_wrapper
    times=triton.testing.do_bench(
          ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/testing.py", line 149, in do_bench
    fn()
  File "/data/users/willfeng/helion/benchmarks/run.py", line 335, in _inner
    result = kernel_func(*args)
             ^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/examples/matmul_split_k.py", line 54, in matmul_split_k
    return matmul_split_k_no_bias(x, y)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/helion/runtime/kernel.py", line 252, in __call__
    return self.bind(args)(*args)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/helion/runtime/kernel.py", line 535, in __call__
    self.autotune(args)
  File "/data/users/willfeng/helion/helion/runtime/kernel.py", line 447, in autotune
    ).autotune()
      ^^^^^^^^^^
  File "/data/users/willfeng/helion/helion/autotuner/base_search.py", line 242, in autotune
    best = self._autotune()
           ^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/helion/autotuner/differential_evolution.py", line 97, in _autotune
    self.initial_two_generations()
  File "/data/users/willfeng/helion/helion/autotuner/differential_evolution.py", line 59, in initial_two_generations
    self.parallel_benchmark_flat(
  File "/data/users/willfeng/helion/helion/autotuner/base_search.py", line 358, in parallel_benchmark_flat
    to_check, configs, self.parallel_benchmark(configs), strict=True
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/helion/autotuner/base_search.py", line 226, in parallel_benchmark
    results.append((config, self.benchmark_function(config, fn)))
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/helion/autotuner/base_search.py", line 124, in benchmark_function
    res = do_bench(
          ^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/testing.py", line 188, in do_bench
    di.synchronize()
  File "/home/willfeng/local/pytorch-nightly/torch/cuda/__init__.py", line 1083, in synchronize
    return torch._C._cuda_synchronize()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
Exception ignored in atexit callback: <function shutdown_compile_workers at 0x7fba9e22c720>
Traceback (most recent call last):
  File "/home/willfeng/local/pytorch-nightly/torch/_inductor/async_compile.py", line 145, in shutdown_compile_workers
    pool.shutdown()
  File "/home/willfeng/local/pytorch-nightly/torch/_inductor/compile_worker/subproc_pool.py", line 262, in shutdown
    self.process.wait(300)
  File "/home/willfeng/local/miniconda3/lib/python3.12/subprocess.py", line 1277, in wait
    self._wait(timeout=sigint_timeout)
  File "/home/willfeng/local/miniconda3/lib/python3.12/subprocess.py", line 2045, in _wait
    time.sleep(delay)
KeyboardInterrupt: 
