Running softmax benchmark with Helion implementation...

Running input shard 2/4: inputs 25 to 49 (of 98 total)
  0%|          | 0/25 [00:00<?, ?it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.07ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.04ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.04ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, n, _m, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < n
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    _mask_to = tl.where(mask_0[:, None] & mask_1[None, :], load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [_BLOCK_SIZE_0, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(mask_0[:, None] & mask_1[None, :], v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [_BLOCK_SIZE_0, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_0[:, None] & mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    """
    Simple Helion kernel wrapping PyTorch's softmax function.
    Args:
        x (torch.Tensor): Input tensor of shape [n, m].
    Returns:
        torch.Tensor: Softmax output tensor of the same shape.
    """
    n, _m = x.size()
    out = torch.empty_like(x)
    _BLOCK_SIZE_0 = 8
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (triton.cdiv(n, _BLOCK_SIZE_0),), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), n, _m, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=8, num_stages=8)
    return out
INFO:tritonbench.utils.triton_op:Took 0.13ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, n, _m, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < n
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    _mask_to = tl.where(mask_0[:, None] & mask_1[None, :], load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [_BLOCK_SIZE_0, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(mask_0[:, None] & mask_1[None, :], v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [_BLOCK_SIZE_0, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_0[:, None] & mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    """
    Simple Helion kernel wrapping PyTorch's softmax function.
    Args:
        x (torch.Tensor): Input tensor of shape [n, m].
    Returns:
        torch.Tensor: Softmax output tensor of the same shape.
    """
    n, _m = x.size()
    out = torch.empty_like(x)
    _BLOCK_SIZE_0 = 8
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (triton.cdiv(n, _BLOCK_SIZE_0),), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), n, _m, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=8, num_stages=8)
    return out
  4%|▍         | 1/25 [00:01<00:28,  1.20s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.03ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.02ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.10ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, n, _m, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < n
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    _mask_to = tl.where(mask_0[:, None] & mask_1[None, :], load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [_BLOCK_SIZE_0, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(mask_0[:, None] & mask_1[None, :], v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [_BLOCK_SIZE_0, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_0[:, None] & mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    """
    Simple Helion kernel wrapping PyTorch's softmax function.
    Args:
        x (torch.Tensor): Input tensor of shape [n, m].
    Returns:
        torch.Tensor: Softmax output tensor of the same shape.
    """
    n, _m = x.size()
    out = torch.empty_like(x)
    _BLOCK_SIZE_0 = 8
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (triton.cdiv(n, _BLOCK_SIZE_0),), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), n, _m, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=8, num_stages=8)
    return out
INFO:tritonbench.utils.triton_op:Took 0.11ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, n, _m, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < n
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    _mask_to = tl.where(mask_0[:, None] & mask_1[None, :], load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [_BLOCK_SIZE_0, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(mask_0[:, None] & mask_1[None, :], v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [_BLOCK_SIZE_0, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_0[:, None] & mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    """
    Simple Helion kernel wrapping PyTorch's softmax function.
    Args:
        x (torch.Tensor): Input tensor of shape [n, m].
    Returns:
        torch.Tensor: Softmax output tensor of the same shape.
    """
    n, _m = x.size()
    out = torch.empty_like(x)
    _BLOCK_SIZE_0 = 8
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (triton.cdiv(n, _BLOCK_SIZE_0),), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), n, _m, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=8, num_stages=8)
    return out
  8%|▊         | 2/25 [00:01<00:17,  1.31it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.03ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.02ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.11ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, n, _m, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < n
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    _mask_to = tl.where(mask_0[:, None] & mask_1[None, :], load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [_BLOCK_SIZE_0, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(mask_0[:, None] & mask_1[None, :], v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [_BLOCK_SIZE_0, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_0[:, None] & mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    """
    Simple Helion kernel wrapping PyTorch's softmax function.
    Args:
        x (torch.Tensor): Input tensor of shape [n, m].
    Returns:
        torch.Tensor: Softmax output tensor of the same shape.
    """
    n, _m = x.size()
    out = torch.empty_like(x)
    _BLOCK_SIZE_0 = 8
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (triton.cdiv(n, _BLOCK_SIZE_0),), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), n, _m, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=8, num_stages=8)
    return out
INFO:tritonbench.utils.triton_op:Took 0.12ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, n, _m, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < n
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    _mask_to = tl.where(mask_0[:, None] & mask_1[None, :], load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [_BLOCK_SIZE_0, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(mask_0[:, None] & mask_1[None, :], v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [_BLOCK_SIZE_0, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_0[:, None] & mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    """
    Simple Helion kernel wrapping PyTorch's softmax function.
    Args:
        x (torch.Tensor): Input tensor of shape [n, m].
    Returns:
        torch.Tensor: Softmax output tensor of the same shape.
    """
    n, _m = x.size()
    out = torch.empty_like(x)
    _BLOCK_SIZE_0 = 8
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (triton.cdiv(n, _BLOCK_SIZE_0),), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), n, _m, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=8, num_stages=8)
    return out
 12%|█▏        | 3/25 [00:02<00:13,  1.61it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.03ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.02ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.12ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, n, _m, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < n
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    _mask_to = tl.where(mask_0[:, None] & mask_1[None, :], load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [_BLOCK_SIZE_0, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(mask_0[:, None] & mask_1[None, :], v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [_BLOCK_SIZE_0, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_0[:, None] & mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    """
    Simple Helion kernel wrapping PyTorch's softmax function.
    Args:
        x (torch.Tensor): Input tensor of shape [n, m].
    Returns:
        torch.Tensor: Softmax output tensor of the same shape.
    """
    n, _m = x.size()
    out = torch.empty_like(x)
    _BLOCK_SIZE_0 = 8
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (triton.cdiv(n, _BLOCK_SIZE_0),), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), n, _m, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=8, num_stages=8)
    return out
INFO:tritonbench.utils.triton_op:Took 0.11ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, n, _m, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < n
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    _mask_to = tl.where(mask_0[:, None] & mask_1[None, :], load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [_BLOCK_SIZE_0, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(mask_0[:, None] & mask_1[None, :], v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [_BLOCK_SIZE_0, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_0[:, None] & mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    """
    Simple Helion kernel wrapping PyTorch's softmax function.
    Args:
        x (torch.Tensor): Input tensor of shape [n, m].
    Returns:
        torch.Tensor: Softmax output tensor of the same shape.
    """
    n, _m = x.size()
    out = torch.empty_like(x)
    _BLOCK_SIZE_0 = 8
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (triton.cdiv(n, _BLOCK_SIZE_0),), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), n, _m, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=8, num_stages=8)
    return out
 16%|█▌        | 4/25 [00:02<00:11,  1.80it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.03ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.02ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.11ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, n, _m, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < n
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    _mask_to = tl.where(mask_0[:, None] & mask_1[None, :], load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [_BLOCK_SIZE_0, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(mask_0[:, None] & mask_1[None, :], v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [_BLOCK_SIZE_0, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_0[:, None] & mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    """
    Simple Helion kernel wrapping PyTorch's softmax function.
    Args:
        x (torch.Tensor): Input tensor of shape [n, m].
    Returns:
        torch.Tensor: Softmax output tensor of the same shape.
    """
    n, _m = x.size()
    out = torch.empty_like(x)
    _BLOCK_SIZE_0 = 8
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (triton.cdiv(n, _BLOCK_SIZE_0),), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), n, _m, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=8, num_stages=8)
    return out
INFO:tritonbench.utils.triton_op:Took 0.12ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, n, _m, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < n
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    _mask_to = tl.where(mask_0[:, None] & mask_1[None, :], load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [_BLOCK_SIZE_0, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(mask_0[:, None] & mask_1[None, :], v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [_BLOCK_SIZE_0, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_0[:, None] & mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    """
    Simple Helion kernel wrapping PyTorch's softmax function.
    Args:
        x (torch.Tensor): Input tensor of shape [n, m].
    Returns:
        torch.Tensor: Softmax output tensor of the same shape.
    """
    n, _m = x.size()
    out = torch.empty_like(x)
    _BLOCK_SIZE_0 = 8
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (triton.cdiv(n, _BLOCK_SIZE_0),), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), n, _m, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=8, num_stages=8)
    return out
 20%|██        | 5/25 [00:03<00:10,  1.92it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.03ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.02ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.13ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, n, _m, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < n
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    _mask_to = tl.where(mask_0[:, None] & mask_1[None, :], load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [_BLOCK_SIZE_0, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(mask_0[:, None] & mask_1[None, :], v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [_BLOCK_SIZE_0, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_0[:, None] & mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    """
    Simple Helion kernel wrapping PyTorch's softmax function.
    Args:
        x (torch.Tensor): Input tensor of shape [n, m].
    Returns:
        torch.Tensor: Softmax output tensor of the same shape.
    """
    n, _m = x.size()
    out = torch.empty_like(x)
    _BLOCK_SIZE_0 = 8
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (triton.cdiv(n, _BLOCK_SIZE_0),), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), n, _m, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=8, num_stages=8)
    return out
INFO:tritonbench.utils.triton_op:Took 0.11ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, n, _m, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < n
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    _mask_to = tl.where(mask_0[:, None] & mask_1[None, :], load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [_BLOCK_SIZE_0, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(mask_0[:, None] & mask_1[None, :], v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [_BLOCK_SIZE_0, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_0[:, None] & mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    """
    Simple Helion kernel wrapping PyTorch's softmax function.
    Args:
        x (torch.Tensor): Input tensor of shape [n, m].
    Returns:
        torch.Tensor: Softmax output tensor of the same shape.
    """
    n, _m = x.size()
    out = torch.empty_like(x)
    _BLOCK_SIZE_0 = 8
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (triton.cdiv(n, _BLOCK_SIZE_0),), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), n, _m, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=8, num_stages=8)
    return out
 24%|██▍       | 6/25 [00:03<00:09,  2.01it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.03ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.02ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.11ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, n, _m, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < n
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    _mask_to = tl.where(mask_0[:, None] & mask_1[None, :], load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [_BLOCK_SIZE_0, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(mask_0[:, None] & mask_1[None, :], v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [_BLOCK_SIZE_0, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_0[:, None] & mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    """
    Simple Helion kernel wrapping PyTorch's softmax function.
    Args:
        x (torch.Tensor): Input tensor of shape [n, m].
    Returns:
        torch.Tensor: Softmax output tensor of the same shape.
    """
    n, _m = x.size()
    out = torch.empty_like(x)
    _BLOCK_SIZE_0 = 8
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (triton.cdiv(n, _BLOCK_SIZE_0),), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), n, _m, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=8, num_stages=8)
    return out
INFO:tritonbench.utils.triton_op:Took 0.11ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, n, _m, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < n
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    _mask_to = tl.where(mask_0[:, None] & mask_1[None, :], load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [_BLOCK_SIZE_0, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(mask_0[:, None] & mask_1[None, :], v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [_BLOCK_SIZE_0, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_0[:, None] & mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    """
    Simple Helion kernel wrapping PyTorch's softmax function.
    Args:
        x (torch.Tensor): Input tensor of shape [n, m].
    Returns:
        torch.Tensor: Softmax output tensor of the same shape.
    """
    n, _m = x.size()
    out = torch.empty_like(x)
    _BLOCK_SIZE_0 = 8
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (triton.cdiv(n, _BLOCK_SIZE_0),), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), n, _m, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=8, num_stages=8)
    return out
 28%|██▊       | 7/25 [00:03<00:08,  2.04it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.03ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.02ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.10ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, n, _m, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < n
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    _mask_to = tl.where(mask_0[:, None] & mask_1[None, :], load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [_BLOCK_SIZE_0, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(mask_0[:, None] & mask_1[None, :], v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [_BLOCK_SIZE_0, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_0[:, None] & mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    """
    Simple Helion kernel wrapping PyTorch's softmax function.
    Args:
        x (torch.Tensor): Input tensor of shape [n, m].
    Returns:
        torch.Tensor: Softmax output tensor of the same shape.
    """
    n, _m = x.size()
    out = torch.empty_like(x)
    _BLOCK_SIZE_0 = 8
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (triton.cdiv(n, _BLOCK_SIZE_0),), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), n, _m, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=8, num_stages=8)
    return out
INFO:tritonbench.utils.triton_op:Took 0.12ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, n, _m, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < n
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    _mask_to = tl.where(mask_0[:, None] & mask_1[None, :], load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [_BLOCK_SIZE_0, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(mask_0[:, None] & mask_1[None, :], v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [_BLOCK_SIZE_0, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_0[:, None] & mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    """
    Simple Helion kernel wrapping PyTorch's softmax function.
    Args:
        x (torch.Tensor): Input tensor of shape [n, m].
    Returns:
        torch.Tensor: Softmax output tensor of the same shape.
    """
    n, _m = x.size()
    out = torch.empty_like(x)
    _BLOCK_SIZE_0 = 8
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (triton.cdiv(n, _BLOCK_SIZE_0),), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), n, _m, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=8, num_stages=8)
    return out
 32%|███▏      | 8/25 [00:04<00:08,  2.06it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.03ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.02ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.12ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, n, _m, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < n
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    _mask_to = tl.where(mask_0[:, None] & mask_1[None, :], load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [_BLOCK_SIZE_0, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(mask_0[:, None] & mask_1[None, :], v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [_BLOCK_SIZE_0, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_0[:, None] & mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    """
    Simple Helion kernel wrapping PyTorch's softmax function.
    Args:
        x (torch.Tensor): Input tensor of shape [n, m].
    Returns:
        torch.Tensor: Softmax output tensor of the same shape.
    """
    n, _m = x.size()
    out = torch.empty_like(x)
    _BLOCK_SIZE_0 = 8
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (triton.cdiv(n, _BLOCK_SIZE_0),), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), n, _m, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=8, num_stages=8)
    return out
INFO:tritonbench.utils.triton_op:Took 0.12ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, n, _m, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < n
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    _mask_to = tl.where(mask_0[:, None] & mask_1[None, :], load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [_BLOCK_SIZE_0, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(mask_0[:, None] & mask_1[None, :], v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [_BLOCK_SIZE_0, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_0[:, None] & mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    """
    Simple Helion kernel wrapping PyTorch's softmax function.
    Args:
        x (torch.Tensor): Input tensor of shape [n, m].
    Returns:
        torch.Tensor: Softmax output tensor of the same shape.
    """
    n, _m = x.size()
    out = torch.empty_like(x)
    _BLOCK_SIZE_0 = 8
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (triton.cdiv(n, _BLOCK_SIZE_0),), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), n, _m, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=8, num_stages=8)
    return out
 36%|███▌      | 9/25 [00:04<00:07,  2.07it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.03ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.02ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.12ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, n, _m, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < n
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    _mask_to = tl.where(mask_0[:, None] & mask_1[None, :], load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [_BLOCK_SIZE_0, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(mask_0[:, None] & mask_1[None, :], v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [_BLOCK_SIZE_0, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_0[:, None] & mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    """
    Simple Helion kernel wrapping PyTorch's softmax function.
    Args:
        x (torch.Tensor): Input tensor of shape [n, m].
    Returns:
        torch.Tensor: Softmax output tensor of the same shape.
    """
    n, _m = x.size()
    out = torch.empty_like(x)
    _BLOCK_SIZE_0 = 8
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (triton.cdiv(n, _BLOCK_SIZE_0),), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), n, _m, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=8, num_stages=8)
    return out
INFO:tritonbench.utils.triton_op:Took 0.11ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, n, _m, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < n
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    _mask_to = tl.where(mask_0[:, None] & mask_1[None, :], load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [_BLOCK_SIZE_0, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(mask_0[:, None] & mask_1[None, :], v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [_BLOCK_SIZE_0, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_0[:, None] & mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    """
    Simple Helion kernel wrapping PyTorch's softmax function.
    Args:
        x (torch.Tensor): Input tensor of shape [n, m].
    Returns:
        torch.Tensor: Softmax output tensor of the same shape.
    """
    n, _m = x.size()
    out = torch.empty_like(x)
    _BLOCK_SIZE_0 = 8
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (triton.cdiv(n, _BLOCK_SIZE_0),), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), n, _m, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=8, num_stages=8)
    return out
 40%|████      | 10/25 [00:05<00:07,  2.09it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.03ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.02ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.11ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, n, _m, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < n
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    _mask_to = tl.where(mask_0[:, None] & mask_1[None, :], load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [_BLOCK_SIZE_0, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(mask_0[:, None] & mask_1[None, :], v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [_BLOCK_SIZE_0, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_0[:, None] & mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    """
    Simple Helion kernel wrapping PyTorch's softmax function.
    Args:
        x (torch.Tensor): Input tensor of shape [n, m].
    Returns:
        torch.Tensor: Softmax output tensor of the same shape.
    """
    n, _m = x.size()
    out = torch.empty_like(x)
    _BLOCK_SIZE_0 = 8
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (triton.cdiv(n, _BLOCK_SIZE_0),), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), n, _m, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=8, num_stages=8)
    return out
INFO:tritonbench.utils.triton_op:Took 0.11ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, n, _m, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < n
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    _mask_to = tl.where(mask_0[:, None] & mask_1[None, :], load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [_BLOCK_SIZE_0, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(mask_0[:, None] & mask_1[None, :], v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [_BLOCK_SIZE_0, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_0[:, None] & mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    """
    Simple Helion kernel wrapping PyTorch's softmax function.
    Args:
        x (torch.Tensor): Input tensor of shape [n, m].
    Returns:
        torch.Tensor: Softmax output tensor of the same shape.
    """
    n, _m = x.size()
    out = torch.empty_like(x)
    _BLOCK_SIZE_0 = 8
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (triton.cdiv(n, _BLOCK_SIZE_0),), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), n, _m, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=8, num_stages=8)
    return out
 44%|████▍     | 11/25 [00:05<00:06,  2.10it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.03ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.02ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.13ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, n, _m, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < n
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    _mask_to = tl.where(mask_0[:, None] & mask_1[None, :], load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [_BLOCK_SIZE_0, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(mask_0[:, None] & mask_1[None, :], v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [_BLOCK_SIZE_0, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_0[:, None] & mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    """
    Simple Helion kernel wrapping PyTorch's softmax function.
    Args:
        x (torch.Tensor): Input tensor of shape [n, m].
    Returns:
        torch.Tensor: Softmax output tensor of the same shape.
    """
    n, _m = x.size()
    out = torch.empty_like(x)
    _BLOCK_SIZE_0 = 8
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (triton.cdiv(n, _BLOCK_SIZE_0),), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), n, _m, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=8, num_stages=8)
    return out
INFO:tritonbench.utils.triton_op:Took 0.11ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, n, _m, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < n
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    _mask_to = tl.where(mask_0[:, None] & mask_1[None, :], load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [_BLOCK_SIZE_0, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(mask_0[:, None] & mask_1[None, :], v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [_BLOCK_SIZE_0, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_0[:, None] & mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    """
    Simple Helion kernel wrapping PyTorch's softmax function.
    Args:
        x (torch.Tensor): Input tensor of shape [n, m].
    Returns:
        torch.Tensor: Softmax output tensor of the same shape.
    """
    n, _m = x.size()
    out = torch.empty_like(x)
    _BLOCK_SIZE_0 = 8
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (triton.cdiv(n, _BLOCK_SIZE_0),), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), n, _m, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=8, num_stages=8)
    return out
 48%|████▊     | 12/25 [00:06<00:06,  2.10it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.03ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.02ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.11ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, n, _m, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < n
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    _mask_to = tl.where(mask_0[:, None] & mask_1[None, :], load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [_BLOCK_SIZE_0, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(mask_0[:, None] & mask_1[None, :], v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [_BLOCK_SIZE_0, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_0[:, None] & mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    """
    Simple Helion kernel wrapping PyTorch's softmax function.
    Args:
        x (torch.Tensor): Input tensor of shape [n, m].
    Returns:
        torch.Tensor: Softmax output tensor of the same shape.
    """
    n, _m = x.size()
    out = torch.empty_like(x)
    _BLOCK_SIZE_0 = 8
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (triton.cdiv(n, _BLOCK_SIZE_0),), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), n, _m, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=8, num_stages=8)
    return out
INFO:tritonbench.utils.triton_op:Took 0.11ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, n, _m, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < n
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    _mask_to = tl.where(mask_0[:, None] & mask_1[None, :], load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [_BLOCK_SIZE_0, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(mask_0[:, None] & mask_1[None, :], v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [_BLOCK_SIZE_0, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_0[:, None] & mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    """
    Simple Helion kernel wrapping PyTorch's softmax function.
    Args:
        x (torch.Tensor): Input tensor of shape [n, m].
    Returns:
        torch.Tensor: Softmax output tensor of the same shape.
    """
    n, _m = x.size()
    out = torch.empty_like(x)
    _BLOCK_SIZE_0 = 8
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (triton.cdiv(n, _BLOCK_SIZE_0),), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), n, _m, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=8, num_stages=8)
    return out
 52%|█████▏    | 13/25 [00:06<00:05,  2.09it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.03ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.02ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.11ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, n, _m, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < n
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    _mask_to = tl.where(mask_0[:, None] & mask_1[None, :], load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [_BLOCK_SIZE_0, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(mask_0[:, None] & mask_1[None, :], v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [_BLOCK_SIZE_0, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_0[:, None] & mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    """
    Simple Helion kernel wrapping PyTorch's softmax function.
    Args:
        x (torch.Tensor): Input tensor of shape [n, m].
    Returns:
        torch.Tensor: Softmax output tensor of the same shape.
    """
    n, _m = x.size()
    out = torch.empty_like(x)
    _BLOCK_SIZE_0 = 8
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (triton.cdiv(n, _BLOCK_SIZE_0),), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), n, _m, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=8, num_stages=8)
    return out
INFO:tritonbench.utils.triton_op:Took 0.11ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, n, _m, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < n
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    _mask_to = tl.where(mask_0[:, None] & mask_1[None, :], load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [_BLOCK_SIZE_0, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(mask_0[:, None] & mask_1[None, :], v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [_BLOCK_SIZE_0, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_0[:, None] & mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    """
    Simple Helion kernel wrapping PyTorch's softmax function.
    Args:
        x (torch.Tensor): Input tensor of shape [n, m].
    Returns:
        torch.Tensor: Softmax output tensor of the same shape.
    """
    n, _m = x.size()
    out = torch.empty_like(x)
    _BLOCK_SIZE_0 = 8
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (triton.cdiv(n, _BLOCK_SIZE_0),), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), n, _m, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=8, num_stages=8)
    return out
 56%|█████▌    | 14/25 [00:07<00:05,  2.10it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.03ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.02ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.11ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, n, _m, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < n
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    _mask_to = tl.where(mask_0[:, None] & mask_1[None, :], load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [_BLOCK_SIZE_0, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(mask_0[:, None] & mask_1[None, :], v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [_BLOCK_SIZE_0, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_0[:, None] & mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    """
    Simple Helion kernel wrapping PyTorch's softmax function.
    Args:
        x (torch.Tensor): Input tensor of shape [n, m].
    Returns:
        torch.Tensor: Softmax output tensor of the same shape.
    """
    n, _m = x.size()
    out = torch.empty_like(x)
    _BLOCK_SIZE_0 = 8
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (triton.cdiv(n, _BLOCK_SIZE_0),), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), n, _m, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=8, num_stages=8)
    return out
INFO:tritonbench.utils.triton_op:Took 0.10ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, n, _m, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < n
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    _mask_to = tl.where(mask_0[:, None] & mask_1[None, :], load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [_BLOCK_SIZE_0, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(mask_0[:, None] & mask_1[None, :], v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [_BLOCK_SIZE_0, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_0[:, None] & mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    """
    Simple Helion kernel wrapping PyTorch's softmax function.
    Args:
        x (torch.Tensor): Input tensor of shape [n, m].
    Returns:
        torch.Tensor: Softmax output tensor of the same shape.
    """
    n, _m = x.size()
    out = torch.empty_like(x)
    _BLOCK_SIZE_0 = 8
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (triton.cdiv(n, _BLOCK_SIZE_0),), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), n, _m, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=8, num_stages=8)
    return out
 60%|██████    | 15/25 [00:07<00:04,  2.10it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.03ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.02ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.12ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, n, _m, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < n
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    _mask_to = tl.where(mask_0[:, None] & mask_1[None, :], load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [_BLOCK_SIZE_0, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(mask_0[:, None] & mask_1[None, :], v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [_BLOCK_SIZE_0, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_0[:, None] & mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    """
    Simple Helion kernel wrapping PyTorch's softmax function.
    Args:
        x (torch.Tensor): Input tensor of shape [n, m].
    Returns:
        torch.Tensor: Softmax output tensor of the same shape.
    """
    n, _m = x.size()
    out = torch.empty_like(x)
    _BLOCK_SIZE_0 = 8
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (triton.cdiv(n, _BLOCK_SIZE_0),), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), n, _m, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=8, num_stages=8)
    return out
INFO:tritonbench.utils.triton_op:Took 0.11ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, n, _m, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < n
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    _mask_to = tl.where(mask_0[:, None] & mask_1[None, :], load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [_BLOCK_SIZE_0, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(mask_0[:, None] & mask_1[None, :], v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [_BLOCK_SIZE_0, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_0[:, None] & mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    """
    Simple Helion kernel wrapping PyTorch's softmax function.
    Args:
        x (torch.Tensor): Input tensor of shape [n, m].
    Returns:
        torch.Tensor: Softmax output tensor of the same shape.
    """
    n, _m = x.size()
    out = torch.empty_like(x)
    _BLOCK_SIZE_0 = 8
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (triton.cdiv(n, _BLOCK_SIZE_0),), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), n, _m, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=8, num_stages=8)
    return out
 64%|██████▍   | 16/25 [00:08<00:04,  1.89it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.03ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.02ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.12ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, n, _m, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < n
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    _mask_to = tl.where(mask_0[:, None] & mask_1[None, :], load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [_BLOCK_SIZE_0, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(mask_0[:, None] & mask_1[None, :], v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [_BLOCK_SIZE_0, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_0[:, None] & mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    """
    Simple Helion kernel wrapping PyTorch's softmax function.
    Args:
        x (torch.Tensor): Input tensor of shape [n, m].
    Returns:
        torch.Tensor: Softmax output tensor of the same shape.
    """
    n, _m = x.size()
    out = torch.empty_like(x)
    _BLOCK_SIZE_0 = 8
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (triton.cdiv(n, _BLOCK_SIZE_0),), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), n, _m, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=8, num_stages=8)
    return out
INFO:tritonbench.utils.triton_op:Took 0.11ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, n, _m, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < n
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    _mask_to = tl.where(mask_0[:, None] & mask_1[None, :], load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [_BLOCK_SIZE_0, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(mask_0[:, None] & mask_1[None, :], v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [_BLOCK_SIZE_0, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_0[:, None] & mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    """
    Simple Helion kernel wrapping PyTorch's softmax function.
    Args:
        x (torch.Tensor): Input tensor of shape [n, m].
    Returns:
        torch.Tensor: Softmax output tensor of the same shape.
    """
    n, _m = x.size()
    out = torch.empty_like(x)
    _BLOCK_SIZE_0 = 8
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (triton.cdiv(n, _BLOCK_SIZE_0),), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), n, _m, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=8, num_stages=8)
    return out
 68%|██████▊   | 17/25 [00:08<00:04,  1.95it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.02ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.02ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.11ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, n, _m, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < n
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    _mask_to = tl.where(mask_0[:, None] & mask_1[None, :], load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [_BLOCK_SIZE_0, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(mask_0[:, None] & mask_1[None, :], v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [_BLOCK_SIZE_0, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_0[:, None] & mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    """
    Simple Helion kernel wrapping PyTorch's softmax function.
    Args:
        x (torch.Tensor): Input tensor of shape [n, m].
    Returns:
        torch.Tensor: Softmax output tensor of the same shape.
    """
    n, _m = x.size()
    out = torch.empty_like(x)
    _BLOCK_SIZE_0 = 8
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (triton.cdiv(n, _BLOCK_SIZE_0),), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), n, _m, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=8, num_stages=8)
    return out
INFO:tritonbench.utils.triton_op:Took 0.13ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, n, _m, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < n
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    _mask_to = tl.where(mask_0[:, None] & mask_1[None, :], load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [_BLOCK_SIZE_0, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(mask_0[:, None] & mask_1[None, :], v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [_BLOCK_SIZE_0, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_0[:, None] & mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    """
    Simple Helion kernel wrapping PyTorch's softmax function.
    Args:
        x (torch.Tensor): Input tensor of shape [n, m].
    Returns:
        torch.Tensor: Softmax output tensor of the same shape.
    """
    n, _m = x.size()
    out = torch.empty_like(x)
    _BLOCK_SIZE_0 = 8
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (triton.cdiv(n, _BLOCK_SIZE_0),), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), n, _m, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=8, num_stages=8)
    return out
 72%|███████▏  | 18/25 [00:09<00:03,  1.99it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.03ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.02ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.14ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, n, _m, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < n
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    _mask_to = tl.where(mask_0[:, None] & mask_1[None, :], load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [_BLOCK_SIZE_0, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(mask_0[:, None] & mask_1[None, :], v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [_BLOCK_SIZE_0, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_0[:, None] & mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    """
    Simple Helion kernel wrapping PyTorch's softmax function.
    Args:
        x (torch.Tensor): Input tensor of shape [n, m].
    Returns:
        torch.Tensor: Softmax output tensor of the same shape.
    """
    n, _m = x.size()
    out = torch.empty_like(x)
    _BLOCK_SIZE_0 = 8
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (triton.cdiv(n, _BLOCK_SIZE_0),), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), n, _m, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=8, num_stages=8)
    return out
INFO:tritonbench.utils.triton_op:Took 0.13ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, n, _m, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < n
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    _mask_to = tl.where(mask_0[:, None] & mask_1[None, :], load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [_BLOCK_SIZE_0, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(mask_0[:, None] & mask_1[None, :], v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [_BLOCK_SIZE_0, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_0[:, None] & mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    """
    Simple Helion kernel wrapping PyTorch's softmax function.
    Args:
        x (torch.Tensor): Input tensor of shape [n, m].
    Returns:
        torch.Tensor: Softmax output tensor of the same shape.
    """
    n, _m = x.size()
    out = torch.empty_like(x)
    _BLOCK_SIZE_0 = 8
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (triton.cdiv(n, _BLOCK_SIZE_0),), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), n, _m, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=8, num_stages=8)
    return out
 76%|███████▌  | 19/25 [00:09<00:02,  2.02it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.03ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.02ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.14ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, n, _m, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < n
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    _mask_to = tl.where(mask_0[:, None] & mask_1[None, :], load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [_BLOCK_SIZE_0, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(mask_0[:, None] & mask_1[None, :], v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [_BLOCK_SIZE_0, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_0[:, None] & mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    """
    Simple Helion kernel wrapping PyTorch's softmax function.
    Args:
        x (torch.Tensor): Input tensor of shape [n, m].
    Returns:
        torch.Tensor: Softmax output tensor of the same shape.
    """
    n, _m = x.size()
    out = torch.empty_like(x)
    _BLOCK_SIZE_0 = 8
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (triton.cdiv(n, _BLOCK_SIZE_0),), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), n, _m, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=8, num_stages=8)
    return out
INFO:tritonbench.utils.triton_op:Took 0.16ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, n, _m, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < n
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    _mask_to = tl.where(mask_0[:, None] & mask_1[None, :], load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [_BLOCK_SIZE_0, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(mask_0[:, None] & mask_1[None, :], v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [_BLOCK_SIZE_0, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_0[:, None] & mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    """
    Simple Helion kernel wrapping PyTorch's softmax function.
    Args:
        x (torch.Tensor): Input tensor of shape [n, m].
    Returns:
        torch.Tensor: Softmax output tensor of the same shape.
    """
    n, _m = x.size()
    out = torch.empty_like(x)
    _BLOCK_SIZE_0 = 8
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (triton.cdiv(n, _BLOCK_SIZE_0),), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), n, _m, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=8, num_stages=8)
    return out
 80%|████████  | 20/25 [00:10<00:02,  2.03it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.03ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.03ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.14ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, n, _m, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < n
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    _mask_to = tl.where(mask_0[:, None] & mask_1[None, :], load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [_BLOCK_SIZE_0, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(mask_0[:, None] & mask_1[None, :], v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [_BLOCK_SIZE_0, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_0[:, None] & mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    """
    Simple Helion kernel wrapping PyTorch's softmax function.
    Args:
        x (torch.Tensor): Input tensor of shape [n, m].
    Returns:
        torch.Tensor: Softmax output tensor of the same shape.
    """
    n, _m = x.size()
    out = torch.empty_like(x)
    _BLOCK_SIZE_0 = 8
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (triton.cdiv(n, _BLOCK_SIZE_0),), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), n, _m, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=8, num_stages=8)
    return out
INFO:tritonbench.utils.triton_op:Took 0.13ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, n, _m, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < n
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    _mask_to = tl.where(mask_0[:, None] & mask_1[None, :], load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [_BLOCK_SIZE_0, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(mask_0[:, None] & mask_1[None, :], v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [_BLOCK_SIZE_0, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_0[:, None] & mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    """
    Simple Helion kernel wrapping PyTorch's softmax function.
    Args:
        x (torch.Tensor): Input tensor of shape [n, m].
    Returns:
        torch.Tensor: Softmax output tensor of the same shape.
    """
    n, _m = x.size()
    out = torch.empty_like(x)
    _BLOCK_SIZE_0 = 8
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (triton.cdiv(n, _BLOCK_SIZE_0),), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), n, _m, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=8, num_stages=8)
    return out
 84%|████████▍ | 21/25 [00:10<00:01,  2.04it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.03ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.02ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.13ms to get benchmark function for helion_softmax
 84%|████████▍ | 21/25 [00:11<00:02,  1.90it/s]
WARNING:tritonbench.utils.triton_op:Caught exception, terminating early with partial results
Traceback (most recent call last):
  File "/data/users/willfeng/helion/benchmarks/tritonbench/tritonbench/utils/triton_op.py", line 913, in run
    y_vals: Dict[str, BenchmarkOperatorMetrics] = functools.reduce(
                                                  ^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/benchmarks/tritonbench/tritonbench/utils/triton_op.py", line 901, in _reduce_benchmarks
    acc[bm_name] = self._do_bench(
                   ^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/benchmarks/tritonbench/tritonbench/utils/triton_op.py", line 1203, in _do_bench
    metrics.latency = do_bench_wrapper(
                      ^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/benchmarks/tritonbench/tritonbench/components/do_bench/run.py", line 202, in do_bench_wrapper
    times=triton.testing.do_bench(
          ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/miniconda3/lib/python3.12/site-packages/triton/testing.py", line 149, in do_bench
    fn()
  File "/data/users/willfeng/helion/benchmarks/run.py", line 441, in _inner
    result = kfunc(*args)
             ^^^^^^^^^^^^
  File "/data/users/willfeng/helion/helion/runtime/kernel.py", line 272, in __call__
    return self.bind(args)(*args)
           ^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/helion/runtime/kernel.py", line 158, in bind
    bound_kernel = BoundKernel(self, args)
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/helion/runtime/kernel.py", line 338, in __init__
    self.host_function: HostFunction = HostFunction(
                                       ^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/helion/_compiler/host_function.py", line 108, in __init__
    propagate_types(self, fake_args)
  File "/data/users/willfeng/helion/helion/_compiler/type_propagation.py", line 2170, in propagate_types
    prop.visit(stmt)
  File "/data/users/willfeng/helion/helion/_compiler/type_propagation.py", line 1499, in visit
    type_info = visitor(node)
                ^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/helion/_compiler/type_propagation.py", line 2005, in visit_For
    body = self._loop_body(node.body)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/helion/_compiler/type_propagation.py", line 1969, in _loop_body
    self.visit(stmt)
  File "/data/users/willfeng/helion/helion/_compiler/type_propagation.py", line 1499, in visit
    type_info = visitor(node)
                ^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/helion/_compiler/type_propagation.py", line 1893, in visit_Assign
    type_info = self.visit(node.value)
                ^^^^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/helion/_compiler/type_propagation.py", line 1499, in visit
    type_info = visitor(node)
                ^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/helion/_compiler/type_propagation.py", line 1820, in visit_Call
    args.append(self.visit(arg))
                ^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/helion/_compiler/type_propagation.py", line 1499, in visit
    type_info = visitor(node)
                ^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/helion/_compiler/type_propagation.py", line 1864, in visit_Subscript
    return value_type.propagate_getitem(slice_type, self.origin())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/helion/_compiler/type_propagation.py", line 501, in propagate_getitem
    origin, self.fake_value.new_empty(self._device_indexing_size(key))
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/miniconda3/lib/python3.12/site-packages/torch/utils/_stats.py", line 28, in wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/miniconda3/lib/python3.12/site-packages/torch/_subclasses/fake_tensor.py", line 1361, in __torch_dispatch__
    return self.dispatch(func, types, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/miniconda3/lib/python3.12/site-packages/torch/_subclasses/fake_tensor.py", line 2077, in dispatch
    return self._cached_dispatch_impl(func, types, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/miniconda3/lib/python3.12/site-packages/torch/_subclasses/fake_tensor.py", line 1496, in _cached_dispatch_impl
    output = self._dispatch_impl(func, types, args, kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/miniconda3/lib/python3.12/site-packages/torch/_subclasses/fake_tensor.py", line 2620, in _dispatch_impl
    decomposition_table[func](*args, **kwargs)
  File "/home/willfeng/local/miniconda3/lib/python3.12/site-packages/torch/_prims_common/wrappers.py", line 309, in _fn
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/miniconda3/lib/python3.12/site-packages/torch/_refs/__init__.py", line 4855, in new_empty
    return torch.empty(
           ^^^^^^^^^^^^
  File "/home/willfeng/local/miniconda3/lib/python3.12/site-packages/torch/utils/_stats.py", line 28, in wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/miniconda3/lib/python3.12/site-packages/torch/_subclasses/fake_tensor.py", line 1361, in __torch_dispatch__
    return self.dispatch(func, types, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/miniconda3/lib/python3.12/site-packages/torch/_subclasses/fake_tensor.py", line 2077, in dispatch
    return self._cached_dispatch_impl(func, types, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/miniconda3/lib/python3.12/site-packages/torch/_subclasses/fake_tensor.py", line 1496, in _cached_dispatch_impl
    output = self._dispatch_impl(func, types, args, kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/miniconda3/lib/python3.12/site-packages/torch/_subclasses/fake_tensor.py", line 2706, in _dispatch_impl
    op_impl_out = op_impl(self, func, *args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/miniconda3/lib/python3.12/site-packages/torch/_subclasses/fake_impls.py", line 198, in constructors
    r = func(*args, **new_kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/miniconda3/lib/python3.12/site-packages/torch/_ops.py", line 840, in __call__
    return self._op(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/miniconda3/lib/python3.12/site-packages/torch/fx/experimental/sym_node.py", line 568, in expect_size
    r = b.expect_true(file, line)
        ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/miniconda3/lib/python3.12/site-packages/torch/fx/experimental/sym_node.py", line 559, in expect_true
    return self.shape_env.guard_or_defer_runtime_assert(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/miniconda3/lib/python3.12/site-packages/torch/fx/experimental/recording.py", line 272, in wrapper
    return retlog(fn(*args, **kwargs))
                  ^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/miniconda3/lib/python3.12/site-packages/torch/fx/experimental/symbolic_shapes.py", line 7724, in guard_or_defer_runtime_assert
    return self.evaluate_expr(new_expr, fx_node=fx_node)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/miniconda3/lib/python3.12/site-packages/torch/fx/experimental/symbolic_shapes.py", line 7339, in evaluate_expr
    return self._inner_evaluate_expr(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/miniconda3/lib/python3.12/site-packages/torch/fx/experimental/recording.py", line 272, in wrapper
    return retlog(fn(*args, **kwargs))
                  ^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/miniconda3/lib/python3.12/site-packages/torch/fx/experimental/symbolic_shapes.py", line 7362, in _inner_evaluate_expr
    return self._evaluate_expr(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/miniconda3/lib/python3.12/site-packages/torch/fx/experimental/symbolic_shapes.py", line 7641, in _evaluate_expr
    self.axioms.update(dict(self.get_implications(self.simplify(g))))
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/miniconda3/lib/python3.12/site-packages/torch/fx/experimental/symbolic_shapes.py", line 6226, in get_implications
    add_expr(sympy.Lt(e.lhs, e.rhs + 1, evaluate=False))
  File "/home/willfeng/local/miniconda3/lib/python3.12/site-packages/torch/fx/experimental/symbolic_shapes.py", line 6197, in add_expr
    expr = canonicalize_bool_expr(expr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/miniconda3/lib/python3.12/site-packages/torch/fx/experimental/symbolic_shapes.py", line 709, in canonicalize_bool_expr
    return _canonicalize_bool_expr_impl(expr)  # type: ignore[arg-type, return-value]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/miniconda3/lib/python3.12/site-packages/torch/fx/experimental/symbolic_shapes.py", line 790, in _canonicalize_bool_expr_impl
    rhs = _reduce_to_lowest_terms(rhs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/miniconda3/lib/python3.12/site-packages/torch/fx/experimental/symbolic_shapes.py", line 844, in _reduce_to_lowest_terms
    atoms = cast(Sequence[sympy.Expr], expr.args)
                                       ^^^^^^^^^
  File "/home/willfeng/local/miniconda3/lib/python3.12/site-packages/sympy/core/basic.py", line 869, in args
    @property
    
KeyboardInterrupt
