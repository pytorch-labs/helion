Running 6 kernels...


============================================================
Kernel: jagged_mean
============================================================

Running jagged_mean benchmark with Helion implementation...

  0%|          | 0/3616 [00:00<?, ?it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.02ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 33.92ms to get benchmark function for torch_compile_nested_tensor_integration
/home/willfeng/local/pytorch-nightly/torch/backends/cuda/__init__.py:131: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:78.)
  return torch._C._get_cublas_allow_tf32()
INFO:tritonbench.utils.triton_op:Took 0.05ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out
  0%|          | 1/3616 [00:06<6:58:54,  6.95s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.32ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.58ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out
  0%|          | 2/3616 [00:08<3:37:12,  3.61s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.28ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.60ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out
  0%|          | 3/3616 [00:09<2:32:44,  2.54s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.29ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.62ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out
  0%|          | 4/3616 [00:10<2:02:11,  2.03s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.30ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.63ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out
  0%|          | 5/3616 [00:11<1:45:09,  1.75s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.31ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.64ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out
  0%|          | 6/3616 [00:13<1:35:15,  1.58s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.29ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.61ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out
  0%|          | 7/3616 [00:14<1:28:47,  1.48s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.32ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.62ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out
  0%|          | 8/3616 [00:15<1:24:41,  1.41s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.28ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.63ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out
  0%|          | 9/3616 [00:17<1:36:05,  1.60s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.34ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.65ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out
  0%|          | 10/3616 [00:19<1:29:29,  1.49s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.30ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.63ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out
  0%|          | 11/3616 [00:20<1:25:03,  1.42s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.32ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.62ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out
  0%|          | 12/3616 [00:21<1:21:55,  1.36s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.29ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.62ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out
  0%|          | 13/3616 [00:22<1:19:56,  1.33s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.28ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.62ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out
  0%|          | 14/3616 [00:24<1:18:24,  1.31s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.30ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.64ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out
  0%|          | 15/3616 [00:25<1:17:27,  1.29s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.30ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.61ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out
  0%|          | 16/3616 [00:26<1:16:50,  1.28s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.29ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.61ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out
  0%|          | 17/3616 [00:27<1:16:19,  1.27s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.29ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.60ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out
  0%|          | 18/3616 [00:29<1:15:54,  1.27s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.30ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.64ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out
  1%|          | 19/3616 [00:30<1:15:32,  1.26s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.29ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.64ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out
  1%|          | 20/3616 [00:31<1:15:36,  1.26s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.02ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.32ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.62ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out
  1%|          | 21/3616 [00:32<1:16:43,  1.28s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.31ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.61ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out
  1%|          | 22/3616 [00:34<1:16:05,  1.27s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.30ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.64ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out
  1%|          | 23/3616 [00:35<1:15:38,  1.26s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.31ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.65ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out
  1%|          | 24/3616 [00:37<1:22:21,  1.38s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.30ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.60ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out
  1%|          | 25/3616 [00:38<1:20:08,  1.34s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.33ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.66ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out
  1%|          | 26/3616 [00:39<1:18:26,  1.31s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.41ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.68ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out
  1%|          | 27/3616 [00:40<1:17:11,  1.29s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.36ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.64ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out
  1%|          | 28/3616 [00:42<1:16:30,  1.28s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.33ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.67ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out
  1%|          | 29/3616 [00:43<1:15:59,  1.27s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.32ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.65ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out
  1%|          | 30/3616 [00:44<1:15:37,  1.27s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.31ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.68ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out
  1%|          | 31/3616 [01:40<17:29:58, 17.57s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.39ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.68ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out
  1%|          | 32/3616 [01:41<12:37:44, 12.69s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.02ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.31ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.67ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out
  1%|          | 33/3616 [01:42<9:13:17,  9.27s/it] INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.34ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.67ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out
Removed 21 outliers from 380 samples
Removed 16 outliers from 473 samples
Removed 12 outliers from 731 samples
Removed 15 outliers from 433 samples
Removed 14 outliers from 443 samples
Removed 8 outliers from 330 samples
Removed 21 outliers from 776 samples
Removed 12 outliers from 439 samples
Removed 11 outliers from 666 samples
Removed 17 outliers from 750 samples
Removed 25 outliers from 661 samples
Removed 23 outliers from 684 samples
Removed 12 outliers from 397 samples
Removed 8 outliers from 783 samples
Removed 32 outliers from 454 samples
Removed 21 outliers from 669 samples
Removed 18 outliers from 743 samples
Removed 35 outliers from 626 samples
Removed 40 outliers from 683 samples
Removed 10 outliers from 387 samples
Removed 18 outliers from 783 samples
Removed 26 outliers from 445 samples
Removed 20 outliers from 668 samples
Removed 24 outliers from 749 samples
Removed 32 outliers from 615 samples
Removed 16 outliers from 669 samples
Removed 11 outliers from 401 samples
Removed 9 outliers from 790 samples
Removed 25 outliers from 434 samples
Removed 25 outliers from 654 samples
Removed 34 outliers from 748 samples
Removed 20 outliers from 629 samples
Removed 29 outliers from 634 samples
Removed 13 outliers from 399 samples
Removed 6 outliers from 796 samples
Removed 22 outliers from 437 samples
Removed 28 outliers from 668 samples
Removed 27 outliers from 765 samples
Removed 13 outliers from 688 samples
Removed 7 outliers from 386 samples
Removed 14 outliers from 709 samples
Removed 36 outliers from 439 samples
Removed 13 outliers from 683 samples
Removed 22 outliers from 766 samples
Removed 2 outliers from 621 samples
Removed 3 outliers from 404 samples
Removed 29 outliers from 436 samples
Removed 2 outliers from 679 samples
Removed 29 outliers from 757 samples
Removed 2 outliers from 654 samples
Removed 3 outliers from 718 samples
Removed 6 outliers from 398 samples
Removed 9 outliers from 727 samples
Removed 31 outliers from 439 samples
Removed 6 outliers from 680 samples
Removed 27 outliers from 766 samples
Removed 3 outliers from 627 samples
Removed 3 outliers from 680 samples
Removed 3 outliers from 310 samples
Removed 1 outliers from 767 samples
Removed 37 outliers from 409 samples
Removed 15 outliers from 679 samples
Removed 18 outliers from 764 samples
Removed 2 outliers from 616 samples
Removed 2 outliers from 683 samples
Removed 4 outliers from 389 samples
Removed 29 outliers from 439 samples
Removed 20 outliers from 670 samples
Removed 20 outliers from 751 samples
Removed 4 outliers from 605 samples
Removed 4 outliers from 672 samples
Removed 5 outliers from 393 samples
Removed 7 outliers from 653 samples
Removed 7 outliers from 438 samples
Removed 12 outliers from 673 samples
Removed 20 outliers from 742 samples
Removed 4 outliers from 599 samples
Removed 6 outliers from 394 samples
Removed 5 outliers from 627 samples
Removed 25 outliers from 443 samples
Removed 6 outliers from 673 samples
Removed 26 outliers from 749 samples
Removed 3 outliers from 627 samples
Removed 3 outliers from 678 samples
Removed 4 outliers from 401 samples
Removed 14 outliers from 670 samples
Removed 41 outliers from 408 samples
Removed 8 outliers from 673 samples
Removed 33 outliers from 758 samples
Removed 3 outliers from 622 samples
Removed 3 outliers from 683 samples
Removed 4 outliers from 389 samples
Removed 4 outliers from 757 samples
Removed 26 outliers from 440 samples
Removed 4 outliers from 669 samples
Removed 20 outliers from 752 samples
Removed 2 outliers from 627 samples
Removed 2 outliers from 674 samples
Removed 3 outliers from 403 samples
Removed 12 outliers from 741 samples
Removed 18 outliers from 448 samples
Removed 12 outliers from 629 samples
Removed 20 outliers from 730 samples
Removed 6 outliers from 631 samples
Removed 2 outliers from 686 samples
Removed 3 outliers from 398 samples
Removed 21 outliers from 434 samples
Removed 12 outliers from 627 samples
Removed 14 outliers from 727 samples
Removed 2 outliers from 620 samples
Removed 5 outliers from 689 samples
Removed 6 outliers from 401 samples
Removed 18 outliers from 434 samples
Removed 2 outliers from 628 samples
Removed 20 outliers from 730 samples
Removed 2 outliers from 620 samples
Removed 7 outliers from 401 samples
Removed 9 outliers from 499 samples
Removed 24 outliers from 436 samples
Removed 8 outliers from 627 samples
Removed 16 outliers from 734 samples
Removed 2 outliers from 676 samples
Removed 5 outliers from 396 samples
Removed 8 outliers from 584 samples
Removed 21 outliers from 440 samples
Removed 9 outliers from 628 samples
Removed 20 outliers from 730 samples
Removed 4 outliers from 643 samples
Removed 1 outliers from 698 samples
Removed 4 outliers from 402 samples
Removed 4 outliers from 606 samples
Removed 19 outliers from 434 samples
Removed 24 outliers from 634 samples
Removed 8 outliers from 740 samples
Removed 1 outliers from 412 samples
Removed 6 outliers from 630 samples
Removed 6 outliers from 387 samples
Removed 2 outliers from 206 samples
Removed 8 outliers from 440 samples
Removed 8 outliers from 640 samples
Removed 12 outliers from 740 samples
Removed 6 outliers from 552 samples
Removed 13 outliers from 635 samples
Removed 4 outliers from 395 samples
Removed 39 outliers from 441 samples
Removed 4 outliers from 639 samples
Removed 17 outliers from 725 samples
Removed 2 outliers from 572 samples
Removed 14 outliers from 641 samples
Removed 5 outliers from 385 samples
Removed 4 outliers from 254 samples
Removed 17 outliers from 445 samples
Removed 12 outliers from 636 samples
Removed 9 outliers from 740 samples
Removed 10 outliers from 602 samples
Removed 5 outliers from 655 samples
Removed 7 outliers from 391 samples
Removed 40 outliers from 435 samples
Removed 4 outliers from 638 samples
Removed 19 outliers from 728 samples
Removed 30 outliers from 588 samples
Removed 5 outliers from 718 samples
Removed 5 outliers from 402 samples
Removed 4 outliers from 393 samples
Removed 66 outliers from 398 samples
Removed 16 outliers from 626 samples
Removed 14 outliers from 723 samples
Removed 4 outliers from 478 samples
Removed 3 outliers from 541 samples
Removed 5 outliers from 377 samples
Removed 1 outliers from 132 samples
Removed 51 outliers from 399 samples
Removed 12 outliers from 634 samples
Removed 17 outliers from 718 samples
Removed 4 outliers from 481 samples
Removed 4 outliers from 534 samples
Removed 4 outliers from 373 samples
Removed 4 outliers from 130 samples
Removed 8 outliers from 426 samples
Removed 4 outliers from 638 samples
Removed 11 outliers from 739 samples
Removed 1 outliers from 509 samples
Removed 7 outliers from 568 samples
Removed 5 outliers from 382 samples
Removed 35 outliers from 436 samples
Removed 10 outliers from 639 samples
Removed 9 outliers from 739 samples
Removed 10 outliers from 627 samples
Removed 4 outliers from 386 samples
Removed 6 outliers from 209 samples
Removed 45 outliers from 435 samples
Removed 33 outliers from 630 samples
Removed 12 outliers from 736 samples
Removed 1 outliers from 564 samples
Removed 33 outliers from 656 samples
Removed 4 outliers from 387 samples
Removed 17 outliers from 267 samples
Removed 31 outliers from 431 samples
Removed 16 outliers from 682 samples
Removed 30 outliers from 758 samples
Removed 12 outliers from 438 samples
Removed 15 outliers from 442 samples
Removed 2 outliers from 295 samples
Removed 29 outliers from 633 samples
Removed 33 outliers from 431 samples
Removed 14 outliers from 682 samples
Removed 18 outliers from 759 samples
Removed 20 outliers from 650 samples
Removed 16 outliers from 683 samples
Removed 4 outliers from 367 samples
Removed 9 outliers from 705 samples
Removed 23 outliers from 443 samples
Removed 19 outliers from 682 samples
Removed 44 outliers from 760 samples
Removed 12 outliers from 628 samples
Removed 18 outliers from 713 samples
Removed 4 outliers from 358 samples
Removed 25 outliers from 740 samples
Removed 27 outliers from 442 samples
Removed 12 outliers from 682 samples
Removed 16 outliers from 764 samples
Removed 9 outliers from 613 samples
Removed 17 outliers from 683 samples
Removed 7 outliers from 355 samples
  1%|          | 34/3616 [01:43<6:50:09,  6.87s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.32ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.62ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out
  1%|          | 35/3616 [01:45<5:17:27,  5.32s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.33ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.68ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out
  1%|          | 36/3616 [01:46<4:05:01,  4.11s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.34ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.72ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out
  1%|          | 37/3616 [01:48<3:14:25,  3.26s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.34ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.72ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out
  1%|          | 38/3616 [01:49<2:38:56,  2.67s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.32ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.72ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out
  1%|          | 39/3616 [01:50<2:14:06,  2.25s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.31ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.68ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out
  1%|          | 40/3616 [01:52<1:56:47,  1.96s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.30ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.68ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out
  1%|          | 41/3616 [01:53<1:44:27,  1.75s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.31ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.68ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out
  1%|          | 42/3616 [01:54<1:35:52,  1.61s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.32ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.67ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out
  1%|          | 43/3616 [01:55<1:29:51,  1.51s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.32ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.65ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out
  1%|          | 44/3616 [01:57<1:25:44,  1.44s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.30ms to get benchmark function for torch_compile_nested_tensor_integration
terminate called after throwing an instance of 'python_error'
  what():  
