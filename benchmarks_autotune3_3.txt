Running part 4/4: kernels 7 to 7 of 7 total
Running 1 kernels...


============================================================
Kernel: fp8_attention
============================================================

TMA benchmarks will be running without grid constant TMA descriptor.
Running fp8_attention benchmark with Helion implementation...

  0%|          | 0/8 [00:00<?, ?it/s]WARNING:tritonbench.utils.env_utils:[tritonbench] Precision fp8 is handled by operator.
INFO:tritonbench.utils.triton_op:Took 8.08ms to get benchmark function for triton_flash_v2
INFO:tritonbench.utils.triton_op:Took 0.11ms to get benchmark function for triton_flash_v2_tma
INFO:tritonbench.utils.triton_op:Took 0.11ms to get benchmark function for triton_flash_v2_ws
INFO:tritonbench.utils.triton_op:Took 0.06ms to get benchmark function for helion_fp8_attention
[0s] Starting DifferentialEvolutionSearch with population=40, generations=20, crossover_rate=0.8
Removed 10 outliers from 566 samples
Removed 40 outliers from 410 samples
Removed 68 outliers from 593 samples
python: /root/.triton/llvm/llvm-8957e64a-almalinux-x64/include/llvm/ADT/SmallVector.h:296: const_reference llvm::SmallVectorTemplateCommon<long>::operator[](size_type) const [T = long]: Assertion `idx < size()' failed.
module {
  tt.func public @_fp8_attention_kernel_kernel(%arg0: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %c1_i32 = arith.constant 1 : i32
    %c32_i32 = arith.constant 32 : i32
    %c64_i32 = arith.constant 64 : i32
    %c128_i32 = arith.constant 128 : i32
    %c0_i32 = arith.constant 0 : i32
    %cst = arith.constant dense<64> : tensor<1x32xi32>
    %cst_0 = arith.constant dense<0.180336878> : tensor<64x32xf32>
    %cst_1 = arith.constant dense<0.000000e+00> : tensor<64x32xf32>
    %cst_2 = arith.constant dense<64> : tensor<32x1xi32>
    %cst_3 = arith.constant dense<64> : tensor<64x1xi32>
    %c8192_i32 = arith.constant 8192 : i32
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<64x64xf32>
    %cst_5 = arith.constant dense<0.000000e+00> : tensor<64xf32>
    %cst_6 = arith.constant dense<0xFF800000> : tensor<64xf32>
    %0 = tt.get_program_id x : i32
    %1 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
    scf.for %arg6 = %c0_i32 to %c128_i32 step %c64_i32  : i32 {
      %2 = tt.splat %arg6 : i32 -> tensor<64xi32>
      %3 = arith.addi %2, %1 : tensor<64xi32>
      %4 = arith.muli %0, %c8192_i32 : i32
      %5 = tt.expand_dims %3 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
      %6 = arith.muli %5, %cst_3 : tensor<64x1xi32>
      %7 = tt.splat %4 : i32 -> tensor<64x1xi32>
      %8 = arith.addi %7, %6 : tensor<64x1xi32>
      %9 = tt.expand_dims %1 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %10 = tt.broadcast %8 : tensor<64x1xi32> -> tensor<64x64xi32>
      %11 = tt.broadcast %9 : tensor<1x64xi32> -> tensor<64x64xi32>
      %12 = arith.addi %10, %11 : tensor<64x64xi32>
      %13 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %14 = tt.addptr %13, %12 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      %15 = tt.load %14 : tensor<64x64x!tt.ptr<f8E5M2>>
      %16:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c32_i32 iter_args(%arg8 = %cst_5, %arg9 = %cst_4, %arg10 = %cst_6) -> (tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>)  : i32 {
        %42 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32>
        %43 = tt.splat %arg7 : i32 -> tensor<32xi32>
        %44 = arith.addi %43, %42 : tensor<32xi32>
        %45 = tt.expand_dims %44 {axis = 1 : i32} : tensor<32xi32> -> tensor<32x1xi32>
        %46 = arith.muli %45, %cst_2 : tensor<32x1xi32>
        %47 = tt.splat %4 : i32 -> tensor<32x1xi32>
        %48 = arith.addi %47, %46 : tensor<32x1xi32>
        %49 = tt.broadcast %48 : tensor<32x1xi32> -> tensor<32x64xi32>
        %50 = tt.broadcast %9 : tensor<1x64xi32> -> tensor<32x64xi32>
        %51 = arith.addi %49, %50 : tensor<32x64xi32>
        %52 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<32x64x!tt.ptr<f8E5M2>>
        %53 = tt.addptr %52, %51 : tensor<32x64x!tt.ptr<f8E5M2>>, tensor<32x64xi32>
        %54 = tt.load %53 : tensor<32x64x!tt.ptr<f8E5M2>>
        %55 = tt.trans %54 {order = array<i32: 1, 0>} : tensor<32x64xf8E5M2> -> tensor<64x32xf8E5M2>
        %56 = tt.dot %15, %55, %cst_1, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x64xf8E5M2> * tensor<64x32xf8E5M2> -> tensor<64x32xf32>
        %57 = arith.mulf %56, %cst_0 : tensor<64x32xf32>
        %58 = "tt.reduce"(%57) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %89 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %89 : f32
        }) : (tensor<64x32xf32>) -> tensor<64xf32>
        %59 = arith.cmpf ogt, %arg10, %58 : tensor<64xf32>
        %60 = arith.cmpf une, %arg10, %arg10 : tensor<64xf32>
        %61 = arith.ori %59, %60 : tensor<64xi1>
        %62 = arith.select %61, %arg10, %58 : tensor<64xi1>, tensor<64xf32>
        %63 = tt.expand_dims %62 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %64 = tt.broadcast %63 : tensor<64x1xf32> -> tensor<64x32xf32>
        %65 = arith.subf %57, %64 : tensor<64x32xf32>
        %66 = tt.extern_elementwise %65 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64x32xf32>) -> tensor<64x32xf32>
        %67 = "tt.reduce"(%66) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %89 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %89 : f32
        }) : (tensor<64x32xf32>) -> tensor<64xf32>
        %68 = arith.subf %arg10, %62 : tensor<64xf32>
        %69 = tt.extern_elementwise %68 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64xf32>) -> tensor<64xf32>
        %70 = arith.mulf %arg8, %69 : tensor<64xf32>
        %71 = arith.addf %70, %67 : tensor<64xf32>
        %72 = tt.expand_dims %69 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %73 = tt.broadcast %72 : tensor<64x1xf32> -> tensor<64x64xf32>
        %74 = arith.mulf %arg9, %73 : tensor<64x64xf32>
        %75 = tt.expand_dims %1 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %76 = arith.addi %7, %75 : tensor<64x1xi32>
        %77 = tt.expand_dims %44 {axis = 0 : i32} : tensor<32xi32> -> tensor<1x32xi32>
        %78 = arith.muli %77, %cst : tensor<1x32xi32>
        %79 = tt.broadcast %76 : tensor<64x1xi32> -> tensor<64x32xi32>
        %80 = tt.broadcast %78 : tensor<1x32xi32> -> tensor<64x32xi32>
        %81 = arith.addi %79, %80 : tensor<64x32xi32>
        %82 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x32x!tt.ptr<f8E5M2>>
        %83 = tt.addptr %82, %81 : tensor<64x32x!tt.ptr<f8E5M2>>, tensor<64x32xi32>
        %84 = tt.load %83 : tensor<64x32x!tt.ptr<f8E5M2>>
        %85 = tt.fp_to_fp %66, rounding = rtne : tensor<64x32xf32> -> tensor<64x32xf8E5M2>
        %86 = tt.trans %84 {order = array<i32: 1, 0>} : tensor<64x32xf8E5M2> -> tensor<32x64xf8E5M2>
        %87 = tt.dot %85, %86, %cst_4, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x32xf8E5M2> * tensor<32x64xf8E5M2> -> tensor<64x64xf32>
        %88 = arith.addf %74, %87 : tensor<64x64xf32>
        scf.yield %71, %88, %62 : tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>
      } {tt.loop_unroll_factor = 1 : i32, tt.num_stages = 3 : i32}
      %17 = tt.expand_dims %16#0 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
      %18 = tt.broadcast %17 : tensor<64x1xf32> -> tensor<64x64xf32>
      %19 = arith.divf %16#1, %18 : tensor<64x64xf32>
      %20 = tt.fp_to_fp %19, rounding = rtne : tensor<64x64xf32> -> tensor<64x64xf8E5M2>
      %21 = arith.divsi %0, %arg5 : i32
      %22 = arith.remsi %0, %arg5 : i32
      %23 = arith.cmpi ne, %22, %c0_i32 : i32
      %24 = arith.subi %21, %c1_i32 : i32
      %25 = arith.select %23, %24, %21 : i32
      %26 = arith.cmpi slt, %0, %c0_i32 : i32
      %27 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %28 = arith.cmpi ne, %26, %27 : i1
      %29 = arith.select %28, %25, %21 : i32
      %30 = arith.andi %23, %28 : i1
      %31 = arith.addi %22, %arg5 : i32
      %32 = arith.select %30, %31, %22 : i32
      %33 = arith.muli %29, %arg4 : i32
      %34 = arith.muli %32, %c8192_i32 : i32
      %35 = arith.addi %33, %34 : i32
      %36 = tt.splat %35 : i32 -> tensor<64x1xi32>
      %37 = arith.addi %36, %6 : tensor<64x1xi32>
      %38 = tt.broadcast %37 : tensor<64x1xi32> -> tensor<64x64xi32>
      %39 = arith.addi %38, %11 : tensor<64x64xi32>
      %40 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %41 = tt.addptr %40, %39 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      tt.store %41, %20 : tensor<64x64x!tt.ptr<f8E5M2>>
    } {tt.flatten, tt.loop_unroll_factor = 1 : i32, tt.num_stages = 1 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(convert-triton-to-tritongpu{enable-source-remat=false num-ctas=1 num-warps=16 target=cuda:90 threads-per-warp=32}, tritongpu-coalesce, tritongpu-F32DotTC, triton-nvidia-gpu-plan-cta, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritongpu-accelerate-matmul, tritongpu-remove-layout-conversions, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, triton-nvidia-optimize-descriptor-encoding, triton-loop-aware-cse, tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-combine-tensor-select-and-if, nvgpu-warp-specialization{dump-intermediate-steps=false num-stages=3}, tritongpu-assign-latencies{num-stages=3}, tritongpu-schedule-loops, tritongpu-pipeline{dump-intermediate-steps=false num-stages=3}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-loop-aware-cse, tritongpu-prefetch, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-coalesce-async-copy, triton-nvidia-optimize-tmem-layouts, tritongpu-remove-layout-conversions, triton-nvidia-interleave-tmem, tritongpu-reduce-data-duplication, tritongpu-reorder-instructions, triton-loop-aware-cse, symbol-dce, triton-nvidia-tma-lowering, triton-nvidia-gpu-fence-insertion{compute-capability=90}, sccp, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true})",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_willfeng/p2/cp2u64bfriahxmdebo5cyr3zc52vk4ryjn6onzm5iwpcqf7lun5n.py:12:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_willfeng/p2/cp2u64bfriahxmdebo5cyr3zc52vk4ryjn6onzm5iwpcqf7lun5n.py:12:0: note: Pipeline failed while executing [`TritonGPUAssignLatencies` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
Process ForkProcess-56:
Traceback (most recent call last):
  File "/home/willfeng/local/miniconda3/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/willfeng/local/miniconda3/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/data/users/willfeng/helion/helion/runtime/precompile_shim.py", line 54, in finish_it
    kernel_cache[key] = fn.compile(
                        ^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/compiler/compiler.py", line 359, in compile
    next_module = compile_ir(module, metadata)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/backends/nvidia/compiler.py", line 456, in <lambda>
    stages["ttgir"] = lambda src, metadata: self.make_ttgir(src, metadata, options, capability)
                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/backends/nvidia/compiler.py", line 298, in make_ttgir
    pm.run(mod)
RuntimeError: PassManager::run failed
python: /root/.triton/llvm/llvm-8957e64a-almalinux-x64/include/llvm/ADT/SmallVector.h:296: const_reference llvm::SmallVectorTemplateCommon<long>::operator[](size_type) const [T = long]: Assertion `idx < size()' failed.
module {
  tt.func public @_fp8_attention_kernel_kernel(%arg0: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %c1_i32 = arith.constant 1 : i32
    %c32_i32 = arith.constant 32 : i32
    %c64_i32 = arith.constant 64 : i32
    %c128_i32 = arith.constant 128 : i32
    %c0_i32 = arith.constant 0 : i32
    %cst = arith.constant dense<64> : tensor<1x32xi32>
    %cst_0 = arith.constant dense<0.180336878> : tensor<64x32xf32>
    %cst_1 = arith.constant dense<0.000000e+00> : tensor<64x32xf32>
    %cst_2 = arith.constant dense<64> : tensor<32x1xi32>
    %cst_3 = arith.constant dense<64> : tensor<64x1xi32>
    %c8192_i32 = arith.constant 8192 : i32
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<64x64xf32>
    %cst_5 = arith.constant dense<0.000000e+00> : tensor<64xf32>
    %cst_6 = arith.constant dense<0xFF800000> : tensor<64xf32>
    %0 = tt.get_program_id x : i32
    %1 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
    scf.for %arg6 = %c0_i32 to %c128_i32 step %c64_i32  : i32 {
      %2 = tt.splat %arg6 : i32 -> tensor<64xi32>
      %3 = arith.addi %2, %1 : tensor<64xi32>
      %4 = arith.muli %0, %c8192_i32 : i32
      %5 = tt.expand_dims %3 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
      %6 = arith.muli %5, %cst_3 : tensor<64x1xi32>
      %7 = tt.splat %4 : i32 -> tensor<64x1xi32>
      %8 = arith.addi %7, %6 : tensor<64x1xi32>
      %9 = tt.expand_dims %1 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %10 = tt.broadcast %8 : tensor<64x1xi32> -> tensor<64x64xi32>
      %11 = tt.broadcast %9 : tensor<1x64xi32> -> tensor<64x64xi32>
      %12 = arith.addi %10, %11 : tensor<64x64xi32>
      %13 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %14 = tt.addptr %13, %12 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      %15 = tt.load %14 : tensor<64x64x!tt.ptr<f8E5M2>>
      %16:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c32_i32 iter_args(%arg8 = %cst_5, %arg9 = %cst_4, %arg10 = %cst_6) -> (tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>)  : i32 {
        %42 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32>
        %43 = tt.splat %arg7 : i32 -> tensor<32xi32>
        %44 = arith.addi %43, %42 : tensor<32xi32>
        %45 = tt.expand_dims %44 {axis = 1 : i32} : tensor<32xi32> -> tensor<32x1xi32>
        %46 = arith.muli %45, %cst_2 : tensor<32x1xi32>
        %47 = tt.splat %4 : i32 -> tensor<32x1xi32>
        %48 = arith.addi %47, %46 : tensor<32x1xi32>
        %49 = tt.broadcast %48 : tensor<32x1xi32> -> tensor<32x64xi32>
        %50 = tt.broadcast %9 : tensor<1x64xi32> -> tensor<32x64xi32>
        %51 = arith.addi %49, %50 : tensor<32x64xi32>
        %52 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<32x64x!tt.ptr<f8E5M2>>
        %53 = tt.addptr %52, %51 : tensor<32x64x!tt.ptr<f8E5M2>>, tensor<32x64xi32>
        %54 = tt.load %53 : tensor<32x64x!tt.ptr<f8E5M2>>
        %55 = tt.trans %54 {order = array<i32: 1, 0>} : tensor<32x64xf8E5M2> -> tensor<64x32xf8E5M2>
        %56 = tt.dot %15, %55, %cst_1, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x64xf8E5M2> * tensor<64x32xf8E5M2> -> tensor<64x32xf32>
        %57 = arith.mulf %56, %cst_0 : tensor<64x32xf32>
        %58 = "tt.reduce"(%57) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %89 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %89 : f32
        }) : (tensor<64x32xf32>) -> tensor<64xf32>
        %59 = arith.cmpf ogt, %arg10, %58 : tensor<64xf32>
        %60 = arith.cmpf une, %arg10, %arg10 : tensor<64xf32>
        %61 = arith.ori %59, %60 : tensor<64xi1>
        %62 = arith.select %61, %arg10, %58 : tensor<64xi1>, tensor<64xf32>
        %63 = tt.expand_dims %62 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %64 = tt.broadcast %63 : tensor<64x1xf32> -> tensor<64x32xf32>
        %65 = arith.subf %57, %64 : tensor<64x32xf32>
        %66 = tt.extern_elementwise %65 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64x32xf32>) -> tensor<64x32xf32>
        %67 = "tt.reduce"(%66) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %89 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %89 : f32
        }) : (tensor<64x32xf32>) -> tensor<64xf32>
        %68 = arith.subf %arg10, %62 : tensor<64xf32>
        %69 = tt.extern_elementwise %68 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64xf32>) -> tensor<64xf32>
        %70 = arith.mulf %arg8, %69 : tensor<64xf32>
        %71 = arith.addf %70, %67 : tensor<64xf32>
        %72 = tt.expand_dims %69 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %73 = tt.broadcast %72 : tensor<64x1xf32> -> tensor<64x64xf32>
        %74 = arith.mulf %arg9, %73 : tensor<64x64xf32>
        %75 = tt.expand_dims %1 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %76 = arith.addi %7, %75 : tensor<64x1xi32>
        %77 = tt.expand_dims %44 {axis = 0 : i32} : tensor<32xi32> -> tensor<1x32xi32>
        %78 = arith.muli %77, %cst : tensor<1x32xi32>
        %79 = tt.broadcast %76 : tensor<64x1xi32> -> tensor<64x32xi32>
        %80 = tt.broadcast %78 : tensor<1x32xi32> -> tensor<64x32xi32>
        %81 = arith.addi %79, %80 : tensor<64x32xi32>
        %82 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x32x!tt.ptr<f8E5M2>>
        %83 = tt.addptr %82, %81 : tensor<64x32x!tt.ptr<f8E5M2>>, tensor<64x32xi32>
        %84 = tt.load %83 : tensor<64x32x!tt.ptr<f8E5M2>>
        %85 = tt.fp_to_fp %66, rounding = rtne : tensor<64x32xf32> -> tensor<64x32xf8E5M2>
        %86 = tt.trans %84 {order = array<i32: 1, 0>} : tensor<64x32xf8E5M2> -> tensor<32x64xf8E5M2>
        %87 = tt.dot %85, %86, %cst_4, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x32xf8E5M2> * tensor<32x64xf8E5M2> -> tensor<64x64xf32>
        %88 = arith.addf %74, %87 : tensor<64x64xf32>
        scf.yield %71, %88, %62 : tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>
      } {tt.loop_unroll_factor = 1 : i32, tt.num_stages = 3 : i32}
      %17 = tt.expand_dims %16#0 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
      %18 = tt.broadcast %17 : tensor<64x1xf32> -> tensor<64x64xf32>
      %19 = arith.divf %16#1, %18 : tensor<64x64xf32>
      %20 = tt.fp_to_fp %19, rounding = rtne : tensor<64x64xf32> -> tensor<64x64xf8E5M2>
      %21 = arith.divsi %0, %arg5 : i32
      %22 = arith.remsi %0, %arg5 : i32
      %23 = arith.cmpi ne, %22, %c0_i32 : i32
      %24 = arith.subi %21, %c1_i32 : i32
      %25 = arith.select %23, %24, %21 : i32
      %26 = arith.cmpi slt, %0, %c0_i32 : i32
      %27 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %28 = arith.cmpi ne, %26, %27 : i1
      %29 = arith.select %28, %25, %21 : i32
      %30 = arith.andi %23, %28 : i1
      %31 = arith.addi %22, %arg5 : i32
      %32 = arith.select %30, %31, %22 : i32
      %33 = arith.muli %29, %arg4 : i32
      %34 = arith.muli %32, %c8192_i32 : i32
      %35 = arith.addi %33, %34 : i32
      %36 = tt.splat %35 : i32 -> tensor<64x1xi32>
      %37 = arith.addi %36, %6 : tensor<64x1xi32>
      %38 = tt.broadcast %37 : tensor<64x1xi32> -> tensor<64x64xi32>
      %39 = arith.addi %38, %11 : tensor<64x64xi32>
      %40 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %41 = tt.addptr %40, %39 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      tt.store %41, %20 : tensor<64x64x!tt.ptr<f8E5M2>>
    } {tt.flatten, tt.loop_unroll_factor = 1 : i32, tt.num_stages = 1 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(convert-triton-to-tritongpu{enable-source-remat=false num-ctas=1 num-warps=16 target=cuda:90 threads-per-warp=32}, tritongpu-coalesce, tritongpu-F32DotTC, triton-nvidia-gpu-plan-cta, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritongpu-accelerate-matmul, tritongpu-remove-layout-conversions, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, triton-nvidia-optimize-descriptor-encoding, triton-loop-aware-cse, tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-combine-tensor-select-and-if, nvgpu-warp-specialization{dump-intermediate-steps=false num-stages=3}, tritongpu-assign-latencies{num-stages=3}, tritongpu-schedule-loops, tritongpu-pipeline{dump-intermediate-steps=false num-stages=3}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-loop-aware-cse, tritongpu-prefetch, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-coalesce-async-copy, triton-nvidia-optimize-tmem-layouts, tritongpu-remove-layout-conversions, triton-nvidia-interleave-tmem, tritongpu-reduce-data-duplication, tritongpu-reorder-instructions, triton-loop-aware-cse, symbol-dce, triton-nvidia-tma-lowering, triton-nvidia-gpu-fence-insertion{compute-capability=90}, sccp, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true})",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_willfeng/p2/cp2u64bfriahxmdebo5cyr3zc52vk4ryjn6onzm5iwpcqf7lun5n.py:12:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_willfeng/p2/cp2u64bfriahxmdebo5cyr3zc52vk4ryjn6onzm5iwpcqf7lun5n.py:12:0: note: Pipeline failed while executing [`TritonGPUAssignLatencies` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
[12s] Initial population: failed=1 min=0.0137 mid=0.0446 max=1.0988 best=Config(block_sizes=[128, 128], range_unroll_factors=[2, 3, 4], range_num_stages=[1, 4, 2], range_multi_buffers=[False, None, True], range_flattens=[True, True, False], num_warps=8, num_stages=2, indexing='block_ptr', pid_type='persistent_interleaved')
[139s] Generation 2: replaced=16 min=0.0121 mid=0.0227 max=0.0417 best=Config(block_sizes=[128, 64], range_unroll_factors=[0, 0, 1], range_num_stages=[0, 0, 2], range_multi_buffers=[None, False, False], range_flattens=[None, None, None], num_warps=4, num_stages=5, indexing='block_ptr', pid_type='flat', range_warp_specializes=[])
python: /root/.triton/llvm/llvm-8957e64a-almalinux-x64/include/llvm/ADT/SmallVector.h:296: const_reference llvm::SmallVectorTemplateCommon<long>::operator[](size_type) const [T = long]: Assertion `idx < size()' failed.
module {
  tt.func public @_fp8_attention_kernel_kernel(%arg0: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %c1_i32 = arith.constant 1 : i32
    %c64_i32 = arith.constant 64 : i32
    %c128_i32 = arith.constant 128 : i32
    %c0_i32 = arith.constant 0 : i32
    %c132_i32 = arith.constant 132 : i32
    %c192_i32 = arith.constant 192 : i32
    %cst = arith.constant dense<64> : tensor<1x64xi32>
    %cst_0 = arith.constant dense<0.180336878> : tensor<128x64xf32>
    %cst_1 = arith.constant dense<64> : tensor<64x1xi32>
    %cst_2 = arith.constant dense<64> : tensor<128x1xi32>
    %c8192_i32 = arith.constant 8192 : i32
    %cst_3 = arith.constant dense<0.000000e+00> : tensor<128x64xf32>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<128xf32>
    %cst_5 = arith.constant dense<0xFF800000> : tensor<128xf32>
    %0 = tt.get_program_id x : i32
    %1 = arith.subi %c192_i32, %0 : i32
    %c1_i32_6 = arith.constant 1 : i32
    %2 = arith.subi %c132_i32, %c1_i32_6 : i32
    %3 = arith.addi %1, %2 : i32
    %4 = arith.divui %3, %c132_i32 : i32
    %c4_i32 = arith.constant 4 : i32
    %5 = arith.remsi %4, %c4_i32 : i32
    %6 = arith.subi %4, %5 : i32
    %7 = arith.muli %6, %c132_i32 : i32
    %8 = arith.addi %0, %7 : i32
    %9 = arith.muli %c132_i32, %c4_i32 : i32
    scf.for %arg6 = %0 to %8 step %9  : i32 {
      %10 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
      %11 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>
      %12 = arith.muli %arg6, %c8192_i32 : i32
      %13 = tt.expand_dims %11 {axis = 1 : i32} : tensor<128xi32> -> tensor<128x1xi32>
      %14 = arith.muli %13, %cst_2 : tensor<128x1xi32>
      %15 = tt.splat %12 : i32 -> tensor<128x1xi32>
      %16 = arith.addi %15, %14 : tensor<128x1xi32>
      %17 = tt.expand_dims %10 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %18 = tt.broadcast %16 : tensor<128x1xi32> -> tensor<128x64xi32>
      %19 = tt.broadcast %17 : tensor<1x64xi32> -> tensor<128x64xi32>
      %20 = arith.addi %18, %19 : tensor<128x64xi32>
      %21 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %22 = tt.addptr %21, %20 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      %23 = tt.load %22 : tensor<128x64x!tt.ptr<f8E5M2>>
      %c0_i32_7 = arith.constant 0 : i32
      %c256_i32 = arith.constant 256 : i32
      %24:3 = scf.for %arg7 = %c0_i32 to %c0_i32_7 step %c256_i32 iter_args(%arg8 = %cst_4, %arg9 = %cst_3, %arg10 = %cst_5) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %180 = tt.splat %arg7 : i32 -> tensor<64xi32>
        %181 = arith.addi %180, %10 : tensor<64xi32>
        %182 = tt.expand_dims %181 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %183 = arith.muli %182, %cst_1 : tensor<64x1xi32>
        %184 = tt.splat %12 : i32 -> tensor<64x1xi32>
        %185 = arith.addi %184, %183 : tensor<64x1xi32>
        %186 = tt.broadcast %185 : tensor<64x1xi32> -> tensor<64x64xi32>
        %187 = tt.broadcast %17 : tensor<1x64xi32> -> tensor<64x64xi32>
        %188 = arith.addi %186, %187 : tensor<64x64xi32>
        %189 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %190 = tt.addptr %189, %188 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %191 = tt.load %190 : tensor<64x64x!tt.ptr<f8E5M2>>
        %192 = tt.trans %191 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %193 = tt.dot %23, %192, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %194 = arith.mulf %193, %cst_0 : tensor<128x64xf32>
        %195 = "tt.reduce"(%194) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %196 = arith.cmpf ogt, %arg10, %195 : tensor<128xf32>
        %197 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %198 = arith.ori %196, %197 : tensor<128xi1>
        %199 = arith.select %198, %arg10, %195 : tensor<128xi1>, tensor<128xf32>
        %200 = tt.expand_dims %199 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %201 = tt.broadcast %200 : tensor<128x1xf32> -> tensor<128x64xf32>
        %202 = arith.subf %194, %201 : tensor<128x64xf32>
        %203 = tt.extern_elementwise %202 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %204 = "tt.reduce"(%203) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %205 = arith.subf %arg10, %199 : tensor<128xf32>
        %206 = tt.extern_elementwise %205 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %207 = arith.mulf %arg8, %206 : tensor<128xf32>
        %208 = arith.addf %207, %204 : tensor<128xf32>
        %209 = tt.expand_dims %206 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %210 = tt.broadcast %209 : tensor<128x1xf32> -> tensor<128x64xf32>
        %211 = arith.mulf %arg9, %210 : tensor<128x64xf32>
        %212 = tt.expand_dims %10 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %213 = arith.addi %184, %212 : tensor<64x1xi32>
        %214 = tt.expand_dims %181 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %215 = arith.muli %214, %cst : tensor<1x64xi32>
        %216 = tt.broadcast %213 : tensor<64x1xi32> -> tensor<64x64xi32>
        %217 = tt.broadcast %215 : tensor<1x64xi32> -> tensor<64x64xi32>
        %218 = arith.addi %216, %217 : tensor<64x64xi32>
        %219 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %220 = tt.addptr %219, %218 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %221 = tt.load %220 : tensor<64x64x!tt.ptr<f8E5M2>>
        %222 = tt.fp_to_fp %203, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %223 = tt.trans %221 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %224 = tt.dot %222, %223, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %225 = arith.addf %211, %224 : tensor<128x64xf32>
        %c1_i32_15 = arith.constant 1 : i32
        %226 = arith.muli %c64_i32, %c1_i32_15 : i32
        %227 = arith.addi %arg7, %226 : i32
        %228 = tt.splat %227 : i32 -> tensor<64xi32>
        %229 = arith.addi %228, %10 : tensor<64xi32>
        %230 = tt.expand_dims %229 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %231 = arith.muli %230, %cst_1 : tensor<64x1xi32>
        %232 = tt.splat %12 : i32 -> tensor<64x1xi32>
        %233 = arith.addi %232, %231 : tensor<64x1xi32>
        %234 = tt.broadcast %233 : tensor<64x1xi32> -> tensor<64x64xi32>
        %235 = tt.broadcast %17 : tensor<1x64xi32> -> tensor<64x64xi32>
        %236 = arith.addi %234, %235 : tensor<64x64xi32>
        %237 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %238 = tt.addptr %237, %236 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %239 = tt.load %238 : tensor<64x64x!tt.ptr<f8E5M2>>
        %240 = tt.trans %239 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %241 = tt.dot %23, %240, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %242 = arith.mulf %241, %cst_0 : tensor<128x64xf32>
        %243 = "tt.reduce"(%242) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %244 = arith.cmpf ogt, %199, %243 : tensor<128xf32>
        %245 = arith.cmpf une, %199, %199 : tensor<128xf32>
        %246 = arith.ori %244, %245 : tensor<128xi1>
        %247 = arith.select %246, %199, %243 : tensor<128xi1>, tensor<128xf32>
        %248 = tt.expand_dims %247 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %249 = tt.broadcast %248 : tensor<128x1xf32> -> tensor<128x64xf32>
        %250 = arith.subf %242, %249 : tensor<128x64xf32>
        %251 = tt.extern_elementwise %250 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %252 = "tt.reduce"(%251) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %253 = arith.subf %199, %247 : tensor<128xf32>
        %254 = tt.extern_elementwise %253 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %255 = arith.mulf %208, %254 : tensor<128xf32>
        %256 = arith.addf %255, %252 : tensor<128xf32>
        %257 = tt.expand_dims %254 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %258 = tt.broadcast %257 : tensor<128x1xf32> -> tensor<128x64xf32>
        %259 = arith.mulf %225, %258 : tensor<128x64xf32>
        %260 = tt.expand_dims %10 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %261 = arith.addi %232, %260 : tensor<64x1xi32>
        %262 = tt.expand_dims %229 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %263 = arith.muli %262, %cst : tensor<1x64xi32>
        %264 = tt.broadcast %261 : tensor<64x1xi32> -> tensor<64x64xi32>
        %265 = tt.broadcast %263 : tensor<1x64xi32> -> tensor<64x64xi32>
        %266 = arith.addi %264, %265 : tensor<64x64xi32>
        %267 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %268 = tt.addptr %267, %266 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %269 = tt.load %268 : tensor<64x64x!tt.ptr<f8E5M2>>
        %270 = tt.fp_to_fp %251, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %271 = tt.trans %269 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %272 = tt.dot %270, %271, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %273 = arith.addf %259, %272 : tensor<128x64xf32>
        %c2_i32_16 = arith.constant 2 : i32
        %274 = arith.muli %c64_i32, %c2_i32_16 : i32
        %275 = arith.addi %arg7, %274 : i32
        %276 = tt.splat %275 : i32 -> tensor<64xi32>
        %277 = arith.addi %276, %10 : tensor<64xi32>
        %278 = tt.expand_dims %277 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %279 = arith.muli %278, %cst_1 : tensor<64x1xi32>
        %280 = tt.splat %12 : i32 -> tensor<64x1xi32>
        %281 = arith.addi %280, %279 : tensor<64x1xi32>
        %282 = tt.broadcast %281 : tensor<64x1xi32> -> tensor<64x64xi32>
        %283 = tt.broadcast %17 : tensor<1x64xi32> -> tensor<64x64xi32>
        %284 = arith.addi %282, %283 : tensor<64x64xi32>
        %285 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %286 = tt.addptr %285, %284 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %287 = tt.load %286 : tensor<64x64x!tt.ptr<f8E5M2>>
        %288 = tt.trans %287 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %289 = tt.dot %23, %288, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %290 = arith.mulf %289, %cst_0 : tensor<128x64xf32>
        %291 = "tt.reduce"(%290) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %292 = arith.cmpf ogt, %247, %291 : tensor<128xf32>
        %293 = arith.cmpf une, %247, %247 : tensor<128xf32>
        %294 = arith.ori %292, %293 : tensor<128xi1>
        %295 = arith.select %294, %247, %291 : tensor<128xi1>, tensor<128xf32>
        %296 = tt.expand_dims %295 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %297 = tt.broadcast %296 : tensor<128x1xf32> -> tensor<128x64xf32>
        %298 = arith.subf %290, %297 : tensor<128x64xf32>
        %299 = tt.extern_elementwise %298 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %300 = "tt.reduce"(%299) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %301 = arith.subf %247, %295 : tensor<128xf32>
        %302 = tt.extern_elementwise %301 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %303 = arith.mulf %256, %302 : tensor<128xf32>
        %304 = arith.addf %303, %300 : tensor<128xf32>
        %305 = tt.expand_dims %302 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %306 = tt.broadcast %305 : tensor<128x1xf32> -> tensor<128x64xf32>
        %307 = arith.mulf %273, %306 : tensor<128x64xf32>
        %308 = tt.expand_dims %10 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %309 = arith.addi %280, %308 : tensor<64x1xi32>
        %310 = tt.expand_dims %277 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %311 = arith.muli %310, %cst : tensor<1x64xi32>
        %312 = tt.broadcast %309 : tensor<64x1xi32> -> tensor<64x64xi32>
        %313 = tt.broadcast %311 : tensor<1x64xi32> -> tensor<64x64xi32>
        %314 = arith.addi %312, %313 : tensor<64x64xi32>
        %315 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %316 = tt.addptr %315, %314 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %317 = tt.load %316 : tensor<64x64x!tt.ptr<f8E5M2>>
        %318 = tt.fp_to_fp %299, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %319 = tt.trans %317 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %320 = tt.dot %318, %319, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %321 = arith.addf %307, %320 : tensor<128x64xf32>
        %c3_i32_17 = arith.constant 3 : i32
        %322 = arith.muli %c64_i32, %c3_i32_17 : i32
        %323 = arith.addi %arg7, %322 : i32
        %324 = tt.splat %323 : i32 -> tensor<64xi32>
        %325 = arith.addi %324, %10 : tensor<64xi32>
        %326 = tt.expand_dims %325 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %327 = arith.muli %326, %cst_1 : tensor<64x1xi32>
        %328 = tt.splat %12 : i32 -> tensor<64x1xi32>
        %329 = arith.addi %328, %327 : tensor<64x1xi32>
        %330 = tt.broadcast %329 : tensor<64x1xi32> -> tensor<64x64xi32>
        %331 = tt.broadcast %17 : tensor<1x64xi32> -> tensor<64x64xi32>
        %332 = arith.addi %330, %331 : tensor<64x64xi32>
        %333 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %334 = tt.addptr %333, %332 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %335 = tt.load %334 : tensor<64x64x!tt.ptr<f8E5M2>>
        %336 = tt.trans %335 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %337 = tt.dot %23, %336, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %338 = arith.mulf %337, %cst_0 : tensor<128x64xf32>
        %339 = "tt.reduce"(%338) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %340 = arith.cmpf ogt, %295, %339 : tensor<128xf32>
        %341 = arith.cmpf une, %295, %295 : tensor<128xf32>
        %342 = arith.ori %340, %341 : tensor<128xi1>
        %343 = arith.select %342, %295, %339 : tensor<128xi1>, tensor<128xf32>
        %344 = tt.expand_dims %343 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %345 = tt.broadcast %344 : tensor<128x1xf32> -> tensor<128x64xf32>
        %346 = arith.subf %338, %345 : tensor<128x64xf32>
        %347 = tt.extern_elementwise %346 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %348 = "tt.reduce"(%347) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %349 = arith.subf %295, %343 : tensor<128xf32>
        %350 = tt.extern_elementwise %349 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %351 = arith.mulf %304, %350 : tensor<128xf32>
        %352 = arith.addf %351, %348 : tensor<128xf32>
        %353 = tt.expand_dims %350 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %354 = tt.broadcast %353 : tensor<128x1xf32> -> tensor<128x64xf32>
        %355 = arith.mulf %321, %354 : tensor<128x64xf32>
        %356 = tt.expand_dims %10 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %357 = arith.addi %328, %356 : tensor<64x1xi32>
        %358 = tt.expand_dims %325 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %359 = arith.muli %358, %cst : tensor<1x64xi32>
        %360 = tt.broadcast %357 : tensor<64x1xi32> -> tensor<64x64xi32>
        %361 = tt.broadcast %359 : tensor<1x64xi32> -> tensor<64x64xi32>
        %362 = arith.addi %360, %361 : tensor<64x64xi32>
        %363 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %364 = tt.addptr %363, %362 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %365 = tt.load %364 : tensor<64x64x!tt.ptr<f8E5M2>>
        %366 = tt.fp_to_fp %347, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %367 = tt.trans %365 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %368 = tt.dot %366, %367, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %369 = arith.addf %355, %368 : tensor<128x64xf32>
        scf.yield %352, %369, %343 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.disallow_acc_multi_buffer, tt.num_stages = 1 : i32}
      %25:3 = scf.for %arg7 = %c0_i32_7 to %c128_i32 step %c64_i32 iter_args(%arg8 = %24#0, %arg9 = %24#1, %arg10 = %24#2) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %180 = tt.splat %arg7 : i32 -> tensor<64xi32>
        %181 = arith.addi %180, %10 : tensor<64xi32>
        %182 = tt.expand_dims %181 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %183 = arith.muli %182, %cst_1 : tensor<64x1xi32>
        %184 = tt.splat %12 : i32 -> tensor<64x1xi32>
        %185 = arith.addi %184, %183 : tensor<64x1xi32>
        %186 = tt.broadcast %185 : tensor<64x1xi32> -> tensor<64x64xi32>
        %187 = tt.broadcast %17 : tensor<1x64xi32> -> tensor<64x64xi32>
        %188 = arith.addi %186, %187 : tensor<64x64xi32>
        %189 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %190 = tt.addptr %189, %188 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %191 = tt.load %190 : tensor<64x64x!tt.ptr<f8E5M2>>
        %192 = tt.trans %191 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %193 = tt.dot %23, %192, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %194 = arith.mulf %193, %cst_0 : tensor<128x64xf32>
        %195 = "tt.reduce"(%194) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %226 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %226 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %196 = arith.cmpf ogt, %arg10, %195 : tensor<128xf32>
        %197 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %198 = arith.ori %196, %197 : tensor<128xi1>
        %199 = arith.select %198, %arg10, %195 : tensor<128xi1>, tensor<128xf32>
        %200 = tt.expand_dims %199 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %201 = tt.broadcast %200 : tensor<128x1xf32> -> tensor<128x64xf32>
        %202 = arith.subf %194, %201 : tensor<128x64xf32>
        %203 = tt.extern_elementwise %202 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %204 = "tt.reduce"(%203) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %226 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %226 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %205 = arith.subf %arg10, %199 : tensor<128xf32>
        %206 = tt.extern_elementwise %205 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %207 = arith.mulf %arg8, %206 : tensor<128xf32>
        %208 = arith.addf %207, %204 : tensor<128xf32>
        %209 = tt.expand_dims %206 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %210 = tt.broadcast %209 : tensor<128x1xf32> -> tensor<128x64xf32>
        %211 = arith.mulf %arg9, %210 : tensor<128x64xf32>
        %212 = tt.expand_dims %10 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %213 = arith.addi %184, %212 : tensor<64x1xi32>
        %214 = tt.expand_dims %181 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %215 = arith.muli %214, %cst : tensor<1x64xi32>
        %216 = tt.broadcast %213 : tensor<64x1xi32> -> tensor<64x64xi32>
        %217 = tt.broadcast %215 : tensor<1x64xi32> -> tensor<64x64xi32>
        %218 = arith.addi %216, %217 : tensor<64x64xi32>
        %219 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %220 = tt.addptr %219, %218 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %221 = tt.load %220 : tensor<64x64x!tt.ptr<f8E5M2>>
        %222 = tt.fp_to_fp %203, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %223 = tt.trans %221 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %224 = tt.dot %222, %223, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %225 = arith.addf %211, %224 : tensor<128x64xf32>
        scf.yield %208, %225, %199 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.disallow_acc_multi_buffer, tt.num_stages = 1 : i32}
      %26 = tt.expand_dims %25#0 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
      %27 = tt.broadcast %26 : tensor<128x1xf32> -> tensor<128x64xf32>
      %28 = arith.divf %25#1, %27 : tensor<128x64xf32>
      %29 = tt.fp_to_fp %28, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
      %30 = arith.divsi %arg6, %arg5 : i32
      %31 = arith.remsi %arg6, %arg5 : i32
      %32 = arith.cmpi ne, %31, %c0_i32 : i32
      %33 = arith.subi %30, %c1_i32 : i32
      %34 = arith.select %32, %33, %30 : i32
      %35 = arith.cmpi slt, %arg6, %c0_i32 : i32
      %36 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %37 = arith.cmpi ne, %35, %36 : i1
      %38 = arith.select %37, %34, %30 : i32
      %39 = arith.andi %32, %37 : i1
      %40 = arith.addi %31, %arg5 : i32
      %41 = arith.select %39, %40, %31 : i32
      %42 = arith.muli %38, %arg4 : i32
      %43 = arith.muli %41, %c8192_i32 : i32
      %44 = arith.addi %42, %43 : i32
      %45 = tt.splat %44 : i32 -> tensor<128x1xi32>
      %46 = arith.addi %45, %14 : tensor<128x1xi32>
      %47 = tt.broadcast %46 : tensor<128x1xi32> -> tensor<128x64xi32>
      %48 = arith.addi %47, %19 : tensor<128x64xi32>
      %49 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %50 = tt.addptr %49, %48 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      tt.store %50, %29 : tensor<128x64x!tt.ptr<f8E5M2>>
      %c1_i32_8 = arith.constant 1 : i32
      %51 = arith.muli %c132_i32, %c1_i32_8 : i32
      %52 = arith.addi %arg6, %51 : i32
      %53 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
      %54 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>
      %55 = arith.muli %52, %c8192_i32 : i32
      %56 = tt.expand_dims %54 {axis = 1 : i32} : tensor<128xi32> -> tensor<128x1xi32>
      %57 = arith.muli %56, %cst_2 : tensor<128x1xi32>
      %58 = tt.splat %55 : i32 -> tensor<128x1xi32>
      %59 = arith.addi %58, %57 : tensor<128x1xi32>
      %60 = tt.expand_dims %53 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %61 = tt.broadcast %59 : tensor<128x1xi32> -> tensor<128x64xi32>
      %62 = tt.broadcast %60 : tensor<1x64xi32> -> tensor<128x64xi32>
      %63 = arith.addi %61, %62 : tensor<128x64xi32>
      %64 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %65 = tt.addptr %64, %63 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      %66 = tt.load %65 : tensor<128x64x!tt.ptr<f8E5M2>>
      %c0_i32_9 = arith.constant 0 : i32
      %c256_i32_10 = arith.constant 256 : i32
      %67:3 = scf.for %arg7 = %c0_i32 to %c0_i32_9 step %c256_i32_10 iter_args(%arg8 = %cst_4, %arg9 = %cst_3, %arg10 = %cst_5) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %180 = tt.splat %arg7 : i32 -> tensor<64xi32>
        %181 = arith.addi %180, %53 : tensor<64xi32>
        %182 = tt.expand_dims %181 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %183 = arith.muli %182, %cst_1 : tensor<64x1xi32>
        %184 = tt.splat %55 : i32 -> tensor<64x1xi32>
        %185 = arith.addi %184, %183 : tensor<64x1xi32>
        %186 = tt.broadcast %185 : tensor<64x1xi32> -> tensor<64x64xi32>
        %187 = tt.broadcast %60 : tensor<1x64xi32> -> tensor<64x64xi32>
        %188 = arith.addi %186, %187 : tensor<64x64xi32>
        %189 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %190 = tt.addptr %189, %188 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %191 = tt.load %190 : tensor<64x64x!tt.ptr<f8E5M2>>
        %192 = tt.trans %191 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %193 = tt.dot %66, %192, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %194 = arith.mulf %193, %cst_0 : tensor<128x64xf32>
        %195 = "tt.reduce"(%194) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %196 = arith.cmpf ogt, %arg10, %195 : tensor<128xf32>
        %197 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %198 = arith.ori %196, %197 : tensor<128xi1>
        %199 = arith.select %198, %arg10, %195 : tensor<128xi1>, tensor<128xf32>
        %200 = tt.expand_dims %199 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %201 = tt.broadcast %200 : tensor<128x1xf32> -> tensor<128x64xf32>
        %202 = arith.subf %194, %201 : tensor<128x64xf32>
        %203 = tt.extern_elementwise %202 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %204 = "tt.reduce"(%203) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %205 = arith.subf %arg10, %199 : tensor<128xf32>
        %206 = tt.extern_elementwise %205 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %207 = arith.mulf %arg8, %206 : tensor<128xf32>
        %208 = arith.addf %207, %204 : tensor<128xf32>
        %209 = tt.expand_dims %206 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %210 = tt.broadcast %209 : tensor<128x1xf32> -> tensor<128x64xf32>
        %211 = arith.mulf %arg9, %210 : tensor<128x64xf32>
        %212 = tt.expand_dims %53 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %213 = arith.addi %184, %212 : tensor<64x1xi32>
        %214 = tt.expand_dims %181 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %215 = arith.muli %214, %cst : tensor<1x64xi32>
        %216 = tt.broadcast %213 : tensor<64x1xi32> -> tensor<64x64xi32>
        %217 = tt.broadcast %215 : tensor<1x64xi32> -> tensor<64x64xi32>
        %218 = arith.addi %216, %217 : tensor<64x64xi32>
        %219 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %220 = tt.addptr %219, %218 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %221 = tt.load %220 : tensor<64x64x!tt.ptr<f8E5M2>>
        %222 = tt.fp_to_fp %203, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %223 = tt.trans %221 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %224 = tt.dot %222, %223, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %225 = arith.addf %211, %224 : tensor<128x64xf32>
        %c1_i32_15 = arith.constant 1 : i32
        %226 = arith.muli %c64_i32, %c1_i32_15 : i32
        %227 = arith.addi %arg7, %226 : i32
        %228 = tt.splat %227 : i32 -> tensor<64xi32>
        %229 = arith.addi %228, %53 : tensor<64xi32>
        %230 = tt.expand_dims %229 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %231 = arith.muli %230, %cst_1 : tensor<64x1xi32>
        %232 = tt.splat %55 : i32 -> tensor<64x1xi32>
        %233 = arith.addi %232, %231 : tensor<64x1xi32>
        %234 = tt.broadcast %233 : tensor<64x1xi32> -> tensor<64x64xi32>
        %235 = tt.broadcast %60 : tensor<1x64xi32> -> tensor<64x64xi32>
        %236 = arith.addi %234, %235 : tensor<64x64xi32>
        %237 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %238 = tt.addptr %237, %236 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %239 = tt.load %238 : tensor<64x64x!tt.ptr<f8E5M2>>
        %240 = tt.trans %239 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %241 = tt.dot %66, %240, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %242 = arith.mulf %241, %cst_0 : tensor<128x64xf32>
        %243 = "tt.reduce"(%242) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %244 = arith.cmpf ogt, %199, %243 : tensor<128xf32>
        %245 = arith.cmpf une, %199, %199 : tensor<128xf32>
        %246 = arith.ori %244, %245 : tensor<128xi1>
        %247 = arith.select %246, %199, %243 : tensor<128xi1>, tensor<128xf32>
        %248 = tt.expand_dims %247 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %249 = tt.broadcast %248 : tensor<128x1xf32> -> tensor<128x64xf32>
        %250 = arith.subf %242, %249 : tensor<128x64xf32>
        %251 = tt.extern_elementwise %250 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %252 = "tt.reduce"(%251) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %253 = arith.subf %199, %247 : tensor<128xf32>
        %254 = tt.extern_elementwise %253 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %255 = arith.mulf %208, %254 : tensor<128xf32>
        %256 = arith.addf %255, %252 : tensor<128xf32>
        %257 = tt.expand_dims %254 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %258 = tt.broadcast %257 : tensor<128x1xf32> -> tensor<128x64xf32>
        %259 = arith.mulf %225, %258 : tensor<128x64xf32>
        %260 = tt.expand_dims %53 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %261 = arith.addi %232, %260 : tensor<64x1xi32>
        %262 = tt.expand_dims %229 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %263 = arith.muli %262, %cst : tensor<1x64xi32>
        %264 = tt.broadcast %261 : tensor<64x1xi32> -> tensor<64x64xi32>
        %265 = tt.broadcast %263 : tensor<1x64xi32> -> tensor<64x64xi32>
        %266 = arith.addi %264, %265 : tensor<64x64xi32>
        %267 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %268 = tt.addptr %267, %266 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %269 = tt.load %268 : tensor<64x64x!tt.ptr<f8E5M2>>
        %270 = tt.fp_to_fp %251, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %271 = tt.trans %269 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %272 = tt.dot %270, %271, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %273 = arith.addf %259, %272 : tensor<128x64xf32>
        %c2_i32_16 = arith.constant 2 : i32
        %274 = arith.muli %c64_i32, %c2_i32_16 : i32
        %275 = arith.addi %arg7, %274 : i32
        %276 = tt.splat %275 : i32 -> tensor<64xi32>
        %277 = arith.addi %276, %53 : tensor<64xi32>
        %278 = tt.expand_dims %277 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %279 = arith.muli %278, %cst_1 : tensor<64x1xi32>
        %280 = tt.splat %55 : i32 -> tensor<64x1xi32>
        %281 = arith.addi %280, %279 : tensor<64x1xi32>
        %282 = tt.broadcast %281 : tensor<64x1xi32> -> tensor<64x64xi32>
        %283 = tt.broadcast %60 : tensor<1x64xi32> -> tensor<64x64xi32>
        %284 = arith.addi %282, %283 : tensor<64x64xi32>
        %285 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %286 = tt.addptr %285, %284 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %287 = tt.load %286 : tensor<64x64x!tt.ptr<f8E5M2>>
        %288 = tt.trans %287 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %289 = tt.dot %66, %288, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %290 = arith.mulf %289, %cst_0 : tensor<128x64xf32>
        %291 = "tt.reduce"(%290) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %292 = arith.cmpf ogt, %247, %291 : tensor<128xf32>
        %293 = arith.cmpf une, %247, %247 : tensor<128xf32>
        %294 = arith.ori %292, %293 : tensor<128xi1>
        %295 = arith.select %294, %247, %291 : tensor<128xi1>, tensor<128xf32>
        %296 = tt.expand_dims %295 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %297 = tt.broadcast %296 : tensor<128x1xf32> -> tensor<128x64xf32>
        %298 = arith.subf %290, %297 : tensor<128x64xf32>
        %299 = tt.extern_elementwise %298 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %300 = "tt.reduce"(%299) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %301 = arith.subf %247, %295 : tensor<128xf32>
        %302 = tt.extern_elementwise %301 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %303 = arith.mulf %256, %302 : tensor<128xf32>
        %304 = arith.addf %303, %300 : tensor<128xf32>
        %305 = tt.expand_dims %302 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %306 = tt.broadcast %305 : tensor<128x1xf32> -> tensor<128x64xf32>
        %307 = arith.mulf %273, %306 : tensor<128x64xf32>
        %308 = tt.expand_dims %53 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %309 = arith.addi %280, %308 : tensor<64x1xi32>
        %310 = tt.expand_dims %277 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %311 = arith.muli %310, %cst : tensor<1x64xi32>
        %312 = tt.broadcast %309 : tensor<64x1xi32> -> tensor<64x64xi32>
        %313 = tt.broadcast %311 : tensor<1x64xi32> -> tensor<64x64xi32>
        %314 = arith.addi %312, %313 : tensor<64x64xi32>
        %315 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %316 = tt.addptr %315, %314 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %317 = tt.load %316 : tensor<64x64x!tt.ptr<f8E5M2>>
        %318 = tt.fp_to_fp %299, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %319 = tt.trans %317 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %320 = tt.dot %318, %319, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %321 = arith.addf %307, %320 : tensor<128x64xf32>
        %c3_i32_17 = arith.constant 3 : i32
        %322 = arith.muli %c64_i32, %c3_i32_17 : i32
        %323 = arith.addi %arg7, %322 : i32
        %324 = tt.splat %323 : i32 -> tensor<64xi32>
        %325 = arith.addi %324, %53 : tensor<64xi32>
        %326 = tt.expand_dims %325 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %327 = arith.muli %326, %cst_1 : tensor<64x1xi32>
        %328 = tt.splat %55 : i32 -> tensor<64x1xi32>
        %329 = arith.addi %328, %327 : tensor<64x1xi32>
        %330 = tt.broadcast %329 : tensor<64x1xi32> -> tensor<64x64xi32>
        %331 = tt.broadcast %60 : tensor<1x64xi32> -> tensor<64x64xi32>
        %332 = arith.addi %330, %331 : tensor<64x64xi32>
        %333 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %334 = tt.addptr %333, %332 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %335 = tt.load %334 : tensor<64x64x!tt.ptr<f8E5M2>>
        %336 = tt.trans %335 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %337 = tt.dot %66, %336, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %338 = arith.mulf %337, %cst_0 : tensor<128x64xf32>
        %339 = "tt.reduce"(%338) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %340 = arith.cmpf ogt, %295, %339 : tensor<128xf32>
        %341 = arith.cmpf une, %295, %295 : tensor<128xf32>
        %342 = arith.ori %340, %341 : tensor<128xi1>
        %343 = arith.select %342, %295, %339 : tensor<128xi1>, tensor<128xf32>
        %344 = tt.expand_dims %343 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %345 = tt.broadcast %344 : tensor<128x1xf32> -> tensor<128x64xf32>
        %346 = arith.subf %338, %345 : tensor<128x64xf32>
        %347 = tt.extern_elementwise %346 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %348 = "tt.reduce"(%347) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %349 = arith.subf %295, %343 : tensor<128xf32>
        %350 = tt.extern_elementwise %349 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %351 = arith.mulf %304, %350 : tensor<128xf32>
        %352 = arith.addf %351, %348 : tensor<128xf32>
        %353 = tt.expand_dims %350 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %354 = tt.broadcast %353 : tensor<128x1xf32> -> tensor<128x64xf32>
        %355 = arith.mulf %321, %354 : tensor<128x64xf32>
        %356 = tt.expand_dims %53 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %357 = arith.addi %328, %356 : tensor<64x1xi32>
        %358 = tt.expand_dims %325 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %359 = arith.muli %358, %cst : tensor<1x64xi32>
        %360 = tt.broadcast %357 : tensor<64x1xi32> -> tensor<64x64xi32>
        %361 = tt.broadcast %359 : tensor<1x64xi32> -> tensor<64x64xi32>
        %362 = arith.addi %360, %361 : tensor<64x64xi32>
        %363 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %364 = tt.addptr %363, %362 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %365 = tt.load %364 : tensor<64x64x!tt.ptr<f8E5M2>>
        %366 = tt.fp_to_fp %347, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %367 = tt.trans %365 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %368 = tt.dot %366, %367, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %369 = arith.addf %355, %368 : tensor<128x64xf32>
        scf.yield %352, %369, %343 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.disallow_acc_multi_buffer, tt.num_stages = 1 : i32}
      %68:3 = scf.for %arg7 = %c0_i32_9 to %c128_i32 step %c64_i32 iter_args(%arg8 = %67#0, %arg9 = %67#1, %arg10 = %67#2) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %180 = tt.splat %arg7 : i32 -> tensor<64xi32>
        %181 = arith.addi %180, %53 : tensor<64xi32>
        %182 = tt.expand_dims %181 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %183 = arith.muli %182, %cst_1 : tensor<64x1xi32>
        %184 = tt.splat %55 : i32 -> tensor<64x1xi32>
        %185 = arith.addi %184, %183 : tensor<64x1xi32>
        %186 = tt.broadcast %185 : tensor<64x1xi32> -> tensor<64x64xi32>
        %187 = tt.broadcast %60 : tensor<1x64xi32> -> tensor<64x64xi32>
        %188 = arith.addi %186, %187 : tensor<64x64xi32>
        %189 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %190 = tt.addptr %189, %188 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %191 = tt.load %190 : tensor<64x64x!tt.ptr<f8E5M2>>
        %192 = tt.trans %191 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %193 = tt.dot %66, %192, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %194 = arith.mulf %193, %cst_0 : tensor<128x64xf32>
        %195 = "tt.reduce"(%194) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %226 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %226 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %196 = arith.cmpf ogt, %arg10, %195 : tensor<128xf32>
        %197 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %198 = arith.ori %196, %197 : tensor<128xi1>
        %199 = arith.select %198, %arg10, %195 : tensor<128xi1>, tensor<128xf32>
        %200 = tt.expand_dims %199 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %201 = tt.broadcast %200 : tensor<128x1xf32> -> tensor<128x64xf32>
        %202 = arith.subf %194, %201 : tensor<128x64xf32>
        %203 = tt.extern_elementwise %202 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %204 = "tt.reduce"(%203) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %226 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %226 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %205 = arith.subf %arg10, %199 : tensor<128xf32>
        %206 = tt.extern_elementwise %205 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %207 = arith.mulf %arg8, %206 : tensor<128xf32>
        %208 = arith.addf %207, %204 : tensor<128xf32>
        %209 = tt.expand_dims %206 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %210 = tt.broadcast %209 : tensor<128x1xf32> -> tensor<128x64xf32>
        %211 = arith.mulf %arg9, %210 : tensor<128x64xf32>
        %212 = tt.expand_dims %53 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %213 = arith.addi %184, %212 : tensor<64x1xi32>
        %214 = tt.expand_dims %181 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %215 = arith.muli %214, %cst : tensor<1x64xi32>
        %216 = tt.broadcast %213 : tensor<64x1xi32> -> tensor<64x64xi32>
        %217 = tt.broadcast %215 : tensor<1x64xi32> -> tensor<64x64xi32>
        %218 = arith.addi %216, %217 : tensor<64x64xi32>
        %219 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %220 = tt.addptr %219, %218 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %221 = tt.load %220 : tensor<64x64x!tt.ptr<f8E5M2>>
        %222 = tt.fp_to_fp %203, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %223 = tt.trans %221 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %224 = tt.dot %222, %223, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %225 = arith.addf %211, %224 : tensor<128x64xf32>
        scf.yield %208, %225, %199 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.disallow_acc_multi_buffer, tt.num_stages = 1 : i32}
      %69 = tt.expand_dims %68#0 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
      %70 = tt.broadcast %69 : tensor<128x1xf32> -> tensor<128x64xf32>
      %71 = arith.divf %68#1, %70 : tensor<128x64xf32>
      %72 = tt.fp_to_fp %71, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
      %73 = arith.divsi %52, %arg5 : i32
      %74 = arith.remsi %52, %arg5 : i32
      %75 = arith.cmpi ne, %74, %c0_i32 : i32
      %76 = arith.subi %73, %c1_i32 : i32
      %77 = arith.select %75, %76, %73 : i32
      %78 = arith.cmpi slt, %52, %c0_i32 : i32
      %79 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %80 = arith.cmpi ne, %78, %79 : i1
      %81 = arith.select %80, %77, %73 : i32
      %82 = arith.andi %75, %80 : i1
      %83 = arith.addi %74, %arg5 : i32
      %84 = arith.select %82, %83, %74 : i32
      %85 = arith.muli %81, %arg4 : i32
      %86 = arith.muli %84, %c8192_i32 : i32
      %87 = arith.addi %85, %86 : i32
      %88 = tt.splat %87 : i32 -> tensor<128x1xi32>
      %89 = arith.addi %88, %57 : tensor<128x1xi32>
      %90 = tt.broadcast %89 : tensor<128x1xi32> -> tensor<128x64xi32>
      %91 = arith.addi %90, %62 : tensor<128x64xi32>
      %92 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %93 = tt.addptr %92, %91 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      tt.store %93, %72 : tensor<128x64x!tt.ptr<f8E5M2>>
      %c2_i32 = arith.constant 2 : i32
      %94 = arith.muli %c132_i32, %c2_i32 : i32
      %95 = arith.addi %arg6, %94 : i32
      %96 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
      %97 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>
      %98 = arith.muli %95, %c8192_i32 : i32
      %99 = tt.expand_dims %97 {axis = 1 : i32} : tensor<128xi32> -> tensor<128x1xi32>
      %100 = arith.muli %99, %cst_2 : tensor<128x1xi32>
      %101 = tt.splat %98 : i32 -> tensor<128x1xi32>
      %102 = arith.addi %101, %100 : tensor<128x1xi32>
      %103 = tt.expand_dims %96 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %104 = tt.broadcast %102 : tensor<128x1xi32> -> tensor<128x64xi32>
      %105 = tt.broadcast %103 : tensor<1x64xi32> -> tensor<128x64xi32>
      %106 = arith.addi %104, %105 : tensor<128x64xi32>
      %107 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %108 = tt.addptr %107, %106 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      %109 = tt.load %108 : tensor<128x64x!tt.ptr<f8E5M2>>
      %c0_i32_11 = arith.constant 0 : i32
      %c256_i32_12 = arith.constant 256 : i32
      %110:3 = scf.for %arg7 = %c0_i32 to %c0_i32_11 step %c256_i32_12 iter_args(%arg8 = %cst_4, %arg9 = %cst_3, %arg10 = %cst_5) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %180 = tt.splat %arg7 : i32 -> tensor<64xi32>
        %181 = arith.addi %180, %96 : tensor<64xi32>
        %182 = tt.expand_dims %181 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %183 = arith.muli %182, %cst_1 : tensor<64x1xi32>
        %184 = tt.splat %98 : i32 -> tensor<64x1xi32>
        %185 = arith.addi %184, %183 : tensor<64x1xi32>
        %186 = tt.broadcast %185 : tensor<64x1xi32> -> tensor<64x64xi32>
        %187 = tt.broadcast %103 : tensor<1x64xi32> -> tensor<64x64xi32>
        %188 = arith.addi %186, %187 : tensor<64x64xi32>
        %189 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %190 = tt.addptr %189, %188 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %191 = tt.load %190 : tensor<64x64x!tt.ptr<f8E5M2>>
        %192 = tt.trans %191 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %193 = tt.dot %109, %192, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %194 = arith.mulf %193, %cst_0 : tensor<128x64xf32>
        %195 = "tt.reduce"(%194) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %196 = arith.cmpf ogt, %arg10, %195 : tensor<128xf32>
        %197 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %198 = arith.ori %196, %197 : tensor<128xi1>
        %199 = arith.select %198, %arg10, %195 : tensor<128xi1>, tensor<128xf32>
        %200 = tt.expand_dims %199 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %201 = tt.broadcast %200 : tensor<128x1xf32> -> tensor<128x64xf32>
        %202 = arith.subf %194, %201 : tensor<128x64xf32>
        %203 = tt.extern_elementwise %202 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %204 = "tt.reduce"(%203) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %205 = arith.subf %arg10, %199 : tensor<128xf32>
        %206 = tt.extern_elementwise %205 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %207 = arith.mulf %arg8, %206 : tensor<128xf32>
        %208 = arith.addf %207, %204 : tensor<128xf32>
        %209 = tt.expand_dims %206 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %210 = tt.broadcast %209 : tensor<128x1xf32> -> tensor<128x64xf32>
        %211 = arith.mulf %arg9, %210 : tensor<128x64xf32>
        %212 = tt.expand_dims %96 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %213 = arith.addi %184, %212 : tensor<64x1xi32>
        %214 = tt.expand_dims %181 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %215 = arith.muli %214, %cst : tensor<1x64xi32>
        %216 = tt.broadcast %213 : tensor<64x1xi32> -> tensor<64x64xi32>
        %217 = tt.broadcast %215 : tensor<1x64xi32> -> tensor<64x64xi32>
        %218 = arith.addi %216, %217 : tensor<64x64xi32>
        %219 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %220 = tt.addptr %219, %218 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %221 = tt.load %220 : tensor<64x64x!tt.ptr<f8E5M2>>
        %222 = tt.fp_to_fp %203, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %223 = tt.trans %221 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %224 = tt.dot %222, %223, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %225 = arith.addf %211, %224 : tensor<128x64xf32>
        %c1_i32_15 = arith.constant 1 : i32
        %226 = arith.muli %c64_i32, %c1_i32_15 : i32
        %227 = arith.addi %arg7, %226 : i32
        %228 = tt.splat %227 : i32 -> tensor<64xi32>
        %229 = arith.addi %228, %96 : tensor<64xi32>
        %230 = tt.expand_dims %229 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %231 = arith.muli %230, %cst_1 : tensor<64x1xi32>
        %232 = tt.splat %98 : i32 -> tensor<64x1xi32>
        %233 = arith.addi %232, %231 : tensor<64x1xi32>
        %234 = tt.broadcast %233 : tensor<64x1xi32> -> tensor<64x64xi32>
        %235 = tt.broadcast %103 : tensor<1x64xi32> -> tensor<64x64xi32>
        %236 = arith.addi %234, %235 : tensor<64x64xi32>
        %237 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %238 = tt.addptr %237, %236 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %239 = tt.load %238 : tensor<64x64x!tt.ptr<f8E5M2>>
        %240 = tt.trans %239 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %241 = tt.dot %109, %240, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %242 = arith.mulf %241, %cst_0 : tensor<128x64xf32>
        %243 = "tt.reduce"(%242) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %244 = arith.cmpf ogt, %199, %243 : tensor<128xf32>
        %245 = arith.cmpf une, %199, %199 : tensor<128xf32>
        %246 = arith.ori %244, %245 : tensor<128xi1>
        %247 = arith.select %246, %199, %243 : tensor<128xi1>, tensor<128xf32>
        %248 = tt.expand_dims %247 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %249 = tt.broadcast %248 : tensor<128x1xf32> -> tensor<128x64xf32>
        %250 = arith.subf %242, %249 : tensor<128x64xf32>
        %251 = tt.extern_elementwise %250 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %252 = "tt.reduce"(%251) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %253 = arith.subf %199, %247 : tensor<128xf32>
        %254 = tt.extern_elementwise %253 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %255 = arith.mulf %208, %254 : tensor<128xf32>
        %256 = arith.addf %255, %252 : tensor<128xf32>
        %257 = tt.expand_dims %254 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %258 = tt.broadcast %257 : tensor<128x1xf32> -> tensor<128x64xf32>
        %259 = arith.mulf %225, %258 : tensor<128x64xf32>
        %260 = tt.expand_dims %96 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %261 = arith.addi %232, %260 : tensor<64x1xi32>
        %262 = tt.expand_dims %229 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %263 = arith.muli %262, %cst : tensor<1x64xi32>
        %264 = tt.broadcast %261 : tensor<64x1xi32> -> tensor<64x64xi32>
        %265 = tt.broadcast %263 : tensor<1x64xi32> -> tensor<64x64xi32>
        %266 = arith.addi %264, %265 : tensor<64x64xi32>
        %267 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %268 = tt.addptr %267, %266 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %269 = tt.load %268 : tensor<64x64x!tt.ptr<f8E5M2>>
        %270 = tt.fp_to_fp %251, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %271 = tt.trans %269 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %272 = tt.dot %270, %271, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %273 = arith.addf %259, %272 : tensor<128x64xf32>
        %c2_i32_16 = arith.constant 2 : i32
        %274 = arith.muli %c64_i32, %c2_i32_16 : i32
        %275 = arith.addi %arg7, %274 : i32
        %276 = tt.splat %275 : i32 -> tensor<64xi32>
        %277 = arith.addi %276, %96 : tensor<64xi32>
        %278 = tt.expand_dims %277 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %279 = arith.muli %278, %cst_1 : tensor<64x1xi32>
        %280 = tt.splat %98 : i32 -> tensor<64x1xi32>
        %281 = arith.addi %280, %279 : tensor<64x1xi32>
        %282 = tt.broadcast %281 : tensor<64x1xi32> -> tensor<64x64xi32>
        %283 = tt.broadcast %103 : tensor<1x64xi32> -> tensor<64x64xi32>
        %284 = arith.addi %282, %283 : tensor<64x64xi32>
        %285 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %286 = tt.addptr %285, %284 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %287 = tt.load %286 : tensor<64x64x!tt.ptr<f8E5M2>>
        %288 = tt.trans %287 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %289 = tt.dot %109, %288, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %290 = arith.mulf %289, %cst_0 : tensor<128x64xf32>
        %291 = "tt.reduce"(%290) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %292 = arith.cmpf ogt, %247, %291 : tensor<128xf32>
        %293 = arith.cmpf une, %247, %247 : tensor<128xf32>
        %294 = arith.ori %292, %293 : tensor<128xi1>
        %295 = arith.select %294, %247, %291 : tensor<128xi1>, tensor<128xf32>
        %296 = tt.expand_dims %295 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %297 = tt.broadcast %296 : tensor<128x1xf32> -> tensor<128x64xf32>
        %298 = arith.subf %290, %297 : tensor<128x64xf32>
        %299 = tt.extern_elementwise %298 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %300 = "tt.reduce"(%299) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %301 = arith.subf %247, %295 : tensor<128xf32>
        %302 = tt.extern_elementwise %301 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %303 = arith.mulf %256, %302 : tensor<128xf32>
        %304 = arith.addf %303, %300 : tensor<128xf32>
        %305 = tt.expand_dims %302 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %306 = tt.broadcast %305 : tensor<128x1xf32> -> tensor<128x64xf32>
        %307 = arith.mulf %273, %306 : tensor<128x64xf32>
        %308 = tt.expand_dims %96 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %309 = arith.addi %280, %308 : tensor<64x1xi32>
        %310 = tt.expand_dims %277 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %311 = arith.muli %310, %cst : tensor<1x64xi32>
        %312 = tt.broadcast %309 : tensor<64x1xi32> -> tensor<64x64xi32>
        %313 = tt.broadcast %311 : tensor<1x64xi32> -> tensor<64x64xi32>
        %314 = arith.addi %312, %313 : tensor<64x64xi32>
        %315 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %316 = tt.addptr %315, %314 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %317 = tt.load %316 : tensor<64x64x!tt.ptr<f8E5M2>>
        %318 = tt.fp_to_fp %299, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %319 = tt.trans %317 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %320 = tt.dot %318, %319, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %321 = arith.addf %307, %320 : tensor<128x64xf32>
        %c3_i32_17 = arith.constant 3 : i32
        %322 = arith.muli %c64_i32, %c3_i32_17 : i32
        %323 = arith.addi %arg7, %322 : i32
        %324 = tt.splat %323 : i32 -> tensor<64xi32>
        %325 = arith.addi %324, %96 : tensor<64xi32>
        %326 = tt.expand_dims %325 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %327 = arith.muli %326, %cst_1 : tensor<64x1xi32>
        %328 = tt.splat %98 : i32 -> tensor<64x1xi32>
        %329 = arith.addi %328, %327 : tensor<64x1xi32>
        %330 = tt.broadcast %329 : tensor<64x1xi32> -> tensor<64x64xi32>
        %331 = tt.broadcast %103 : tensor<1x64xi32> -> tensor<64x64xi32>
        %332 = arith.addi %330, %331 : tensor<64x64xi32>
        %333 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %334 = tt.addptr %333, %332 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %335 = tt.load %334 : tensor<64x64x!tt.ptr<f8E5M2>>
        %336 = tt.trans %335 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %337 = tt.dot %109, %336, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %338 = arith.mulf %337, %cst_0 : tensor<128x64xf32>
        %339 = "tt.reduce"(%338) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %340 = arith.cmpf ogt, %295, %339 : tensor<128xf32>
        %341 = arith.cmpf une, %295, %295 : tensor<128xf32>
        %342 = arith.ori %340, %341 : tensor<128xi1>
        %343 = arith.select %342, %295, %339 : tensor<128xi1>, tensor<128xf32>
        %344 = tt.expand_dims %343 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %345 = tt.broadcast %344 : tensor<128x1xf32> -> tensor<128x64xf32>
        %346 = arith.subf %338, %345 : tensor<128x64xf32>
        %347 = tt.extern_elementwise %346 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %348 = "tt.reduce"(%347) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %349 = arith.subf %295, %343 : tensor<128xf32>
        %350 = tt.extern_elementwise %349 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %351 = arith.mulf %304, %350 : tensor<128xf32>
        %352 = arith.addf %351, %348 : tensor<128xf32>
        %353 = tt.expand_dims %350 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %354 = tt.broadcast %353 : tensor<128x1xf32> -> tensor<128x64xf32>
        %355 = arith.mulf %321, %354 : tensor<128x64xf32>
        %356 = tt.expand_dims %96 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %357 = arith.addi %328, %356 : tensor<64x1xi32>
        %358 = tt.expand_dims %325 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %359 = arith.muli %358, %cst : tensor<1x64xi32>
        %360 = tt.broadcast %357 : tensor<64x1xi32> -> tensor<64x64xi32>
        %361 = tt.broadcast %359 : tensor<1x64xi32> -> tensor<64x64xi32>
        %362 = arith.addi %360, %361 : tensor<64x64xi32>
        %363 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %364 = tt.addptr %363, %362 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %365 = tt.load %364 : tensor<64x64x!tt.ptr<f8E5M2>>
        %366 = tt.fp_to_fp %347, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %367 = tt.trans %365 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %368 = tt.dot %366, %367, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %369 = arith.addf %355, %368 : tensor<128x64xf32>
        scf.yield %352, %369, %343 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.disallow_acc_multi_buffer, tt.num_stages = 1 : i32}
      %111:3 = scf.for %arg7 = %c0_i32_11 to %c128_i32 step %c64_i32 iter_args(%arg8 = %110#0, %arg9 = %110#1, %arg10 = %110#2) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %180 = tt.splat %arg7 : i32 -> tensor<64xi32>
        %181 = arith.addi %180, %96 : tensor<64xi32>
        %182 = tt.expand_dims %181 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %183 = arith.muli %182, %cst_1 : tensor<64x1xi32>
        %184 = tt.splat %98 : i32 -> tensor<64x1xi32>
        %185 = arith.addi %184, %183 : tensor<64x1xi32>
        %186 = tt.broadcast %185 : tensor<64x1xi32> -> tensor<64x64xi32>
        %187 = tt.broadcast %103 : tensor<1x64xi32> -> tensor<64x64xi32>
        %188 = arith.addi %186, %187 : tensor<64x64xi32>
        %189 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %190 = tt.addptr %189, %188 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %191 = tt.load %190 : tensor<64x64x!tt.ptr<f8E5M2>>
        %192 = tt.trans %191 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %193 = tt.dot %109, %192, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %194 = arith.mulf %193, %cst_0 : tensor<128x64xf32>
        %195 = "tt.reduce"(%194) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %226 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %226 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %196 = arith.cmpf ogt, %arg10, %195 : tensor<128xf32>
        %197 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %198 = arith.ori %196, %197 : tensor<128xi1>
        %199 = arith.select %198, %arg10, %195 : tensor<128xi1>, tensor<128xf32>
        %200 = tt.expand_dims %199 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %201 = tt.broadcast %200 : tensor<128x1xf32> -> tensor<128x64xf32>
        %202 = arith.subf %194, %201 : tensor<128x64xf32>
        %203 = tt.extern_elementwise %202 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %204 = "tt.reduce"(%203) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %226 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %226 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %205 = arith.subf %arg10, %199 : tensor<128xf32>
        %206 = tt.extern_elementwise %205 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %207 = arith.mulf %arg8, %206 : tensor<128xf32>
        %208 = arith.addf %207, %204 : tensor<128xf32>
        %209 = tt.expand_dims %206 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %210 = tt.broadcast %209 : tensor<128x1xf32> -> tensor<128x64xf32>
        %211 = arith.mulf %arg9, %210 : tensor<128x64xf32>
        %212 = tt.expand_dims %96 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %213 = arith.addi %184, %212 : tensor<64x1xi32>
        %214 = tt.expand_dims %181 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %215 = arith.muli %214, %cst : tensor<1x64xi32>
        %216 = tt.broadcast %213 : tensor<64x1xi32> -> tensor<64x64xi32>
        %217 = tt.broadcast %215 : tensor<1x64xi32> -> tensor<64x64xi32>
        %218 = arith.addi %216, %217 : tensor<64x64xi32>
        %219 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %220 = tt.addptr %219, %218 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %221 = tt.load %220 : tensor<64x64x!tt.ptr<f8E5M2>>
        %222 = tt.fp_to_fp %203, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %223 = tt.trans %221 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %224 = tt.dot %222, %223, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %225 = arith.addf %211, %224 : tensor<128x64xf32>
        scf.yield %208, %225, %199 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.disallow_acc_multi_buffer, tt.num_stages = 1 : i32}
      %112 = tt.expand_dims %111#0 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
      %113 = tt.broadcast %112 : tensor<128x1xf32> -> tensor<128x64xf32>
      %114 = arith.divf %111#1, %113 : tensor<128x64xf32>
      %115 = tt.fp_to_fp %114, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
      %116 = arith.divsi %95, %arg5 : i32
      %117 = arith.remsi %95, %arg5 : i32
      %118 = arith.cmpi ne, %117, %c0_i32 : i32
      %119 = arith.subi %116, %c1_i32 : i32
      %120 = arith.select %118, %119, %116 : i32
      %121 = arith.cmpi slt, %95, %c0_i32 : i32
      %122 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %123 = arith.cmpi ne, %121, %122 : i1
      %124 = arith.select %123, %120, %116 : i32
      %125 = arith.andi %118, %123 : i1
      %126 = arith.addi %117, %arg5 : i32
      %127 = arith.select %125, %126, %117 : i32
      %128 = arith.muli %124, %arg4 : i32
      %129 = arith.muli %127, %c8192_i32 : i32
      %130 = arith.addi %128, %129 : i32
      %131 = tt.splat %130 : i32 -> tensor<128x1xi32>
      %132 = arith.addi %131, %100 : tensor<128x1xi32>
      %133 = tt.broadcast %132 : tensor<128x1xi32> -> tensor<128x64xi32>
      %134 = arith.addi %133, %105 : tensor<128x64xi32>
      %135 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %136 = tt.addptr %135, %134 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      tt.store %136, %115 : tensor<128x64x!tt.ptr<f8E5M2>>
      %c3_i32 = arith.constant 3 : i32
      %137 = arith.muli %c132_i32, %c3_i32 : i32
      %138 = arith.addi %arg6, %137 : i32
      %139 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
      %140 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>
      %141 = arith.muli %138, %c8192_i32 : i32
      %142 = tt.expand_dims %140 {axis = 1 : i32} : tensor<128xi32> -> tensor<128x1xi32>
      %143 = arith.muli %142, %cst_2 : tensor<128x1xi32>
      %144 = tt.splat %141 : i32 -> tensor<128x1xi32>
      %145 = arith.addi %144, %143 : tensor<128x1xi32>
      %146 = tt.expand_dims %139 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %147 = tt.broadcast %145 : tensor<128x1xi32> -> tensor<128x64xi32>
      %148 = tt.broadcast %146 : tensor<1x64xi32> -> tensor<128x64xi32>
      %149 = arith.addi %147, %148 : tensor<128x64xi32>
      %150 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %151 = tt.addptr %150, %149 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      %152 = tt.load %151 : tensor<128x64x!tt.ptr<f8E5M2>>
      %c0_i32_13 = arith.constant 0 : i32
      %c256_i32_14 = arith.constant 256 : i32
      %153:3 = scf.for %arg7 = %c0_i32 to %c0_i32_13 step %c256_i32_14 iter_args(%arg8 = %cst_4, %arg9 = %cst_3, %arg10 = %cst_5) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %180 = tt.splat %arg7 : i32 -> tensor<64xi32>
        %181 = arith.addi %180, %139 : tensor<64xi32>
        %182 = tt.expand_dims %181 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %183 = arith.muli %182, %cst_1 : tensor<64x1xi32>
        %184 = tt.splat %141 : i32 -> tensor<64x1xi32>
        %185 = arith.addi %184, %183 : tensor<64x1xi32>
        %186 = tt.broadcast %185 : tensor<64x1xi32> -> tensor<64x64xi32>
        %187 = tt.broadcast %146 : tensor<1x64xi32> -> tensor<64x64xi32>
        %188 = arith.addi %186, %187 : tensor<64x64xi32>
        %189 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %190 = tt.addptr %189, %188 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %191 = tt.load %190 : tensor<64x64x!tt.ptr<f8E5M2>>
        %192 = tt.trans %191 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %193 = tt.dot %152, %192, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %194 = arith.mulf %193, %cst_0 : tensor<128x64xf32>
        %195 = "tt.reduce"(%194) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %196 = arith.cmpf ogt, %arg10, %195 : tensor<128xf32>
        %197 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %198 = arith.ori %196, %197 : tensor<128xi1>
        %199 = arith.select %198, %arg10, %195 : tensor<128xi1>, tensor<128xf32>
        %200 = tt.expand_dims %199 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %201 = tt.broadcast %200 : tensor<128x1xf32> -> tensor<128x64xf32>
        %202 = arith.subf %194, %201 : tensor<128x64xf32>
        %203 = tt.extern_elementwise %202 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %204 = "tt.reduce"(%203) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %205 = arith.subf %arg10, %199 : tensor<128xf32>
        %206 = tt.extern_elementwise %205 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %207 = arith.mulf %arg8, %206 : tensor<128xf32>
        %208 = arith.addf %207, %204 : tensor<128xf32>
        %209 = tt.expand_dims %206 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %210 = tt.broadcast %209 : tensor<128x1xf32> -> tensor<128x64xf32>
        %211 = arith.mulf %arg9, %210 : tensor<128x64xf32>
        %212 = tt.expand_dims %139 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %213 = arith.addi %184, %212 : tensor<64x1xi32>
        %214 = tt.expand_dims %181 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %215 = arith.muli %214, %cst : tensor<1x64xi32>
        %216 = tt.broadcast %213 : tensor<64x1xi32> -> tensor<64x64xi32>
        %217 = tt.broadcast %215 : tensor<1x64xi32> -> tensor<64x64xi32>
        %218 = arith.addi %216, %217 : tensor<64x64xi32>
        %219 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %220 = tt.addptr %219, %218 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %221 = tt.load %220 : tensor<64x64x!tt.ptr<f8E5M2>>
        %222 = tt.fp_to_fp %203, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %223 = tt.trans %221 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %224 = tt.dot %222, %223, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %225 = arith.addf %211, %224 : tensor<128x64xf32>
        %c1_i32_15 = arith.constant 1 : i32
        %226 = arith.muli %c64_i32, %c1_i32_15 : i32
        %227 = arith.addi %arg7, %226 : i32
        %228 = tt.splat %227 : i32 -> tensor<64xi32>
        %229 = arith.addi %228, %139 : tensor<64xi32>
        %230 = tt.expand_dims %229 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %231 = arith.muli %230, %cst_1 : tensor<64x1xi32>
        %232 = tt.splat %141 : i32 -> tensor<64x1xi32>
        %233 = arith.addi %232, %231 : tensor<64x1xi32>
        %234 = tt.broadcast %233 : tensor<64x1xi32> -> tensor<64x64xi32>
        %235 = tt.broadcast %146 : tensor<1x64xi32> -> tensor<64x64xi32>
        %236 = arith.addi %234, %235 : tensor<64x64xi32>
        %237 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %238 = tt.addptr %237, %236 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %239 = tt.load %238 : tensor<64x64x!tt.ptr<f8E5M2>>
        %240 = tt.trans %239 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %241 = tt.dot %152, %240, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %242 = arith.mulf %241, %cst_0 : tensor<128x64xf32>
        %243 = "tt.reduce"(%242) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %244 = arith.cmpf ogt, %199, %243 : tensor<128xf32>
        %245 = arith.cmpf une, %199, %199 : tensor<128xf32>
        %246 = arith.ori %244, %245 : tensor<128xi1>
        %247 = arith.select %246, %199, %243 : tensor<128xi1>, tensor<128xf32>
        %248 = tt.expand_dims %247 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %249 = tt.broadcast %248 : tensor<128x1xf32> -> tensor<128x64xf32>
        %250 = arith.subf %242, %249 : tensor<128x64xf32>
        %251 = tt.extern_elementwise %250 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %252 = "tt.reduce"(%251) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %253 = arith.subf %199, %247 : tensor<128xf32>
        %254 = tt.extern_elementwise %253 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %255 = arith.mulf %208, %254 : tensor<128xf32>
        %256 = arith.addf %255, %252 : tensor<128xf32>
        %257 = tt.expand_dims %254 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %258 = tt.broadcast %257 : tensor<128x1xf32> -> tensor<128x64xf32>
        %259 = arith.mulf %225, %258 : tensor<128x64xf32>
        %260 = tt.expand_dims %139 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %261 = arith.addi %232, %260 : tensor<64x1xi32>
        %262 = tt.expand_dims %229 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %263 = arith.muli %262, %cst : tensor<1x64xi32>
        %264 = tt.broadcast %261 : tensor<64x1xi32> -> tensor<64x64xi32>
        %265 = tt.broadcast %263 : tensor<1x64xi32> -> tensor<64x64xi32>
        %266 = arith.addi %264, %265 : tensor<64x64xi32>
        %267 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %268 = tt.addptr %267, %266 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %269 = tt.load %268 : tensor<64x64x!tt.ptr<f8E5M2>>
        %270 = tt.fp_to_fp %251, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %271 = tt.trans %269 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %272 = tt.dot %270, %271, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %273 = arith.addf %259, %272 : tensor<128x64xf32>
        %c2_i32_16 = arith.constant 2 : i32
        %274 = arith.muli %c64_i32, %c2_i32_16 : i32
        %275 = arith.addi %arg7, %274 : i32
        %276 = tt.splat %275 : i32 -> tensor<64xi32>
        %277 = arith.addi %276, %139 : tensor<64xi32>
        %278 = tt.expand_dims %277 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %279 = arith.muli %278, %cst_1 : tensor<64x1xi32>
        %280 = tt.splat %141 : i32 -> tensor<64x1xi32>
        %281 = arith.addi %280, %279 : tensor<64x1xi32>
        %282 = tt.broadcast %281 : tensor<64x1xi32> -> tensor<64x64xi32>
        %283 = tt.broadcast %146 : tensor<1x64xi32> -> tensor<64x64xi32>
        %284 = arith.addi %282, %283 : tensor<64x64xi32>
        %285 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %286 = tt.addptr %285, %284 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %287 = tt.load %286 : tensor<64x64x!tt.ptr<f8E5M2>>
        %288 = tt.trans %287 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %289 = tt.dot %152, %288, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %290 = arith.mulf %289, %cst_0 : tensor<128x64xf32>
        %291 = "tt.reduce"(%290) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %292 = arith.cmpf ogt, %247, %291 : tensor<128xf32>
        %293 = arith.cmpf une, %247, %247 : tensor<128xf32>
        %294 = arith.ori %292, %293 : tensor<128xi1>
        %295 = arith.select %294, %247, %291 : tensor<128xi1>, tensor<128xf32>
        %296 = tt.expand_dims %295 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %297 = tt.broadcast %296 : tensor<128x1xf32> -> tensor<128x64xf32>
        %298 = arith.subf %290, %297 : tensor<128x64xf32>
        %299 = tt.extern_elementwise %298 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %300 = "tt.reduce"(%299) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %301 = arith.subf %247, %295 : tensor<128xf32>
        %302 = tt.extern_elementwise %301 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %303 = arith.mulf %256, %302 : tensor<128xf32>
        %304 = arith.addf %303, %300 : tensor<128xf32>
        %305 = tt.expand_dims %302 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %306 = tt.broadcast %305 : tensor<128x1xf32> -> tensor<128x64xf32>
        %307 = arith.mulf %273, %306 : tensor<128x64xf32>
        %308 = tt.expand_dims %139 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %309 = arith.addi %280, %308 : tensor<64x1xi32>
        %310 = tt.expand_dims %277 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %311 = arith.muli %310, %cst : tensor<1x64xi32>
        %312 = tt.broadcast %309 : tensor<64x1xi32> -> tensor<64x64xi32>
        %313 = tt.broadcast %311 : tensor<1x64xi32> -> tensor<64x64xi32>
        %314 = arith.addi %312, %313 : tensor<64x64xi32>
        %315 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %316 = tt.addptr %315, %314 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %317 = tt.load %316 : tensor<64x64x!tt.ptr<f8E5M2>>
        %318 = tt.fp_to_fp %299, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %319 = tt.trans %317 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %320 = tt.dot %318, %319, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %321 = arith.addf %307, %320 : tensor<128x64xf32>
        %c3_i32_17 = arith.constant 3 : i32
        %322 = arith.muli %c64_i32, %c3_i32_17 : i32
        %323 = arith.addi %arg7, %322 : i32
        %324 = tt.splat %323 : i32 -> tensor<64xi32>
        %325 = arith.addi %324, %139 : tensor<64xi32>
        %326 = tt.expand_dims %325 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %327 = arith.muli %326, %cst_1 : tensor<64x1xi32>
        %328 = tt.splat %141 : i32 -> tensor<64x1xi32>
        %329 = arith.addi %328, %327 : tensor<64x1xi32>
        %330 = tt.broadcast %329 : tensor<64x1xi32> -> tensor<64x64xi32>
        %331 = tt.broadcast %146 : tensor<1x64xi32> -> tensor<64x64xi32>
        %332 = arith.addi %330, %331 : tensor<64x64xi32>
        %333 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %334 = tt.addptr %333, %332 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %335 = tt.load %334 : tensor<64x64x!tt.ptr<f8E5M2>>
        %336 = tt.trans %335 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %337 = tt.dot %152, %336, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %338 = arith.mulf %337, %cst_0 : tensor<128x64xf32>
        %339 = "tt.reduce"(%338) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %340 = arith.cmpf ogt, %295, %339 : tensor<128xf32>
        %341 = arith.cmpf une, %295, %295 : tensor<128xf32>
        %342 = arith.ori %340, %341 : tensor<128xi1>
        %343 = arith.select %342, %295, %339 : tensor<128xi1>, tensor<128xf32>
        %344 = tt.expand_dims %343 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %345 = tt.broadcast %344 : tensor<128x1xf32> -> tensor<128x64xf32>
        %346 = arith.subf %338, %345 : tensor<128x64xf32>
        %347 = tt.extern_elementwise %346 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %348 = "tt.reduce"(%347) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %349 = arith.subf %295, %343 : tensor<128xf32>
        %350 = tt.extern_elementwise %349 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %351 = arith.mulf %304, %350 : tensor<128xf32>
        %352 = arith.addf %351, %348 : tensor<128xf32>
        %353 = tt.expand_dims %350 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %354 = tt.broadcast %353 : tensor<128x1xf32> -> tensor<128x64xf32>
        %355 = arith.mulf %321, %354 : tensor<128x64xf32>
        %356 = tt.expand_dims %139 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %357 = arith.addi %328, %356 : tensor<64x1xi32>
        %358 = tt.expand_dims %325 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %359 = arith.muli %358, %cst : tensor<1x64xi32>
        %360 = tt.broadcast %357 : tensor<64x1xi32> -> tensor<64x64xi32>
        %361 = tt.broadcast %359 : tensor<1x64xi32> -> tensor<64x64xi32>
        %362 = arith.addi %360, %361 : tensor<64x64xi32>
        %363 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %364 = tt.addptr %363, %362 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %365 = tt.load %364 : tensor<64x64x!tt.ptr<f8E5M2>>
        %366 = tt.fp_to_fp %347, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %367 = tt.trans %365 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %368 = tt.dot %366, %367, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %369 = arith.addf %355, %368 : tensor<128x64xf32>
        scf.yield %352, %369, %343 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.disallow_acc_multi_buffer, tt.num_stages = 1 : i32}
      %154:3 = scf.for %arg7 = %c0_i32_13 to %c128_i32 step %c64_i32 iter_args(%arg8 = %153#0, %arg9 = %153#1, %arg10 = %153#2) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %180 = tt.splat %arg7 : i32 -> tensor<64xi32>
        %181 = arith.addi %180, %139 : tensor<64xi32>
        %182 = tt.expand_dims %181 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %183 = arith.muli %182, %cst_1 : tensor<64x1xi32>
        %184 = tt.splat %141 : i32 -> tensor<64x1xi32>
        %185 = arith.addi %184, %183 : tensor<64x1xi32>
        %186 = tt.broadcast %185 : tensor<64x1xi32> -> tensor<64x64xi32>
        %187 = tt.broadcast %146 : tensor<1x64xi32> -> tensor<64x64xi32>
        %188 = arith.addi %186, %187 : tensor<64x64xi32>
        %189 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %190 = tt.addptr %189, %188 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %191 = tt.load %190 : tensor<64x64x!tt.ptr<f8E5M2>>
        %192 = tt.trans %191 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %193 = tt.dot %152, %192, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %194 = arith.mulf %193, %cst_0 : tensor<128x64xf32>
        %195 = "tt.reduce"(%194) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %226 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %226 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %196 = arith.cmpf ogt, %arg10, %195 : tensor<128xf32>
        %197 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %198 = arith.ori %196, %197 : tensor<128xi1>
        %199 = arith.select %198, %arg10, %195 : tensor<128xi1>, tensor<128xf32>
        %200 = tt.expand_dims %199 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %201 = tt.broadcast %200 : tensor<128x1xf32> -> tensor<128x64xf32>
        %202 = arith.subf %194, %201 : tensor<128x64xf32>
        %203 = tt.extern_elementwise %202 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %204 = "tt.reduce"(%203) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %226 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %226 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %205 = arith.subf %arg10, %199 : tensor<128xf32>
        %206 = tt.extern_elementwise %205 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %207 = arith.mulf %arg8, %206 : tensor<128xf32>
        %208 = arith.addf %207, %204 : tensor<128xf32>
        %209 = tt.expand_dims %206 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %210 = tt.broadcast %209 : tensor<128x1xf32> -> tensor<128x64xf32>
        %211 = arith.mulf %arg9, %210 : tensor<128x64xf32>
        %212 = tt.expand_dims %139 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %213 = arith.addi %184, %212 : tensor<64x1xi32>
        %214 = tt.expand_dims %181 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %215 = arith.muli %214, %cst : tensor<1x64xi32>
        %216 = tt.broadcast %213 : tensor<64x1xi32> -> tensor<64x64xi32>
        %217 = tt.broadcast %215 : tensor<1x64xi32> -> tensor<64x64xi32>
        %218 = arith.addi %216, %217 : tensor<64x64xi32>
        %219 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %220 = tt.addptr %219, %218 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %221 = tt.load %220 : tensor<64x64x!tt.ptr<f8E5M2>>
        %222 = tt.fp_to_fp %203, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %223 = tt.trans %221 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %224 = tt.dot %222, %223, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %225 = arith.addf %211, %224 : tensor<128x64xf32>
        scf.yield %208, %225, %199 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.disallow_acc_multi_buffer, tt.num_stages = 1 : i32}
      %155 = tt.expand_dims %154#0 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
      %156 = tt.broadcast %155 : tensor<128x1xf32> -> tensor<128x64xf32>
      %157 = arith.divf %154#1, %156 : tensor<128x64xf32>
      %158 = tt.fp_to_fp %157, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
      %159 = arith.divsi %138, %arg5 : i32
      %160 = arith.remsi %138, %arg5 : i32
      %161 = arith.cmpi ne, %160, %c0_i32 : i32
      %162 = arith.subi %159, %c1_i32 : i32
      %163 = arith.select %161, %162, %159 : i32
      %164 = arith.cmpi slt, %138, %c0_i32 : i32
      %165 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %166 = arith.cmpi ne, %164, %165 : i1
      %167 = arith.select %166, %163, %159 : i32
      %168 = arith.andi %161, %166 : i1
      %169 = arith.addi %160, %arg5 : i32
      %170 = arith.select %168, %169, %160 : i32
      %171 = arith.muli %167, %arg4 : i32
      %172 = arith.muli %170, %c8192_i32 : i32
      %173 = arith.addi %171, %172 : i32
      %174 = tt.splat %173 : i32 -> tensor<128x1xi32>
      %175 = arith.addi %174, %143 : tensor<128x1xi32>
      %176 = tt.broadcast %175 : tensor<128x1xi32> -> tensor<128x64xi32>
      %177 = arith.addi %176, %148 : tensor<128x64xi32>
      %178 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %179 = tt.addptr %178, %177 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      tt.store %179, %158 : tensor<128x64x!tt.ptr<f8E5M2>>
    } {tt.disallow_acc_multi_buffer, tt.flatten}
    scf.for %arg6 = %8 to %c192_i32 step %c132_i32  : i32 {
      %10 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
      %11 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>
      %12 = arith.muli %arg6, %c8192_i32 : i32
      %13 = tt.expand_dims %11 {axis = 1 : i32} : tensor<128xi32> -> tensor<128x1xi32>
      %14 = arith.muli %13, %cst_2 : tensor<128x1xi32>
      %15 = tt.splat %12 : i32 -> tensor<128x1xi32>
      %16 = arith.addi %15, %14 : tensor<128x1xi32>
      %17 = tt.expand_dims %10 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %18 = tt.broadcast %16 : tensor<128x1xi32> -> tensor<128x64xi32>
      %19 = tt.broadcast %17 : tensor<1x64xi32> -> tensor<128x64xi32>
      %20 = arith.addi %18, %19 : tensor<128x64xi32>
      %21 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %22 = tt.addptr %21, %20 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      %23 = tt.load %22 : tensor<128x64x!tt.ptr<f8E5M2>>
      %c0_i32_7 = arith.constant 0 : i32
      %c256_i32 = arith.constant 256 : i32
      %24:3 = scf.for %arg7 = %c0_i32 to %c0_i32_7 step %c256_i32 iter_args(%arg8 = %cst_4, %arg9 = %cst_3, %arg10 = %cst_5) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %51 = tt.splat %arg7 : i32 -> tensor<64xi32>
        %52 = arith.addi %51, %10 : tensor<64xi32>
        %53 = tt.expand_dims %52 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %54 = arith.muli %53, %cst_1 : tensor<64x1xi32>
        %55 = tt.splat %12 : i32 -> tensor<64x1xi32>
        %56 = arith.addi %55, %54 : tensor<64x1xi32>
        %57 = tt.broadcast %56 : tensor<64x1xi32> -> tensor<64x64xi32>
        %58 = tt.broadcast %17 : tensor<1x64xi32> -> tensor<64x64xi32>
        %59 = arith.addi %57, %58 : tensor<64x64xi32>
        %60 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %61 = tt.addptr %60, %59 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %62 = tt.load %61 : tensor<64x64x!tt.ptr<f8E5M2>>
        %63 = tt.trans %62 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %64 = tt.dot %23, %63, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %65 = arith.mulf %64, %cst_0 : tensor<128x64xf32>
        %66 = "tt.reduce"(%65) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %241 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %241 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %67 = arith.cmpf ogt, %arg10, %66 : tensor<128xf32>
        %68 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %69 = arith.ori %67, %68 : tensor<128xi1>
        %70 = arith.select %69, %arg10, %66 : tensor<128xi1>, tensor<128xf32>
        %71 = tt.expand_dims %70 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %72 = tt.broadcast %71 : tensor<128x1xf32> -> tensor<128x64xf32>
        %73 = arith.subf %65, %72 : tensor<128x64xf32>
        %74 = tt.extern_elementwise %73 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %75 = "tt.reduce"(%74) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %241 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %241 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %76 = arith.subf %arg10, %70 : tensor<128xf32>
        %77 = tt.extern_elementwise %76 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %78 = arith.mulf %arg8, %77 : tensor<128xf32>
        %79 = arith.addf %78, %75 : tensor<128xf32>
        %80 = tt.expand_dims %77 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %81 = tt.broadcast %80 : tensor<128x1xf32> -> tensor<128x64xf32>
        %82 = arith.mulf %arg9, %81 : tensor<128x64xf32>
        %83 = tt.expand_dims %10 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %84 = arith.addi %55, %83 : tensor<64x1xi32>
        %85 = tt.expand_dims %52 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %86 = arith.muli %85, %cst : tensor<1x64xi32>
        %87 = tt.broadcast %84 : tensor<64x1xi32> -> tensor<64x64xi32>
        %88 = tt.broadcast %86 : tensor<1x64xi32> -> tensor<64x64xi32>
        %89 = arith.addi %87, %88 : tensor<64x64xi32>
        %90 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %91 = tt.addptr %90, %89 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %92 = tt.load %91 : tensor<64x64x!tt.ptr<f8E5M2>>
        %93 = tt.fp_to_fp %74, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %94 = tt.trans %92 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %95 = tt.dot %93, %94, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %96 = arith.addf %82, %95 : tensor<128x64xf32>
        %c1_i32_8 = arith.constant 1 : i32
        %97 = arith.muli %c64_i32, %c1_i32_8 : i32
        %98 = arith.addi %arg7, %97 : i32
        %99 = tt.splat %98 : i32 -> tensor<64xi32>
        %100 = arith.addi %99, %10 : tensor<64xi32>
        %101 = tt.expand_dims %100 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %102 = arith.muli %101, %cst_1 : tensor<64x1xi32>
        %103 = tt.splat %12 : i32 -> tensor<64x1xi32>
        %104 = arith.addi %103, %102 : tensor<64x1xi32>
        %105 = tt.broadcast %104 : tensor<64x1xi32> -> tensor<64x64xi32>
        %106 = tt.broadcast %17 : tensor<1x64xi32> -> tensor<64x64xi32>
        %107 = arith.addi %105, %106 : tensor<64x64xi32>
        %108 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %109 = tt.addptr %108, %107 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %110 = tt.load %109 : tensor<64x64x!tt.ptr<f8E5M2>>
        %111 = tt.trans %110 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %112 = tt.dot %23, %111, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %113 = arith.mulf %112, %cst_0 : tensor<128x64xf32>
        %114 = "tt.reduce"(%113) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %241 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %241 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %115 = arith.cmpf ogt, %70, %114 : tensor<128xf32>
        %116 = arith.cmpf une, %70, %70 : tensor<128xf32>
        %117 = arith.ori %115, %116 : tensor<128xi1>
        %118 = arith.select %117, %70, %114 : tensor<128xi1>, tensor<128xf32>
        %119 = tt.expand_dims %118 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %120 = tt.broadcast %119 : tensor<128x1xf32> -> tensor<128x64xf32>
        %121 = arith.subf %113, %120 : tensor<128x64xf32>
        %122 = tt.extern_elementwise %121 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %123 = "tt.reduce"(%122) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %241 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %241 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %124 = arith.subf %70, %118 : tensor<128xf32>
        %125 = tt.extern_elementwise %124 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %126 = arith.mulf %79, %125 : tensor<128xf32>
        %127 = arith.addf %126, %123 : tensor<128xf32>
        %128 = tt.expand_dims %125 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %129 = tt.broadcast %128 : tensor<128x1xf32> -> tensor<128x64xf32>
        %130 = arith.mulf %96, %129 : tensor<128x64xf32>
        %131 = tt.expand_dims %10 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %132 = arith.addi %103, %131 : tensor<64x1xi32>
        %133 = tt.expand_dims %100 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %134 = arith.muli %133, %cst : tensor<1x64xi32>
        %135 = tt.broadcast %132 : tensor<64x1xi32> -> tensor<64x64xi32>
        %136 = tt.broadcast %134 : tensor<1x64xi32> -> tensor<64x64xi32>
        %137 = arith.addi %135, %136 : tensor<64x64xi32>
        %138 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %139 = tt.addptr %138, %137 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %140 = tt.load %139 : tensor<64x64x!tt.ptr<f8E5M2>>
        %141 = tt.fp_to_fp %122, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %142 = tt.trans %140 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %143 = tt.dot %141, %142, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %144 = arith.addf %130, %143 : tensor<128x64xf32>
        %c2_i32 = arith.constant 2 : i32
        %145 = arith.muli %c64_i32, %c2_i32 : i32
        %146 = arith.addi %arg7, %145 : i32
        %147 = tt.splat %146 : i32 -> tensor<64xi32>
        %148 = arith.addi %147, %10 : tensor<64xi32>
        %149 = tt.expand_dims %148 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %150 = arith.muli %149, %cst_1 : tensor<64x1xi32>
        %151 = tt.splat %12 : i32 -> tensor<64x1xi32>
        %152 = arith.addi %151, %150 : tensor<64x1xi32>
        %153 = tt.broadcast %152 : tensor<64x1xi32> -> tensor<64x64xi32>
        %154 = tt.broadcast %17 : tensor<1x64xi32> -> tensor<64x64xi32>
        %155 = arith.addi %153, %154 : tensor<64x64xi32>
        %156 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %157 = tt.addptr %156, %155 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %158 = tt.load %157 : tensor<64x64x!tt.ptr<f8E5M2>>
        %159 = tt.trans %158 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %160 = tt.dot %23, %159, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %161 = arith.mulf %160, %cst_0 : tensor<128x64xf32>
        %162 = "tt.reduce"(%161) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %241 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %241 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %163 = arith.cmpf ogt, %118, %162 : tensor<128xf32>
        %164 = arith.cmpf une, %118, %118 : tensor<128xf32>
        %165 = arith.ori %163, %164 : tensor<128xi1>
        %166 = arith.select %165, %118, %162 : tensor<128xi1>, tensor<128xf32>
        %167 = tt.expand_dims %166 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %168 = tt.broadcast %167 : tensor<128x1xf32> -> tensor<128x64xf32>
        %169 = arith.subf %161, %168 : tensor<128x64xf32>
        %170 = tt.extern_elementwise %169 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %171 = "tt.reduce"(%170) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %241 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %241 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %172 = arith.subf %118, %166 : tensor<128xf32>
        %173 = tt.extern_elementwise %172 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %174 = arith.mulf %127, %173 : tensor<128xf32>
        %175 = arith.addf %174, %171 : tensor<128xf32>
        %176 = tt.expand_dims %173 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %177 = tt.broadcast %176 : tensor<128x1xf32> -> tensor<128x64xf32>
        %178 = arith.mulf %144, %177 : tensor<128x64xf32>
        %179 = tt.expand_dims %10 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %180 = arith.addi %151, %179 : tensor<64x1xi32>
        %181 = tt.expand_dims %148 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %182 = arith.muli %181, %cst : tensor<1x64xi32>
        %183 = tt.broadcast %180 : tensor<64x1xi32> -> tensor<64x64xi32>
        %184 = tt.broadcast %182 : tensor<1x64xi32> -> tensor<64x64xi32>
        %185 = arith.addi %183, %184 : tensor<64x64xi32>
        %186 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %187 = tt.addptr %186, %185 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %188 = tt.load %187 : tensor<64x64x!tt.ptr<f8E5M2>>
        %189 = tt.fp_to_fp %170, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %190 = tt.trans %188 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %191 = tt.dot %189, %190, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %192 = arith.addf %178, %191 : tensor<128x64xf32>
        %c3_i32 = arith.constant 3 : i32
        %193 = arith.muli %c64_i32, %c3_i32 : i32
        %194 = arith.addi %arg7, %193 : i32
        %195 = tt.splat %194 : i32 -> tensor<64xi32>
        %196 = arith.addi %195, %10 : tensor<64xi32>
        %197 = tt.expand_dims %196 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %198 = arith.muli %197, %cst_1 : tensor<64x1xi32>
        %199 = tt.splat %12 : i32 -> tensor<64x1xi32>
        %200 = arith.addi %199, %198 : tensor<64x1xi32>
        %201 = tt.broadcast %200 : tensor<64x1xi32> -> tensor<64x64xi32>
        %202 = tt.broadcast %17 : tensor<1x64xi32> -> tensor<64x64xi32>
        %203 = arith.addi %201, %202 : tensor<64x64xi32>
        %204 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %205 = tt.addptr %204, %203 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %206 = tt.load %205 : tensor<64x64x!tt.ptr<f8E5M2>>
        %207 = tt.trans %206 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %208 = tt.dot %23, %207, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %209 = arith.mulf %208, %cst_0 : tensor<128x64xf32>
        %210 = "tt.reduce"(%209) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %241 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %241 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %211 = arith.cmpf ogt, %166, %210 : tensor<128xf32>
        %212 = arith.cmpf une, %166, %166 : tensor<128xf32>
        %213 = arith.ori %211, %212 : tensor<128xi1>
        %214 = arith.select %213, %166, %210 : tensor<128xi1>, tensor<128xf32>
        %215 = tt.expand_dims %214 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %216 = tt.broadcast %215 : tensor<128x1xf32> -> tensor<128x64xf32>
        %217 = arith.subf %209, %216 : tensor<128x64xf32>
        %218 = tt.extern_elementwise %217 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %219 = "tt.reduce"(%218) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %241 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %241 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %220 = arith.subf %166, %214 : tensor<128xf32>
        %221 = tt.extern_elementwise %220 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %222 = arith.mulf %175, %221 : tensor<128xf32>
        %223 = arith.addf %222, %219 : tensor<128xf32>
        %224 = tt.expand_dims %221 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %225 = tt.broadcast %224 : tensor<128x1xf32> -> tensor<128x64xf32>
        %226 = arith.mulf %192, %225 : tensor<128x64xf32>
        %227 = tt.expand_dims %10 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %228 = arith.addi %199, %227 : tensor<64x1xi32>
        %229 = tt.expand_dims %196 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %230 = arith.muli %229, %cst : tensor<1x64xi32>
        %231 = tt.broadcast %228 : tensor<64x1xi32> -> tensor<64x64xi32>
        %232 = tt.broadcast %230 : tensor<1x64xi32> -> tensor<64x64xi32>
        %233 = arith.addi %231, %232 : tensor<64x64xi32>
        %234 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %235 = tt.addptr %234, %233 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %236 = tt.load %235 : tensor<64x64x!tt.ptr<f8E5M2>>
        %237 = tt.fp_to_fp %218, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %238 = tt.trans %236 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %239 = tt.dot %237, %238, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %240 = arith.addf %226, %239 : tensor<128x64xf32>
        scf.yield %223, %240, %214 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.disallow_acc_multi_buffer, tt.num_stages = 1 : i32}
      %25:3 = scf.for %arg7 = %c0_i32_7 to %c128_i32 step %c64_i32 iter_args(%arg8 = %24#0, %arg9 = %24#1, %arg10 = %24#2) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %51 = tt.splat %arg7 : i32 -> tensor<64xi32>
        %52 = arith.addi %51, %10 : tensor<64xi32>
        %53 = tt.expand_dims %52 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %54 = arith.muli %53, %cst_1 : tensor<64x1xi32>
        %55 = tt.splat %12 : i32 -> tensor<64x1xi32>
        %56 = arith.addi %55, %54 : tensor<64x1xi32>
        %57 = tt.broadcast %56 : tensor<64x1xi32> -> tensor<64x64xi32>
        %58 = tt.broadcast %17 : tensor<1x64xi32> -> tensor<64x64xi32>
        %59 = arith.addi %57, %58 : tensor<64x64xi32>
        %60 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %61 = tt.addptr %60, %59 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %62 = tt.load %61 : tensor<64x64x!tt.ptr<f8E5M2>>
        %63 = tt.trans %62 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %64 = tt.dot %23, %63, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %65 = arith.mulf %64, %cst_0 : tensor<128x64xf32>
        %66 = "tt.reduce"(%65) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %97 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %97 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %67 = arith.cmpf ogt, %arg10, %66 : tensor<128xf32>
        %68 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %69 = arith.ori %67, %68 : tensor<128xi1>
        %70 = arith.select %69, %arg10, %66 : tensor<128xi1>, tensor<128xf32>
        %71 = tt.expand_dims %70 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %72 = tt.broadcast %71 : tensor<128x1xf32> -> tensor<128x64xf32>
        %73 = arith.subf %65, %72 : tensor<128x64xf32>
        %74 = tt.extern_elementwise %73 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %75 = "tt.reduce"(%74) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %97 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %97 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %76 = arith.subf %arg10, %70 : tensor<128xf32>
        %77 = tt.extern_elementwise %76 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %78 = arith.mulf %arg8, %77 : tensor<128xf32>
        %79 = arith.addf %78, %75 : tensor<128xf32>
        %80 = tt.expand_dims %77 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %81 = tt.broadcast %80 : tensor<128x1xf32> -> tensor<128x64xf32>
        %82 = arith.mulf %arg9, %81 : tensor<128x64xf32>
        %83 = tt.expand_dims %10 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %84 = arith.addi %55, %83 : tensor<64x1xi32>
        %85 = tt.expand_dims %52 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %86 = arith.muli %85, %cst : tensor<1x64xi32>
        %87 = tt.broadcast %84 : tensor<64x1xi32> -> tensor<64x64xi32>
        %88 = tt.broadcast %86 : tensor<1x64xi32> -> tensor<64x64xi32>
        %89 = arith.addi %87, %88 : tensor<64x64xi32>
        %90 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %91 = tt.addptr %90, %89 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %92 = tt.load %91 : tensor<64x64x!tt.ptr<f8E5M2>>
        %93 = tt.fp_to_fp %74, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %94 = tt.trans %92 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %95 = tt.dot %93, %94, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %96 = arith.addf %82, %95 : tensor<128x64xf32>
        scf.yield %79, %96, %70 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.disallow_acc_multi_buffer, tt.num_stages = 1 : i32}
      %26 = tt.expand_dims %25#0 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
      %27 = tt.broadcast %26 : tensor<128x1xf32> -> tensor<128x64xf32>
      %28 = arith.divf %25#1, %27 : tensor<128x64xf32>
      %29 = tt.fp_to_fp %28, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
      %30 = arith.divsi %arg6, %arg5 : i32
      %31 = arith.remsi %arg6, %arg5 : i32
      %32 = arith.cmpi ne, %31, %c0_i32 : i32
      %33 = arith.subi %30, %c1_i32 : i32
      %34 = arith.select %32, %33, %30 : i32
      %35 = arith.cmpi slt, %arg6, %c0_i32 : i32
      %36 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %37 = arith.cmpi ne, %35, %36 : i1
      %38 = arith.select %37, %34, %30 : i32
      %39 = arith.andi %32, %37 : i1
      %40 = arith.addi %31, %arg5 : i32
      %41 = arith.select %39, %40, %31 : i32
      %42 = arith.muli %38, %arg4 : i32
      %43 = arith.muli %41, %c8192_i32 : i32
      %44 = arith.addi %42, %43 : i32
      %45 = tt.splat %44 : i32 -> tensor<128x1xi32>
      %46 = arith.addi %45, %14 : tensor<128x1xi32>
      %47 = tt.broadcast %46 : tensor<128x1xi32> -> tensor<128x64xi32>
      %48 = arith.addi %47, %19 : tensor<128x64xi32>
      %49 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %50 = tt.addptr %49, %48 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      tt.store %50, %29 : tensor<128x64x!tt.ptr<f8E5M2>>
    } {tt.disallow_acc_multi_buffer, tt.flatten, tt.num_stages = 1 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(convert-triton-to-tritongpu{enable-source-remat=false num-ctas=1 num-warps=16 target=cuda:90 threads-per-warp=32}, tritongpu-coalesce, tritongpu-F32DotTC, triton-nvidia-gpu-plan-cta, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritongpu-accelerate-matmul, tritongpu-remove-layout-conversions, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, triton-nvidia-optimize-descriptor-encoding, triton-loop-aware-cse, tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-combine-tensor-select-and-if, nvgpu-warp-specialization{dump-intermediate-steps=false num-stages=4}, tritongpu-assign-latencies{num-stages=4}, tritongpu-schedule-loops, tritongpu-pipeline{dump-intermediate-steps=false num-stages=4}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-loop-aware-cse, tritongpu-prefetch, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-coalesce-async-copy, triton-nvidia-optimize-tmem-layouts, tritongpu-remove-layout-conversions, triton-nvidia-interleave-tmem, tritongpu-reduce-data-duplication, tritongpu-reorder-instructions, triton-loop-aware-cse, symbol-dce, triton-nvidia-tma-lowering, triton-nvidia-gpu-fence-insertion{compute-capability=90}, sccp, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true})",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_willfeng/3w/c3wooj3qjzyiukhwuvkk5um2qqi5ql2mrok7myhpr6qbpmbowxcv.py:13:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_willfeng/3w/c3wooj3qjzyiukhwuvkk5um2qqi5ql2mrok7myhpr6qbpmbowxcv.py:13:0: note: Pipeline failed while executing [`TritonGPUAssignLatencies` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
Process ForkProcess-161:
Traceback (most recent call last):
  File "/home/willfeng/local/miniconda3/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/willfeng/local/miniconda3/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/data/users/willfeng/helion/helion/runtime/precompile_shim.py", line 54, in finish_it
    kernel_cache[key] = fn.compile(
                        ^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/compiler/compiler.py", line 359, in compile
    next_module = compile_ir(module, metadata)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/backends/nvidia/compiler.py", line 456, in <lambda>
    stages["ttgir"] = lambda src, metadata: self.make_ttgir(src, metadata, options, capability)
                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/backends/nvidia/compiler.py", line 298, in make_ttgir
    pm.run(mod)
RuntimeError: PassManager::run failed
python: /root/.triton/llvm/llvm-8957e64a-almalinux-x64/include/llvm/ADT/SmallVector.h:296: const_reference llvm::SmallVectorTemplateCommon<long>::operator[](size_type) const [T = long]: Assertion `idx < size()' failed.
module {
  tt.func public @_fp8_attention_kernel_kernel(%arg0: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %c1_i32 = arith.constant 1 : i32
    %c64_i32 = arith.constant 64 : i32
    %c128_i32 = arith.constant 128 : i32
    %c0_i32 = arith.constant 0 : i32
    %c132_i32 = arith.constant 132 : i32
    %c192_i32 = arith.constant 192 : i32
    %cst = arith.constant dense<64> : tensor<1x64xi32>
    %cst_0 = arith.constant dense<0.180336878> : tensor<128x64xf32>
    %cst_1 = arith.constant dense<64> : tensor<64x1xi32>
    %cst_2 = arith.constant dense<64> : tensor<128x1xi32>
    %c8192_i32 = arith.constant 8192 : i32
    %cst_3 = arith.constant dense<0.000000e+00> : tensor<128x64xf32>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<128xf32>
    %cst_5 = arith.constant dense<0xFF800000> : tensor<128xf32>
    %0 = tt.get_program_id x : i32
    %1 = arith.subi %c192_i32, %0 : i32
    %c1_i32_6 = arith.constant 1 : i32
    %2 = arith.subi %c132_i32, %c1_i32_6 : i32
    %3 = arith.addi %1, %2 : i32
    %4 = arith.divui %3, %c132_i32 : i32
    %c4_i32 = arith.constant 4 : i32
    %5 = arith.remsi %4, %c4_i32 : i32
    %6 = arith.subi %4, %5 : i32
    %7 = arith.muli %6, %c132_i32 : i32
    %8 = arith.addi %0, %7 : i32
    %9 = arith.muli %c132_i32, %c4_i32 : i32
    scf.for %arg6 = %0 to %8 step %9  : i32 {
      %10 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
      %11 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>
      %12 = arith.muli %arg6, %c8192_i32 : i32
      %13 = tt.expand_dims %11 {axis = 1 : i32} : tensor<128xi32> -> tensor<128x1xi32>
      %14 = arith.muli %13, %cst_2 : tensor<128x1xi32>
      %15 = tt.splat %12 : i32 -> tensor<128x1xi32>
      %16 = arith.addi %15, %14 : tensor<128x1xi32>
      %17 = tt.expand_dims %10 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %18 = tt.broadcast %16 : tensor<128x1xi32> -> tensor<128x64xi32>
      %19 = tt.broadcast %17 : tensor<1x64xi32> -> tensor<128x64xi32>
      %20 = arith.addi %18, %19 : tensor<128x64xi32>
      %21 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %22 = tt.addptr %21, %20 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      %23 = tt.load %22 : tensor<128x64x!tt.ptr<f8E5M2>>
      %c0_i32_7 = arith.constant 0 : i32
      %c256_i32 = arith.constant 256 : i32
      %24:3 = scf.for %arg7 = %c0_i32 to %c0_i32_7 step %c256_i32 iter_args(%arg8 = %cst_4, %arg9 = %cst_3, %arg10 = %cst_5) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %180 = tt.splat %arg7 : i32 -> tensor<64xi32>
        %181 = arith.addi %180, %10 : tensor<64xi32>
        %182 = tt.expand_dims %181 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %183 = arith.muli %182, %cst_1 : tensor<64x1xi32>
        %184 = tt.splat %12 : i32 -> tensor<64x1xi32>
        %185 = arith.addi %184, %183 : tensor<64x1xi32>
        %186 = tt.broadcast %185 : tensor<64x1xi32> -> tensor<64x64xi32>
        %187 = tt.broadcast %17 : tensor<1x64xi32> -> tensor<64x64xi32>
        %188 = arith.addi %186, %187 : tensor<64x64xi32>
        %189 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %190 = tt.addptr %189, %188 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %191 = tt.load %190 : tensor<64x64x!tt.ptr<f8E5M2>>
        %192 = tt.trans %191 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %193 = tt.dot %23, %192, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %194 = arith.mulf %193, %cst_0 : tensor<128x64xf32>
        %195 = "tt.reduce"(%194) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %196 = arith.cmpf ogt, %arg10, %195 : tensor<128xf32>
        %197 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %198 = arith.ori %196, %197 : tensor<128xi1>
        %199 = arith.select %198, %arg10, %195 : tensor<128xi1>, tensor<128xf32>
        %200 = tt.expand_dims %199 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %201 = tt.broadcast %200 : tensor<128x1xf32> -> tensor<128x64xf32>
        %202 = arith.subf %194, %201 : tensor<128x64xf32>
        %203 = tt.extern_elementwise %202 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %204 = "tt.reduce"(%203) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %205 = arith.subf %arg10, %199 : tensor<128xf32>
        %206 = tt.extern_elementwise %205 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %207 = arith.mulf %arg8, %206 : tensor<128xf32>
        %208 = arith.addf %207, %204 : tensor<128xf32>
        %209 = tt.expand_dims %206 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %210 = tt.broadcast %209 : tensor<128x1xf32> -> tensor<128x64xf32>
        %211 = arith.mulf %arg9, %210 : tensor<128x64xf32>
        %212 = tt.expand_dims %10 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %213 = arith.addi %184, %212 : tensor<64x1xi32>
        %214 = tt.expand_dims %181 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %215 = arith.muli %214, %cst : tensor<1x64xi32>
        %216 = tt.broadcast %213 : tensor<64x1xi32> -> tensor<64x64xi32>
        %217 = tt.broadcast %215 : tensor<1x64xi32> -> tensor<64x64xi32>
        %218 = arith.addi %216, %217 : tensor<64x64xi32>
        %219 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %220 = tt.addptr %219, %218 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %221 = tt.load %220 : tensor<64x64x!tt.ptr<f8E5M2>>
        %222 = tt.fp_to_fp %203, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %223 = tt.trans %221 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %224 = tt.dot %222, %223, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %225 = arith.addf %211, %224 : tensor<128x64xf32>
        %c1_i32_15 = arith.constant 1 : i32
        %226 = arith.muli %c64_i32, %c1_i32_15 : i32
        %227 = arith.addi %arg7, %226 : i32
        %228 = tt.splat %227 : i32 -> tensor<64xi32>
        %229 = arith.addi %228, %10 : tensor<64xi32>
        %230 = tt.expand_dims %229 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %231 = arith.muli %230, %cst_1 : tensor<64x1xi32>
        %232 = tt.splat %12 : i32 -> tensor<64x1xi32>
        %233 = arith.addi %232, %231 : tensor<64x1xi32>
        %234 = tt.broadcast %233 : tensor<64x1xi32> -> tensor<64x64xi32>
        %235 = tt.broadcast %17 : tensor<1x64xi32> -> tensor<64x64xi32>
        %236 = arith.addi %234, %235 : tensor<64x64xi32>
        %237 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %238 = tt.addptr %237, %236 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %239 = tt.load %238 : tensor<64x64x!tt.ptr<f8E5M2>>
        %240 = tt.trans %239 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %241 = tt.dot %23, %240, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %242 = arith.mulf %241, %cst_0 : tensor<128x64xf32>
        %243 = "tt.reduce"(%242) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %244 = arith.cmpf ogt, %199, %243 : tensor<128xf32>
        %245 = arith.cmpf une, %199, %199 : tensor<128xf32>
        %246 = arith.ori %244, %245 : tensor<128xi1>
        %247 = arith.select %246, %199, %243 : tensor<128xi1>, tensor<128xf32>
        %248 = tt.expand_dims %247 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %249 = tt.broadcast %248 : tensor<128x1xf32> -> tensor<128x64xf32>
        %250 = arith.subf %242, %249 : tensor<128x64xf32>
        %251 = tt.extern_elementwise %250 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %252 = "tt.reduce"(%251) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %253 = arith.subf %199, %247 : tensor<128xf32>
        %254 = tt.extern_elementwise %253 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %255 = arith.mulf %208, %254 : tensor<128xf32>
        %256 = arith.addf %255, %252 : tensor<128xf32>
        %257 = tt.expand_dims %254 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %258 = tt.broadcast %257 : tensor<128x1xf32> -> tensor<128x64xf32>
        %259 = arith.mulf %225, %258 : tensor<128x64xf32>
        %260 = tt.expand_dims %10 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %261 = arith.addi %232, %260 : tensor<64x1xi32>
        %262 = tt.expand_dims %229 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %263 = arith.muli %262, %cst : tensor<1x64xi32>
        %264 = tt.broadcast %261 : tensor<64x1xi32> -> tensor<64x64xi32>
        %265 = tt.broadcast %263 : tensor<1x64xi32> -> tensor<64x64xi32>
        %266 = arith.addi %264, %265 : tensor<64x64xi32>
        %267 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %268 = tt.addptr %267, %266 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %269 = tt.load %268 : tensor<64x64x!tt.ptr<f8E5M2>>
        %270 = tt.fp_to_fp %251, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %271 = tt.trans %269 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %272 = tt.dot %270, %271, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %273 = arith.addf %259, %272 : tensor<128x64xf32>
        %c2_i32_16 = arith.constant 2 : i32
        %274 = arith.muli %c64_i32, %c2_i32_16 : i32
        %275 = arith.addi %arg7, %274 : i32
        %276 = tt.splat %275 : i32 -> tensor<64xi32>
        %277 = arith.addi %276, %10 : tensor<64xi32>
        %278 = tt.expand_dims %277 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %279 = arith.muli %278, %cst_1 : tensor<64x1xi32>
        %280 = tt.splat %12 : i32 -> tensor<64x1xi32>
        %281 = arith.addi %280, %279 : tensor<64x1xi32>
        %282 = tt.broadcast %281 : tensor<64x1xi32> -> tensor<64x64xi32>
        %283 = tt.broadcast %17 : tensor<1x64xi32> -> tensor<64x64xi32>
        %284 = arith.addi %282, %283 : tensor<64x64xi32>
        %285 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %286 = tt.addptr %285, %284 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %287 = tt.load %286 : tensor<64x64x!tt.ptr<f8E5M2>>
        %288 = tt.trans %287 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %289 = tt.dot %23, %288, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %290 = arith.mulf %289, %cst_0 : tensor<128x64xf32>
        %291 = "tt.reduce"(%290) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %292 = arith.cmpf ogt, %247, %291 : tensor<128xf32>
        %293 = arith.cmpf une, %247, %247 : tensor<128xf32>
        %294 = arith.ori %292, %293 : tensor<128xi1>
        %295 = arith.select %294, %247, %291 : tensor<128xi1>, tensor<128xf32>
        %296 = tt.expand_dims %295 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %297 = tt.broadcast %296 : tensor<128x1xf32> -> tensor<128x64xf32>
        %298 = arith.subf %290, %297 : tensor<128x64xf32>
        %299 = tt.extern_elementwise %298 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %300 = "tt.reduce"(%299) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %301 = arith.subf %247, %295 : tensor<128xf32>
        %302 = tt.extern_elementwise %301 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %303 = arith.mulf %256, %302 : tensor<128xf32>
        %304 = arith.addf %303, %300 : tensor<128xf32>
        %305 = tt.expand_dims %302 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %306 = tt.broadcast %305 : tensor<128x1xf32> -> tensor<128x64xf32>
        %307 = arith.mulf %273, %306 : tensor<128x64xf32>
        %308 = tt.expand_dims %10 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %309 = arith.addi %280, %308 : tensor<64x1xi32>
        %310 = tt.expand_dims %277 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %311 = arith.muli %310, %cst : tensor<1x64xi32>
        %312 = tt.broadcast %309 : tensor<64x1xi32> -> tensor<64x64xi32>
        %313 = tt.broadcast %311 : tensor<1x64xi32> -> tensor<64x64xi32>
        %314 = arith.addi %312, %313 : tensor<64x64xi32>
        %315 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %316 = tt.addptr %315, %314 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %317 = tt.load %316 : tensor<64x64x!tt.ptr<f8E5M2>>
        %318 = tt.fp_to_fp %299, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %319 = tt.trans %317 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %320 = tt.dot %318, %319, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %321 = arith.addf %307, %320 : tensor<128x64xf32>
        %c3_i32_17 = arith.constant 3 : i32
        %322 = arith.muli %c64_i32, %c3_i32_17 : i32
        %323 = arith.addi %arg7, %322 : i32
        %324 = tt.splat %323 : i32 -> tensor<64xi32>
        %325 = arith.addi %324, %10 : tensor<64xi32>
        %326 = tt.expand_dims %325 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %327 = arith.muli %326, %cst_1 : tensor<64x1xi32>
        %328 = tt.splat %12 : i32 -> tensor<64x1xi32>
        %329 = arith.addi %328, %327 : tensor<64x1xi32>
        %330 = tt.broadcast %329 : tensor<64x1xi32> -> tensor<64x64xi32>
        %331 = tt.broadcast %17 : tensor<1x64xi32> -> tensor<64x64xi32>
        %332 = arith.addi %330, %331 : tensor<64x64xi32>
        %333 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %334 = tt.addptr %333, %332 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %335 = tt.load %334 : tensor<64x64x!tt.ptr<f8E5M2>>
        %336 = tt.trans %335 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %337 = tt.dot %23, %336, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %338 = arith.mulf %337, %cst_0 : tensor<128x64xf32>
        %339 = "tt.reduce"(%338) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %340 = arith.cmpf ogt, %295, %339 : tensor<128xf32>
        %341 = arith.cmpf une, %295, %295 : tensor<128xf32>
        %342 = arith.ori %340, %341 : tensor<128xi1>
        %343 = arith.select %342, %295, %339 : tensor<128xi1>, tensor<128xf32>
        %344 = tt.expand_dims %343 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %345 = tt.broadcast %344 : tensor<128x1xf32> -> tensor<128x64xf32>
        %346 = arith.subf %338, %345 : tensor<128x64xf32>
        %347 = tt.extern_elementwise %346 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %348 = "tt.reduce"(%347) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %349 = arith.subf %295, %343 : tensor<128xf32>
        %350 = tt.extern_elementwise %349 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %351 = arith.mulf %304, %350 : tensor<128xf32>
        %352 = arith.addf %351, %348 : tensor<128xf32>
        %353 = tt.expand_dims %350 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %354 = tt.broadcast %353 : tensor<128x1xf32> -> tensor<128x64xf32>
        %355 = arith.mulf %321, %354 : tensor<128x64xf32>
        %356 = tt.expand_dims %10 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %357 = arith.addi %328, %356 : tensor<64x1xi32>
        %358 = tt.expand_dims %325 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %359 = arith.muli %358, %cst : tensor<1x64xi32>
        %360 = tt.broadcast %357 : tensor<64x1xi32> -> tensor<64x64xi32>
        %361 = tt.broadcast %359 : tensor<1x64xi32> -> tensor<64x64xi32>
        %362 = arith.addi %360, %361 : tensor<64x64xi32>
        %363 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %364 = tt.addptr %363, %362 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %365 = tt.load %364 : tensor<64x64x!tt.ptr<f8E5M2>>
        %366 = tt.fp_to_fp %347, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %367 = tt.trans %365 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %368 = tt.dot %366, %367, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %369 = arith.addf %355, %368 : tensor<128x64xf32>
        scf.yield %352, %369, %343 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.disallow_acc_multi_buffer, tt.num_stages = 1 : i32}
      %25:3 = scf.for %arg7 = %c0_i32_7 to %c128_i32 step %c64_i32 iter_args(%arg8 = %24#0, %arg9 = %24#1, %arg10 = %24#2) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %180 = tt.splat %arg7 : i32 -> tensor<64xi32>
        %181 = arith.addi %180, %10 : tensor<64xi32>
        %182 = tt.expand_dims %181 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %183 = arith.muli %182, %cst_1 : tensor<64x1xi32>
        %184 = tt.splat %12 : i32 -> tensor<64x1xi32>
        %185 = arith.addi %184, %183 : tensor<64x1xi32>
        %186 = tt.broadcast %185 : tensor<64x1xi32> -> tensor<64x64xi32>
        %187 = tt.broadcast %17 : tensor<1x64xi32> -> tensor<64x64xi32>
        %188 = arith.addi %186, %187 : tensor<64x64xi32>
        %189 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %190 = tt.addptr %189, %188 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %191 = tt.load %190 : tensor<64x64x!tt.ptr<f8E5M2>>
        %192 = tt.trans %191 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %193 = tt.dot %23, %192, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %194 = arith.mulf %193, %cst_0 : tensor<128x64xf32>
        %195 = "tt.reduce"(%194) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %226 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %226 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %196 = arith.cmpf ogt, %arg10, %195 : tensor<128xf32>
        %197 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %198 = arith.ori %196, %197 : tensor<128xi1>
        %199 = arith.select %198, %arg10, %195 : tensor<128xi1>, tensor<128xf32>
        %200 = tt.expand_dims %199 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %201 = tt.broadcast %200 : tensor<128x1xf32> -> tensor<128x64xf32>
        %202 = arith.subf %194, %201 : tensor<128x64xf32>
        %203 = tt.extern_elementwise %202 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %204 = "tt.reduce"(%203) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %226 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %226 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %205 = arith.subf %arg10, %199 : tensor<128xf32>
        %206 = tt.extern_elementwise %205 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %207 = arith.mulf %arg8, %206 : tensor<128xf32>
        %208 = arith.addf %207, %204 : tensor<128xf32>
        %209 = tt.expand_dims %206 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %210 = tt.broadcast %209 : tensor<128x1xf32> -> tensor<128x64xf32>
        %211 = arith.mulf %arg9, %210 : tensor<128x64xf32>
        %212 = tt.expand_dims %10 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %213 = arith.addi %184, %212 : tensor<64x1xi32>
        %214 = tt.expand_dims %181 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %215 = arith.muli %214, %cst : tensor<1x64xi32>
        %216 = tt.broadcast %213 : tensor<64x1xi32> -> tensor<64x64xi32>
        %217 = tt.broadcast %215 : tensor<1x64xi32> -> tensor<64x64xi32>
        %218 = arith.addi %216, %217 : tensor<64x64xi32>
        %219 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %220 = tt.addptr %219, %218 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %221 = tt.load %220 : tensor<64x64x!tt.ptr<f8E5M2>>
        %222 = tt.fp_to_fp %203, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %223 = tt.trans %221 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %224 = tt.dot %222, %223, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %225 = arith.addf %211, %224 : tensor<128x64xf32>
        scf.yield %208, %225, %199 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.disallow_acc_multi_buffer, tt.num_stages = 1 : i32}
      %26 = tt.expand_dims %25#0 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
      %27 = tt.broadcast %26 : tensor<128x1xf32> -> tensor<128x64xf32>
      %28 = arith.divf %25#1, %27 : tensor<128x64xf32>
      %29 = tt.fp_to_fp %28, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
      %30 = arith.divsi %arg6, %arg5 : i32
      %31 = arith.remsi %arg6, %arg5 : i32
      %32 = arith.cmpi ne, %31, %c0_i32 : i32
      %33 = arith.subi %30, %c1_i32 : i32
      %34 = arith.select %32, %33, %30 : i32
      %35 = arith.cmpi slt, %arg6, %c0_i32 : i32
      %36 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %37 = arith.cmpi ne, %35, %36 : i1
      %38 = arith.select %37, %34, %30 : i32
      %39 = arith.andi %32, %37 : i1
      %40 = arith.addi %31, %arg5 : i32
      %41 = arith.select %39, %40, %31 : i32
      %42 = arith.muli %38, %arg4 : i32
      %43 = arith.muli %41, %c8192_i32 : i32
      %44 = arith.addi %42, %43 : i32
      %45 = tt.splat %44 : i32 -> tensor<128x1xi32>
      %46 = arith.addi %45, %14 : tensor<128x1xi32>
      %47 = tt.broadcast %46 : tensor<128x1xi32> -> tensor<128x64xi32>
      %48 = arith.addi %47, %19 : tensor<128x64xi32>
      %49 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %50 = tt.addptr %49, %48 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      tt.store %50, %29 : tensor<128x64x!tt.ptr<f8E5M2>>
      %c1_i32_8 = arith.constant 1 : i32
      %51 = arith.muli %c132_i32, %c1_i32_8 : i32
      %52 = arith.addi %arg6, %51 : i32
      %53 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
      %54 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>
      %55 = arith.muli %52, %c8192_i32 : i32
      %56 = tt.expand_dims %54 {axis = 1 : i32} : tensor<128xi32> -> tensor<128x1xi32>
      %57 = arith.muli %56, %cst_2 : tensor<128x1xi32>
      %58 = tt.splat %55 : i32 -> tensor<128x1xi32>
      %59 = arith.addi %58, %57 : tensor<128x1xi32>
      %60 = tt.expand_dims %53 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %61 = tt.broadcast %59 : tensor<128x1xi32> -> tensor<128x64xi32>
      %62 = tt.broadcast %60 : tensor<1x64xi32> -> tensor<128x64xi32>
      %63 = arith.addi %61, %62 : tensor<128x64xi32>
      %64 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %65 = tt.addptr %64, %63 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      %66 = tt.load %65 : tensor<128x64x!tt.ptr<f8E5M2>>
      %c0_i32_9 = arith.constant 0 : i32
      %c256_i32_10 = arith.constant 256 : i32
      %67:3 = scf.for %arg7 = %c0_i32 to %c0_i32_9 step %c256_i32_10 iter_args(%arg8 = %cst_4, %arg9 = %cst_3, %arg10 = %cst_5) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %180 = tt.splat %arg7 : i32 -> tensor<64xi32>
        %181 = arith.addi %180, %53 : tensor<64xi32>
        %182 = tt.expand_dims %181 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %183 = arith.muli %182, %cst_1 : tensor<64x1xi32>
        %184 = tt.splat %55 : i32 -> tensor<64x1xi32>
        %185 = arith.addi %184, %183 : tensor<64x1xi32>
        %186 = tt.broadcast %185 : tensor<64x1xi32> -> tensor<64x64xi32>
        %187 = tt.broadcast %60 : tensor<1x64xi32> -> tensor<64x64xi32>
        %188 = arith.addi %186, %187 : tensor<64x64xi32>
        %189 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %190 = tt.addptr %189, %188 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %191 = tt.load %190 : tensor<64x64x!tt.ptr<f8E5M2>>
        %192 = tt.trans %191 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %193 = tt.dot %66, %192, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %194 = arith.mulf %193, %cst_0 : tensor<128x64xf32>
        %195 = "tt.reduce"(%194) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %196 = arith.cmpf ogt, %arg10, %195 : tensor<128xf32>
        %197 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %198 = arith.ori %196, %197 : tensor<128xi1>
        %199 = arith.select %198, %arg10, %195 : tensor<128xi1>, tensor<128xf32>
        %200 = tt.expand_dims %199 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %201 = tt.broadcast %200 : tensor<128x1xf32> -> tensor<128x64xf32>
        %202 = arith.subf %194, %201 : tensor<128x64xf32>
        %203 = tt.extern_elementwise %202 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %204 = "tt.reduce"(%203) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %205 = arith.subf %arg10, %199 : tensor<128xf32>
        %206 = tt.extern_elementwise %205 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %207 = arith.mulf %arg8, %206 : tensor<128xf32>
        %208 = arith.addf %207, %204 : tensor<128xf32>
        %209 = tt.expand_dims %206 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %210 = tt.broadcast %209 : tensor<128x1xf32> -> tensor<128x64xf32>
        %211 = arith.mulf %arg9, %210 : tensor<128x64xf32>
        %212 = tt.expand_dims %53 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %213 = arith.addi %184, %212 : tensor<64x1xi32>
        %214 = tt.expand_dims %181 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %215 = arith.muli %214, %cst : tensor<1x64xi32>
        %216 = tt.broadcast %213 : tensor<64x1xi32> -> tensor<64x64xi32>
        %217 = tt.broadcast %215 : tensor<1x64xi32> -> tensor<64x64xi32>
        %218 = arith.addi %216, %217 : tensor<64x64xi32>
        %219 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %220 = tt.addptr %219, %218 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %221 = tt.load %220 : tensor<64x64x!tt.ptr<f8E5M2>>
        %222 = tt.fp_to_fp %203, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %223 = tt.trans %221 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %224 = tt.dot %222, %223, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %225 = arith.addf %211, %224 : tensor<128x64xf32>
        %c1_i32_15 = arith.constant 1 : i32
        %226 = arith.muli %c64_i32, %c1_i32_15 : i32
        %227 = arith.addi %arg7, %226 : i32
        %228 = tt.splat %227 : i32 -> tensor<64xi32>
        %229 = arith.addi %228, %53 : tensor<64xi32>
        %230 = tt.expand_dims %229 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %231 = arith.muli %230, %cst_1 : tensor<64x1xi32>
        %232 = tt.splat %55 : i32 -> tensor<64x1xi32>
        %233 = arith.addi %232, %231 : tensor<64x1xi32>
        %234 = tt.broadcast %233 : tensor<64x1xi32> -> tensor<64x64xi32>
        %235 = tt.broadcast %60 : tensor<1x64xi32> -> tensor<64x64xi32>
        %236 = arith.addi %234, %235 : tensor<64x64xi32>
        %237 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %238 = tt.addptr %237, %236 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %239 = tt.load %238 : tensor<64x64x!tt.ptr<f8E5M2>>
        %240 = tt.trans %239 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %241 = tt.dot %66, %240, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %242 = arith.mulf %241, %cst_0 : tensor<128x64xf32>
        %243 = "tt.reduce"(%242) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %244 = arith.cmpf ogt, %199, %243 : tensor<128xf32>
        %245 = arith.cmpf une, %199, %199 : tensor<128xf32>
        %246 = arith.ori %244, %245 : tensor<128xi1>
        %247 = arith.select %246, %199, %243 : tensor<128xi1>, tensor<128xf32>
        %248 = tt.expand_dims %247 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %249 = tt.broadcast %248 : tensor<128x1xf32> -> tensor<128x64xf32>
        %250 = arith.subf %242, %249 : tensor<128x64xf32>
        %251 = tt.extern_elementwise %250 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %252 = "tt.reduce"(%251) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %253 = arith.subf %199, %247 : tensor<128xf32>
        %254 = tt.extern_elementwise %253 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %255 = arith.mulf %208, %254 : tensor<128xf32>
        %256 = arith.addf %255, %252 : tensor<128xf32>
        %257 = tt.expand_dims %254 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %258 = tt.broadcast %257 : tensor<128x1xf32> -> tensor<128x64xf32>
        %259 = arith.mulf %225, %258 : tensor<128x64xf32>
        %260 = tt.expand_dims %53 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %261 = arith.addi %232, %260 : tensor<64x1xi32>
        %262 = tt.expand_dims %229 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %263 = arith.muli %262, %cst : tensor<1x64xi32>
        %264 = tt.broadcast %261 : tensor<64x1xi32> -> tensor<64x64xi32>
        %265 = tt.broadcast %263 : tensor<1x64xi32> -> tensor<64x64xi32>
        %266 = arith.addi %264, %265 : tensor<64x64xi32>
        %267 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %268 = tt.addptr %267, %266 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %269 = tt.load %268 : tensor<64x64x!tt.ptr<f8E5M2>>
        %270 = tt.fp_to_fp %251, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %271 = tt.trans %269 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %272 = tt.dot %270, %271, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %273 = arith.addf %259, %272 : tensor<128x64xf32>
        %c2_i32_16 = arith.constant 2 : i32
        %274 = arith.muli %c64_i32, %c2_i32_16 : i32
        %275 = arith.addi %arg7, %274 : i32
        %276 = tt.splat %275 : i32 -> tensor<64xi32>
        %277 = arith.addi %276, %53 : tensor<64xi32>
        %278 = tt.expand_dims %277 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %279 = arith.muli %278, %cst_1 : tensor<64x1xi32>
        %280 = tt.splat %55 : i32 -> tensor<64x1xi32>
        %281 = arith.addi %280, %279 : tensor<64x1xi32>
        %282 = tt.broadcast %281 : tensor<64x1xi32> -> tensor<64x64xi32>
        %283 = tt.broadcast %60 : tensor<1x64xi32> -> tensor<64x64xi32>
        %284 = arith.addi %282, %283 : tensor<64x64xi32>
        %285 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %286 = tt.addptr %285, %284 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %287 = tt.load %286 : tensor<64x64x!tt.ptr<f8E5M2>>
        %288 = tt.trans %287 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %289 = tt.dot %66, %288, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %290 = arith.mulf %289, %cst_0 : tensor<128x64xf32>
        %291 = "tt.reduce"(%290) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %292 = arith.cmpf ogt, %247, %291 : tensor<128xf32>
        %293 = arith.cmpf une, %247, %247 : tensor<128xf32>
        %294 = arith.ori %292, %293 : tensor<128xi1>
        %295 = arith.select %294, %247, %291 : tensor<128xi1>, tensor<128xf32>
        %296 = tt.expand_dims %295 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %297 = tt.broadcast %296 : tensor<128x1xf32> -> tensor<128x64xf32>
        %298 = arith.subf %290, %297 : tensor<128x64xf32>
        %299 = tt.extern_elementwise %298 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %300 = "tt.reduce"(%299) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %301 = arith.subf %247, %295 : tensor<128xf32>
        %302 = tt.extern_elementwise %301 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %303 = arith.mulf %256, %302 : tensor<128xf32>
        %304 = arith.addf %303, %300 : tensor<128xf32>
        %305 = tt.expand_dims %302 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %306 = tt.broadcast %305 : tensor<128x1xf32> -> tensor<128x64xf32>
        %307 = arith.mulf %273, %306 : tensor<128x64xf32>
        %308 = tt.expand_dims %53 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %309 = arith.addi %280, %308 : tensor<64x1xi32>
        %310 = tt.expand_dims %277 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %311 = arith.muli %310, %cst : tensor<1x64xi32>
        %312 = tt.broadcast %309 : tensor<64x1xi32> -> tensor<64x64xi32>
        %313 = tt.broadcast %311 : tensor<1x64xi32> -> tensor<64x64xi32>
        %314 = arith.addi %312, %313 : tensor<64x64xi32>
        %315 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %316 = tt.addptr %315, %314 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %317 = tt.load %316 : tensor<64x64x!tt.ptr<f8E5M2>>
        %318 = tt.fp_to_fp %299, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %319 = tt.trans %317 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %320 = tt.dot %318, %319, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %321 = arith.addf %307, %320 : tensor<128x64xf32>
        %c3_i32_17 = arith.constant 3 : i32
        %322 = arith.muli %c64_i32, %c3_i32_17 : i32
        %323 = arith.addi %arg7, %322 : i32
        %324 = tt.splat %323 : i32 -> tensor<64xi32>
        %325 = arith.addi %324, %53 : tensor<64xi32>
        %326 = tt.expand_dims %325 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %327 = arith.muli %326, %cst_1 : tensor<64x1xi32>
        %328 = tt.splat %55 : i32 -> tensor<64x1xi32>
        %329 = arith.addi %328, %327 : tensor<64x1xi32>
        %330 = tt.broadcast %329 : tensor<64x1xi32> -> tensor<64x64xi32>
        %331 = tt.broadcast %60 : tensor<1x64xi32> -> tensor<64x64xi32>
        %332 = arith.addi %330, %331 : tensor<64x64xi32>
        %333 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %334 = tt.addptr %333, %332 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %335 = tt.load %334 : tensor<64x64x!tt.ptr<f8E5M2>>
        %336 = tt.trans %335 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %337 = tt.dot %66, %336, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %338 = arith.mulf %337, %cst_0 : tensor<128x64xf32>
        %339 = "tt.reduce"(%338) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %340 = arith.cmpf ogt, %295, %339 : tensor<128xf32>
        %341 = arith.cmpf une, %295, %295 : tensor<128xf32>
        %342 = arith.ori %340, %341 : tensor<128xi1>
        %343 = arith.select %342, %295, %339 : tensor<128xi1>, tensor<128xf32>
        %344 = tt.expand_dims %343 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %345 = tt.broadcast %344 : tensor<128x1xf32> -> tensor<128x64xf32>
        %346 = arith.subf %338, %345 : tensor<128x64xf32>
        %347 = tt.extern_elementwise %346 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %348 = "tt.reduce"(%347) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %349 = arith.subf %295, %343 : tensor<128xf32>
        %350 = tt.extern_elementwise %349 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %351 = arith.mulf %304, %350 : tensor<128xf32>
        %352 = arith.addf %351, %348 : tensor<128xf32>
        %353 = tt.expand_dims %350 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %354 = tt.broadcast %353 : tensor<128x1xf32> -> tensor<128x64xf32>
        %355 = arith.mulf %321, %354 : tensor<128x64xf32>
        %356 = tt.expand_dims %53 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %357 = arith.addi %328, %356 : tensor<64x1xi32>
        %358 = tt.expand_dims %325 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %359 = arith.muli %358, %cst : tensor<1x64xi32>
        %360 = tt.broadcast %357 : tensor<64x1xi32> -> tensor<64x64xi32>
        %361 = tt.broadcast %359 : tensor<1x64xi32> -> tensor<64x64xi32>
        %362 = arith.addi %360, %361 : tensor<64x64xi32>
        %363 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %364 = tt.addptr %363, %362 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %365 = tt.load %364 : tensor<64x64x!tt.ptr<f8E5M2>>
        %366 = tt.fp_to_fp %347, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %367 = tt.trans %365 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %368 = tt.dot %366, %367, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %369 = arith.addf %355, %368 : tensor<128x64xf32>
        scf.yield %352, %369, %343 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.disallow_acc_multi_buffer, tt.num_stages = 1 : i32}
      %68:3 = scf.for %arg7 = %c0_i32_9 to %c128_i32 step %c64_i32 iter_args(%arg8 = %67#0, %arg9 = %67#1, %arg10 = %67#2) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %180 = tt.splat %arg7 : i32 -> tensor<64xi32>
        %181 = arith.addi %180, %53 : tensor<64xi32>
        %182 = tt.expand_dims %181 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %183 = arith.muli %182, %cst_1 : tensor<64x1xi32>
        %184 = tt.splat %55 : i32 -> tensor<64x1xi32>
        %185 = arith.addi %184, %183 : tensor<64x1xi32>
        %186 = tt.broadcast %185 : tensor<64x1xi32> -> tensor<64x64xi32>
        %187 = tt.broadcast %60 : tensor<1x64xi32> -> tensor<64x64xi32>
        %188 = arith.addi %186, %187 : tensor<64x64xi32>
        %189 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %190 = tt.addptr %189, %188 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %191 = tt.load %190 : tensor<64x64x!tt.ptr<f8E5M2>>
        %192 = tt.trans %191 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %193 = tt.dot %66, %192, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %194 = arith.mulf %193, %cst_0 : tensor<128x64xf32>
        %195 = "tt.reduce"(%194) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %226 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %226 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %196 = arith.cmpf ogt, %arg10, %195 : tensor<128xf32>
        %197 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %198 = arith.ori %196, %197 : tensor<128xi1>
        %199 = arith.select %198, %arg10, %195 : tensor<128xi1>, tensor<128xf32>
        %200 = tt.expand_dims %199 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %201 = tt.broadcast %200 : tensor<128x1xf32> -> tensor<128x64xf32>
        %202 = arith.subf %194, %201 : tensor<128x64xf32>
        %203 = tt.extern_elementwise %202 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %204 = "tt.reduce"(%203) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %226 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %226 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %205 = arith.subf %arg10, %199 : tensor<128xf32>
        %206 = tt.extern_elementwise %205 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %207 = arith.mulf %arg8, %206 : tensor<128xf32>
        %208 = arith.addf %207, %204 : tensor<128xf32>
        %209 = tt.expand_dims %206 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %210 = tt.broadcast %209 : tensor<128x1xf32> -> tensor<128x64xf32>
        %211 = arith.mulf %arg9, %210 : tensor<128x64xf32>
        %212 = tt.expand_dims %53 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %213 = arith.addi %184, %212 : tensor<64x1xi32>
        %214 = tt.expand_dims %181 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %215 = arith.muli %214, %cst : tensor<1x64xi32>
        %216 = tt.broadcast %213 : tensor<64x1xi32> -> tensor<64x64xi32>
        %217 = tt.broadcast %215 : tensor<1x64xi32> -> tensor<64x64xi32>
        %218 = arith.addi %216, %217 : tensor<64x64xi32>
        %219 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %220 = tt.addptr %219, %218 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %221 = tt.load %220 : tensor<64x64x!tt.ptr<f8E5M2>>
        %222 = tt.fp_to_fp %203, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %223 = tt.trans %221 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %224 = tt.dot %222, %223, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %225 = arith.addf %211, %224 : tensor<128x64xf32>
        scf.yield %208, %225, %199 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.disallow_acc_multi_buffer, tt.num_stages = 1 : i32}
      %69 = tt.expand_dims %68#0 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
      %70 = tt.broadcast %69 : tensor<128x1xf32> -> tensor<128x64xf32>
      %71 = arith.divf %68#1, %70 : tensor<128x64xf32>
      %72 = tt.fp_to_fp %71, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
      %73 = arith.divsi %52, %arg5 : i32
      %74 = arith.remsi %52, %arg5 : i32
      %75 = arith.cmpi ne, %74, %c0_i32 : i32
      %76 = arith.subi %73, %c1_i32 : i32
      %77 = arith.select %75, %76, %73 : i32
      %78 = arith.cmpi slt, %52, %c0_i32 : i32
      %79 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %80 = arith.cmpi ne, %78, %79 : i1
      %81 = arith.select %80, %77, %73 : i32
      %82 = arith.andi %75, %80 : i1
      %83 = arith.addi %74, %arg5 : i32
      %84 = arith.select %82, %83, %74 : i32
      %85 = arith.muli %81, %arg4 : i32
      %86 = arith.muli %84, %c8192_i32 : i32
      %87 = arith.addi %85, %86 : i32
      %88 = tt.splat %87 : i32 -> tensor<128x1xi32>
      %89 = arith.addi %88, %57 : tensor<128x1xi32>
      %90 = tt.broadcast %89 : tensor<128x1xi32> -> tensor<128x64xi32>
      %91 = arith.addi %90, %62 : tensor<128x64xi32>
      %92 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %93 = tt.addptr %92, %91 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      tt.store %93, %72 : tensor<128x64x!tt.ptr<f8E5M2>>
      %c2_i32 = arith.constant 2 : i32
      %94 = arith.muli %c132_i32, %c2_i32 : i32
      %95 = arith.addi %arg6, %94 : i32
      %96 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
      %97 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>
      %98 = arith.muli %95, %c8192_i32 : i32
      %99 = tt.expand_dims %97 {axis = 1 : i32} : tensor<128xi32> -> tensor<128x1xi32>
      %100 = arith.muli %99, %cst_2 : tensor<128x1xi32>
      %101 = tt.splat %98 : i32 -> tensor<128x1xi32>
      %102 = arith.addi %101, %100 : tensor<128x1xi32>
      %103 = tt.expand_dims %96 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %104 = tt.broadcast %102 : tensor<128x1xi32> -> tensor<128x64xi32>
      %105 = tt.broadcast %103 : tensor<1x64xi32> -> tensor<128x64xi32>
      %106 = arith.addi %104, %105 : tensor<128x64xi32>
      %107 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %108 = tt.addptr %107, %106 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      %109 = tt.load %108 : tensor<128x64x!tt.ptr<f8E5M2>>
      %c0_i32_11 = arith.constant 0 : i32
      %c256_i32_12 = arith.constant 256 : i32
      %110:3 = scf.for %arg7 = %c0_i32 to %c0_i32_11 step %c256_i32_12 iter_args(%arg8 = %cst_4, %arg9 = %cst_3, %arg10 = %cst_5) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %180 = tt.splat %arg7 : i32 -> tensor<64xi32>
        %181 = arith.addi %180, %96 : tensor<64xi32>
        %182 = tt.expand_dims %181 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %183 = arith.muli %182, %cst_1 : tensor<64x1xi32>
        %184 = tt.splat %98 : i32 -> tensor<64x1xi32>
        %185 = arith.addi %184, %183 : tensor<64x1xi32>
        %186 = tt.broadcast %185 : tensor<64x1xi32> -> tensor<64x64xi32>
        %187 = tt.broadcast %103 : tensor<1x64xi32> -> tensor<64x64xi32>
        %188 = arith.addi %186, %187 : tensor<64x64xi32>
        %189 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %190 = tt.addptr %189, %188 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %191 = tt.load %190 : tensor<64x64x!tt.ptr<f8E5M2>>
        %192 = tt.trans %191 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %193 = tt.dot %109, %192, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %194 = arith.mulf %193, %cst_0 : tensor<128x64xf32>
        %195 = "tt.reduce"(%194) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %196 = arith.cmpf ogt, %arg10, %195 : tensor<128xf32>
        %197 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %198 = arith.ori %196, %197 : tensor<128xi1>
        %199 = arith.select %198, %arg10, %195 : tensor<128xi1>, tensor<128xf32>
        %200 = tt.expand_dims %199 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %201 = tt.broadcast %200 : tensor<128x1xf32> -> tensor<128x64xf32>
        %202 = arith.subf %194, %201 : tensor<128x64xf32>
        %203 = tt.extern_elementwise %202 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %204 = "tt.reduce"(%203) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %205 = arith.subf %arg10, %199 : tensor<128xf32>
        %206 = tt.extern_elementwise %205 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %207 = arith.mulf %arg8, %206 : tensor<128xf32>
        %208 = arith.addf %207, %204 : tensor<128xf32>
        %209 = tt.expand_dims %206 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %210 = tt.broadcast %209 : tensor<128x1xf32> -> tensor<128x64xf32>
        %211 = arith.mulf %arg9, %210 : tensor<128x64xf32>
        %212 = tt.expand_dims %96 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %213 = arith.addi %184, %212 : tensor<64x1xi32>
        %214 = tt.expand_dims %181 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %215 = arith.muli %214, %cst : tensor<1x64xi32>
        %216 = tt.broadcast %213 : tensor<64x1xi32> -> tensor<64x64xi32>
        %217 = tt.broadcast %215 : tensor<1x64xi32> -> tensor<64x64xi32>
        %218 = arith.addi %216, %217 : tensor<64x64xi32>
        %219 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %220 = tt.addptr %219, %218 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %221 = tt.load %220 : tensor<64x64x!tt.ptr<f8E5M2>>
        %222 = tt.fp_to_fp %203, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %223 = tt.trans %221 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %224 = tt.dot %222, %223, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %225 = arith.addf %211, %224 : tensor<128x64xf32>
        %c1_i32_15 = arith.constant 1 : i32
        %226 = arith.muli %c64_i32, %c1_i32_15 : i32
        %227 = arith.addi %arg7, %226 : i32
        %228 = tt.splat %227 : i32 -> tensor<64xi32>
        %229 = arith.addi %228, %96 : tensor<64xi32>
        %230 = tt.expand_dims %229 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %231 = arith.muli %230, %cst_1 : tensor<64x1xi32>
        %232 = tt.splat %98 : i32 -> tensor<64x1xi32>
        %233 = arith.addi %232, %231 : tensor<64x1xi32>
        %234 = tt.broadcast %233 : tensor<64x1xi32> -> tensor<64x64xi32>
        %235 = tt.broadcast %103 : tensor<1x64xi32> -> tensor<64x64xi32>
        %236 = arith.addi %234, %235 : tensor<64x64xi32>
        %237 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %238 = tt.addptr %237, %236 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %239 = tt.load %238 : tensor<64x64x!tt.ptr<f8E5M2>>
        %240 = tt.trans %239 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %241 = tt.dot %109, %240, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %242 = arith.mulf %241, %cst_0 : tensor<128x64xf32>
        %243 = "tt.reduce"(%242) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %244 = arith.cmpf ogt, %199, %243 : tensor<128xf32>
        %245 = arith.cmpf une, %199, %199 : tensor<128xf32>
        %246 = arith.ori %244, %245 : tensor<128xi1>
        %247 = arith.select %246, %199, %243 : tensor<128xi1>, tensor<128xf32>
        %248 = tt.expand_dims %247 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %249 = tt.broadcast %248 : tensor<128x1xf32> -> tensor<128x64xf32>
        %250 = arith.subf %242, %249 : tensor<128x64xf32>
        %251 = tt.extern_elementwise %250 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %252 = "tt.reduce"(%251) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %253 = arith.subf %199, %247 : tensor<128xf32>
        %254 = tt.extern_elementwise %253 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %255 = arith.mulf %208, %254 : tensor<128xf32>
        %256 = arith.addf %255, %252 : tensor<128xf32>
        %257 = tt.expand_dims %254 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %258 = tt.broadcast %257 : tensor<128x1xf32> -> tensor<128x64xf32>
        %259 = arith.mulf %225, %258 : tensor<128x64xf32>
        %260 = tt.expand_dims %96 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %261 = arith.addi %232, %260 : tensor<64x1xi32>
        %262 = tt.expand_dims %229 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %263 = arith.muli %262, %cst : tensor<1x64xi32>
        %264 = tt.broadcast %261 : tensor<64x1xi32> -> tensor<64x64xi32>
        %265 = tt.broadcast %263 : tensor<1x64xi32> -> tensor<64x64xi32>
        %266 = arith.addi %264, %265 : tensor<64x64xi32>
        %267 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %268 = tt.addptr %267, %266 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %269 = tt.load %268 : tensor<64x64x!tt.ptr<f8E5M2>>
        %270 = tt.fp_to_fp %251, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %271 = tt.trans %269 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %272 = tt.dot %270, %271, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %273 = arith.addf %259, %272 : tensor<128x64xf32>
        %c2_i32_16 = arith.constant 2 : i32
        %274 = arith.muli %c64_i32, %c2_i32_16 : i32
        %275 = arith.addi %arg7, %274 : i32
        %276 = tt.splat %275 : i32 -> tensor<64xi32>
        %277 = arith.addi %276, %96 : tensor<64xi32>
        %278 = tt.expand_dims %277 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %279 = arith.muli %278, %cst_1 : tensor<64x1xi32>
        %280 = tt.splat %98 : i32 -> tensor<64x1xi32>
        %281 = arith.addi %280, %279 : tensor<64x1xi32>
        %282 = tt.broadcast %281 : tensor<64x1xi32> -> tensor<64x64xi32>
        %283 = tt.broadcast %103 : tensor<1x64xi32> -> tensor<64x64xi32>
        %284 = arith.addi %282, %283 : tensor<64x64xi32>
        %285 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %286 = tt.addptr %285, %284 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %287 = tt.load %286 : tensor<64x64x!tt.ptr<f8E5M2>>
        %288 = tt.trans %287 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %289 = tt.dot %109, %288, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %290 = arith.mulf %289, %cst_0 : tensor<128x64xf32>
        %291 = "tt.reduce"(%290) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %292 = arith.cmpf ogt, %247, %291 : tensor<128xf32>
        %293 = arith.cmpf une, %247, %247 : tensor<128xf32>
        %294 = arith.ori %292, %293 : tensor<128xi1>
        %295 = arith.select %294, %247, %291 : tensor<128xi1>, tensor<128xf32>
        %296 = tt.expand_dims %295 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %297 = tt.broadcast %296 : tensor<128x1xf32> -> tensor<128x64xf32>
        %298 = arith.subf %290, %297 : tensor<128x64xf32>
        %299 = tt.extern_elementwise %298 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %300 = "tt.reduce"(%299) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %301 = arith.subf %247, %295 : tensor<128xf32>
        %302 = tt.extern_elementwise %301 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %303 = arith.mulf %256, %302 : tensor<128xf32>
        %304 = arith.addf %303, %300 : tensor<128xf32>
        %305 = tt.expand_dims %302 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %306 = tt.broadcast %305 : tensor<128x1xf32> -> tensor<128x64xf32>
        %307 = arith.mulf %273, %306 : tensor<128x64xf32>
        %308 = tt.expand_dims %96 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %309 = arith.addi %280, %308 : tensor<64x1xi32>
        %310 = tt.expand_dims %277 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %311 = arith.muli %310, %cst : tensor<1x64xi32>
        %312 = tt.broadcast %309 : tensor<64x1xi32> -> tensor<64x64xi32>
        %313 = tt.broadcast %311 : tensor<1x64xi32> -> tensor<64x64xi32>
        %314 = arith.addi %312, %313 : tensor<64x64xi32>
        %315 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %316 = tt.addptr %315, %314 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %317 = tt.load %316 : tensor<64x64x!tt.ptr<f8E5M2>>
        %318 = tt.fp_to_fp %299, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %319 = tt.trans %317 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %320 = tt.dot %318, %319, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %321 = arith.addf %307, %320 : tensor<128x64xf32>
        %c3_i32_17 = arith.constant 3 : i32
        %322 = arith.muli %c64_i32, %c3_i32_17 : i32
        %323 = arith.addi %arg7, %322 : i32
        %324 = tt.splat %323 : i32 -> tensor<64xi32>
        %325 = arith.addi %324, %96 : tensor<64xi32>
        %326 = tt.expand_dims %325 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %327 = arith.muli %326, %cst_1 : tensor<64x1xi32>
        %328 = tt.splat %98 : i32 -> tensor<64x1xi32>
        %329 = arith.addi %328, %327 : tensor<64x1xi32>
        %330 = tt.broadcast %329 : tensor<64x1xi32> -> tensor<64x64xi32>
        %331 = tt.broadcast %103 : tensor<1x64xi32> -> tensor<64x64xi32>
        %332 = arith.addi %330, %331 : tensor<64x64xi32>
        %333 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %334 = tt.addptr %333, %332 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %335 = tt.load %334 : tensor<64x64x!tt.ptr<f8E5M2>>
        %336 = tt.trans %335 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %337 = tt.dot %109, %336, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %338 = arith.mulf %337, %cst_0 : tensor<128x64xf32>
        %339 = "tt.reduce"(%338) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %340 = arith.cmpf ogt, %295, %339 : tensor<128xf32>
        %341 = arith.cmpf une, %295, %295 : tensor<128xf32>
        %342 = arith.ori %340, %341 : tensor<128xi1>
        %343 = arith.select %342, %295, %339 : tensor<128xi1>, tensor<128xf32>
        %344 = tt.expand_dims %343 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %345 = tt.broadcast %344 : tensor<128x1xf32> -> tensor<128x64xf32>
        %346 = arith.subf %338, %345 : tensor<128x64xf32>
        %347 = tt.extern_elementwise %346 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %348 = "tt.reduce"(%347) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %349 = arith.subf %295, %343 : tensor<128xf32>
        %350 = tt.extern_elementwise %349 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %351 = arith.mulf %304, %350 : tensor<128xf32>
        %352 = arith.addf %351, %348 : tensor<128xf32>
        %353 = tt.expand_dims %350 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %354 = tt.broadcast %353 : tensor<128x1xf32> -> tensor<128x64xf32>
        %355 = arith.mulf %321, %354 : tensor<128x64xf32>
        %356 = tt.expand_dims %96 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %357 = arith.addi %328, %356 : tensor<64x1xi32>
        %358 = tt.expand_dims %325 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %359 = arith.muli %358, %cst : tensor<1x64xi32>
        %360 = tt.broadcast %357 : tensor<64x1xi32> -> tensor<64x64xi32>
        %361 = tt.broadcast %359 : tensor<1x64xi32> -> tensor<64x64xi32>
        %362 = arith.addi %360, %361 : tensor<64x64xi32>
        %363 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %364 = tt.addptr %363, %362 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %365 = tt.load %364 : tensor<64x64x!tt.ptr<f8E5M2>>
        %366 = tt.fp_to_fp %347, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %367 = tt.trans %365 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %368 = tt.dot %366, %367, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %369 = arith.addf %355, %368 : tensor<128x64xf32>
        scf.yield %352, %369, %343 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.disallow_acc_multi_buffer, tt.num_stages = 1 : i32}
      %111:3 = scf.for %arg7 = %c0_i32_11 to %c128_i32 step %c64_i32 iter_args(%arg8 = %110#0, %arg9 = %110#1, %arg10 = %110#2) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %180 = tt.splat %arg7 : i32 -> tensor<64xi32>
        %181 = arith.addi %180, %96 : tensor<64xi32>
        %182 = tt.expand_dims %181 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %183 = arith.muli %182, %cst_1 : tensor<64x1xi32>
        %184 = tt.splat %98 : i32 -> tensor<64x1xi32>
        %185 = arith.addi %184, %183 : tensor<64x1xi32>
        %186 = tt.broadcast %185 : tensor<64x1xi32> -> tensor<64x64xi32>
        %187 = tt.broadcast %103 : tensor<1x64xi32> -> tensor<64x64xi32>
        %188 = arith.addi %186, %187 : tensor<64x64xi32>
        %189 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %190 = tt.addptr %189, %188 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %191 = tt.load %190 : tensor<64x64x!tt.ptr<f8E5M2>>
        %192 = tt.trans %191 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %193 = tt.dot %109, %192, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %194 = arith.mulf %193, %cst_0 : tensor<128x64xf32>
        %195 = "tt.reduce"(%194) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %226 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %226 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %196 = arith.cmpf ogt, %arg10, %195 : tensor<128xf32>
        %197 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %198 = arith.ori %196, %197 : tensor<128xi1>
        %199 = arith.select %198, %arg10, %195 : tensor<128xi1>, tensor<128xf32>
        %200 = tt.expand_dims %199 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %201 = tt.broadcast %200 : tensor<128x1xf32> -> tensor<128x64xf32>
        %202 = arith.subf %194, %201 : tensor<128x64xf32>
        %203 = tt.extern_elementwise %202 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %204 = "tt.reduce"(%203) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %226 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %226 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %205 = arith.subf %arg10, %199 : tensor<128xf32>
        %206 = tt.extern_elementwise %205 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %207 = arith.mulf %arg8, %206 : tensor<128xf32>
        %208 = arith.addf %207, %204 : tensor<128xf32>
        %209 = tt.expand_dims %206 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %210 = tt.broadcast %209 : tensor<128x1xf32> -> tensor<128x64xf32>
        %211 = arith.mulf %arg9, %210 : tensor<128x64xf32>
        %212 = tt.expand_dims %96 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %213 = arith.addi %184, %212 : tensor<64x1xi32>
        %214 = tt.expand_dims %181 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %215 = arith.muli %214, %cst : tensor<1x64xi32>
        %216 = tt.broadcast %213 : tensor<64x1xi32> -> tensor<64x64xi32>
        %217 = tt.broadcast %215 : tensor<1x64xi32> -> tensor<64x64xi32>
        %218 = arith.addi %216, %217 : tensor<64x64xi32>
        %219 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %220 = tt.addptr %219, %218 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %221 = tt.load %220 : tensor<64x64x!tt.ptr<f8E5M2>>
        %222 = tt.fp_to_fp %203, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %223 = tt.trans %221 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %224 = tt.dot %222, %223, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %225 = arith.addf %211, %224 : tensor<128x64xf32>
        scf.yield %208, %225, %199 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.disallow_acc_multi_buffer, tt.num_stages = 1 : i32}
      %112 = tt.expand_dims %111#0 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
      %113 = tt.broadcast %112 : tensor<128x1xf32> -> tensor<128x64xf32>
      %114 = arith.divf %111#1, %113 : tensor<128x64xf32>
      %115 = tt.fp_to_fp %114, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
      %116 = arith.divsi %95, %arg5 : i32
      %117 = arith.remsi %95, %arg5 : i32
      %118 = arith.cmpi ne, %117, %c0_i32 : i32
      %119 = arith.subi %116, %c1_i32 : i32
      %120 = arith.select %118, %119, %116 : i32
      %121 = arith.cmpi slt, %95, %c0_i32 : i32
      %122 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %123 = arith.cmpi ne, %121, %122 : i1
      %124 = arith.select %123, %120, %116 : i32
      %125 = arith.andi %118, %123 : i1
      %126 = arith.addi %117, %arg5 : i32
      %127 = arith.select %125, %126, %117 : i32
      %128 = arith.muli %124, %arg4 : i32
      %129 = arith.muli %127, %c8192_i32 : i32
      %130 = arith.addi %128, %129 : i32
      %131 = tt.splat %130 : i32 -> tensor<128x1xi32>
      %132 = arith.addi %131, %100 : tensor<128x1xi32>
      %133 = tt.broadcast %132 : tensor<128x1xi32> -> tensor<128x64xi32>
      %134 = arith.addi %133, %105 : tensor<128x64xi32>
      %135 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %136 = tt.addptr %135, %134 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      tt.store %136, %115 : tensor<128x64x!tt.ptr<f8E5M2>>
      %c3_i32 = arith.constant 3 : i32
      %137 = arith.muli %c132_i32, %c3_i32 : i32
      %138 = arith.addi %arg6, %137 : i32
      %139 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
      %140 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>
      %141 = arith.muli %138, %c8192_i32 : i32
      %142 = tt.expand_dims %140 {axis = 1 : i32} : tensor<128xi32> -> tensor<128x1xi32>
      %143 = arith.muli %142, %cst_2 : tensor<128x1xi32>
      %144 = tt.splat %141 : i32 -> tensor<128x1xi32>
      %145 = arith.addi %144, %143 : tensor<128x1xi32>
      %146 = tt.expand_dims %139 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %147 = tt.broadcast %145 : tensor<128x1xi32> -> tensor<128x64xi32>
      %148 = tt.broadcast %146 : tensor<1x64xi32> -> tensor<128x64xi32>
      %149 = arith.addi %147, %148 : tensor<128x64xi32>
      %150 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %151 = tt.addptr %150, %149 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      %152 = tt.load %151 : tensor<128x64x!tt.ptr<f8E5M2>>
      %c0_i32_13 = arith.constant 0 : i32
      %c256_i32_14 = arith.constant 256 : i32
      %153:3 = scf.for %arg7 = %c0_i32 to %c0_i32_13 step %c256_i32_14 iter_args(%arg8 = %cst_4, %arg9 = %cst_3, %arg10 = %cst_5) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %180 = tt.splat %arg7 : i32 -> tensor<64xi32>
        %181 = arith.addi %180, %139 : tensor<64xi32>
        %182 = tt.expand_dims %181 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %183 = arith.muli %182, %cst_1 : tensor<64x1xi32>
        %184 = tt.splat %141 : i32 -> tensor<64x1xi32>
        %185 = arith.addi %184, %183 : tensor<64x1xi32>
        %186 = tt.broadcast %185 : tensor<64x1xi32> -> tensor<64x64xi32>
        %187 = tt.broadcast %146 : tensor<1x64xi32> -> tensor<64x64xi32>
        %188 = arith.addi %186, %187 : tensor<64x64xi32>
        %189 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %190 = tt.addptr %189, %188 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %191 = tt.load %190 : tensor<64x64x!tt.ptr<f8E5M2>>
        %192 = tt.trans %191 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %193 = tt.dot %152, %192, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %194 = arith.mulf %193, %cst_0 : tensor<128x64xf32>
        %195 = "tt.reduce"(%194) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %196 = arith.cmpf ogt, %arg10, %195 : tensor<128xf32>
        %197 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %198 = arith.ori %196, %197 : tensor<128xi1>
        %199 = arith.select %198, %arg10, %195 : tensor<128xi1>, tensor<128xf32>
        %200 = tt.expand_dims %199 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %201 = tt.broadcast %200 : tensor<128x1xf32> -> tensor<128x64xf32>
        %202 = arith.subf %194, %201 : tensor<128x64xf32>
        %203 = tt.extern_elementwise %202 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %204 = "tt.reduce"(%203) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %205 = arith.subf %arg10, %199 : tensor<128xf32>
        %206 = tt.extern_elementwise %205 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %207 = arith.mulf %arg8, %206 : tensor<128xf32>
        %208 = arith.addf %207, %204 : tensor<128xf32>
        %209 = tt.expand_dims %206 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %210 = tt.broadcast %209 : tensor<128x1xf32> -> tensor<128x64xf32>
        %211 = arith.mulf %arg9, %210 : tensor<128x64xf32>
        %212 = tt.expand_dims %139 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %213 = arith.addi %184, %212 : tensor<64x1xi32>
        %214 = tt.expand_dims %181 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %215 = arith.muli %214, %cst : tensor<1x64xi32>
        %216 = tt.broadcast %213 : tensor<64x1xi32> -> tensor<64x64xi32>
        %217 = tt.broadcast %215 : tensor<1x64xi32> -> tensor<64x64xi32>
        %218 = arith.addi %216, %217 : tensor<64x64xi32>
        %219 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %220 = tt.addptr %219, %218 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %221 = tt.load %220 : tensor<64x64x!tt.ptr<f8E5M2>>
        %222 = tt.fp_to_fp %203, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %223 = tt.trans %221 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %224 = tt.dot %222, %223, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %225 = arith.addf %211, %224 : tensor<128x64xf32>
        %c1_i32_15 = arith.constant 1 : i32
        %226 = arith.muli %c64_i32, %c1_i32_15 : i32
        %227 = arith.addi %arg7, %226 : i32
        %228 = tt.splat %227 : i32 -> tensor<64xi32>
        %229 = arith.addi %228, %139 : tensor<64xi32>
        %230 = tt.expand_dims %229 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %231 = arith.muli %230, %cst_1 : tensor<64x1xi32>
        %232 = tt.splat %141 : i32 -> tensor<64x1xi32>
        %233 = arith.addi %232, %231 : tensor<64x1xi32>
        %234 = tt.broadcast %233 : tensor<64x1xi32> -> tensor<64x64xi32>
        %235 = tt.broadcast %146 : tensor<1x64xi32> -> tensor<64x64xi32>
        %236 = arith.addi %234, %235 : tensor<64x64xi32>
        %237 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %238 = tt.addptr %237, %236 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %239 = tt.load %238 : tensor<64x64x!tt.ptr<f8E5M2>>
        %240 = tt.trans %239 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %241 = tt.dot %152, %240, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %242 = arith.mulf %241, %cst_0 : tensor<128x64xf32>
        %243 = "tt.reduce"(%242) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %244 = arith.cmpf ogt, %199, %243 : tensor<128xf32>
        %245 = arith.cmpf une, %199, %199 : tensor<128xf32>
        %246 = arith.ori %244, %245 : tensor<128xi1>
        %247 = arith.select %246, %199, %243 : tensor<128xi1>, tensor<128xf32>
        %248 = tt.expand_dims %247 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %249 = tt.broadcast %248 : tensor<128x1xf32> -> tensor<128x64xf32>
        %250 = arith.subf %242, %249 : tensor<128x64xf32>
        %251 = tt.extern_elementwise %250 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %252 = "tt.reduce"(%251) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %253 = arith.subf %199, %247 : tensor<128xf32>
        %254 = tt.extern_elementwise %253 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %255 = arith.mulf %208, %254 : tensor<128xf32>
        %256 = arith.addf %255, %252 : tensor<128xf32>
        %257 = tt.expand_dims %254 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %258 = tt.broadcast %257 : tensor<128x1xf32> -> tensor<128x64xf32>
        %259 = arith.mulf %225, %258 : tensor<128x64xf32>
        %260 = tt.expand_dims %139 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %261 = arith.addi %232, %260 : tensor<64x1xi32>
        %262 = tt.expand_dims %229 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %263 = arith.muli %262, %cst : tensor<1x64xi32>
        %264 = tt.broadcast %261 : tensor<64x1xi32> -> tensor<64x64xi32>
        %265 = tt.broadcast %263 : tensor<1x64xi32> -> tensor<64x64xi32>
        %266 = arith.addi %264, %265 : tensor<64x64xi32>
        %267 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %268 = tt.addptr %267, %266 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %269 = tt.load %268 : tensor<64x64x!tt.ptr<f8E5M2>>
        %270 = tt.fp_to_fp %251, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %271 = tt.trans %269 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %272 = tt.dot %270, %271, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %273 = arith.addf %259, %272 : tensor<128x64xf32>
        %c2_i32_16 = arith.constant 2 : i32
        %274 = arith.muli %c64_i32, %c2_i32_16 : i32
        %275 = arith.addi %arg7, %274 : i32
        %276 = tt.splat %275 : i32 -> tensor<64xi32>
        %277 = arith.addi %276, %139 : tensor<64xi32>
        %278 = tt.expand_dims %277 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %279 = arith.muli %278, %cst_1 : tensor<64x1xi32>
        %280 = tt.splat %141 : i32 -> tensor<64x1xi32>
        %281 = arith.addi %280, %279 : tensor<64x1xi32>
        %282 = tt.broadcast %281 : tensor<64x1xi32> -> tensor<64x64xi32>
        %283 = tt.broadcast %146 : tensor<1x64xi32> -> tensor<64x64xi32>
        %284 = arith.addi %282, %283 : tensor<64x64xi32>
        %285 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %286 = tt.addptr %285, %284 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %287 = tt.load %286 : tensor<64x64x!tt.ptr<f8E5M2>>
        %288 = tt.trans %287 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %289 = tt.dot %152, %288, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %290 = arith.mulf %289, %cst_0 : tensor<128x64xf32>
        %291 = "tt.reduce"(%290) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %292 = arith.cmpf ogt, %247, %291 : tensor<128xf32>
        %293 = arith.cmpf une, %247, %247 : tensor<128xf32>
        %294 = arith.ori %292, %293 : tensor<128xi1>
        %295 = arith.select %294, %247, %291 : tensor<128xi1>, tensor<128xf32>
        %296 = tt.expand_dims %295 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %297 = tt.broadcast %296 : tensor<128x1xf32> -> tensor<128x64xf32>
        %298 = arith.subf %290, %297 : tensor<128x64xf32>
        %299 = tt.extern_elementwise %298 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %300 = "tt.reduce"(%299) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %301 = arith.subf %247, %295 : tensor<128xf32>
        %302 = tt.extern_elementwise %301 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %303 = arith.mulf %256, %302 : tensor<128xf32>
        %304 = arith.addf %303, %300 : tensor<128xf32>
        %305 = tt.expand_dims %302 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %306 = tt.broadcast %305 : tensor<128x1xf32> -> tensor<128x64xf32>
        %307 = arith.mulf %273, %306 : tensor<128x64xf32>
        %308 = tt.expand_dims %139 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %309 = arith.addi %280, %308 : tensor<64x1xi32>
        %310 = tt.expand_dims %277 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %311 = arith.muli %310, %cst : tensor<1x64xi32>
        %312 = tt.broadcast %309 : tensor<64x1xi32> -> tensor<64x64xi32>
        %313 = tt.broadcast %311 : tensor<1x64xi32> -> tensor<64x64xi32>
        %314 = arith.addi %312, %313 : tensor<64x64xi32>
        %315 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %316 = tt.addptr %315, %314 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %317 = tt.load %316 : tensor<64x64x!tt.ptr<f8E5M2>>
        %318 = tt.fp_to_fp %299, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %319 = tt.trans %317 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %320 = tt.dot %318, %319, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %321 = arith.addf %307, %320 : tensor<128x64xf32>
        %c3_i32_17 = arith.constant 3 : i32
        %322 = arith.muli %c64_i32, %c3_i32_17 : i32
        %323 = arith.addi %arg7, %322 : i32
        %324 = tt.splat %323 : i32 -> tensor<64xi32>
        %325 = arith.addi %324, %139 : tensor<64xi32>
        %326 = tt.expand_dims %325 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %327 = arith.muli %326, %cst_1 : tensor<64x1xi32>
        %328 = tt.splat %141 : i32 -> tensor<64x1xi32>
        %329 = arith.addi %328, %327 : tensor<64x1xi32>
        %330 = tt.broadcast %329 : tensor<64x1xi32> -> tensor<64x64xi32>
        %331 = tt.broadcast %146 : tensor<1x64xi32> -> tensor<64x64xi32>
        %332 = arith.addi %330, %331 : tensor<64x64xi32>
        %333 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %334 = tt.addptr %333, %332 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %335 = tt.load %334 : tensor<64x64x!tt.ptr<f8E5M2>>
        %336 = tt.trans %335 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %337 = tt.dot %152, %336, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %338 = arith.mulf %337, %cst_0 : tensor<128x64xf32>
        %339 = "tt.reduce"(%338) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %340 = arith.cmpf ogt, %295, %339 : tensor<128xf32>
        %341 = arith.cmpf une, %295, %295 : tensor<128xf32>
        %342 = arith.ori %340, %341 : tensor<128xi1>
        %343 = arith.select %342, %295, %339 : tensor<128xi1>, tensor<128xf32>
        %344 = tt.expand_dims %343 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %345 = tt.broadcast %344 : tensor<128x1xf32> -> tensor<128x64xf32>
        %346 = arith.subf %338, %345 : tensor<128x64xf32>
        %347 = tt.extern_elementwise %346 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %348 = "tt.reduce"(%347) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %370 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %370 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %349 = arith.subf %295, %343 : tensor<128xf32>
        %350 = tt.extern_elementwise %349 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %351 = arith.mulf %304, %350 : tensor<128xf32>
        %352 = arith.addf %351, %348 : tensor<128xf32>
        %353 = tt.expand_dims %350 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %354 = tt.broadcast %353 : tensor<128x1xf32> -> tensor<128x64xf32>
        %355 = arith.mulf %321, %354 : tensor<128x64xf32>
        %356 = tt.expand_dims %139 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %357 = arith.addi %328, %356 : tensor<64x1xi32>
        %358 = tt.expand_dims %325 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %359 = arith.muli %358, %cst : tensor<1x64xi32>
        %360 = tt.broadcast %357 : tensor<64x1xi32> -> tensor<64x64xi32>
        %361 = tt.broadcast %359 : tensor<1x64xi32> -> tensor<64x64xi32>
        %362 = arith.addi %360, %361 : tensor<64x64xi32>
        %363 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %364 = tt.addptr %363, %362 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %365 = tt.load %364 : tensor<64x64x!tt.ptr<f8E5M2>>
        %366 = tt.fp_to_fp %347, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %367 = tt.trans %365 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %368 = tt.dot %366, %367, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %369 = arith.addf %355, %368 : tensor<128x64xf32>
        scf.yield %352, %369, %343 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.disallow_acc_multi_buffer, tt.num_stages = 1 : i32}
      %154:3 = scf.for %arg7 = %c0_i32_13 to %c128_i32 step %c64_i32 iter_args(%arg8 = %153#0, %arg9 = %153#1, %arg10 = %153#2) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %180 = tt.splat %arg7 : i32 -> tensor<64xi32>
        %181 = arith.addi %180, %139 : tensor<64xi32>
        %182 = tt.expand_dims %181 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %183 = arith.muli %182, %cst_1 : tensor<64x1xi32>
        %184 = tt.splat %141 : i32 -> tensor<64x1xi32>
        %185 = arith.addi %184, %183 : tensor<64x1xi32>
        %186 = tt.broadcast %185 : tensor<64x1xi32> -> tensor<64x64xi32>
        %187 = tt.broadcast %146 : tensor<1x64xi32> -> tensor<64x64xi32>
        %188 = arith.addi %186, %187 : tensor<64x64xi32>
        %189 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %190 = tt.addptr %189, %188 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %191 = tt.load %190 : tensor<64x64x!tt.ptr<f8E5M2>>
        %192 = tt.trans %191 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %193 = tt.dot %152, %192, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %194 = arith.mulf %193, %cst_0 : tensor<128x64xf32>
        %195 = "tt.reduce"(%194) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %226 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %226 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %196 = arith.cmpf ogt, %arg10, %195 : tensor<128xf32>
        %197 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %198 = arith.ori %196, %197 : tensor<128xi1>
        %199 = arith.select %198, %arg10, %195 : tensor<128xi1>, tensor<128xf32>
        %200 = tt.expand_dims %199 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %201 = tt.broadcast %200 : tensor<128x1xf32> -> tensor<128x64xf32>
        %202 = arith.subf %194, %201 : tensor<128x64xf32>
        %203 = tt.extern_elementwise %202 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %204 = "tt.reduce"(%203) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %226 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %226 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %205 = arith.subf %arg10, %199 : tensor<128xf32>
        %206 = tt.extern_elementwise %205 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %207 = arith.mulf %arg8, %206 : tensor<128xf32>
        %208 = arith.addf %207, %204 : tensor<128xf32>
        %209 = tt.expand_dims %206 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %210 = tt.broadcast %209 : tensor<128x1xf32> -> tensor<128x64xf32>
        %211 = arith.mulf %arg9, %210 : tensor<128x64xf32>
        %212 = tt.expand_dims %139 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %213 = arith.addi %184, %212 : tensor<64x1xi32>
        %214 = tt.expand_dims %181 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %215 = arith.muli %214, %cst : tensor<1x64xi32>
        %216 = tt.broadcast %213 : tensor<64x1xi32> -> tensor<64x64xi32>
        %217 = tt.broadcast %215 : tensor<1x64xi32> -> tensor<64x64xi32>
        %218 = arith.addi %216, %217 : tensor<64x64xi32>
        %219 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %220 = tt.addptr %219, %218 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %221 = tt.load %220 : tensor<64x64x!tt.ptr<f8E5M2>>
        %222 = tt.fp_to_fp %203, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %223 = tt.trans %221 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %224 = tt.dot %222, %223, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %225 = arith.addf %211, %224 : tensor<128x64xf32>
        scf.yield %208, %225, %199 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.disallow_acc_multi_buffer, tt.num_stages = 1 : i32}
      %155 = tt.expand_dims %154#0 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
      %156 = tt.broadcast %155 : tensor<128x1xf32> -> tensor<128x64xf32>
      %157 = arith.divf %154#1, %156 : tensor<128x64xf32>
      %158 = tt.fp_to_fp %157, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
      %159 = arith.divsi %138, %arg5 : i32
      %160 = arith.remsi %138, %arg5 : i32
      %161 = arith.cmpi ne, %160, %c0_i32 : i32
      %162 = arith.subi %159, %c1_i32 : i32
      %163 = arith.select %161, %162, %159 : i32
      %164 = arith.cmpi slt, %138, %c0_i32 : i32
      %165 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %166 = arith.cmpi ne, %164, %165 : i1
      %167 = arith.select %166, %163, %159 : i32
      %168 = arith.andi %161, %166 : i1
      %169 = arith.addi %160, %arg5 : i32
      %170 = arith.select %168, %169, %160 : i32
      %171 = arith.muli %167, %arg4 : i32
      %172 = arith.muli %170, %c8192_i32 : i32
      %173 = arith.addi %171, %172 : i32
      %174 = tt.splat %173 : i32 -> tensor<128x1xi32>
      %175 = arith.addi %174, %143 : tensor<128x1xi32>
      %176 = tt.broadcast %175 : tensor<128x1xi32> -> tensor<128x64xi32>
      %177 = arith.addi %176, %148 : tensor<128x64xi32>
      %178 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %179 = tt.addptr %178, %177 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      tt.store %179, %158 : tensor<128x64x!tt.ptr<f8E5M2>>
    } {tt.disallow_acc_multi_buffer, tt.flatten}
    scf.for %arg6 = %8 to %c192_i32 step %c132_i32  : i32 {
      %10 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
      %11 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>
      %12 = arith.muli %arg6, %c8192_i32 : i32
      %13 = tt.expand_dims %11 {axis = 1 : i32} : tensor<128xi32> -> tensor<128x1xi32>
      %14 = arith.muli %13, %cst_2 : tensor<128x1xi32>
      %15 = tt.splat %12 : i32 -> tensor<128x1xi32>
      %16 = arith.addi %15, %14 : tensor<128x1xi32>
      %17 = tt.expand_dims %10 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %18 = tt.broadcast %16 : tensor<128x1xi32> -> tensor<128x64xi32>
      %19 = tt.broadcast %17 : tensor<1x64xi32> -> tensor<128x64xi32>
      %20 = arith.addi %18, %19 : tensor<128x64xi32>
      %21 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %22 = tt.addptr %21, %20 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      %23 = tt.load %22 : tensor<128x64x!tt.ptr<f8E5M2>>
      %c0_i32_7 = arith.constant 0 : i32
      %c256_i32 = arith.constant 256 : i32
      %24:3 = scf.for %arg7 = %c0_i32 to %c0_i32_7 step %c256_i32 iter_args(%arg8 = %cst_4, %arg9 = %cst_3, %arg10 = %cst_5) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %51 = tt.splat %arg7 : i32 -> tensor<64xi32>
        %52 = arith.addi %51, %10 : tensor<64xi32>
        %53 = tt.expand_dims %52 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %54 = arith.muli %53, %cst_1 : tensor<64x1xi32>
        %55 = tt.splat %12 : i32 -> tensor<64x1xi32>
        %56 = arith.addi %55, %54 : tensor<64x1xi32>
        %57 = tt.broadcast %56 : tensor<64x1xi32> -> tensor<64x64xi32>
        %58 = tt.broadcast %17 : tensor<1x64xi32> -> tensor<64x64xi32>
        %59 = arith.addi %57, %58 : tensor<64x64xi32>
        %60 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %61 = tt.addptr %60, %59 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %62 = tt.load %61 : tensor<64x64x!tt.ptr<f8E5M2>>
        %63 = tt.trans %62 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %64 = tt.dot %23, %63, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %65 = arith.mulf %64, %cst_0 : tensor<128x64xf32>
        %66 = "tt.reduce"(%65) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %241 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %241 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %67 = arith.cmpf ogt, %arg10, %66 : tensor<128xf32>
        %68 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %69 = arith.ori %67, %68 : tensor<128xi1>
        %70 = arith.select %69, %arg10, %66 : tensor<128xi1>, tensor<128xf32>
        %71 = tt.expand_dims %70 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %72 = tt.broadcast %71 : tensor<128x1xf32> -> tensor<128x64xf32>
        %73 = arith.subf %65, %72 : tensor<128x64xf32>
        %74 = tt.extern_elementwise %73 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %75 = "tt.reduce"(%74) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %241 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %241 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %76 = arith.subf %arg10, %70 : tensor<128xf32>
        %77 = tt.extern_elementwise %76 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %78 = arith.mulf %arg8, %77 : tensor<128xf32>
        %79 = arith.addf %78, %75 : tensor<128xf32>
        %80 = tt.expand_dims %77 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %81 = tt.broadcast %80 : tensor<128x1xf32> -> tensor<128x64xf32>
        %82 = arith.mulf %arg9, %81 : tensor<128x64xf32>
        %83 = tt.expand_dims %10 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %84 = arith.addi %55, %83 : tensor<64x1xi32>
        %85 = tt.expand_dims %52 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %86 = arith.muli %85, %cst : tensor<1x64xi32>
        %87 = tt.broadcast %84 : tensor<64x1xi32> -> tensor<64x64xi32>
        %88 = tt.broadcast %86 : tensor<1x64xi32> -> tensor<64x64xi32>
        %89 = arith.addi %87, %88 : tensor<64x64xi32>
        %90 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %91 = tt.addptr %90, %89 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %92 = tt.load %91 : tensor<64x64x!tt.ptr<f8E5M2>>
        %93 = tt.fp_to_fp %74, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %94 = tt.trans %92 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %95 = tt.dot %93, %94, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %96 = arith.addf %82, %95 : tensor<128x64xf32>
        %c1_i32_8 = arith.constant 1 : i32
        %97 = arith.muli %c64_i32, %c1_i32_8 : i32
        %98 = arith.addi %arg7, %97 : i32
        %99 = tt.splat %98 : i32 -> tensor<64xi32>
        %100 = arith.addi %99, %10 : tensor<64xi32>
        %101 = tt.expand_dims %100 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %102 = arith.muli %101, %cst_1 : tensor<64x1xi32>
        %103 = tt.splat %12 : i32 -> tensor<64x1xi32>
        %104 = arith.addi %103, %102 : tensor<64x1xi32>
        %105 = tt.broadcast %104 : tensor<64x1xi32> -> tensor<64x64xi32>
        %106 = tt.broadcast %17 : tensor<1x64xi32> -> tensor<64x64xi32>
        %107 = arith.addi %105, %106 : tensor<64x64xi32>
        %108 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %109 = tt.addptr %108, %107 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %110 = tt.load %109 : tensor<64x64x!tt.ptr<f8E5M2>>
        %111 = tt.trans %110 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %112 = tt.dot %23, %111, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %113 = arith.mulf %112, %cst_0 : tensor<128x64xf32>
        %114 = "tt.reduce"(%113) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %241 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %241 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %115 = arith.cmpf ogt, %70, %114 : tensor<128xf32>
        %116 = arith.cmpf une, %70, %70 : tensor<128xf32>
        %117 = arith.ori %115, %116 : tensor<128xi1>
        %118 = arith.select %117, %70, %114 : tensor<128xi1>, tensor<128xf32>
        %119 = tt.expand_dims %118 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %120 = tt.broadcast %119 : tensor<128x1xf32> -> tensor<128x64xf32>
        %121 = arith.subf %113, %120 : tensor<128x64xf32>
        %122 = tt.extern_elementwise %121 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %123 = "tt.reduce"(%122) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %241 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %241 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %124 = arith.subf %70, %118 : tensor<128xf32>
        %125 = tt.extern_elementwise %124 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %126 = arith.mulf %79, %125 : tensor<128xf32>
        %127 = arith.addf %126, %123 : tensor<128xf32>
        %128 = tt.expand_dims %125 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %129 = tt.broadcast %128 : tensor<128x1xf32> -> tensor<128x64xf32>
        %130 = arith.mulf %96, %129 : tensor<128x64xf32>
        %131 = tt.expand_dims %10 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %132 = arith.addi %103, %131 : tensor<64x1xi32>
        %133 = tt.expand_dims %100 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %134 = arith.muli %133, %cst : tensor<1x64xi32>
        %135 = tt.broadcast %132 : tensor<64x1xi32> -> tensor<64x64xi32>
        %136 = tt.broadcast %134 : tensor<1x64xi32> -> tensor<64x64xi32>
        %137 = arith.addi %135, %136 : tensor<64x64xi32>
        %138 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %139 = tt.addptr %138, %137 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %140 = tt.load %139 : tensor<64x64x!tt.ptr<f8E5M2>>
        %141 = tt.fp_to_fp %122, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %142 = tt.trans %140 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %143 = tt.dot %141, %142, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %144 = arith.addf %130, %143 : tensor<128x64xf32>
        %c2_i32 = arith.constant 2 : i32
        %145 = arith.muli %c64_i32, %c2_i32 : i32
        %146 = arith.addi %arg7, %145 : i32
        %147 = tt.splat %146 : i32 -> tensor<64xi32>
        %148 = arith.addi %147, %10 : tensor<64xi32>
        %149 = tt.expand_dims %148 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %150 = arith.muli %149, %cst_1 : tensor<64x1xi32>
        %151 = tt.splat %12 : i32 -> tensor<64x1xi32>
        %152 = arith.addi %151, %150 : tensor<64x1xi32>
        %153 = tt.broadcast %152 : tensor<64x1xi32> -> tensor<64x64xi32>
        %154 = tt.broadcast %17 : tensor<1x64xi32> -> tensor<64x64xi32>
        %155 = arith.addi %153, %154 : tensor<64x64xi32>
        %156 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %157 = tt.addptr %156, %155 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %158 = tt.load %157 : tensor<64x64x!tt.ptr<f8E5M2>>
        %159 = tt.trans %158 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %160 = tt.dot %23, %159, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %161 = arith.mulf %160, %cst_0 : tensor<128x64xf32>
        %162 = "tt.reduce"(%161) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %241 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %241 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %163 = arith.cmpf ogt, %118, %162 : tensor<128xf32>
        %164 = arith.cmpf une, %118, %118 : tensor<128xf32>
        %165 = arith.ori %163, %164 : tensor<128xi1>
        %166 = arith.select %165, %118, %162 : tensor<128xi1>, tensor<128xf32>
        %167 = tt.expand_dims %166 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %168 = tt.broadcast %167 : tensor<128x1xf32> -> tensor<128x64xf32>
        %169 = arith.subf %161, %168 : tensor<128x64xf32>
        %170 = tt.extern_elementwise %169 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %171 = "tt.reduce"(%170) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %241 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %241 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %172 = arith.subf %118, %166 : tensor<128xf32>
        %173 = tt.extern_elementwise %172 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %174 = arith.mulf %127, %173 : tensor<128xf32>
        %175 = arith.addf %174, %171 : tensor<128xf32>
        %176 = tt.expand_dims %173 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %177 = tt.broadcast %176 : tensor<128x1xf32> -> tensor<128x64xf32>
        %178 = arith.mulf %144, %177 : tensor<128x64xf32>
        %179 = tt.expand_dims %10 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %180 = arith.addi %151, %179 : tensor<64x1xi32>
        %181 = tt.expand_dims %148 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %182 = arith.muli %181, %cst : tensor<1x64xi32>
        %183 = tt.broadcast %180 : tensor<64x1xi32> -> tensor<64x64xi32>
        %184 = tt.broadcast %182 : tensor<1x64xi32> -> tensor<64x64xi32>
        %185 = arith.addi %183, %184 : tensor<64x64xi32>
        %186 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %187 = tt.addptr %186, %185 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %188 = tt.load %187 : tensor<64x64x!tt.ptr<f8E5M2>>
        %189 = tt.fp_to_fp %170, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %190 = tt.trans %188 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %191 = tt.dot %189, %190, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %192 = arith.addf %178, %191 : tensor<128x64xf32>
        %c3_i32 = arith.constant 3 : i32
        %193 = arith.muli %c64_i32, %c3_i32 : i32
        %194 = arith.addi %arg7, %193 : i32
        %195 = tt.splat %194 : i32 -> tensor<64xi32>
        %196 = arith.addi %195, %10 : tensor<64xi32>
        %197 = tt.expand_dims %196 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %198 = arith.muli %197, %cst_1 : tensor<64x1xi32>
        %199 = tt.splat %12 : i32 -> tensor<64x1xi32>
        %200 = arith.addi %199, %198 : tensor<64x1xi32>
        %201 = tt.broadcast %200 : tensor<64x1xi32> -> tensor<64x64xi32>
        %202 = tt.broadcast %17 : tensor<1x64xi32> -> tensor<64x64xi32>
        %203 = arith.addi %201, %202 : tensor<64x64xi32>
        %204 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %205 = tt.addptr %204, %203 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %206 = tt.load %205 : tensor<64x64x!tt.ptr<f8E5M2>>
        %207 = tt.trans %206 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %208 = tt.dot %23, %207, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %209 = arith.mulf %208, %cst_0 : tensor<128x64xf32>
        %210 = "tt.reduce"(%209) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %241 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %241 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %211 = arith.cmpf ogt, %166, %210 : tensor<128xf32>
        %212 = arith.cmpf une, %166, %166 : tensor<128xf32>
        %213 = arith.ori %211, %212 : tensor<128xi1>
        %214 = arith.select %213, %166, %210 : tensor<128xi1>, tensor<128xf32>
        %215 = tt.expand_dims %214 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %216 = tt.broadcast %215 : tensor<128x1xf32> -> tensor<128x64xf32>
        %217 = arith.subf %209, %216 : tensor<128x64xf32>
        %218 = tt.extern_elementwise %217 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %219 = "tt.reduce"(%218) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %241 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %241 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %220 = arith.subf %166, %214 : tensor<128xf32>
        %221 = tt.extern_elementwise %220 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %222 = arith.mulf %175, %221 : tensor<128xf32>
        %223 = arith.addf %222, %219 : tensor<128xf32>
        %224 = tt.expand_dims %221 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %225 = tt.broadcast %224 : tensor<128x1xf32> -> tensor<128x64xf32>
        %226 = arith.mulf %192, %225 : tensor<128x64xf32>
        %227 = tt.expand_dims %10 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %228 = arith.addi %199, %227 : tensor<64x1xi32>
        %229 = tt.expand_dims %196 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %230 = arith.muli %229, %cst : tensor<1x64xi32>
        %231 = tt.broadcast %228 : tensor<64x1xi32> -> tensor<64x64xi32>
        %232 = tt.broadcast %230 : tensor<1x64xi32> -> tensor<64x64xi32>
        %233 = arith.addi %231, %232 : tensor<64x64xi32>
        %234 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %235 = tt.addptr %234, %233 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %236 = tt.load %235 : tensor<64x64x!tt.ptr<f8E5M2>>
        %237 = tt.fp_to_fp %218, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %238 = tt.trans %236 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %239 = tt.dot %237, %238, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %240 = arith.addf %226, %239 : tensor<128x64xf32>
        scf.yield %223, %240, %214 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.disallow_acc_multi_buffer, tt.num_stages = 1 : i32}
      %25:3 = scf.for %arg7 = %c0_i32_7 to %c128_i32 step %c64_i32 iter_args(%arg8 = %24#0, %arg9 = %24#1, %arg10 = %24#2) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %51 = tt.splat %arg7 : i32 -> tensor<64xi32>
        %52 = arith.addi %51, %10 : tensor<64xi32>
        %53 = tt.expand_dims %52 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %54 = arith.muli %53, %cst_1 : tensor<64x1xi32>
        %55 = tt.splat %12 : i32 -> tensor<64x1xi32>
        %56 = arith.addi %55, %54 : tensor<64x1xi32>
        %57 = tt.broadcast %56 : tensor<64x1xi32> -> tensor<64x64xi32>
        %58 = tt.broadcast %17 : tensor<1x64xi32> -> tensor<64x64xi32>
        %59 = arith.addi %57, %58 : tensor<64x64xi32>
        %60 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %61 = tt.addptr %60, %59 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %62 = tt.load %61 : tensor<64x64x!tt.ptr<f8E5M2>>
        %63 = tt.trans %62 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %64 = tt.dot %23, %63, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %65 = arith.mulf %64, %cst_0 : tensor<128x64xf32>
        %66 = "tt.reduce"(%65) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %97 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %97 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %67 = arith.cmpf ogt, %arg10, %66 : tensor<128xf32>
        %68 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %69 = arith.ori %67, %68 : tensor<128xi1>
        %70 = arith.select %69, %arg10, %66 : tensor<128xi1>, tensor<128xf32>
        %71 = tt.expand_dims %70 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %72 = tt.broadcast %71 : tensor<128x1xf32> -> tensor<128x64xf32>
        %73 = arith.subf %65, %72 : tensor<128x64xf32>
        %74 = tt.extern_elementwise %73 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %75 = "tt.reduce"(%74) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %97 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %97 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %76 = arith.subf %arg10, %70 : tensor<128xf32>
        %77 = tt.extern_elementwise %76 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %78 = arith.mulf %arg8, %77 : tensor<128xf32>
        %79 = arith.addf %78, %75 : tensor<128xf32>
        %80 = tt.expand_dims %77 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %81 = tt.broadcast %80 : tensor<128x1xf32> -> tensor<128x64xf32>
        %82 = arith.mulf %arg9, %81 : tensor<128x64xf32>
        %83 = tt.expand_dims %10 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %84 = arith.addi %55, %83 : tensor<64x1xi32>
        %85 = tt.expand_dims %52 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %86 = arith.muli %85, %cst : tensor<1x64xi32>
        %87 = tt.broadcast %84 : tensor<64x1xi32> -> tensor<64x64xi32>
        %88 = tt.broadcast %86 : tensor<1x64xi32> -> tensor<64x64xi32>
        %89 = arith.addi %87, %88 : tensor<64x64xi32>
        %90 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %91 = tt.addptr %90, %89 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %92 = tt.load %91 : tensor<64x64x!tt.ptr<f8E5M2>>
        %93 = tt.fp_to_fp %74, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %94 = tt.trans %92 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %95 = tt.dot %93, %94, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %96 = arith.addf %82, %95 : tensor<128x64xf32>
        scf.yield %79, %96, %70 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.disallow_acc_multi_buffer, tt.num_stages = 1 : i32}
      %26 = tt.expand_dims %25#0 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
      %27 = tt.broadcast %26 : tensor<128x1xf32> -> tensor<128x64xf32>
      %28 = arith.divf %25#1, %27 : tensor<128x64xf32>
      %29 = tt.fp_to_fp %28, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
      %30 = arith.divsi %arg6, %arg5 : i32
      %31 = arith.remsi %arg6, %arg5 : i32
      %32 = arith.cmpi ne, %31, %c0_i32 : i32
      %33 = arith.subi %30, %c1_i32 : i32
      %34 = arith.select %32, %33, %30 : i32
      %35 = arith.cmpi slt, %arg6, %c0_i32 : i32
      %36 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %37 = arith.cmpi ne, %35, %36 : i1
      %38 = arith.select %37, %34, %30 : i32
      %39 = arith.andi %32, %37 : i1
      %40 = arith.addi %31, %arg5 : i32
      %41 = arith.select %39, %40, %31 : i32
      %42 = arith.muli %38, %arg4 : i32
      %43 = arith.muli %41, %c8192_i32 : i32
      %44 = arith.addi %42, %43 : i32
      %45 = tt.splat %44 : i32 -> tensor<128x1xi32>
      %46 = arith.addi %45, %14 : tensor<128x1xi32>
      %47 = tt.broadcast %46 : tensor<128x1xi32> -> tensor<128x64xi32>
      %48 = arith.addi %47, %19 : tensor<128x64xi32>
      %49 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %50 = tt.addptr %49, %48 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      tt.store %50, %29 : tensor<128x64x!tt.ptr<f8E5M2>>
    } {tt.disallow_acc_multi_buffer, tt.flatten, tt.num_stages = 1 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(convert-triton-to-tritongpu{enable-source-remat=false num-ctas=1 num-warps=16 target=cuda:90 threads-per-warp=32}, tritongpu-coalesce, tritongpu-F32DotTC, triton-nvidia-gpu-plan-cta, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritongpu-accelerate-matmul, tritongpu-remove-layout-conversions, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, triton-nvidia-optimize-descriptor-encoding, triton-loop-aware-cse, tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-combine-tensor-select-and-if, nvgpu-warp-specialization{dump-intermediate-steps=false num-stages=4}, tritongpu-assign-latencies{num-stages=4}, tritongpu-schedule-loops, tritongpu-pipeline{dump-intermediate-steps=false num-stages=4}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-loop-aware-cse, tritongpu-prefetch, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-coalesce-async-copy, triton-nvidia-optimize-tmem-layouts, tritongpu-remove-layout-conversions, triton-nvidia-interleave-tmem, tritongpu-reduce-data-duplication, tritongpu-reorder-instructions, triton-loop-aware-cse, symbol-dce, triton-nvidia-tma-lowering, triton-nvidia-gpu-fence-insertion{compute-capability=90}, sccp, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true})",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_willfeng/3w/c3wooj3qjzyiukhwuvkk5um2qqi5ql2mrok7myhpr6qbpmbowxcv.py:13:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_willfeng/3w/c3wooj3qjzyiukhwuvkk5um2qqi5ql2mrok7myhpr6qbpmbowxcv.py:13:0: note: Pipeline failed while executing [`TritonGPUAssignLatencies` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
python: /root/.triton/llvm/llvm-8957e64a-almalinux-x64/include/llvm/ADT/SmallVector.h:296: const_reference llvm::SmallVectorTemplateCommon<long>::operator[](size_type) const [T = long]: Assertion `idx < size()' failed.
module {
  tt.func public @_fp8_attention_kernel_kernel(%arg0: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %c1_i32 = arith.constant 1 : i32
    %c32_i32 = arith.constant 32 : i32
    %c64_i32 = arith.constant 64 : i32
    %c128_i32 = arith.constant 128 : i32
    %c0_i32 = arith.constant 0 : i32
    %cst = arith.constant dense<64> : tensor<1x32xi32>
    %cst_0 = arith.constant dense<0.180336878> : tensor<64x32xf32>
    %cst_1 = arith.constant dense<0.000000e+00> : tensor<64x32xf32>
    %cst_2 = arith.constant dense<64> : tensor<32x1xi32>
    %cst_3 = arith.constant dense<64> : tensor<64x1xi32>
    %c8192_i32 = arith.constant 8192 : i32
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<64x64xf32>
    %cst_5 = arith.constant dense<0.000000e+00> : tensor<64xf32>
    %cst_6 = arith.constant dense<0xFF800000> : tensor<64xf32>
    %0 = tt.get_program_id x : i32
    %1 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
    %c0_i32_7 = arith.constant 0 : i32
    %c256_i32 = arith.constant 256 : i32
    scf.for %arg6 = %c0_i32 to %c0_i32_7 step %c256_i32  : i32 {
      %2 = tt.splat %arg6 : i32 -> tensor<64xi32>
      %3 = arith.addi %2, %1 : tensor<64xi32>
      %4 = arith.muli %0, %c8192_i32 : i32
      %5 = tt.expand_dims %3 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
      %6 = arith.muli %5, %cst_3 : tensor<64x1xi32>
      %7 = tt.splat %4 : i32 -> tensor<64x1xi32>
      %8 = arith.addi %7, %6 : tensor<64x1xi32>
      %9 = tt.expand_dims %1 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %10 = tt.broadcast %8 : tensor<64x1xi32> -> tensor<64x64xi32>
      %11 = tt.broadcast %9 : tensor<1x64xi32> -> tensor<64x64xi32>
      %12 = arith.addi %10, %11 : tensor<64x64xi32>
      %13 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %14 = tt.addptr %13, %12 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      %15 = tt.load %14 : tensor<64x64x!tt.ptr<f8E5M2>>
      %16:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c32_i32 iter_args(%arg8 = %cst_5, %arg9 = %cst_4, %arg10 = %cst_6) -> (tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>)  : i32 {
        %168 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32>
        %169 = tt.splat %arg7 : i32 -> tensor<32xi32>
        %170 = arith.addi %169, %168 : tensor<32xi32>
        %171 = tt.expand_dims %170 {axis = 1 : i32} : tensor<32xi32> -> tensor<32x1xi32>
        %172 = arith.muli %171, %cst_2 : tensor<32x1xi32>
        %173 = tt.splat %4 : i32 -> tensor<32x1xi32>
        %174 = arith.addi %173, %172 : tensor<32x1xi32>
        %175 = tt.broadcast %174 : tensor<32x1xi32> -> tensor<32x64xi32>
        %176 = tt.broadcast %9 : tensor<1x64xi32> -> tensor<32x64xi32>
        %177 = arith.addi %175, %176 : tensor<32x64xi32>
        %178 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<32x64x!tt.ptr<f8E5M2>>
        %179 = tt.addptr %178, %177 : tensor<32x64x!tt.ptr<f8E5M2>>, tensor<32x64xi32>
        %180 = tt.load %179 : tensor<32x64x!tt.ptr<f8E5M2>>
        %181 = tt.trans %180 {order = array<i32: 1, 0>} : tensor<32x64xf8E5M2> -> tensor<64x32xf8E5M2>
        %182 = tt.dot %15, %181, %cst_1, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x64xf8E5M2> * tensor<64x32xf8E5M2> -> tensor<64x32xf32>
        %183 = arith.mulf %182, %cst_0 : tensor<64x32xf32>
        %184 = "tt.reduce"(%183) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %215 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %215 : f32
        }) : (tensor<64x32xf32>) -> tensor<64xf32>
        %185 = arith.cmpf ogt, %arg10, %184 : tensor<64xf32>
        %186 = arith.cmpf une, %arg10, %arg10 : tensor<64xf32>
        %187 = arith.ori %185, %186 : tensor<64xi1>
        %188 = arith.select %187, %arg10, %184 : tensor<64xi1>, tensor<64xf32>
        %189 = tt.expand_dims %188 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %190 = tt.broadcast %189 : tensor<64x1xf32> -> tensor<64x32xf32>
        %191 = arith.subf %183, %190 : tensor<64x32xf32>
        %192 = tt.extern_elementwise %191 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64x32xf32>) -> tensor<64x32xf32>
        %193 = "tt.reduce"(%192) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %215 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %215 : f32
        }) : (tensor<64x32xf32>) -> tensor<64xf32>
        %194 = arith.subf %arg10, %188 : tensor<64xf32>
        %195 = tt.extern_elementwise %194 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64xf32>) -> tensor<64xf32>
        %196 = arith.mulf %arg8, %195 : tensor<64xf32>
        %197 = arith.addf %196, %193 : tensor<64xf32>
        %198 = tt.expand_dims %195 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %199 = tt.broadcast %198 : tensor<64x1xf32> -> tensor<64x64xf32>
        %200 = arith.mulf %arg9, %199 : tensor<64x64xf32>
        %201 = tt.expand_dims %1 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %202 = arith.addi %7, %201 : tensor<64x1xi32>
        %203 = tt.expand_dims %170 {axis = 0 : i32} : tensor<32xi32> -> tensor<1x32xi32>
        %204 = arith.muli %203, %cst : tensor<1x32xi32>
        %205 = tt.broadcast %202 : tensor<64x1xi32> -> tensor<64x32xi32>
        %206 = tt.broadcast %204 : tensor<1x32xi32> -> tensor<64x32xi32>
        %207 = arith.addi %205, %206 : tensor<64x32xi32>
        %208 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x32x!tt.ptr<f8E5M2>>
        %209 = tt.addptr %208, %207 : tensor<64x32x!tt.ptr<f8E5M2>>, tensor<64x32xi32>
        %210 = tt.load %209 : tensor<64x32x!tt.ptr<f8E5M2>>
        %211 = tt.fp_to_fp %192, rounding = rtne : tensor<64x32xf32> -> tensor<64x32xf8E5M2>
        %212 = tt.trans %210 {order = array<i32: 1, 0>} : tensor<64x32xf8E5M2> -> tensor<32x64xf8E5M2>
        %213 = tt.dot %211, %212, %cst_4, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x32xf8E5M2> * tensor<32x64xf8E5M2> -> tensor<64x64xf32>
        %214 = arith.addf %200, %213 : tensor<64x64xf32>
        scf.yield %197, %214, %188 : tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>
      } {tt.flatten, tt.loop_unroll_factor = 1 : i32}
      %17 = tt.expand_dims %16#0 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
      %18 = tt.broadcast %17 : tensor<64x1xf32> -> tensor<64x64xf32>
      %19 = arith.divf %16#1, %18 : tensor<64x64xf32>
      %20 = tt.fp_to_fp %19, rounding = rtne : tensor<64x64xf32> -> tensor<64x64xf8E5M2>
      %21 = arith.divsi %0, %arg5 : i32
      %22 = arith.remsi %0, %arg5 : i32
      %23 = arith.cmpi ne, %22, %c0_i32 : i32
      %24 = arith.subi %21, %c1_i32 : i32
      %25 = arith.select %23, %24, %21 : i32
      %26 = arith.cmpi slt, %0, %c0_i32 : i32
      %27 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %28 = arith.cmpi ne, %26, %27 : i1
      %29 = arith.select %28, %25, %21 : i32
      %30 = arith.andi %23, %28 : i1
      %31 = arith.addi %22, %arg5 : i32
      %32 = arith.select %30, %31, %22 : i32
      %33 = arith.muli %29, %arg4 : i32
      %34 = arith.muli %32, %c8192_i32 : i32
      %35 = arith.addi %33, %34 : i32
      %36 = tt.splat %35 : i32 -> tensor<64x1xi32>
      %37 = arith.addi %36, %6 : tensor<64x1xi32>
      %38 = tt.broadcast %37 : tensor<64x1xi32> -> tensor<64x64xi32>
      %39 = arith.addi %38, %11 : tensor<64x64xi32>
      %40 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %41 = tt.addptr %40, %39 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      tt.store %41, %20 : tensor<64x64x!tt.ptr<f8E5M2>>
      %c1_i32_8 = arith.constant 1 : i32
      %42 = arith.muli %c64_i32, %c1_i32_8 : i32
      %43 = arith.addi %arg6, %42 : i32
      %44 = tt.splat %43 : i32 -> tensor<64xi32>
      %45 = arith.addi %44, %1 : tensor<64xi32>
      %46 = arith.muli %0, %c8192_i32 : i32
      %47 = tt.expand_dims %45 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
      %48 = arith.muli %47, %cst_3 : tensor<64x1xi32>
      %49 = tt.splat %46 : i32 -> tensor<64x1xi32>
      %50 = arith.addi %49, %48 : tensor<64x1xi32>
      %51 = tt.expand_dims %1 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %52 = tt.broadcast %50 : tensor<64x1xi32> -> tensor<64x64xi32>
      %53 = tt.broadcast %51 : tensor<1x64xi32> -> tensor<64x64xi32>
      %54 = arith.addi %52, %53 : tensor<64x64xi32>
      %55 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %56 = tt.addptr %55, %54 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      %57 = tt.load %56 : tensor<64x64x!tt.ptr<f8E5M2>>
      %58:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c32_i32 iter_args(%arg8 = %cst_5, %arg9 = %cst_4, %arg10 = %cst_6) -> (tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>)  : i32 {
        %168 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32>
        %169 = tt.splat %arg7 : i32 -> tensor<32xi32>
        %170 = arith.addi %169, %168 : tensor<32xi32>
        %171 = tt.expand_dims %170 {axis = 1 : i32} : tensor<32xi32> -> tensor<32x1xi32>
        %172 = arith.muli %171, %cst_2 : tensor<32x1xi32>
        %173 = tt.splat %46 : i32 -> tensor<32x1xi32>
        %174 = arith.addi %173, %172 : tensor<32x1xi32>
        %175 = tt.broadcast %174 : tensor<32x1xi32> -> tensor<32x64xi32>
        %176 = tt.broadcast %51 : tensor<1x64xi32> -> tensor<32x64xi32>
        %177 = arith.addi %175, %176 : tensor<32x64xi32>
        %178 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<32x64x!tt.ptr<f8E5M2>>
        %179 = tt.addptr %178, %177 : tensor<32x64x!tt.ptr<f8E5M2>>, tensor<32x64xi32>
        %180 = tt.load %179 : tensor<32x64x!tt.ptr<f8E5M2>>
        %181 = tt.trans %180 {order = array<i32: 1, 0>} : tensor<32x64xf8E5M2> -> tensor<64x32xf8E5M2>
        %182 = tt.dot %57, %181, %cst_1, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x64xf8E5M2> * tensor<64x32xf8E5M2> -> tensor<64x32xf32>
        %183 = arith.mulf %182, %cst_0 : tensor<64x32xf32>
        %184 = "tt.reduce"(%183) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %215 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %215 : f32
        }) : (tensor<64x32xf32>) -> tensor<64xf32>
        %185 = arith.cmpf ogt, %arg10, %184 : tensor<64xf32>
        %186 = arith.cmpf une, %arg10, %arg10 : tensor<64xf32>
        %187 = arith.ori %185, %186 : tensor<64xi1>
        %188 = arith.select %187, %arg10, %184 : tensor<64xi1>, tensor<64xf32>
        %189 = tt.expand_dims %188 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %190 = tt.broadcast %189 : tensor<64x1xf32> -> tensor<64x32xf32>
        %191 = arith.subf %183, %190 : tensor<64x32xf32>
        %192 = tt.extern_elementwise %191 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64x32xf32>) -> tensor<64x32xf32>
        %193 = "tt.reduce"(%192) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %215 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %215 : f32
        }) : (tensor<64x32xf32>) -> tensor<64xf32>
        %194 = arith.subf %arg10, %188 : tensor<64xf32>
        %195 = tt.extern_elementwise %194 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64xf32>) -> tensor<64xf32>
        %196 = arith.mulf %arg8, %195 : tensor<64xf32>
        %197 = arith.addf %196, %193 : tensor<64xf32>
        %198 = tt.expand_dims %195 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %199 = tt.broadcast %198 : tensor<64x1xf32> -> tensor<64x64xf32>
        %200 = arith.mulf %arg9, %199 : tensor<64x64xf32>
        %201 = tt.expand_dims %1 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %202 = arith.addi %49, %201 : tensor<64x1xi32>
        %203 = tt.expand_dims %170 {axis = 0 : i32} : tensor<32xi32> -> tensor<1x32xi32>
        %204 = arith.muli %203, %cst : tensor<1x32xi32>
        %205 = tt.broadcast %202 : tensor<64x1xi32> -> tensor<64x32xi32>
        %206 = tt.broadcast %204 : tensor<1x32xi32> -> tensor<64x32xi32>
        %207 = arith.addi %205, %206 : tensor<64x32xi32>
        %208 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x32x!tt.ptr<f8E5M2>>
        %209 = tt.addptr %208, %207 : tensor<64x32x!tt.ptr<f8E5M2>>, tensor<64x32xi32>
        %210 = tt.load %209 : tensor<64x32x!tt.ptr<f8E5M2>>
        %211 = tt.fp_to_fp %192, rounding = rtne : tensor<64x32xf32> -> tensor<64x32xf8E5M2>
        %212 = tt.trans %210 {order = array<i32: 1, 0>} : tensor<64x32xf8E5M2> -> tensor<32x64xf8E5M2>
        %213 = tt.dot %211, %212, %cst_4, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x32xf8E5M2> * tensor<32x64xf8E5M2> -> tensor<64x64xf32>
        %214 = arith.addf %200, %213 : tensor<64x64xf32>
        scf.yield %197, %214, %188 : tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>
      } {tt.flatten, tt.loop_unroll_factor = 1 : i32}
      %59 = tt.expand_dims %58#0 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
      %60 = tt.broadcast %59 : tensor<64x1xf32> -> tensor<64x64xf32>
      %61 = arith.divf %58#1, %60 : tensor<64x64xf32>
      %62 = tt.fp_to_fp %61, rounding = rtne : tensor<64x64xf32> -> tensor<64x64xf8E5M2>
      %63 = arith.divsi %0, %arg5 : i32
      %64 = arith.remsi %0, %arg5 : i32
      %65 = arith.cmpi ne, %64, %c0_i32 : i32
      %66 = arith.subi %63, %c1_i32 : i32
      %67 = arith.select %65, %66, %63 : i32
      %68 = arith.cmpi slt, %0, %c0_i32 : i32
      %69 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %70 = arith.cmpi ne, %68, %69 : i1
      %71 = arith.select %70, %67, %63 : i32
      %72 = arith.andi %65, %70 : i1
      %73 = arith.addi %64, %arg5 : i32
      %74 = arith.select %72, %73, %64 : i32
      %75 = arith.muli %71, %arg4 : i32
      %76 = arith.muli %74, %c8192_i32 : i32
      %77 = arith.addi %75, %76 : i32
      %78 = tt.splat %77 : i32 -> tensor<64x1xi32>
      %79 = arith.addi %78, %48 : tensor<64x1xi32>
      %80 = tt.broadcast %79 : tensor<64x1xi32> -> tensor<64x64xi32>
      %81 = arith.addi %80, %53 : tensor<64x64xi32>
      %82 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %83 = tt.addptr %82, %81 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      tt.store %83, %62 : tensor<64x64x!tt.ptr<f8E5M2>>
      %c2_i32 = arith.constant 2 : i32
      %84 = arith.muli %c64_i32, %c2_i32 : i32
      %85 = arith.addi %arg6, %84 : i32
      %86 = tt.splat %85 : i32 -> tensor<64xi32>
      %87 = arith.addi %86, %1 : tensor<64xi32>
      %88 = arith.muli %0, %c8192_i32 : i32
      %89 = tt.expand_dims %87 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
      %90 = arith.muli %89, %cst_3 : tensor<64x1xi32>
      %91 = tt.splat %88 : i32 -> tensor<64x1xi32>
      %92 = arith.addi %91, %90 : tensor<64x1xi32>
      %93 = tt.expand_dims %1 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %94 = tt.broadcast %92 : tensor<64x1xi32> -> tensor<64x64xi32>
      %95 = tt.broadcast %93 : tensor<1x64xi32> -> tensor<64x64xi32>
      %96 = arith.addi %94, %95 : tensor<64x64xi32>
      %97 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %98 = tt.addptr %97, %96 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      %99 = tt.load %98 : tensor<64x64x!tt.ptr<f8E5M2>>
      %100:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c32_i32 iter_args(%arg8 = %cst_5, %arg9 = %cst_4, %arg10 = %cst_6) -> (tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>)  : i32 {
        %168 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32>
        %169 = tt.splat %arg7 : i32 -> tensor<32xi32>
        %170 = arith.addi %169, %168 : tensor<32xi32>
        %171 = tt.expand_dims %170 {axis = 1 : i32} : tensor<32xi32> -> tensor<32x1xi32>
        %172 = arith.muli %171, %cst_2 : tensor<32x1xi32>
        %173 = tt.splat %88 : i32 -> tensor<32x1xi32>
        %174 = arith.addi %173, %172 : tensor<32x1xi32>
        %175 = tt.broadcast %174 : tensor<32x1xi32> -> tensor<32x64xi32>
        %176 = tt.broadcast %93 : tensor<1x64xi32> -> tensor<32x64xi32>
        %177 = arith.addi %175, %176 : tensor<32x64xi32>
        %178 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<32x64x!tt.ptr<f8E5M2>>
        %179 = tt.addptr %178, %177 : tensor<32x64x!tt.ptr<f8E5M2>>, tensor<32x64xi32>
        %180 = tt.load %179 : tensor<32x64x!tt.ptr<f8E5M2>>
        %181 = tt.trans %180 {order = array<i32: 1, 0>} : tensor<32x64xf8E5M2> -> tensor<64x32xf8E5M2>
        %182 = tt.dot %99, %181, %cst_1, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x64xf8E5M2> * tensor<64x32xf8E5M2> -> tensor<64x32xf32>
        %183 = arith.mulf %182, %cst_0 : tensor<64x32xf32>
        %184 = "tt.reduce"(%183) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %215 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %215 : f32
        }) : (tensor<64x32xf32>) -> tensor<64xf32>
        %185 = arith.cmpf ogt, %arg10, %184 : tensor<64xf32>
        %186 = arith.cmpf une, %arg10, %arg10 : tensor<64xf32>
        %187 = arith.ori %185, %186 : tensor<64xi1>
        %188 = arith.select %187, %arg10, %184 : tensor<64xi1>, tensor<64xf32>
        %189 = tt.expand_dims %188 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %190 = tt.broadcast %189 : tensor<64x1xf32> -> tensor<64x32xf32>
        %191 = arith.subf %183, %190 : tensor<64x32xf32>
        %192 = tt.extern_elementwise %191 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64x32xf32>) -> tensor<64x32xf32>
        %193 = "tt.reduce"(%192) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %215 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %215 : f32
        }) : (tensor<64x32xf32>) -> tensor<64xf32>
        %194 = arith.subf %arg10, %188 : tensor<64xf32>
        %195 = tt.extern_elementwise %194 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64xf32>) -> tensor<64xf32>
        %196 = arith.mulf %arg8, %195 : tensor<64xf32>
        %197 = arith.addf %196, %193 : tensor<64xf32>
        %198 = tt.expand_dims %195 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %199 = tt.broadcast %198 : tensor<64x1xf32> -> tensor<64x64xf32>
        %200 = arith.mulf %arg9, %199 : tensor<64x64xf32>
        %201 = tt.expand_dims %1 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %202 = arith.addi %91, %201 : tensor<64x1xi32>
        %203 = tt.expand_dims %170 {axis = 0 : i32} : tensor<32xi32> -> tensor<1x32xi32>
        %204 = arith.muli %203, %cst : tensor<1x32xi32>
        %205 = tt.broadcast %202 : tensor<64x1xi32> -> tensor<64x32xi32>
        %206 = tt.broadcast %204 : tensor<1x32xi32> -> tensor<64x32xi32>
        %207 = arith.addi %205, %206 : tensor<64x32xi32>
        %208 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x32x!tt.ptr<f8E5M2>>
        %209 = tt.addptr %208, %207 : tensor<64x32x!tt.ptr<f8E5M2>>, tensor<64x32xi32>
        %210 = tt.load %209 : tensor<64x32x!tt.ptr<f8E5M2>>
        %211 = tt.fp_to_fp %192, rounding = rtne : tensor<64x32xf32> -> tensor<64x32xf8E5M2>
        %212 = tt.trans %210 {order = array<i32: 1, 0>} : tensor<64x32xf8E5M2> -> tensor<32x64xf8E5M2>
        %213 = tt.dot %211, %212, %cst_4, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x32xf8E5M2> * tensor<32x64xf8E5M2> -> tensor<64x64xf32>
        %214 = arith.addf %200, %213 : tensor<64x64xf32>
        scf.yield %197, %214, %188 : tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>
      } {tt.flatten, tt.loop_unroll_factor = 1 : i32}
      %101 = tt.expand_dims %100#0 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
      %102 = tt.broadcast %101 : tensor<64x1xf32> -> tensor<64x64xf32>
      %103 = arith.divf %100#1, %102 : tensor<64x64xf32>
      %104 = tt.fp_to_fp %103, rounding = rtne : tensor<64x64xf32> -> tensor<64x64xf8E5M2>
      %105 = arith.divsi %0, %arg5 : i32
      %106 = arith.remsi %0, %arg5 : i32
      %107 = arith.cmpi ne, %106, %c0_i32 : i32
      %108 = arith.subi %105, %c1_i32 : i32
      %109 = arith.select %107, %108, %105 : i32
      %110 = arith.cmpi slt, %0, %c0_i32 : i32
      %111 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %112 = arith.cmpi ne, %110, %111 : i1
      %113 = arith.select %112, %109, %105 : i32
      %114 = arith.andi %107, %112 : i1
      %115 = arith.addi %106, %arg5 : i32
      %116 = arith.select %114, %115, %106 : i32
      %117 = arith.muli %113, %arg4 : i32
      %118 = arith.muli %116, %c8192_i32 : i32
      %119 = arith.addi %117, %118 : i32
      %120 = tt.splat %119 : i32 -> tensor<64x1xi32>
      %121 = arith.addi %120, %90 : tensor<64x1xi32>
      %122 = tt.broadcast %121 : tensor<64x1xi32> -> tensor<64x64xi32>
      %123 = arith.addi %122, %95 : tensor<64x64xi32>
      %124 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %125 = tt.addptr %124, %123 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      tt.store %125, %104 : tensor<64x64x!tt.ptr<f8E5M2>>
      %c3_i32 = arith.constant 3 : i32
      %126 = arith.muli %c64_i32, %c3_i32 : i32
      %127 = arith.addi %arg6, %126 : i32
      %128 = tt.splat %127 : i32 -> tensor<64xi32>
      %129 = arith.addi %128, %1 : tensor<64xi32>
      %130 = arith.muli %0, %c8192_i32 : i32
      %131 = tt.expand_dims %129 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
      %132 = arith.muli %131, %cst_3 : tensor<64x1xi32>
      %133 = tt.splat %130 : i32 -> tensor<64x1xi32>
      %134 = arith.addi %133, %132 : tensor<64x1xi32>
      %135 = tt.expand_dims %1 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %136 = tt.broadcast %134 : tensor<64x1xi32> -> tensor<64x64xi32>
      %137 = tt.broadcast %135 : tensor<1x64xi32> -> tensor<64x64xi32>
      %138 = arith.addi %136, %137 : tensor<64x64xi32>
      %139 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %140 = tt.addptr %139, %138 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      %141 = tt.load %140 : tensor<64x64x!tt.ptr<f8E5M2>>
      %142:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c32_i32 iter_args(%arg8 = %cst_5, %arg9 = %cst_4, %arg10 = %cst_6) -> (tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>)  : i32 {
        %168 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32>
        %169 = tt.splat %arg7 : i32 -> tensor<32xi32>
        %170 = arith.addi %169, %168 : tensor<32xi32>
        %171 = tt.expand_dims %170 {axis = 1 : i32} : tensor<32xi32> -> tensor<32x1xi32>
        %172 = arith.muli %171, %cst_2 : tensor<32x1xi32>
        %173 = tt.splat %130 : i32 -> tensor<32x1xi32>
        %174 = arith.addi %173, %172 : tensor<32x1xi32>
        %175 = tt.broadcast %174 : tensor<32x1xi32> -> tensor<32x64xi32>
        %176 = tt.broadcast %135 : tensor<1x64xi32> -> tensor<32x64xi32>
        %177 = arith.addi %175, %176 : tensor<32x64xi32>
        %178 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<32x64x!tt.ptr<f8E5M2>>
        %179 = tt.addptr %178, %177 : tensor<32x64x!tt.ptr<f8E5M2>>, tensor<32x64xi32>
        %180 = tt.load %179 : tensor<32x64x!tt.ptr<f8E5M2>>
        %181 = tt.trans %180 {order = array<i32: 1, 0>} : tensor<32x64xf8E5M2> -> tensor<64x32xf8E5M2>
        %182 = tt.dot %141, %181, %cst_1, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x64xf8E5M2> * tensor<64x32xf8E5M2> -> tensor<64x32xf32>
        %183 = arith.mulf %182, %cst_0 : tensor<64x32xf32>
        %184 = "tt.reduce"(%183) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %215 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %215 : f32
        }) : (tensor<64x32xf32>) -> tensor<64xf32>
        %185 = arith.cmpf ogt, %arg10, %184 : tensor<64xf32>
        %186 = arith.cmpf une, %arg10, %arg10 : tensor<64xf32>
        %187 = arith.ori %185, %186 : tensor<64xi1>
        %188 = arith.select %187, %arg10, %184 : tensor<64xi1>, tensor<64xf32>
        %189 = tt.expand_dims %188 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %190 = tt.broadcast %189 : tensor<64x1xf32> -> tensor<64x32xf32>
        %191 = arith.subf %183, %190 : tensor<64x32xf32>
        %192 = tt.extern_elementwise %191 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64x32xf32>) -> tensor<64x32xf32>
        %193 = "tt.reduce"(%192) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %215 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %215 : f32
        }) : (tensor<64x32xf32>) -> tensor<64xf32>
        %194 = arith.subf %arg10, %188 : tensor<64xf32>
        %195 = tt.extern_elementwise %194 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64xf32>) -> tensor<64xf32>
        %196 = arith.mulf %arg8, %195 : tensor<64xf32>
        %197 = arith.addf %196, %193 : tensor<64xf32>
        %198 = tt.expand_dims %195 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %199 = tt.broadcast %198 : tensor<64x1xf32> -> tensor<64x64xf32>
        %200 = arith.mulf %arg9, %199 : tensor<64x64xf32>
        %201 = tt.expand_dims %1 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %202 = arith.addi %133, %201 : tensor<64x1xi32>
        %203 = tt.expand_dims %170 {axis = 0 : i32} : tensor<32xi32> -> tensor<1x32xi32>
        %204 = arith.muli %203, %cst : tensor<1x32xi32>
        %205 = tt.broadcast %202 : tensor<64x1xi32> -> tensor<64x32xi32>
        %206 = tt.broadcast %204 : tensor<1x32xi32> -> tensor<64x32xi32>
        %207 = arith.addi %205, %206 : tensor<64x32xi32>
        %208 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x32x!tt.ptr<f8E5M2>>
        %209 = tt.addptr %208, %207 : tensor<64x32x!tt.ptr<f8E5M2>>, tensor<64x32xi32>
        %210 = tt.load %209 : tensor<64x32x!tt.ptr<f8E5M2>>
        %211 = tt.fp_to_fp %192, rounding = rtne : tensor<64x32xf32> -> tensor<64x32xf8E5M2>
        %212 = tt.trans %210 {order = array<i32: 1, 0>} : tensor<64x32xf8E5M2> -> tensor<32x64xf8E5M2>
        %213 = tt.dot %211, %212, %cst_4, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x32xf8E5M2> * tensor<32x64xf8E5M2> -> tensor<64x64xf32>
        %214 = arith.addf %200, %213 : tensor<64x64xf32>
        scf.yield %197, %214, %188 : tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>
      } {tt.flatten, tt.loop_unroll_factor = 1 : i32}
      %143 = tt.expand_dims %142#0 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
      %144 = tt.broadcast %143 : tensor<64x1xf32> -> tensor<64x64xf32>
      %145 = arith.divf %142#1, %144 : tensor<64x64xf32>
      %146 = tt.fp_to_fp %145, rounding = rtne : tensor<64x64xf32> -> tensor<64x64xf8E5M2>
      %147 = arith.divsi %0, %arg5 : i32
      %148 = arith.remsi %0, %arg5 : i32
      %149 = arith.cmpi ne, %148, %c0_i32 : i32
      %150 = arith.subi %147, %c1_i32 : i32
      %151 = arith.select %149, %150, %147 : i32
      %152 = arith.cmpi slt, %0, %c0_i32 : i32
      %153 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %154 = arith.cmpi ne, %152, %153 : i1
      %155 = arith.select %154, %151, %147 : i32
      %156 = arith.andi %149, %154 : i1
      %157 = arith.addi %148, %arg5 : i32
      %158 = arith.select %156, %157, %148 : i32
      %159 = arith.muli %155, %arg4 : i32
      %160 = arith.muli %158, %c8192_i32 : i32
      %161 = arith.addi %159, %160 : i32
      %162 = tt.splat %161 : i32 -> tensor<64x1xi32>
      %163 = arith.addi %162, %132 : tensor<64x1xi32>
      %164 = tt.broadcast %163 : tensor<64x1xi32> -> tensor<64x64xi32>
      %165 = arith.addi %164, %137 : tensor<64x64xi32>
      %166 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %167 = tt.addptr %166, %165 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      tt.store %167, %146 : tensor<64x64x!tt.ptr<f8E5M2>>
    } {tt.disallow_acc_multi_buffer, tt.flatten, tt.num_stages = 4 : i32}
    scf.for %arg6 = %c0_i32_7 to %c128_i32 step %c64_i32  : i32 {
      %2 = tt.splat %arg6 : i32 -> tensor<64xi32>
      %3 = arith.addi %2, %1 : tensor<64xi32>
      %4 = arith.muli %0, %c8192_i32 : i32
      %5 = tt.expand_dims %3 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
      %6 = arith.muli %5, %cst_3 : tensor<64x1xi32>
      %7 = tt.splat %4 : i32 -> tensor<64x1xi32>
      %8 = arith.addi %7, %6 : tensor<64x1xi32>
      %9 = tt.expand_dims %1 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %10 = tt.broadcast %8 : tensor<64x1xi32> -> tensor<64x64xi32>
      %11 = tt.broadcast %9 : tensor<1x64xi32> -> tensor<64x64xi32>
      %12 = arith.addi %10, %11 : tensor<64x64xi32>
      %13 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %14 = tt.addptr %13, %12 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      %15 = tt.load %14 : tensor<64x64x!tt.ptr<f8E5M2>>
      %16:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c32_i32 iter_args(%arg8 = %cst_5, %arg9 = %cst_4, %arg10 = %cst_6) -> (tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>)  : i32 {
        %42 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32>
        %43 = tt.splat %arg7 : i32 -> tensor<32xi32>
        %44 = arith.addi %43, %42 : tensor<32xi32>
        %45 = tt.expand_dims %44 {axis = 1 : i32} : tensor<32xi32> -> tensor<32x1xi32>
        %46 = arith.muli %45, %cst_2 : tensor<32x1xi32>
        %47 = tt.splat %4 : i32 -> tensor<32x1xi32>
        %48 = arith.addi %47, %46 : tensor<32x1xi32>
        %49 = tt.broadcast %48 : tensor<32x1xi32> -> tensor<32x64xi32>
        %50 = tt.broadcast %9 : tensor<1x64xi32> -> tensor<32x64xi32>
        %51 = arith.addi %49, %50 : tensor<32x64xi32>
        %52 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<32x64x!tt.ptr<f8E5M2>>
        %53 = tt.addptr %52, %51 : tensor<32x64x!tt.ptr<f8E5M2>>, tensor<32x64xi32>
        %54 = tt.load %53 : tensor<32x64x!tt.ptr<f8E5M2>>
        %55 = tt.trans %54 {order = array<i32: 1, 0>} : tensor<32x64xf8E5M2> -> tensor<64x32xf8E5M2>
        %56 = tt.dot %15, %55, %cst_1, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x64xf8E5M2> * tensor<64x32xf8E5M2> -> tensor<64x32xf32>
        %57 = arith.mulf %56, %cst_0 : tensor<64x32xf32>
        %58 = "tt.reduce"(%57) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %89 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %89 : f32
        }) : (tensor<64x32xf32>) -> tensor<64xf32>
        %59 = arith.cmpf ogt, %arg10, %58 : tensor<64xf32>
        %60 = arith.cmpf une, %arg10, %arg10 : tensor<64xf32>
        %61 = arith.ori %59, %60 : tensor<64xi1>
        %62 = arith.select %61, %arg10, %58 : tensor<64xi1>, tensor<64xf32>
        %63 = tt.expand_dims %62 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %64 = tt.broadcast %63 : tensor<64x1xf32> -> tensor<64x32xf32>
        %65 = arith.subf %57, %64 : tensor<64x32xf32>
        %66 = tt.extern_elementwise %65 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64x32xf32>) -> tensor<64x32xf32>
        %67 = "tt.reduce"(%66) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %89 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %89 : f32
        }) : (tensor<64x32xf32>) -> tensor<64xf32>
        %68 = arith.subf %arg10, %62 : tensor<64xf32>
        %69 = tt.extern_elementwise %68 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64xf32>) -> tensor<64xf32>
        %70 = arith.mulf %arg8, %69 : tensor<64xf32>
        %71 = arith.addf %70, %67 : tensor<64xf32>
        %72 = tt.expand_dims %69 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %73 = tt.broadcast %72 : tensor<64x1xf32> -> tensor<64x64xf32>
        %74 = arith.mulf %arg9, %73 : tensor<64x64xf32>
        %75 = tt.expand_dims %1 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %76 = arith.addi %7, %75 : tensor<64x1xi32>
        %77 = tt.expand_dims %44 {axis = 0 : i32} : tensor<32xi32> -> tensor<1x32xi32>
        %78 = arith.muli %77, %cst : tensor<1x32xi32>
        %79 = tt.broadcast %76 : tensor<64x1xi32> -> tensor<64x32xi32>
        %80 = tt.broadcast %78 : tensor<1x32xi32> -> tensor<64x32xi32>
        %81 = arith.addi %79, %80 : tensor<64x32xi32>
        %82 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x32x!tt.ptr<f8E5M2>>
        %83 = tt.addptr %82, %81 : tensor<64x32x!tt.ptr<f8E5M2>>, tensor<64x32xi32>
        %84 = tt.load %83 : tensor<64x32x!tt.ptr<f8E5M2>>
        %85 = tt.fp_to_fp %66, rounding = rtne : tensor<64x32xf32> -> tensor<64x32xf8E5M2>
        %86 = tt.trans %84 {order = array<i32: 1, 0>} : tensor<64x32xf8E5M2> -> tensor<32x64xf8E5M2>
        %87 = tt.dot %85, %86, %cst_4, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x32xf8E5M2> * tensor<32x64xf8E5M2> -> tensor<64x64xf32>
        %88 = arith.addf %74, %87 : tensor<64x64xf32>
        scf.yield %71, %88, %62 : tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>
      } {tt.flatten, tt.loop_unroll_factor = 1 : i32}
      %17 = tt.expand_dims %16#0 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
      %18 = tt.broadcast %17 : tensor<64x1xf32> -> tensor<64x64xf32>
      %19 = arith.divf %16#1, %18 : tensor<64x64xf32>
      %20 = tt.fp_to_fp %19, rounding = rtne : tensor<64x64xf32> -> tensor<64x64xf8E5M2>
      %21 = arith.divsi %0, %arg5 : i32
      %22 = arith.remsi %0, %arg5 : i32
      %23 = arith.cmpi ne, %22, %c0_i32 : i32
      %24 = arith.subi %21, %c1_i32 : i32
      %25 = arith.select %23, %24, %21 : i32
      %26 = arith.cmpi slt, %0, %c0_i32 : i32
      %27 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %28 = arith.cmpi ne, %26, %27 : i1
      %29 = arith.select %28, %25, %21 : i32
      %30 = arith.andi %23, %28 : i1
      %31 = arith.addi %22, %arg5 : i32
      %32 = arith.select %30, %31, %22 : i32
      %33 = arith.muli %29, %arg4 : i32
      %34 = arith.muli %32, %c8192_i32 : i32
      %35 = arith.addi %33, %34 : i32
      %36 = tt.splat %35 : i32 -> tensor<64x1xi32>
      %37 = arith.addi %36, %6 : tensor<64x1xi32>
      %38 = tt.broadcast %37 : tensor<64x1xi32> -> tensor<64x64xi32>
      %39 = arith.addi %38, %11 : tensor<64x64xi32>
      %40 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %41 = tt.addptr %40, %39 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      tt.store %41, %20 : tensor<64x64x!tt.ptr<f8E5M2>>
    } {tt.disallow_acc_multi_buffer, tt.flatten, tt.num_stages = 1 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(convert-triton-to-tritongpu{enable-source-remat=false num-ctas=1 num-warps=8 target=cuda:90 threads-per-warp=32}, tritongpu-coalesce, tritongpu-F32DotTC, triton-nvidia-gpu-plan-cta, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritongpu-accelerate-matmul, tritongpu-remove-layout-conversions, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, triton-nvidia-optimize-descriptor-encoding, triton-loop-aware-cse, tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-combine-tensor-select-and-if, nvgpu-warp-specialization{dump-intermediate-steps=false num-stages=5}, tritongpu-assign-latencies{num-stages=5}, tritongpu-schedule-loops, tritongpu-pipeline{dump-intermediate-steps=false num-stages=5}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-loop-aware-cse, tritongpu-prefetch, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-coalesce-async-copy, triton-nvidia-optimize-tmem-layouts, tritongpu-remove-layout-conversions, triton-nvidia-interleave-tmem, tritongpu-reduce-data-duplication, tritongpu-reorder-instructions, triton-loop-aware-cse, symbol-dce, triton-nvidia-tma-lowering, triton-nvidia-gpu-fence-insertion{compute-capability=90}, sccp, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true})",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_willfeng/pp/cpp26ah2udj75q6vy3hprdotrsjh2uudml7u7mh37gyzogka6o4u.py:12:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_willfeng/pp/cpp26ah2udj75q6vy3hprdotrsjh2uudml7u7mh37gyzogka6o4u.py:12:0: note: Pipeline failed while executing [`TritonGPUAssignLatencies` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
Process ForkProcess-209:
Traceback (most recent call last):
  File "/home/willfeng/local/miniconda3/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/willfeng/local/miniconda3/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/data/users/willfeng/helion/helion/runtime/precompile_shim.py", line 54, in finish_it
    kernel_cache[key] = fn.compile(
                        ^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/compiler/compiler.py", line 359, in compile
    next_module = compile_ir(module, metadata)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/backends/nvidia/compiler.py", line 456, in <lambda>
    stages["ttgir"] = lambda src, metadata: self.make_ttgir(src, metadata, options, capability)
                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/backends/nvidia/compiler.py", line 298, in make_ttgir
    pm.run(mod)
RuntimeError: PassManager::run failed
python: /root/.triton/llvm/llvm-8957e64a-almalinux-x64/include/llvm/ADT/SmallVector.h:296: const_reference llvm::SmallVectorTemplateCommon<long>::operator[](size_type) const [T = long]: Assertion `idx < size()' failed.
module {
  tt.func public @_fp8_attention_kernel_kernel(%arg0: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %c1_i32 = arith.constant 1 : i32
    %c32_i32 = arith.constant 32 : i32
    %c64_i32 = arith.constant 64 : i32
    %c128_i32 = arith.constant 128 : i32
    %c0_i32 = arith.constant 0 : i32
    %cst = arith.constant dense<64> : tensor<1x32xi32>
    %cst_0 = arith.constant dense<0.180336878> : tensor<64x32xf32>
    %cst_1 = arith.constant dense<0.000000e+00> : tensor<64x32xf32>
    %cst_2 = arith.constant dense<64> : tensor<32x1xi32>
    %cst_3 = arith.constant dense<64> : tensor<64x1xi32>
    %c8192_i32 = arith.constant 8192 : i32
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<64x64xf32>
    %cst_5 = arith.constant dense<0.000000e+00> : tensor<64xf32>
    %cst_6 = arith.constant dense<0xFF800000> : tensor<64xf32>
    %0 = tt.get_program_id x : i32
    %1 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
    %c0_i32_7 = arith.constant 0 : i32
    %c256_i32 = arith.constant 256 : i32
    scf.for %arg6 = %c0_i32 to %c0_i32_7 step %c256_i32  : i32 {
      %2 = tt.splat %arg6 : i32 -> tensor<64xi32>
      %3 = arith.addi %2, %1 : tensor<64xi32>
      %4 = arith.muli %0, %c8192_i32 : i32
      %5 = tt.expand_dims %3 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
      %6 = arith.muli %5, %cst_3 : tensor<64x1xi32>
      %7 = tt.splat %4 : i32 -> tensor<64x1xi32>
      %8 = arith.addi %7, %6 : tensor<64x1xi32>
      %9 = tt.expand_dims %1 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %10 = tt.broadcast %8 : tensor<64x1xi32> -> tensor<64x64xi32>
      %11 = tt.broadcast %9 : tensor<1x64xi32> -> tensor<64x64xi32>
      %12 = arith.addi %10, %11 : tensor<64x64xi32>
      %13 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %14 = tt.addptr %13, %12 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      %15 = tt.load %14 : tensor<64x64x!tt.ptr<f8E5M2>>
      %16:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c32_i32 iter_args(%arg8 = %cst_5, %arg9 = %cst_4, %arg10 = %cst_6) -> (tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>)  : i32 {
        %168 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32>
        %169 = tt.splat %arg7 : i32 -> tensor<32xi32>
        %170 = arith.addi %169, %168 : tensor<32xi32>
        %171 = tt.expand_dims %170 {axis = 1 : i32} : tensor<32xi32> -> tensor<32x1xi32>
        %172 = arith.muli %171, %cst_2 : tensor<32x1xi32>
        %173 = tt.splat %4 : i32 -> tensor<32x1xi32>
        %174 = arith.addi %173, %172 : tensor<32x1xi32>
        %175 = tt.broadcast %174 : tensor<32x1xi32> -> tensor<32x64xi32>
        %176 = tt.broadcast %9 : tensor<1x64xi32> -> tensor<32x64xi32>
        %177 = arith.addi %175, %176 : tensor<32x64xi32>
        %178 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<32x64x!tt.ptr<f8E5M2>>
        %179 = tt.addptr %178, %177 : tensor<32x64x!tt.ptr<f8E5M2>>, tensor<32x64xi32>
        %180 = tt.load %179 : tensor<32x64x!tt.ptr<f8E5M2>>
        %181 = tt.trans %180 {order = array<i32: 1, 0>} : tensor<32x64xf8E5M2> -> tensor<64x32xf8E5M2>
        %182 = tt.dot %15, %181, %cst_1, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x64xf8E5M2> * tensor<64x32xf8E5M2> -> tensor<64x32xf32>
        %183 = arith.mulf %182, %cst_0 : tensor<64x32xf32>
        %184 = "tt.reduce"(%183) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %215 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %215 : f32
        }) : (tensor<64x32xf32>) -> tensor<64xf32>
        %185 = arith.cmpf ogt, %arg10, %184 : tensor<64xf32>
        %186 = arith.cmpf une, %arg10, %arg10 : tensor<64xf32>
        %187 = arith.ori %185, %186 : tensor<64xi1>
        %188 = arith.select %187, %arg10, %184 : tensor<64xi1>, tensor<64xf32>
        %189 = tt.expand_dims %188 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %190 = tt.broadcast %189 : tensor<64x1xf32> -> tensor<64x32xf32>
        %191 = arith.subf %183, %190 : tensor<64x32xf32>
        %192 = tt.extern_elementwise %191 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64x32xf32>) -> tensor<64x32xf32>
        %193 = "tt.reduce"(%192) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %215 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %215 : f32
        }) : (tensor<64x32xf32>) -> tensor<64xf32>
        %194 = arith.subf %arg10, %188 : tensor<64xf32>
        %195 = tt.extern_elementwise %194 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64xf32>) -> tensor<64xf32>
        %196 = arith.mulf %arg8, %195 : tensor<64xf32>
        %197 = arith.addf %196, %193 : tensor<64xf32>
        %198 = tt.expand_dims %195 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %199 = tt.broadcast %198 : tensor<64x1xf32> -> tensor<64x64xf32>
        %200 = arith.mulf %arg9, %199 : tensor<64x64xf32>
        %201 = tt.expand_dims %1 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %202 = arith.addi %7, %201 : tensor<64x1xi32>
        %203 = tt.expand_dims %170 {axis = 0 : i32} : tensor<32xi32> -> tensor<1x32xi32>
        %204 = arith.muli %203, %cst : tensor<1x32xi32>
        %205 = tt.broadcast %202 : tensor<64x1xi32> -> tensor<64x32xi32>
        %206 = tt.broadcast %204 : tensor<1x32xi32> -> tensor<64x32xi32>
        %207 = arith.addi %205, %206 : tensor<64x32xi32>
        %208 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x32x!tt.ptr<f8E5M2>>
        %209 = tt.addptr %208, %207 : tensor<64x32x!tt.ptr<f8E5M2>>, tensor<64x32xi32>
        %210 = tt.load %209 : tensor<64x32x!tt.ptr<f8E5M2>>
        %211 = tt.fp_to_fp %192, rounding = rtne : tensor<64x32xf32> -> tensor<64x32xf8E5M2>
        %212 = tt.trans %210 {order = array<i32: 1, 0>} : tensor<64x32xf8E5M2> -> tensor<32x64xf8E5M2>
        %213 = tt.dot %211, %212, %cst_4, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x32xf8E5M2> * tensor<32x64xf8E5M2> -> tensor<64x64xf32>
        %214 = arith.addf %200, %213 : tensor<64x64xf32>
        scf.yield %197, %214, %188 : tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>
      } {tt.flatten, tt.loop_unroll_factor = 1 : i32}
      %17 = tt.expand_dims %16#0 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
      %18 = tt.broadcast %17 : tensor<64x1xf32> -> tensor<64x64xf32>
      %19 = arith.divf %16#1, %18 : tensor<64x64xf32>
      %20 = tt.fp_to_fp %19, rounding = rtne : tensor<64x64xf32> -> tensor<64x64xf8E5M2>
      %21 = arith.divsi %0, %arg5 : i32
      %22 = arith.remsi %0, %arg5 : i32
      %23 = arith.cmpi ne, %22, %c0_i32 : i32
      %24 = arith.subi %21, %c1_i32 : i32
      %25 = arith.select %23, %24, %21 : i32
      %26 = arith.cmpi slt, %0, %c0_i32 : i32
      %27 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %28 = arith.cmpi ne, %26, %27 : i1
      %29 = arith.select %28, %25, %21 : i32
      %30 = arith.andi %23, %28 : i1
      %31 = arith.addi %22, %arg5 : i32
      %32 = arith.select %30, %31, %22 : i32
      %33 = arith.muli %29, %arg4 : i32
      %34 = arith.muli %32, %c8192_i32 : i32
      %35 = arith.addi %33, %34 : i32
      %36 = tt.splat %35 : i32 -> tensor<64x1xi32>
      %37 = arith.addi %36, %6 : tensor<64x1xi32>
      %38 = tt.broadcast %37 : tensor<64x1xi32> -> tensor<64x64xi32>
      %39 = arith.addi %38, %11 : tensor<64x64xi32>
      %40 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %41 = tt.addptr %40, %39 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      tt.store %41, %20 : tensor<64x64x!tt.ptr<f8E5M2>>
      %c1_i32_8 = arith.constant 1 : i32
      %42 = arith.muli %c64_i32, %c1_i32_8 : i32
      %43 = arith.addi %arg6, %42 : i32
      %44 = tt.splat %43 : i32 -> tensor<64xi32>
      %45 = arith.addi %44, %1 : tensor<64xi32>
      %46 = arith.muli %0, %c8192_i32 : i32
      %47 = tt.expand_dims %45 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
      %48 = arith.muli %47, %cst_3 : tensor<64x1xi32>
      %49 = tt.splat %46 : i32 -> tensor<64x1xi32>
      %50 = arith.addi %49, %48 : tensor<64x1xi32>
      %51 = tt.expand_dims %1 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %52 = tt.broadcast %50 : tensor<64x1xi32> -> tensor<64x64xi32>
      %53 = tt.broadcast %51 : tensor<1x64xi32> -> tensor<64x64xi32>
      %54 = arith.addi %52, %53 : tensor<64x64xi32>
      %55 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %56 = tt.addptr %55, %54 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      %57 = tt.load %56 : tensor<64x64x!tt.ptr<f8E5M2>>
      %58:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c32_i32 iter_args(%arg8 = %cst_5, %arg9 = %cst_4, %arg10 = %cst_6) -> (tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>)  : i32 {
        %168 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32>
        %169 = tt.splat %arg7 : i32 -> tensor<32xi32>
        %170 = arith.addi %169, %168 : tensor<32xi32>
        %171 = tt.expand_dims %170 {axis = 1 : i32} : tensor<32xi32> -> tensor<32x1xi32>
        %172 = arith.muli %171, %cst_2 : tensor<32x1xi32>
        %173 = tt.splat %46 : i32 -> tensor<32x1xi32>
        %174 = arith.addi %173, %172 : tensor<32x1xi32>
        %175 = tt.broadcast %174 : tensor<32x1xi32> -> tensor<32x64xi32>
        %176 = tt.broadcast %51 : tensor<1x64xi32> -> tensor<32x64xi32>
        %177 = arith.addi %175, %176 : tensor<32x64xi32>
        %178 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<32x64x!tt.ptr<f8E5M2>>
        %179 = tt.addptr %178, %177 : tensor<32x64x!tt.ptr<f8E5M2>>, tensor<32x64xi32>
        %180 = tt.load %179 : tensor<32x64x!tt.ptr<f8E5M2>>
        %181 = tt.trans %180 {order = array<i32: 1, 0>} : tensor<32x64xf8E5M2> -> tensor<64x32xf8E5M2>
        %182 = tt.dot %57, %181, %cst_1, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x64xf8E5M2> * tensor<64x32xf8E5M2> -> tensor<64x32xf32>
        %183 = arith.mulf %182, %cst_0 : tensor<64x32xf32>
        %184 = "tt.reduce"(%183) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %215 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %215 : f32
        }) : (tensor<64x32xf32>) -> tensor<64xf32>
        %185 = arith.cmpf ogt, %arg10, %184 : tensor<64xf32>
        %186 = arith.cmpf une, %arg10, %arg10 : tensor<64xf32>
        %187 = arith.ori %185, %186 : tensor<64xi1>
        %188 = arith.select %187, %arg10, %184 : tensor<64xi1>, tensor<64xf32>
        %189 = tt.expand_dims %188 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %190 = tt.broadcast %189 : tensor<64x1xf32> -> tensor<64x32xf32>
        %191 = arith.subf %183, %190 : tensor<64x32xf32>
        %192 = tt.extern_elementwise %191 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64x32xf32>) -> tensor<64x32xf32>
        %193 = "tt.reduce"(%192) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %215 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %215 : f32
        }) : (tensor<64x32xf32>) -> tensor<64xf32>
        %194 = arith.subf %arg10, %188 : tensor<64xf32>
        %195 = tt.extern_elementwise %194 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64xf32>) -> tensor<64xf32>
        %196 = arith.mulf %arg8, %195 : tensor<64xf32>
        %197 = arith.addf %196, %193 : tensor<64xf32>
        %198 = tt.expand_dims %195 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %199 = tt.broadcast %198 : tensor<64x1xf32> -> tensor<64x64xf32>
        %200 = arith.mulf %arg9, %199 : tensor<64x64xf32>
        %201 = tt.expand_dims %1 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %202 = arith.addi %49, %201 : tensor<64x1xi32>
        %203 = tt.expand_dims %170 {axis = 0 : i32} : tensor<32xi32> -> tensor<1x32xi32>
        %204 = arith.muli %203, %cst : tensor<1x32xi32>
        %205 = tt.broadcast %202 : tensor<64x1xi32> -> tensor<64x32xi32>
        %206 = tt.broadcast %204 : tensor<1x32xi32> -> tensor<64x32xi32>
        %207 = arith.addi %205, %206 : tensor<64x32xi32>
        %208 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x32x!tt.ptr<f8E5M2>>
        %209 = tt.addptr %208, %207 : tensor<64x32x!tt.ptr<f8E5M2>>, tensor<64x32xi32>
        %210 = tt.load %209 : tensor<64x32x!tt.ptr<f8E5M2>>
        %211 = tt.fp_to_fp %192, rounding = rtne : tensor<64x32xf32> -> tensor<64x32xf8E5M2>
        %212 = tt.trans %210 {order = array<i32: 1, 0>} : tensor<64x32xf8E5M2> -> tensor<32x64xf8E5M2>
        %213 = tt.dot %211, %212, %cst_4, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x32xf8E5M2> * tensor<32x64xf8E5M2> -> tensor<64x64xf32>
        %214 = arith.addf %200, %213 : tensor<64x64xf32>
        scf.yield %197, %214, %188 : tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>
      } {tt.flatten, tt.loop_unroll_factor = 1 : i32}
      %59 = tt.expand_dims %58#0 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
      %60 = tt.broadcast %59 : tensor<64x1xf32> -> tensor<64x64xf32>
      %61 = arith.divf %58#1, %60 : tensor<64x64xf32>
      %62 = tt.fp_to_fp %61, rounding = rtne : tensor<64x64xf32> -> tensor<64x64xf8E5M2>
      %63 = arith.divsi %0, %arg5 : i32
      %64 = arith.remsi %0, %arg5 : i32
      %65 = arith.cmpi ne, %64, %c0_i32 : i32
      %66 = arith.subi %63, %c1_i32 : i32
      %67 = arith.select %65, %66, %63 : i32
      %68 = arith.cmpi slt, %0, %c0_i32 : i32
      %69 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %70 = arith.cmpi ne, %68, %69 : i1
      %71 = arith.select %70, %67, %63 : i32
      %72 = arith.andi %65, %70 : i1
      %73 = arith.addi %64, %arg5 : i32
      %74 = arith.select %72, %73, %64 : i32
      %75 = arith.muli %71, %arg4 : i32
      %76 = arith.muli %74, %c8192_i32 : i32
      %77 = arith.addi %75, %76 : i32
      %78 = tt.splat %77 : i32 -> tensor<64x1xi32>
      %79 = arith.addi %78, %48 : tensor<64x1xi32>
      %80 = tt.broadcast %79 : tensor<64x1xi32> -> tensor<64x64xi32>
      %81 = arith.addi %80, %53 : tensor<64x64xi32>
      %82 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %83 = tt.addptr %82, %81 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      tt.store %83, %62 : tensor<64x64x!tt.ptr<f8E5M2>>
      %c2_i32 = arith.constant 2 : i32
      %84 = arith.muli %c64_i32, %c2_i32 : i32
      %85 = arith.addi %arg6, %84 : i32
      %86 = tt.splat %85 : i32 -> tensor<64xi32>
      %87 = arith.addi %86, %1 : tensor<64xi32>
      %88 = arith.muli %0, %c8192_i32 : i32
      %89 = tt.expand_dims %87 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
      %90 = arith.muli %89, %cst_3 : tensor<64x1xi32>
      %91 = tt.splat %88 : i32 -> tensor<64x1xi32>
      %92 = arith.addi %91, %90 : tensor<64x1xi32>
      %93 = tt.expand_dims %1 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %94 = tt.broadcast %92 : tensor<64x1xi32> -> tensor<64x64xi32>
      %95 = tt.broadcast %93 : tensor<1x64xi32> -> tensor<64x64xi32>
      %96 = arith.addi %94, %95 : tensor<64x64xi32>
      %97 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %98 = tt.addptr %97, %96 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      %99 = tt.load %98 : tensor<64x64x!tt.ptr<f8E5M2>>
      %100:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c32_i32 iter_args(%arg8 = %cst_5, %arg9 = %cst_4, %arg10 = %cst_6) -> (tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>)  : i32 {
        %168 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32>
        %169 = tt.splat %arg7 : i32 -> tensor<32xi32>
        %170 = arith.addi %169, %168 : tensor<32xi32>
        %171 = tt.expand_dims %170 {axis = 1 : i32} : tensor<32xi32> -> tensor<32x1xi32>
        %172 = arith.muli %171, %cst_2 : tensor<32x1xi32>
        %173 = tt.splat %88 : i32 -> tensor<32x1xi32>
        %174 = arith.addi %173, %172 : tensor<32x1xi32>
        %175 = tt.broadcast %174 : tensor<32x1xi32> -> tensor<32x64xi32>
        %176 = tt.broadcast %93 : tensor<1x64xi32> -> tensor<32x64xi32>
        %177 = arith.addi %175, %176 : tensor<32x64xi32>
        %178 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<32x64x!tt.ptr<f8E5M2>>
        %179 = tt.addptr %178, %177 : tensor<32x64x!tt.ptr<f8E5M2>>, tensor<32x64xi32>
        %180 = tt.load %179 : tensor<32x64x!tt.ptr<f8E5M2>>
        %181 = tt.trans %180 {order = array<i32: 1, 0>} : tensor<32x64xf8E5M2> -> tensor<64x32xf8E5M2>
        %182 = tt.dot %99, %181, %cst_1, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x64xf8E5M2> * tensor<64x32xf8E5M2> -> tensor<64x32xf32>
        %183 = arith.mulf %182, %cst_0 : tensor<64x32xf32>
        %184 = "tt.reduce"(%183) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %215 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %215 : f32
        }) : (tensor<64x32xf32>) -> tensor<64xf32>
        %185 = arith.cmpf ogt, %arg10, %184 : tensor<64xf32>
        %186 = arith.cmpf une, %arg10, %arg10 : tensor<64xf32>
        %187 = arith.ori %185, %186 : tensor<64xi1>
        %188 = arith.select %187, %arg10, %184 : tensor<64xi1>, tensor<64xf32>
        %189 = tt.expand_dims %188 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %190 = tt.broadcast %189 : tensor<64x1xf32> -> tensor<64x32xf32>
        %191 = arith.subf %183, %190 : tensor<64x32xf32>
        %192 = tt.extern_elementwise %191 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64x32xf32>) -> tensor<64x32xf32>
        %193 = "tt.reduce"(%192) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %215 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %215 : f32
        }) : (tensor<64x32xf32>) -> tensor<64xf32>
        %194 = arith.subf %arg10, %188 : tensor<64xf32>
        %195 = tt.extern_elementwise %194 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64xf32>) -> tensor<64xf32>
        %196 = arith.mulf %arg8, %195 : tensor<64xf32>
        %197 = arith.addf %196, %193 : tensor<64xf32>
        %198 = tt.expand_dims %195 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %199 = tt.broadcast %198 : tensor<64x1xf32> -> tensor<64x64xf32>
        %200 = arith.mulf %arg9, %199 : tensor<64x64xf32>
        %201 = tt.expand_dims %1 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %202 = arith.addi %91, %201 : tensor<64x1xi32>
        %203 = tt.expand_dims %170 {axis = 0 : i32} : tensor<32xi32> -> tensor<1x32xi32>
        %204 = arith.muli %203, %cst : tensor<1x32xi32>
        %205 = tt.broadcast %202 : tensor<64x1xi32> -> tensor<64x32xi32>
        %206 = tt.broadcast %204 : tensor<1x32xi32> -> tensor<64x32xi32>
        %207 = arith.addi %205, %206 : tensor<64x32xi32>
        %208 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x32x!tt.ptr<f8E5M2>>
        %209 = tt.addptr %208, %207 : tensor<64x32x!tt.ptr<f8E5M2>>, tensor<64x32xi32>
        %210 = tt.load %209 : tensor<64x32x!tt.ptr<f8E5M2>>
        %211 = tt.fp_to_fp %192, rounding = rtne : tensor<64x32xf32> -> tensor<64x32xf8E5M2>
        %212 = tt.trans %210 {order = array<i32: 1, 0>} : tensor<64x32xf8E5M2> -> tensor<32x64xf8E5M2>
        %213 = tt.dot %211, %212, %cst_4, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x32xf8E5M2> * tensor<32x64xf8E5M2> -> tensor<64x64xf32>
        %214 = arith.addf %200, %213 : tensor<64x64xf32>
        scf.yield %197, %214, %188 : tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>
      } {tt.flatten, tt.loop_unroll_factor = 1 : i32}
      %101 = tt.expand_dims %100#0 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
      %102 = tt.broadcast %101 : tensor<64x1xf32> -> tensor<64x64xf32>
      %103 = arith.divf %100#1, %102 : tensor<64x64xf32>
      %104 = tt.fp_to_fp %103, rounding = rtne : tensor<64x64xf32> -> tensor<64x64xf8E5M2>
      %105 = arith.divsi %0, %arg5 : i32
      %106 = arith.remsi %0, %arg5 : i32
      %107 = arith.cmpi ne, %106, %c0_i32 : i32
      %108 = arith.subi %105, %c1_i32 : i32
      %109 = arith.select %107, %108, %105 : i32
      %110 = arith.cmpi slt, %0, %c0_i32 : i32
      %111 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %112 = arith.cmpi ne, %110, %111 : i1
      %113 = arith.select %112, %109, %105 : i32
      %114 = arith.andi %107, %112 : i1
      %115 = arith.addi %106, %arg5 : i32
      %116 = arith.select %114, %115, %106 : i32
      %117 = arith.muli %113, %arg4 : i32
      %118 = arith.muli %116, %c8192_i32 : i32
      %119 = arith.addi %117, %118 : i32
      %120 = tt.splat %119 : i32 -> tensor<64x1xi32>
      %121 = arith.addi %120, %90 : tensor<64x1xi32>
      %122 = tt.broadcast %121 : tensor<64x1xi32> -> tensor<64x64xi32>
      %123 = arith.addi %122, %95 : tensor<64x64xi32>
      %124 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %125 = tt.addptr %124, %123 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      tt.store %125, %104 : tensor<64x64x!tt.ptr<f8E5M2>>
      %c3_i32 = arith.constant 3 : i32
      %126 = arith.muli %c64_i32, %c3_i32 : i32
      %127 = arith.addi %arg6, %126 : i32
      %128 = tt.splat %127 : i32 -> tensor<64xi32>
      %129 = arith.addi %128, %1 : tensor<64xi32>
      %130 = arith.muli %0, %c8192_i32 : i32
      %131 = tt.expand_dims %129 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
      %132 = arith.muli %131, %cst_3 : tensor<64x1xi32>
      %133 = tt.splat %130 : i32 -> tensor<64x1xi32>
      %134 = arith.addi %133, %132 : tensor<64x1xi32>
      %135 = tt.expand_dims %1 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %136 = tt.broadcast %134 : tensor<64x1xi32> -> tensor<64x64xi32>
      %137 = tt.broadcast %135 : tensor<1x64xi32> -> tensor<64x64xi32>
      %138 = arith.addi %136, %137 : tensor<64x64xi32>
      %139 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %140 = tt.addptr %139, %138 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      %141 = tt.load %140 : tensor<64x64x!tt.ptr<f8E5M2>>
      %142:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c32_i32 iter_args(%arg8 = %cst_5, %arg9 = %cst_4, %arg10 = %cst_6) -> (tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>)  : i32 {
        %168 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32>
        %169 = tt.splat %arg7 : i32 -> tensor<32xi32>
        %170 = arith.addi %169, %168 : tensor<32xi32>
        %171 = tt.expand_dims %170 {axis = 1 : i32} : tensor<32xi32> -> tensor<32x1xi32>
        %172 = arith.muli %171, %cst_2 : tensor<32x1xi32>
        %173 = tt.splat %130 : i32 -> tensor<32x1xi32>
        %174 = arith.addi %173, %172 : tensor<32x1xi32>
        %175 = tt.broadcast %174 : tensor<32x1xi32> -> tensor<32x64xi32>
        %176 = tt.broadcast %135 : tensor<1x64xi32> -> tensor<32x64xi32>
        %177 = arith.addi %175, %176 : tensor<32x64xi32>
        %178 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<32x64x!tt.ptr<f8E5M2>>
        %179 = tt.addptr %178, %177 : tensor<32x64x!tt.ptr<f8E5M2>>, tensor<32x64xi32>
        %180 = tt.load %179 : tensor<32x64x!tt.ptr<f8E5M2>>
        %181 = tt.trans %180 {order = array<i32: 1, 0>} : tensor<32x64xf8E5M2> -> tensor<64x32xf8E5M2>
        %182 = tt.dot %141, %181, %cst_1, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x64xf8E5M2> * tensor<64x32xf8E5M2> -> tensor<64x32xf32>
        %183 = arith.mulf %182, %cst_0 : tensor<64x32xf32>
        %184 = "tt.reduce"(%183) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %215 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %215 : f32
        }) : (tensor<64x32xf32>) -> tensor<64xf32>
        %185 = arith.cmpf ogt, %arg10, %184 : tensor<64xf32>
        %186 = arith.cmpf une, %arg10, %arg10 : tensor<64xf32>
        %187 = arith.ori %185, %186 : tensor<64xi1>
        %188 = arith.select %187, %arg10, %184 : tensor<64xi1>, tensor<64xf32>
        %189 = tt.expand_dims %188 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %190 = tt.broadcast %189 : tensor<64x1xf32> -> tensor<64x32xf32>
        %191 = arith.subf %183, %190 : tensor<64x32xf32>
        %192 = tt.extern_elementwise %191 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64x32xf32>) -> tensor<64x32xf32>
        %193 = "tt.reduce"(%192) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %215 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %215 : f32
        }) : (tensor<64x32xf32>) -> tensor<64xf32>
        %194 = arith.subf %arg10, %188 : tensor<64xf32>
        %195 = tt.extern_elementwise %194 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64xf32>) -> tensor<64xf32>
        %196 = arith.mulf %arg8, %195 : tensor<64xf32>
        %197 = arith.addf %196, %193 : tensor<64xf32>
        %198 = tt.expand_dims %195 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %199 = tt.broadcast %198 : tensor<64x1xf32> -> tensor<64x64xf32>
        %200 = arith.mulf %arg9, %199 : tensor<64x64xf32>
        %201 = tt.expand_dims %1 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %202 = arith.addi %133, %201 : tensor<64x1xi32>
        %203 = tt.expand_dims %170 {axis = 0 : i32} : tensor<32xi32> -> tensor<1x32xi32>
        %204 = arith.muli %203, %cst : tensor<1x32xi32>
        %205 = tt.broadcast %202 : tensor<64x1xi32> -> tensor<64x32xi32>
        %206 = tt.broadcast %204 : tensor<1x32xi32> -> tensor<64x32xi32>
        %207 = arith.addi %205, %206 : tensor<64x32xi32>
        %208 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x32x!tt.ptr<f8E5M2>>
        %209 = tt.addptr %208, %207 : tensor<64x32x!tt.ptr<f8E5M2>>, tensor<64x32xi32>
        %210 = tt.load %209 : tensor<64x32x!tt.ptr<f8E5M2>>
        %211 = tt.fp_to_fp %192, rounding = rtne : tensor<64x32xf32> -> tensor<64x32xf8E5M2>
        %212 = tt.trans %210 {order = array<i32: 1, 0>} : tensor<64x32xf8E5M2> -> tensor<32x64xf8E5M2>
        %213 = tt.dot %211, %212, %cst_4, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x32xf8E5M2> * tensor<32x64xf8E5M2> -> tensor<64x64xf32>
        %214 = arith.addf %200, %213 : tensor<64x64xf32>
        scf.yield %197, %214, %188 : tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>
      } {tt.flatten, tt.loop_unroll_factor = 1 : i32}
      %143 = tt.expand_dims %142#0 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
      %144 = tt.broadcast %143 : tensor<64x1xf32> -> tensor<64x64xf32>
      %145 = arith.divf %142#1, %144 : tensor<64x64xf32>
      %146 = tt.fp_to_fp %145, rounding = rtne : tensor<64x64xf32> -> tensor<64x64xf8E5M2>
      %147 = arith.divsi %0, %arg5 : i32
      %148 = arith.remsi %0, %arg5 : i32
      %149 = arith.cmpi ne, %148, %c0_i32 : i32
      %150 = arith.subi %147, %c1_i32 : i32
      %151 = arith.select %149, %150, %147 : i32
      %152 = arith.cmpi slt, %0, %c0_i32 : i32
      %153 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %154 = arith.cmpi ne, %152, %153 : i1
      %155 = arith.select %154, %151, %147 : i32
      %156 = arith.andi %149, %154 : i1
      %157 = arith.addi %148, %arg5 : i32
      %158 = arith.select %156, %157, %148 : i32
      %159 = arith.muli %155, %arg4 : i32
      %160 = arith.muli %158, %c8192_i32 : i32
      %161 = arith.addi %159, %160 : i32
      %162 = tt.splat %161 : i32 -> tensor<64x1xi32>
      %163 = arith.addi %162, %132 : tensor<64x1xi32>
      %164 = tt.broadcast %163 : tensor<64x1xi32> -> tensor<64x64xi32>
      %165 = arith.addi %164, %137 : tensor<64x64xi32>
      %166 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %167 = tt.addptr %166, %165 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      tt.store %167, %146 : tensor<64x64x!tt.ptr<f8E5M2>>
    } {tt.disallow_acc_multi_buffer, tt.flatten, tt.num_stages = 4 : i32}
    scf.for %arg6 = %c0_i32_7 to %c128_i32 step %c64_i32  : i32 {
      %2 = tt.splat %arg6 : i32 -> tensor<64xi32>
      %3 = arith.addi %2, %1 : tensor<64xi32>
      %4 = arith.muli %0, %c8192_i32 : i32
      %5 = tt.expand_dims %3 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
      %6 = arith.muli %5, %cst_3 : tensor<64x1xi32>
      %7 = tt.splat %4 : i32 -> tensor<64x1xi32>
      %8 = arith.addi %7, %6 : tensor<64x1xi32>
      %9 = tt.expand_dims %1 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %10 = tt.broadcast %8 : tensor<64x1xi32> -> tensor<64x64xi32>
      %11 = tt.broadcast %9 : tensor<1x64xi32> -> tensor<64x64xi32>
      %12 = arith.addi %10, %11 : tensor<64x64xi32>
      %13 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %14 = tt.addptr %13, %12 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      %15 = tt.load %14 : tensor<64x64x!tt.ptr<f8E5M2>>
      %16:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c32_i32 iter_args(%arg8 = %cst_5, %arg9 = %cst_4, %arg10 = %cst_6) -> (tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>)  : i32 {
        %42 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32>
        %43 = tt.splat %arg7 : i32 -> tensor<32xi32>
        %44 = arith.addi %43, %42 : tensor<32xi32>
        %45 = tt.expand_dims %44 {axis = 1 : i32} : tensor<32xi32> -> tensor<32x1xi32>
        %46 = arith.muli %45, %cst_2 : tensor<32x1xi32>
        %47 = tt.splat %4 : i32 -> tensor<32x1xi32>
        %48 = arith.addi %47, %46 : tensor<32x1xi32>
        %49 = tt.broadcast %48 : tensor<32x1xi32> -> tensor<32x64xi32>
        %50 = tt.broadcast %9 : tensor<1x64xi32> -> tensor<32x64xi32>
        %51 = arith.addi %49, %50 : tensor<32x64xi32>
        %52 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<32x64x!tt.ptr<f8E5M2>>
        %53 = tt.addptr %52, %51 : tensor<32x64x!tt.ptr<f8E5M2>>, tensor<32x64xi32>
        %54 = tt.load %53 : tensor<32x64x!tt.ptr<f8E5M2>>
        %55 = tt.trans %54 {order = array<i32: 1, 0>} : tensor<32x64xf8E5M2> -> tensor<64x32xf8E5M2>
        %56 = tt.dot %15, %55, %cst_1, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x64xf8E5M2> * tensor<64x32xf8E5M2> -> tensor<64x32xf32>
        %57 = arith.mulf %56, %cst_0 : tensor<64x32xf32>
        %58 = "tt.reduce"(%57) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %89 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %89 : f32
        }) : (tensor<64x32xf32>) -> tensor<64xf32>
        %59 = arith.cmpf ogt, %arg10, %58 : tensor<64xf32>
        %60 = arith.cmpf une, %arg10, %arg10 : tensor<64xf32>
        %61 = arith.ori %59, %60 : tensor<64xi1>
        %62 = arith.select %61, %arg10, %58 : tensor<64xi1>, tensor<64xf32>
        %63 = tt.expand_dims %62 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %64 = tt.broadcast %63 : tensor<64x1xf32> -> tensor<64x32xf32>
        %65 = arith.subf %57, %64 : tensor<64x32xf32>
        %66 = tt.extern_elementwise %65 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64x32xf32>) -> tensor<64x32xf32>
        %67 = "tt.reduce"(%66) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %89 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %89 : f32
        }) : (tensor<64x32xf32>) -> tensor<64xf32>
        %68 = arith.subf %arg10, %62 : tensor<64xf32>
        %69 = tt.extern_elementwise %68 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64xf32>) -> tensor<64xf32>
        %70 = arith.mulf %arg8, %69 : tensor<64xf32>
        %71 = arith.addf %70, %67 : tensor<64xf32>
        %72 = tt.expand_dims %69 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %73 = tt.broadcast %72 : tensor<64x1xf32> -> tensor<64x64xf32>
        %74 = arith.mulf %arg9, %73 : tensor<64x64xf32>
        %75 = tt.expand_dims %1 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %76 = arith.addi %7, %75 : tensor<64x1xi32>
        %77 = tt.expand_dims %44 {axis = 0 : i32} : tensor<32xi32> -> tensor<1x32xi32>
        %78 = arith.muli %77, %cst : tensor<1x32xi32>
        %79 = tt.broadcast %76 : tensor<64x1xi32> -> tensor<64x32xi32>
        %80 = tt.broadcast %78 : tensor<1x32xi32> -> tensor<64x32xi32>
        %81 = arith.addi %79, %80 : tensor<64x32xi32>
        %82 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x32x!tt.ptr<f8E5M2>>
        %83 = tt.addptr %82, %81 : tensor<64x32x!tt.ptr<f8E5M2>>, tensor<64x32xi32>
        %84 = tt.load %83 : tensor<64x32x!tt.ptr<f8E5M2>>
        %85 = tt.fp_to_fp %66, rounding = rtne : tensor<64x32xf32> -> tensor<64x32xf8E5M2>
        %86 = tt.trans %84 {order = array<i32: 1, 0>} : tensor<64x32xf8E5M2> -> tensor<32x64xf8E5M2>
        %87 = tt.dot %85, %86, %cst_4, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x32xf8E5M2> * tensor<32x64xf8E5M2> -> tensor<64x64xf32>
        %88 = arith.addf %74, %87 : tensor<64x64xf32>
        scf.yield %71, %88, %62 : tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>
      } {tt.flatten, tt.loop_unroll_factor = 1 : i32}
      %17 = tt.expand_dims %16#0 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
      %18 = tt.broadcast %17 : tensor<64x1xf32> -> tensor<64x64xf32>
      %19 = arith.divf %16#1, %18 : tensor<64x64xf32>
      %20 = tt.fp_to_fp %19, rounding = rtne : tensor<64x64xf32> -> tensor<64x64xf8E5M2>
      %21 = arith.divsi %0, %arg5 : i32
      %22 = arith.remsi %0, %arg5 : i32
      %23 = arith.cmpi ne, %22, %c0_i32 : i32
      %24 = arith.subi %21, %c1_i32 : i32
      %25 = arith.select %23, %24, %21 : i32
      %26 = arith.cmpi slt, %0, %c0_i32 : i32
      %27 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %28 = arith.cmpi ne, %26, %27 : i1
      %29 = arith.select %28, %25, %21 : i32
      %30 = arith.andi %23, %28 : i1
      %31 = arith.addi %22, %arg5 : i32
      %32 = arith.select %30, %31, %22 : i32
      %33 = arith.muli %29, %arg4 : i32
      %34 = arith.muli %32, %c8192_i32 : i32
      %35 = arith.addi %33, %34 : i32
      %36 = tt.splat %35 : i32 -> tensor<64x1xi32>
      %37 = arith.addi %36, %6 : tensor<64x1xi32>
      %38 = tt.broadcast %37 : tensor<64x1xi32> -> tensor<64x64xi32>
      %39 = arith.addi %38, %11 : tensor<64x64xi32>
      %40 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %41 = tt.addptr %40, %39 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      tt.store %41, %20 : tensor<64x64x!tt.ptr<f8E5M2>>
    } {tt.disallow_acc_multi_buffer, tt.flatten, tt.num_stages = 1 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(convert-triton-to-tritongpu{enable-source-remat=false num-ctas=1 num-warps=8 target=cuda:90 threads-per-warp=32}, tritongpu-coalesce, tritongpu-F32DotTC, triton-nvidia-gpu-plan-cta, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritongpu-accelerate-matmul, tritongpu-remove-layout-conversions, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, triton-nvidia-optimize-descriptor-encoding, triton-loop-aware-cse, tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-combine-tensor-select-and-if, nvgpu-warp-specialization{dump-intermediate-steps=false num-stages=5}, tritongpu-assign-latencies{num-stages=5}, tritongpu-schedule-loops, tritongpu-pipeline{dump-intermediate-steps=false num-stages=5}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-loop-aware-cse, tritongpu-prefetch, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-coalesce-async-copy, triton-nvidia-optimize-tmem-layouts, tritongpu-remove-layout-conversions, triton-nvidia-interleave-tmem, tritongpu-reduce-data-duplication, tritongpu-reorder-instructions, triton-loop-aware-cse, symbol-dce, triton-nvidia-tma-lowering, triton-nvidia-gpu-fence-insertion{compute-capability=90}, sccp, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true})",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_willfeng/pp/cpp26ah2udj75q6vy3hprdotrsjh2uudml7u7mh37gyzogka6o4u.py:12:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_willfeng/pp/cpp26ah2udj75q6vy3hprdotrsjh2uudml7u7mh37gyzogka6o4u.py:12:0: note: Pipeline failed while executing [`TritonGPUAssignLatencies` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
[276s] Generation 3: replaced=14 min=0.0121 mid=0.0198 max=0.0415 best=Config(block_sizes=[128, 64], range_unroll_factors=[0, 0, 1], range_num_stages=[0, 0, 2], range_multi_buffers=[None, False, False], range_flattens=[None, None, None], num_warps=4, num_stages=5, indexing='block_ptr', pid_type='flat', range_warp_specializes=[])
python: /root/.triton/llvm/llvm-8957e64a-almalinux-x64/include/llvm/ADT/SmallVector.h:296: const_reference llvm::SmallVectorTemplateCommon<long>::operator[](size_type) const [T = long]: Assertion `idx < size()' failed.
module {
  tt.func public @_fp8_attention_kernel_kernel(%arg0: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %c1_i32 = arith.constant 1 : i32
    %c64_i32 = arith.constant 64 : i32
    %c128_i32 = arith.constant 128 : i32
    %c0_i32 = arith.constant 0 : i32
    %c132_i32 = arith.constant 132 : i32
    %c192_i32 = arith.constant 192 : i32
    %cst = arith.constant dense<64> : tensor<1x64xi32>
    %cst_0 = arith.constant dense<0.180336878> : tensor<128x64xf32>
    %cst_1 = arith.constant dense<64> : tensor<64x1xi32>
    %cst_2 = arith.constant dense<64> : tensor<128x1xi32>
    %c8192_i32 = arith.constant 8192 : i32
    %cst_3 = arith.constant dense<0.000000e+00> : tensor<128x64xf32>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<128xf32>
    %cst_5 = arith.constant dense<0xFF800000> : tensor<128xf32>
    %0 = tt.get_program_id x : i32
    scf.for %arg6 = %0 to %c192_i32 step %c132_i32  : i32 {
      %1 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
      %2 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>
      %3 = arith.muli %arg6, %c8192_i32 : i32
      %4 = tt.expand_dims %2 {axis = 1 : i32} : tensor<128xi32> -> tensor<128x1xi32>
      %5 = arith.muli %4, %cst_2 : tensor<128x1xi32>
      %6 = tt.splat %3 : i32 -> tensor<128x1xi32>
      %7 = arith.addi %6, %5 : tensor<128x1xi32>
      %8 = tt.expand_dims %1 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %9 = tt.broadcast %7 : tensor<128x1xi32> -> tensor<128x64xi32>
      %10 = tt.broadcast %8 : tensor<1x64xi32> -> tensor<128x64xi32>
      %11 = arith.addi %9, %10 : tensor<128x64xi32>
      %12 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %13 = tt.addptr %12, %11 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      %14 = tt.load %13 : tensor<128x64x!tt.ptr<f8E5M2>>
      %15:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c64_i32 iter_args(%arg8 = %cst_4, %arg9 = %cst_3, %arg10 = %cst_5) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %41 = tt.splat %arg7 : i32 -> tensor<64xi32>
        %42 = arith.addi %41, %1 : tensor<64xi32>
        %43 = tt.expand_dims %42 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %44 = arith.muli %43, %cst_1 : tensor<64x1xi32>
        %45 = tt.splat %3 : i32 -> tensor<64x1xi32>
        %46 = arith.addi %45, %44 : tensor<64x1xi32>
        %47 = tt.broadcast %46 : tensor<64x1xi32> -> tensor<64x64xi32>
        %48 = tt.broadcast %8 : tensor<1x64xi32> -> tensor<64x64xi32>
        %49 = arith.addi %47, %48 : tensor<64x64xi32>
        %50 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %51 = tt.addptr %50, %49 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %52 = tt.load %51 : tensor<64x64x!tt.ptr<f8E5M2>>
        %53 = tt.trans %52 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %54 = tt.dot %14, %53, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %55 = arith.mulf %54, %cst_0 : tensor<128x64xf32>
        %56 = "tt.reduce"(%55) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %87 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %87 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %57 = arith.cmpf ogt, %arg10, %56 : tensor<128xf32>
        %58 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %59 = arith.ori %57, %58 : tensor<128xi1>
        %60 = arith.select %59, %arg10, %56 : tensor<128xi1>, tensor<128xf32>
        %61 = tt.expand_dims %60 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %62 = tt.broadcast %61 : tensor<128x1xf32> -> tensor<128x64xf32>
        %63 = arith.subf %55, %62 : tensor<128x64xf32>
        %64 = tt.extern_elementwise %63 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %65 = "tt.reduce"(%64) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %87 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %87 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %66 = arith.subf %arg10, %60 : tensor<128xf32>
        %67 = tt.extern_elementwise %66 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %68 = arith.mulf %arg8, %67 : tensor<128xf32>
        %69 = arith.addf %68, %65 : tensor<128xf32>
        %70 = tt.expand_dims %67 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %71 = tt.broadcast %70 : tensor<128x1xf32> -> tensor<128x64xf32>
        %72 = arith.mulf %arg9, %71 : tensor<128x64xf32>
        %73 = tt.expand_dims %1 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %74 = arith.addi %45, %73 : tensor<64x1xi32>
        %75 = tt.expand_dims %42 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %76 = arith.muli %75, %cst : tensor<1x64xi32>
        %77 = tt.broadcast %74 : tensor<64x1xi32> -> tensor<64x64xi32>
        %78 = tt.broadcast %76 : tensor<1x64xi32> -> tensor<64x64xi32>
        %79 = arith.addi %77, %78 : tensor<64x64xi32>
        %80 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %81 = tt.addptr %80, %79 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %82 = tt.load %81 : tensor<64x64x!tt.ptr<f8E5M2>>
        %83 = tt.fp_to_fp %64, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %84 = tt.trans %82 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %85 = tt.dot %83, %84, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %86 = arith.addf %72, %85 : tensor<128x64xf32>
        scf.yield %69, %86, %60 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.disallow_acc_multi_buffer, tt.num_stages = 2 : i32}
      %16 = tt.expand_dims %15#0 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
      %17 = tt.broadcast %16 : tensor<128x1xf32> -> tensor<128x64xf32>
      %18 = arith.divf %15#1, %17 : tensor<128x64xf32>
      %19 = tt.fp_to_fp %18, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
      %20 = arith.divsi %arg6, %arg5 : i32
      %21 = arith.remsi %arg6, %arg5 : i32
      %22 = arith.cmpi ne, %21, %c0_i32 : i32
      %23 = arith.subi %20, %c1_i32 : i32
      %24 = arith.select %22, %23, %20 : i32
      %25 = arith.cmpi slt, %arg6, %c0_i32 : i32
      %26 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %27 = arith.cmpi ne, %25, %26 : i1
      %28 = arith.select %27, %24, %20 : i32
      %29 = arith.andi %22, %27 : i1
      %30 = arith.addi %21, %arg5 : i32
      %31 = arith.select %29, %30, %21 : i32
      %32 = arith.muli %28, %arg4 : i32
      %33 = arith.muli %31, %c8192_i32 : i32
      %34 = arith.addi %32, %33 : i32
      %35 = tt.splat %34 : i32 -> tensor<128x1xi32>
      %36 = arith.addi %35, %5 : tensor<128x1xi32>
      %37 = tt.broadcast %36 : tensor<128x1xi32> -> tensor<128x64xi32>
      %38 = arith.addi %37, %10 : tensor<128x64xi32>
      %39 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %40 = tt.addptr %39, %38 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      tt.store %40, %19 : tensor<128x64x!tt.ptr<f8E5M2>>
    } {tt.disallow_acc_multi_buffer, tt.flatten, tt.num_stages = 2 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(convert-triton-to-tritongpu{enable-source-remat=false num-ctas=1 num-warps=4 target=cuda:90 threads-per-warp=32}, tritongpu-coalesce, tritongpu-F32DotTC, triton-nvidia-gpu-plan-cta, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritongpu-accelerate-matmul, tritongpu-remove-layout-conversions, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, triton-nvidia-optimize-descriptor-encoding, triton-loop-aware-cse, tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-combine-tensor-select-and-if, nvgpu-warp-specialization{dump-intermediate-steps=false num-stages=7}, tritongpu-assign-latencies{num-stages=7}, tritongpu-schedule-loops, tritongpu-pipeline{dump-intermediate-steps=false num-stages=7}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-loop-aware-cse, tritongpu-prefetch, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-coalesce-async-copy, triton-nvidia-optimize-tmem-layouts, tritongpu-remove-layout-conversions, triton-nvidia-interleave-tmem, tritongpu-reduce-data-duplication, tritongpu-reorder-instructions, triton-loop-aware-cse, symbol-dce, triton-nvidia-tma-lowering, triton-nvidia-gpu-fence-insertion{compute-capability=90}, sccp, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true})",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_willfeng/f3/cf3xksss2ynjwxpa4dgjppsnxeloxhnpavjwn2vysy3u4caeykec.py:13:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_willfeng/f3/cf3xksss2ynjwxpa4dgjppsnxeloxhnpavjwn2vysy3u4caeykec.py:13:0: note: Pipeline failed while executing [`TritonGPUAssignLatencies` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
Process ForkProcess-264:
Traceback (most recent call last):
  File "/home/willfeng/local/miniconda3/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/willfeng/local/miniconda3/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/data/users/willfeng/helion/helion/runtime/precompile_shim.py", line 54, in finish_it
    kernel_cache[key] = fn.compile(
                        ^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/compiler/compiler.py", line 359, in compile
    next_module = compile_ir(module, metadata)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/backends/nvidia/compiler.py", line 456, in <lambda>
    stages["ttgir"] = lambda src, metadata: self.make_ttgir(src, metadata, options, capability)
                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/backends/nvidia/compiler.py", line 298, in make_ttgir
    pm.run(mod)
RuntimeError: PassManager::run failed
python: /root/.triton/llvm/llvm-8957e64a-almalinux-x64/include/llvm/ADT/SmallVector.h:296: const_reference llvm::SmallVectorTemplateCommon<long>::operator[](size_type) const [T = long]: Assertion `idx < size()' failed.
module {
  tt.func public @_fp8_attention_kernel_kernel(%arg0: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %c1_i32 = arith.constant 1 : i32
    %c64_i32 = arith.constant 64 : i32
    %c128_i32 = arith.constant 128 : i32
    %c0_i32 = arith.constant 0 : i32
    %c132_i32 = arith.constant 132 : i32
    %c192_i32 = arith.constant 192 : i32
    %cst = arith.constant dense<64> : tensor<1x64xi32>
    %cst_0 = arith.constant dense<0.180336878> : tensor<128x64xf32>
    %cst_1 = arith.constant dense<64> : tensor<64x1xi32>
    %cst_2 = arith.constant dense<64> : tensor<128x1xi32>
    %c8192_i32 = arith.constant 8192 : i32
    %cst_3 = arith.constant dense<0.000000e+00> : tensor<128x64xf32>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<128xf32>
    %cst_5 = arith.constant dense<0xFF800000> : tensor<128xf32>
    %0 = tt.get_program_id x : i32
    scf.for %arg6 = %0 to %c192_i32 step %c132_i32  : i32 {
      %1 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
      %2 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>
      %3 = arith.muli %arg6, %c8192_i32 : i32
      %4 = tt.expand_dims %2 {axis = 1 : i32} : tensor<128xi32> -> tensor<128x1xi32>
      %5 = arith.muli %4, %cst_2 : tensor<128x1xi32>
      %6 = tt.splat %3 : i32 -> tensor<128x1xi32>
      %7 = arith.addi %6, %5 : tensor<128x1xi32>
      %8 = tt.expand_dims %1 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %9 = tt.broadcast %7 : tensor<128x1xi32> -> tensor<128x64xi32>
      %10 = tt.broadcast %8 : tensor<1x64xi32> -> tensor<128x64xi32>
      %11 = arith.addi %9, %10 : tensor<128x64xi32>
      %12 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %13 = tt.addptr %12, %11 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      %14 = tt.load %13 : tensor<128x64x!tt.ptr<f8E5M2>>
      %15:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c64_i32 iter_args(%arg8 = %cst_4, %arg9 = %cst_3, %arg10 = %cst_5) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %41 = tt.splat %arg7 : i32 -> tensor<64xi32>
        %42 = arith.addi %41, %1 : tensor<64xi32>
        %43 = tt.expand_dims %42 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %44 = arith.muli %43, %cst_1 : tensor<64x1xi32>
        %45 = tt.splat %3 : i32 -> tensor<64x1xi32>
        %46 = arith.addi %45, %44 : tensor<64x1xi32>
        %47 = tt.broadcast %46 : tensor<64x1xi32> -> tensor<64x64xi32>
        %48 = tt.broadcast %8 : tensor<1x64xi32> -> tensor<64x64xi32>
        %49 = arith.addi %47, %48 : tensor<64x64xi32>
        %50 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %51 = tt.addptr %50, %49 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %52 = tt.load %51 : tensor<64x64x!tt.ptr<f8E5M2>>
        %53 = tt.trans %52 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %54 = tt.dot %14, %53, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %55 = arith.mulf %54, %cst_0 : tensor<128x64xf32>
        %56 = "tt.reduce"(%55) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %87 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %87 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %57 = arith.cmpf ogt, %arg10, %56 : tensor<128xf32>
        %58 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %59 = arith.ori %57, %58 : tensor<128xi1>
        %60 = arith.select %59, %arg10, %56 : tensor<128xi1>, tensor<128xf32>
        %61 = tt.expand_dims %60 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %62 = tt.broadcast %61 : tensor<128x1xf32> -> tensor<128x64xf32>
        %63 = arith.subf %55, %62 : tensor<128x64xf32>
        %64 = tt.extern_elementwise %63 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %65 = "tt.reduce"(%64) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %87 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %87 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %66 = arith.subf %arg10, %60 : tensor<128xf32>
        %67 = tt.extern_elementwise %66 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %68 = arith.mulf %arg8, %67 : tensor<128xf32>
        %69 = arith.addf %68, %65 : tensor<128xf32>
        %70 = tt.expand_dims %67 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %71 = tt.broadcast %70 : tensor<128x1xf32> -> tensor<128x64xf32>
        %72 = arith.mulf %arg9, %71 : tensor<128x64xf32>
        %73 = tt.expand_dims %1 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %74 = arith.addi %45, %73 : tensor<64x1xi32>
        %75 = tt.expand_dims %42 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %76 = arith.muli %75, %cst : tensor<1x64xi32>
        %77 = tt.broadcast %74 : tensor<64x1xi32> -> tensor<64x64xi32>
        %78 = tt.broadcast %76 : tensor<1x64xi32> -> tensor<64x64xi32>
        %79 = arith.addi %77, %78 : tensor<64x64xi32>
        %80 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %81 = tt.addptr %80, %79 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %82 = tt.load %81 : tensor<64x64x!tt.ptr<f8E5M2>>
        %83 = tt.fp_to_fp %64, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %84 = tt.trans %82 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %85 = tt.dot %83, %84, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %86 = arith.addf %72, %85 : tensor<128x64xf32>
        scf.yield %69, %86, %60 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.disallow_acc_multi_buffer, tt.num_stages = 2 : i32}
      %16 = tt.expand_dims %15#0 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
      %17 = tt.broadcast %16 : tensor<128x1xf32> -> tensor<128x64xf32>
      %18 = arith.divf %15#1, %17 : tensor<128x64xf32>
      %19 = tt.fp_to_fp %18, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
      %20 = arith.divsi %arg6, %arg5 : i32
      %21 = arith.remsi %arg6, %arg5 : i32
      %22 = arith.cmpi ne, %21, %c0_i32 : i32
      %23 = arith.subi %20, %c1_i32 : i32
      %24 = arith.select %22, %23, %20 : i32
      %25 = arith.cmpi slt, %arg6, %c0_i32 : i32
      %26 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %27 = arith.cmpi ne, %25, %26 : i1
      %28 = arith.select %27, %24, %20 : i32
      %29 = arith.andi %22, %27 : i1
      %30 = arith.addi %21, %arg5 : i32
      %31 = arith.select %29, %30, %21 : i32
      %32 = arith.muli %28, %arg4 : i32
      %33 = arith.muli %31, %c8192_i32 : i32
      %34 = arith.addi %32, %33 : i32
      %35 = tt.splat %34 : i32 -> tensor<128x1xi32>
      %36 = arith.addi %35, %5 : tensor<128x1xi32>
      %37 = tt.broadcast %36 : tensor<128x1xi32> -> tensor<128x64xi32>
      %38 = arith.addi %37, %10 : tensor<128x64xi32>
      %39 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %40 = tt.addptr %39, %38 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      tt.store %40, %19 : tensor<128x64x!tt.ptr<f8E5M2>>
    } {tt.disallow_acc_multi_buffer, tt.flatten, tt.num_stages = 2 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(convert-triton-to-tritongpu{enable-source-remat=false num-ctas=1 num-warps=4 target=cuda:90 threads-per-warp=32}, tritongpu-coalesce, tritongpu-F32DotTC, triton-nvidia-gpu-plan-cta, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritongpu-accelerate-matmul, tritongpu-remove-layout-conversions, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, triton-nvidia-optimize-descriptor-encoding, triton-loop-aware-cse, tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-combine-tensor-select-and-if, nvgpu-warp-specialization{dump-intermediate-steps=false num-stages=7}, tritongpu-assign-latencies{num-stages=7}, tritongpu-schedule-loops, tritongpu-pipeline{dump-intermediate-steps=false num-stages=7}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-loop-aware-cse, tritongpu-prefetch, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-coalesce-async-copy, triton-nvidia-optimize-tmem-layouts, tritongpu-remove-layout-conversions, triton-nvidia-interleave-tmem, tritongpu-reduce-data-duplication, tritongpu-reorder-instructions, triton-loop-aware-cse, symbol-dce, triton-nvidia-tma-lowering, triton-nvidia-gpu-fence-insertion{compute-capability=90}, sccp, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true})",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_willfeng/f3/cf3xksss2ynjwxpa4dgjppsnxeloxhnpavjwn2vysy3u4caeykec.py:13:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_willfeng/f3/cf3xksss2ynjwxpa4dgjppsnxeloxhnpavjwn2vysy3u4caeykec.py:13:0: note: Pipeline failed while executing [`TritonGPUAssignLatencies` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
python: /root/.triton/llvm/llvm-8957e64a-almalinux-x64/include/llvm/ADT/SmallVector.h:296: const_reference llvm::SmallVectorTemplateCommon<long>::operator[](size_type) const [T = long]: Assertion `idx < size()' failed.
module {
  tt.func public @_fp8_attention_kernel_kernel(%arg0: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %c1_i32 = arith.constant 1 : i32
    %c64_i32 = arith.constant 64 : i32
    %c128_i32 = arith.constant 128 : i32
    %c0_i32 = arith.constant 0 : i32
    %cst = arith.constant dense<64> : tensor<1x64xi32>
    %cst_0 = arith.constant dense<0.180336878> : tensor<64x64xf32>
    %cst_1 = arith.constant dense<64> : tensor<64x1xi32>
    %c8192_i32 = arith.constant 8192 : i32
    %cst_2 = arith.constant dense<0.000000e+00> : tensor<64x64xf32>
    %cst_3 = arith.constant dense<0.000000e+00> : tensor<64xf32>
    %cst_4 = arith.constant dense<0xFF800000> : tensor<64xf32>
    %0 = tt.get_program_id x : i32
    %1 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
    %c0_i32_5 = arith.constant 0 : i32
    %c192_i32 = arith.constant 192 : i32
    scf.for %arg6 = %c0_i32 to %c0_i32_5 step %c192_i32  : i32 {
      %2 = tt.splat %arg6 : i32 -> tensor<64xi32>
      %3 = arith.addi %2, %1 : tensor<64xi32>
      %4 = arith.muli %0, %c8192_i32 : i32
      %5 = tt.expand_dims %3 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
      %6 = arith.muli %5, %cst_1 : tensor<64x1xi32>
      %7 = tt.splat %4 : i32 -> tensor<64x1xi32>
      %8 = arith.addi %7, %6 : tensor<64x1xi32>
      %9 = tt.expand_dims %1 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %10 = tt.broadcast %8 : tensor<64x1xi32> -> tensor<64x64xi32>
      %11 = tt.broadcast %9 : tensor<1x64xi32> -> tensor<64x64xi32>
      %12 = arith.addi %10, %11 : tensor<64x64xi32>
      %13 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %14 = tt.addptr %13, %12 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      %15 = tt.load %14 : tensor<64x64x!tt.ptr<f8E5M2>>
      %16:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c64_i32 iter_args(%arg8 = %cst_3, %arg9 = %cst_2, %arg10 = %cst_4) -> (tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>)  : i32 {
        %126 = tt.splat %arg7 : i32 -> tensor<64xi32>
        %127 = arith.addi %126, %1 : tensor<64xi32>
        %128 = tt.expand_dims %127 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %129 = arith.muli %128, %cst_1 : tensor<64x1xi32>
        %130 = arith.addi %7, %129 : tensor<64x1xi32>
        %131 = tt.broadcast %130 : tensor<64x1xi32> -> tensor<64x64xi32>
        %132 = arith.addi %131, %11 : tensor<64x64xi32>
        %133 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %134 = tt.addptr %133, %132 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %135 = tt.load %134 : tensor<64x64x!tt.ptr<f8E5M2>>
        %136 = tt.trans %135 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %137 = tt.dot %15, %136, %cst_2, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<64x64xf32>
        %138 = arith.mulf %137, %cst_0 : tensor<64x64xf32>
        %139 = "tt.reduce"(%138) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %170 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %170 : f32
        }) : (tensor<64x64xf32>) -> tensor<64xf32>
        %140 = arith.cmpf ogt, %arg10, %139 : tensor<64xf32>
        %141 = arith.cmpf une, %arg10, %arg10 : tensor<64xf32>
        %142 = arith.ori %140, %141 : tensor<64xi1>
        %143 = arith.select %142, %arg10, %139 : tensor<64xi1>, tensor<64xf32>
        %144 = tt.expand_dims %143 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %145 = tt.broadcast %144 : tensor<64x1xf32> -> tensor<64x64xf32>
        %146 = arith.subf %138, %145 : tensor<64x64xf32>
        %147 = tt.extern_elementwise %146 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64x64xf32>) -> tensor<64x64xf32>
        %148 = "tt.reduce"(%147) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %170 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %170 : f32
        }) : (tensor<64x64xf32>) -> tensor<64xf32>
        %149 = arith.subf %arg10, %143 : tensor<64xf32>
        %150 = tt.extern_elementwise %149 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64xf32>) -> tensor<64xf32>
        %151 = arith.mulf %arg8, %150 : tensor<64xf32>
        %152 = arith.addf %151, %148 : tensor<64xf32>
        %153 = tt.expand_dims %150 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %154 = tt.broadcast %153 : tensor<64x1xf32> -> tensor<64x64xf32>
        %155 = arith.mulf %arg9, %154 : tensor<64x64xf32>
        %156 = tt.expand_dims %1 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %157 = arith.addi %7, %156 : tensor<64x1xi32>
        %158 = tt.expand_dims %127 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %159 = arith.muli %158, %cst : tensor<1x64xi32>
        %160 = tt.broadcast %157 : tensor<64x1xi32> -> tensor<64x64xi32>
        %161 = tt.broadcast %159 : tensor<1x64xi32> -> tensor<64x64xi32>
        %162 = arith.addi %160, %161 : tensor<64x64xi32>
        %163 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %164 = tt.addptr %163, %162 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %165 = tt.load %164 : tensor<64x64x!tt.ptr<f8E5M2>>
        %166 = tt.fp_to_fp %147, rounding = rtne : tensor<64x64xf32> -> tensor<64x64xf8E5M2>
        %167 = tt.trans %165 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %168 = tt.dot %166, %167, %cst_2, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<64x64xf32>
        %169 = arith.addf %155, %168 : tensor<64x64xf32>
        scf.yield %152, %169, %143 : tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>
      } {tt.loop_unroll_factor = 1 : i32, tt.num_stages = 1 : i32}
      %17 = tt.expand_dims %16#0 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
      %18 = tt.broadcast %17 : tensor<64x1xf32> -> tensor<64x64xf32>
      %19 = arith.divf %16#1, %18 : tensor<64x64xf32>
      %20 = tt.fp_to_fp %19, rounding = rtne : tensor<64x64xf32> -> tensor<64x64xf8E5M2>
      %21 = arith.divsi %0, %arg5 : i32
      %22 = arith.remsi %0, %arg5 : i32
      %23 = arith.cmpi ne, %22, %c0_i32 : i32
      %24 = arith.subi %21, %c1_i32 : i32
      %25 = arith.select %23, %24, %21 : i32
      %26 = arith.cmpi slt, %0, %c0_i32 : i32
      %27 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %28 = arith.cmpi ne, %26, %27 : i1
      %29 = arith.select %28, %25, %21 : i32
      %30 = arith.andi %23, %28 : i1
      %31 = arith.addi %22, %arg5 : i32
      %32 = arith.select %30, %31, %22 : i32
      %33 = arith.muli %29, %arg4 : i32
      %34 = arith.muli %32, %c8192_i32 : i32
      %35 = arith.addi %33, %34 : i32
      %36 = tt.splat %35 : i32 -> tensor<64x1xi32>
      %37 = arith.addi %36, %6 : tensor<64x1xi32>
      %38 = tt.broadcast %37 : tensor<64x1xi32> -> tensor<64x64xi32>
      %39 = arith.addi %38, %11 : tensor<64x64xi32>
      %40 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %41 = tt.addptr %40, %39 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      tt.store %41, %20 : tensor<64x64x!tt.ptr<f8E5M2>>
      %c1_i32_6 = arith.constant 1 : i32
      %42 = arith.muli %c64_i32, %c1_i32_6 : i32
      %43 = arith.addi %arg6, %42 : i32
      %44 = tt.splat %43 : i32 -> tensor<64xi32>
      %45 = arith.addi %44, %1 : tensor<64xi32>
      %46 = arith.muli %0, %c8192_i32 : i32
      %47 = tt.expand_dims %45 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
      %48 = arith.muli %47, %cst_1 : tensor<64x1xi32>
      %49 = tt.splat %46 : i32 -> tensor<64x1xi32>
      %50 = arith.addi %49, %48 : tensor<64x1xi32>
      %51 = tt.expand_dims %1 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %52 = tt.broadcast %50 : tensor<64x1xi32> -> tensor<64x64xi32>
      %53 = tt.broadcast %51 : tensor<1x64xi32> -> tensor<64x64xi32>
      %54 = arith.addi %52, %53 : tensor<64x64xi32>
      %55 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %56 = tt.addptr %55, %54 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      %57 = tt.load %56 : tensor<64x64x!tt.ptr<f8E5M2>>
      %58:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c64_i32 iter_args(%arg8 = %cst_3, %arg9 = %cst_2, %arg10 = %cst_4) -> (tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>)  : i32 {
        %126 = tt.splat %arg7 : i32 -> tensor<64xi32>
        %127 = arith.addi %126, %1 : tensor<64xi32>
        %128 = tt.expand_dims %127 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %129 = arith.muli %128, %cst_1 : tensor<64x1xi32>
        %130 = arith.addi %49, %129 : tensor<64x1xi32>
        %131 = tt.broadcast %130 : tensor<64x1xi32> -> tensor<64x64xi32>
        %132 = arith.addi %131, %53 : tensor<64x64xi32>
        %133 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %134 = tt.addptr %133, %132 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %135 = tt.load %134 : tensor<64x64x!tt.ptr<f8E5M2>>
        %136 = tt.trans %135 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %137 = tt.dot %57, %136, %cst_2, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<64x64xf32>
        %138 = arith.mulf %137, %cst_0 : tensor<64x64xf32>
        %139 = "tt.reduce"(%138) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %170 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %170 : f32
        }) : (tensor<64x64xf32>) -> tensor<64xf32>
        %140 = arith.cmpf ogt, %arg10, %139 : tensor<64xf32>
        %141 = arith.cmpf une, %arg10, %arg10 : tensor<64xf32>
        %142 = arith.ori %140, %141 : tensor<64xi1>
        %143 = arith.select %142, %arg10, %139 : tensor<64xi1>, tensor<64xf32>
        %144 = tt.expand_dims %143 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %145 = tt.broadcast %144 : tensor<64x1xf32> -> tensor<64x64xf32>
        %146 = arith.subf %138, %145 : tensor<64x64xf32>
        %147 = tt.extern_elementwise %146 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64x64xf32>) -> tensor<64x64xf32>
        %148 = "tt.reduce"(%147) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %170 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %170 : f32
        }) : (tensor<64x64xf32>) -> tensor<64xf32>
        %149 = arith.subf %arg10, %143 : tensor<64xf32>
        %150 = tt.extern_elementwise %149 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64xf32>) -> tensor<64xf32>
        %151 = arith.mulf %arg8, %150 : tensor<64xf32>
        %152 = arith.addf %151, %148 : tensor<64xf32>
        %153 = tt.expand_dims %150 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %154 = tt.broadcast %153 : tensor<64x1xf32> -> tensor<64x64xf32>
        %155 = arith.mulf %arg9, %154 : tensor<64x64xf32>
        %156 = tt.expand_dims %1 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %157 = arith.addi %49, %156 : tensor<64x1xi32>
        %158 = tt.expand_dims %127 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %159 = arith.muli %158, %cst : tensor<1x64xi32>
        %160 = tt.broadcast %157 : tensor<64x1xi32> -> tensor<64x64xi32>
        %161 = tt.broadcast %159 : tensor<1x64xi32> -> tensor<64x64xi32>
        %162 = arith.addi %160, %161 : tensor<64x64xi32>
        %163 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %164 = tt.addptr %163, %162 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %165 = tt.load %164 : tensor<64x64x!tt.ptr<f8E5M2>>
        %166 = tt.fp_to_fp %147, rounding = rtne : tensor<64x64xf32> -> tensor<64x64xf8E5M2>
        %167 = tt.trans %165 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %168 = tt.dot %166, %167, %cst_2, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<64x64xf32>
        %169 = arith.addf %155, %168 : tensor<64x64xf32>
        scf.yield %152, %169, %143 : tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>
      } {tt.loop_unroll_factor = 1 : i32, tt.num_stages = 1 : i32}
      %59 = tt.expand_dims %58#0 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
      %60 = tt.broadcast %59 : tensor<64x1xf32> -> tensor<64x64xf32>
      %61 = arith.divf %58#1, %60 : tensor<64x64xf32>
      %62 = tt.fp_to_fp %61, rounding = rtne : tensor<64x64xf32> -> tensor<64x64xf8E5M2>
      %63 = arith.divsi %0, %arg5 : i32
      %64 = arith.remsi %0, %arg5 : i32
      %65 = arith.cmpi ne, %64, %c0_i32 : i32
      %66 = arith.subi %63, %c1_i32 : i32
      %67 = arith.select %65, %66, %63 : i32
      %68 = arith.cmpi slt, %0, %c0_i32 : i32
      %69 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %70 = arith.cmpi ne, %68, %69 : i1
      %71 = arith.select %70, %67, %63 : i32
      %72 = arith.andi %65, %70 : i1
      %73 = arith.addi %64, %arg5 : i32
      %74 = arith.select %72, %73, %64 : i32
      %75 = arith.muli %71, %arg4 : i32
      %76 = arith.muli %74, %c8192_i32 : i32
      %77 = arith.addi %75, %76 : i32
      %78 = tt.splat %77 : i32 -> tensor<64x1xi32>
      %79 = arith.addi %78, %48 : tensor<64x1xi32>
      %80 = tt.broadcast %79 : tensor<64x1xi32> -> tensor<64x64xi32>
      %81 = arith.addi %80, %53 : tensor<64x64xi32>
      %82 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %83 = tt.addptr %82, %81 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      tt.store %83, %62 : tensor<64x64x!tt.ptr<f8E5M2>>
      %c2_i32 = arith.constant 2 : i32
      %84 = arith.muli %c64_i32, %c2_i32 : i32
      %85 = arith.addi %arg6, %84 : i32
      %86 = tt.splat %85 : i32 -> tensor<64xi32>
      %87 = arith.addi %86, %1 : tensor<64xi32>
      %88 = arith.muli %0, %c8192_i32 : i32
      %89 = tt.expand_dims %87 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
      %90 = arith.muli %89, %cst_1 : tensor<64x1xi32>
      %91 = tt.splat %88 : i32 -> tensor<64x1xi32>
      %92 = arith.addi %91, %90 : tensor<64x1xi32>
      %93 = tt.expand_dims %1 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %94 = tt.broadcast %92 : tensor<64x1xi32> -> tensor<64x64xi32>
      %95 = tt.broadcast %93 : tensor<1x64xi32> -> tensor<64x64xi32>
      %96 = arith.addi %94, %95 : tensor<64x64xi32>
      %97 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %98 = tt.addptr %97, %96 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      %99 = tt.load %98 : tensor<64x64x!tt.ptr<f8E5M2>>
      %100:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c64_i32 iter_args(%arg8 = %cst_3, %arg9 = %cst_2, %arg10 = %cst_4) -> (tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>)  : i32 {
        %126 = tt.splat %arg7 : i32 -> tensor<64xi32>
        %127 = arith.addi %126, %1 : tensor<64xi32>
        %128 = tt.expand_dims %127 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %129 = arith.muli %128, %cst_1 : tensor<64x1xi32>
        %130 = arith.addi %91, %129 : tensor<64x1xi32>
        %131 = tt.broadcast %130 : tensor<64x1xi32> -> tensor<64x64xi32>
        %132 = arith.addi %131, %95 : tensor<64x64xi32>
        %133 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %134 = tt.addptr %133, %132 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %135 = tt.load %134 : tensor<64x64x!tt.ptr<f8E5M2>>
        %136 = tt.trans %135 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %137 = tt.dot %99, %136, %cst_2, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<64x64xf32>
        %138 = arith.mulf %137, %cst_0 : tensor<64x64xf32>
        %139 = "tt.reduce"(%138) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %170 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %170 : f32
        }) : (tensor<64x64xf32>) -> tensor<64xf32>
        %140 = arith.cmpf ogt, %arg10, %139 : tensor<64xf32>
        %141 = arith.cmpf une, %arg10, %arg10 : tensor<64xf32>
        %142 = arith.ori %140, %141 : tensor<64xi1>
        %143 = arith.select %142, %arg10, %139 : tensor<64xi1>, tensor<64xf32>
        %144 = tt.expand_dims %143 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %145 = tt.broadcast %144 : tensor<64x1xf32> -> tensor<64x64xf32>
        %146 = arith.subf %138, %145 : tensor<64x64xf32>
        %147 = tt.extern_elementwise %146 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64x64xf32>) -> tensor<64x64xf32>
        %148 = "tt.reduce"(%147) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %170 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %170 : f32
        }) : (tensor<64x64xf32>) -> tensor<64xf32>
        %149 = arith.subf %arg10, %143 : tensor<64xf32>
        %150 = tt.extern_elementwise %149 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64xf32>) -> tensor<64xf32>
        %151 = arith.mulf %arg8, %150 : tensor<64xf32>
        %152 = arith.addf %151, %148 : tensor<64xf32>
        %153 = tt.expand_dims %150 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %154 = tt.broadcast %153 : tensor<64x1xf32> -> tensor<64x64xf32>
        %155 = arith.mulf %arg9, %154 : tensor<64x64xf32>
        %156 = tt.expand_dims %1 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %157 = arith.addi %91, %156 : tensor<64x1xi32>
        %158 = tt.expand_dims %127 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %159 = arith.muli %158, %cst : tensor<1x64xi32>
        %160 = tt.broadcast %157 : tensor<64x1xi32> -> tensor<64x64xi32>
        %161 = tt.broadcast %159 : tensor<1x64xi32> -> tensor<64x64xi32>
        %162 = arith.addi %160, %161 : tensor<64x64xi32>
        %163 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %164 = tt.addptr %163, %162 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %165 = tt.load %164 : tensor<64x64x!tt.ptr<f8E5M2>>
        %166 = tt.fp_to_fp %147, rounding = rtne : tensor<64x64xf32> -> tensor<64x64xf8E5M2>
        %167 = tt.trans %165 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %168 = tt.dot %166, %167, %cst_2, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<64x64xf32>
        %169 = arith.addf %155, %168 : tensor<64x64xf32>
        scf.yield %152, %169, %143 : tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>
      } {tt.loop_unroll_factor = 1 : i32, tt.num_stages = 1 : i32}
      %101 = tt.expand_dims %100#0 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
      %102 = tt.broadcast %101 : tensor<64x1xf32> -> tensor<64x64xf32>
      %103 = arith.divf %100#1, %102 : tensor<64x64xf32>
      %104 = tt.fp_to_fp %103, rounding = rtne : tensor<64x64xf32> -> tensor<64x64xf8E5M2>
      %105 = arith.divsi %0, %arg5 : i32
      %106 = arith.remsi %0, %arg5 : i32
      %107 = arith.cmpi ne, %106, %c0_i32 : i32
      %108 = arith.subi %105, %c1_i32 : i32
      %109 = arith.select %107, %108, %105 : i32
      %110 = arith.cmpi slt, %0, %c0_i32 : i32
      %111 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %112 = arith.cmpi ne, %110, %111 : i1
      %113 = arith.select %112, %109, %105 : i32
      %114 = arith.andi %107, %112 : i1
      %115 = arith.addi %106, %arg5 : i32
      %116 = arith.select %114, %115, %106 : i32
      %117 = arith.muli %113, %arg4 : i32
      %118 = arith.muli %116, %c8192_i32 : i32
      %119 = arith.addi %117, %118 : i32
      %120 = tt.splat %119 : i32 -> tensor<64x1xi32>
      %121 = arith.addi %120, %90 : tensor<64x1xi32>
      %122 = tt.broadcast %121 : tensor<64x1xi32> -> tensor<64x64xi32>
      %123 = arith.addi %122, %95 : tensor<64x64xi32>
      %124 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %125 = tt.addptr %124, %123 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      tt.store %125, %104 : tensor<64x64x!tt.ptr<f8E5M2>>
    } {tt.disallow_acc_multi_buffer, tt.flatten, tt.num_stages = 2 : i32}
    scf.for %arg6 = %c0_i32_5 to %c128_i32 step %c64_i32  : i32 {
      %2 = tt.splat %arg6 : i32 -> tensor<64xi32>
      %3 = arith.addi %2, %1 : tensor<64xi32>
      %4 = arith.muli %0, %c8192_i32 : i32
      %5 = tt.expand_dims %3 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
      %6 = arith.muli %5, %cst_1 : tensor<64x1xi32>
      %7 = tt.splat %4 : i32 -> tensor<64x1xi32>
      %8 = arith.addi %7, %6 : tensor<64x1xi32>
      %9 = tt.expand_dims %1 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %10 = tt.broadcast %8 : tensor<64x1xi32> -> tensor<64x64xi32>
      %11 = tt.broadcast %9 : tensor<1x64xi32> -> tensor<64x64xi32>
      %12 = arith.addi %10, %11 : tensor<64x64xi32>
      %13 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %14 = tt.addptr %13, %12 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      %15 = tt.load %14 : tensor<64x64x!tt.ptr<f8E5M2>>
      %16:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c64_i32 iter_args(%arg8 = %cst_3, %arg9 = %cst_2, %arg10 = %cst_4) -> (tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>)  : i32 {
        %42 = tt.splat %arg7 : i32 -> tensor<64xi32>
        %43 = arith.addi %42, %1 : tensor<64xi32>
        %44 = tt.expand_dims %43 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %45 = arith.muli %44, %cst_1 : tensor<64x1xi32>
        %46 = arith.addi %7, %45 : tensor<64x1xi32>
        %47 = tt.broadcast %46 : tensor<64x1xi32> -> tensor<64x64xi32>
        %48 = arith.addi %47, %11 : tensor<64x64xi32>
        %49 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %50 = tt.addptr %49, %48 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %51 = tt.load %50 : tensor<64x64x!tt.ptr<f8E5M2>>
        %52 = tt.trans %51 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %53 = tt.dot %15, %52, %cst_2, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<64x64xf32>
        %54 = arith.mulf %53, %cst_0 : tensor<64x64xf32>
        %55 = "tt.reduce"(%54) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %86 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %86 : f32
        }) : (tensor<64x64xf32>) -> tensor<64xf32>
        %56 = arith.cmpf ogt, %arg10, %55 : tensor<64xf32>
        %57 = arith.cmpf une, %arg10, %arg10 : tensor<64xf32>
        %58 = arith.ori %56, %57 : tensor<64xi1>
        %59 = arith.select %58, %arg10, %55 : tensor<64xi1>, tensor<64xf32>
        %60 = tt.expand_dims %59 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %61 = tt.broadcast %60 : tensor<64x1xf32> -> tensor<64x64xf32>
        %62 = arith.subf %54, %61 : tensor<64x64xf32>
        %63 = tt.extern_elementwise %62 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64x64xf32>) -> tensor<64x64xf32>
        %64 = "tt.reduce"(%63) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %86 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %86 : f32
        }) : (tensor<64x64xf32>) -> tensor<64xf32>
        %65 = arith.subf %arg10, %59 : tensor<64xf32>
        %66 = tt.extern_elementwise %65 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64xf32>) -> tensor<64xf32>
        %67 = arith.mulf %arg8, %66 : tensor<64xf32>
        %68 = arith.addf %67, %64 : tensor<64xf32>
        %69 = tt.expand_dims %66 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %70 = tt.broadcast %69 : tensor<64x1xf32> -> tensor<64x64xf32>
        %71 = arith.mulf %arg9, %70 : tensor<64x64xf32>
        %72 = tt.expand_dims %1 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %73 = arith.addi %7, %72 : tensor<64x1xi32>
        %74 = tt.expand_dims %43 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %75 = arith.muli %74, %cst : tensor<1x64xi32>
        %76 = tt.broadcast %73 : tensor<64x1xi32> -> tensor<64x64xi32>
        %77 = tt.broadcast %75 : tensor<1x64xi32> -> tensor<64x64xi32>
        %78 = arith.addi %76, %77 : tensor<64x64xi32>
        %79 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %80 = tt.addptr %79, %78 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %81 = tt.load %80 : tensor<64x64x!tt.ptr<f8E5M2>>
        %82 = tt.fp_to_fp %63, rounding = rtne : tensor<64x64xf32> -> tensor<64x64xf8E5M2>
        %83 = tt.trans %81 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %84 = tt.dot %82, %83, %cst_2, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<64x64xf32>
        %85 = arith.addf %71, %84 : tensor<64x64xf32>
        scf.yield %68, %85, %59 : tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>
      } {tt.loop_unroll_factor = 1 : i32, tt.num_stages = 1 : i32}
      %17 = tt.expand_dims %16#0 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
      %18 = tt.broadcast %17 : tensor<64x1xf32> -> tensor<64x64xf32>
      %19 = arith.divf %16#1, %18 : tensor<64x64xf32>
      %20 = tt.fp_to_fp %19, rounding = rtne : tensor<64x64xf32> -> tensor<64x64xf8E5M2>
      %21 = arith.divsi %0, %arg5 : i32
      %22 = arith.remsi %0, %arg5 : i32
      %23 = arith.cmpi ne, %22, %c0_i32 : i32
      %24 = arith.subi %21, %c1_i32 : i32
      %25 = arith.select %23, %24, %21 : i32
      %26 = arith.cmpi slt, %0, %c0_i32 : i32
      %27 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %28 = arith.cmpi ne, %26, %27 : i1
      %29 = arith.select %28, %25, %21 : i32
      %30 = arith.andi %23, %28 : i1
      %31 = arith.addi %22, %arg5 : i32
      %32 = arith.select %30, %31, %22 : i32
      %33 = arith.muli %29, %arg4 : i32
      %34 = arith.muli %32, %c8192_i32 : i32
      %35 = arith.addi %33, %34 : i32
      %36 = tt.splat %35 : i32 -> tensor<64x1xi32>
      %37 = arith.addi %36, %6 : tensor<64x1xi32>
      %38 = tt.broadcast %37 : tensor<64x1xi32> -> tensor<64x64xi32>
      %39 = arith.addi %38, %11 : tensor<64x64xi32>
      %40 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %41 = tt.addptr %40, %39 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      tt.store %41, %20 : tensor<64x64x!tt.ptr<f8E5M2>>
    } {tt.disallow_acc_multi_buffer, tt.flatten, tt.num_stages = 1 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(convert-triton-to-tritongpu{enable-source-remat=false num-ctas=1 num-warps=8 target=cuda:90 threads-per-warp=32}, tritongpu-coalesce, tritongpu-F32DotTC, triton-nvidia-gpu-plan-cta, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritongpu-accelerate-matmul, tritongpu-remove-layout-conversions, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, triton-nvidia-optimize-descriptor-encoding, triton-loop-aware-cse, tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-combine-tensor-select-and-if, nvgpu-warp-specialization{dump-intermediate-steps=false num-stages=7}, tritongpu-assign-latencies{num-stages=7}, tritongpu-schedule-loops, tritongpu-pipeline{dump-intermediate-steps=false num-stages=7}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-loop-aware-cse, tritongpu-prefetch, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-coalesce-async-copy, triton-nvidia-optimize-tmem-layouts, tritongpu-remove-layout-conversions, triton-nvidia-interleave-tmem, tritongpu-reduce-data-duplication, tritongpu-reorder-instructions, triton-loop-aware-cse, symbol-dce, triton-nvidia-tma-lowering, triton-nvidia-gpu-fence-insertion{compute-capability=90}, sccp, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true})",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_willfeng/xa/cxapgu7oouw4urqmf4tgmto5wfhn2d6scu6cksoexc3yl5n4uvcd.py:12:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_willfeng/xa/cxapgu7oouw4urqmf4tgmto5wfhn2d6scu6cksoexc3yl5n4uvcd.py:12:0: note: Pipeline failed while executing [`TritonGPUAssignLatencies` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
Process ForkProcess-288:
Traceback (most recent call last):
  File "/home/willfeng/local/miniconda3/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/willfeng/local/miniconda3/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/data/users/willfeng/helion/helion/runtime/precompile_shim.py", line 54, in finish_it
    kernel_cache[key] = fn.compile(
                        ^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/compiler/compiler.py", line 359, in compile
    next_module = compile_ir(module, metadata)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/backends/nvidia/compiler.py", line 456, in <lambda>
    stages["ttgir"] = lambda src, metadata: self.make_ttgir(src, metadata, options, capability)
                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/backends/nvidia/compiler.py", line 298, in make_ttgir
    pm.run(mod)
RuntimeError: PassManager::run failed
python: /root/.triton/llvm/llvm-8957e64a-almalinux-x64/include/llvm/ADT/SmallVector.h:296: const_reference llvm::SmallVectorTemplateCommon<long>::operator[](size_type) const [T = long]: Assertion `idx < size()' failed.
module {
  tt.func public @_fp8_attention_kernel_kernel(%arg0: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %c1_i32 = arith.constant 1 : i32
    %c64_i32 = arith.constant 64 : i32
    %c128_i32 = arith.constant 128 : i32
    %c0_i32 = arith.constant 0 : i32
    %cst = arith.constant dense<64> : tensor<1x64xi32>
    %cst_0 = arith.constant dense<0.180336878> : tensor<64x64xf32>
    %cst_1 = arith.constant dense<64> : tensor<64x1xi32>
    %c8192_i32 = arith.constant 8192 : i32
    %cst_2 = arith.constant dense<0.000000e+00> : tensor<64x64xf32>
    %cst_3 = arith.constant dense<0.000000e+00> : tensor<64xf32>
    %cst_4 = arith.constant dense<0xFF800000> : tensor<64xf32>
    %0 = tt.get_program_id x : i32
    %1 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
    %c0_i32_5 = arith.constant 0 : i32
    %c192_i32 = arith.constant 192 : i32
    scf.for %arg6 = %c0_i32 to %c0_i32_5 step %c192_i32  : i32 {
      %2 = tt.splat %arg6 : i32 -> tensor<64xi32>
      %3 = arith.addi %2, %1 : tensor<64xi32>
      %4 = arith.muli %0, %c8192_i32 : i32
      %5 = tt.expand_dims %3 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
      %6 = arith.muli %5, %cst_1 : tensor<64x1xi32>
      %7 = tt.splat %4 : i32 -> tensor<64x1xi32>
      %8 = arith.addi %7, %6 : tensor<64x1xi32>
      %9 = tt.expand_dims %1 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %10 = tt.broadcast %8 : tensor<64x1xi32> -> tensor<64x64xi32>
      %11 = tt.broadcast %9 : tensor<1x64xi32> -> tensor<64x64xi32>
      %12 = arith.addi %10, %11 : tensor<64x64xi32>
      %13 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %14 = tt.addptr %13, %12 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      %15 = tt.load %14 : tensor<64x64x!tt.ptr<f8E5M2>>
      %16:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c64_i32 iter_args(%arg8 = %cst_3, %arg9 = %cst_2, %arg10 = %cst_4) -> (tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>)  : i32 {
        %126 = tt.splat %arg7 : i32 -> tensor<64xi32>
        %127 = arith.addi %126, %1 : tensor<64xi32>
        %128 = tt.expand_dims %127 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %129 = arith.muli %128, %cst_1 : tensor<64x1xi32>
        %130 = arith.addi %7, %129 : tensor<64x1xi32>
        %131 = tt.broadcast %130 : tensor<64x1xi32> -> tensor<64x64xi32>
        %132 = arith.addi %131, %11 : tensor<64x64xi32>
        %133 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %134 = tt.addptr %133, %132 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %135 = tt.load %134 : tensor<64x64x!tt.ptr<f8E5M2>>
        %136 = tt.trans %135 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %137 = tt.dot %15, %136, %cst_2, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<64x64xf32>
        %138 = arith.mulf %137, %cst_0 : tensor<64x64xf32>
        %139 = "tt.reduce"(%138) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %170 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %170 : f32
        }) : (tensor<64x64xf32>) -> tensor<64xf32>
        %140 = arith.cmpf ogt, %arg10, %139 : tensor<64xf32>
        %141 = arith.cmpf une, %arg10, %arg10 : tensor<64xf32>
        %142 = arith.ori %140, %141 : tensor<64xi1>
        %143 = arith.select %142, %arg10, %139 : tensor<64xi1>, tensor<64xf32>
        %144 = tt.expand_dims %143 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %145 = tt.broadcast %144 : tensor<64x1xf32> -> tensor<64x64xf32>
        %146 = arith.subf %138, %145 : tensor<64x64xf32>
        %147 = tt.extern_elementwise %146 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64x64xf32>) -> tensor<64x64xf32>
        %148 = "tt.reduce"(%147) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %170 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %170 : f32
        }) : (tensor<64x64xf32>) -> tensor<64xf32>
        %149 = arith.subf %arg10, %143 : tensor<64xf32>
        %150 = tt.extern_elementwise %149 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64xf32>) -> tensor<64xf32>
        %151 = arith.mulf %arg8, %150 : tensor<64xf32>
        %152 = arith.addf %151, %148 : tensor<64xf32>
        %153 = tt.expand_dims %150 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %154 = tt.broadcast %153 : tensor<64x1xf32> -> tensor<64x64xf32>
        %155 = arith.mulf %arg9, %154 : tensor<64x64xf32>
        %156 = tt.expand_dims %1 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %157 = arith.addi %7, %156 : tensor<64x1xi32>
        %158 = tt.expand_dims %127 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %159 = arith.muli %158, %cst : tensor<1x64xi32>
        %160 = tt.broadcast %157 : tensor<64x1xi32> -> tensor<64x64xi32>
        %161 = tt.broadcast %159 : tensor<1x64xi32> -> tensor<64x64xi32>
        %162 = arith.addi %160, %161 : tensor<64x64xi32>
        %163 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %164 = tt.addptr %163, %162 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %165 = tt.load %164 : tensor<64x64x!tt.ptr<f8E5M2>>
        %166 = tt.fp_to_fp %147, rounding = rtne : tensor<64x64xf32> -> tensor<64x64xf8E5M2>
        %167 = tt.trans %165 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %168 = tt.dot %166, %167, %cst_2, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<64x64xf32>
        %169 = arith.addf %155, %168 : tensor<64x64xf32>
        scf.yield %152, %169, %143 : tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>
      } {tt.loop_unroll_factor = 1 : i32, tt.num_stages = 1 : i32}
      %17 = tt.expand_dims %16#0 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
      %18 = tt.broadcast %17 : tensor<64x1xf32> -> tensor<64x64xf32>
      %19 = arith.divf %16#1, %18 : tensor<64x64xf32>
      %20 = tt.fp_to_fp %19, rounding = rtne : tensor<64x64xf32> -> tensor<64x64xf8E5M2>
      %21 = arith.divsi %0, %arg5 : i32
      %22 = arith.remsi %0, %arg5 : i32
      %23 = arith.cmpi ne, %22, %c0_i32 : i32
      %24 = arith.subi %21, %c1_i32 : i32
      %25 = arith.select %23, %24, %21 : i32
      %26 = arith.cmpi slt, %0, %c0_i32 : i32
      %27 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %28 = arith.cmpi ne, %26, %27 : i1
      %29 = arith.select %28, %25, %21 : i32
      %30 = arith.andi %23, %28 : i1
      %31 = arith.addi %22, %arg5 : i32
      %32 = arith.select %30, %31, %22 : i32
      %33 = arith.muli %29, %arg4 : i32
      %34 = arith.muli %32, %c8192_i32 : i32
      %35 = arith.addi %33, %34 : i32
      %36 = tt.splat %35 : i32 -> tensor<64x1xi32>
      %37 = arith.addi %36, %6 : tensor<64x1xi32>
      %38 = tt.broadcast %37 : tensor<64x1xi32> -> tensor<64x64xi32>
      %39 = arith.addi %38, %11 : tensor<64x64xi32>
      %40 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %41 = tt.addptr %40, %39 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      tt.store %41, %20 : tensor<64x64x!tt.ptr<f8E5M2>>
      %c1_i32_6 = arith.constant 1 : i32
      %42 = arith.muli %c64_i32, %c1_i32_6 : i32
      %43 = arith.addi %arg6, %42 : i32
      %44 = tt.splat %43 : i32 -> tensor<64xi32>
      %45 = arith.addi %44, %1 : tensor<64xi32>
      %46 = arith.muli %0, %c8192_i32 : i32
      %47 = tt.expand_dims %45 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
      %48 = arith.muli %47, %cst_1 : tensor<64x1xi32>
      %49 = tt.splat %46 : i32 -> tensor<64x1xi32>
      %50 = arith.addi %49, %48 : tensor<64x1xi32>
      %51 = tt.expand_dims %1 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %52 = tt.broadcast %50 : tensor<64x1xi32> -> tensor<64x64xi32>
      %53 = tt.broadcast %51 : tensor<1x64xi32> -> tensor<64x64xi32>
      %54 = arith.addi %52, %53 : tensor<64x64xi32>
      %55 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %56 = tt.addptr %55, %54 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      %57 = tt.load %56 : tensor<64x64x!tt.ptr<f8E5M2>>
      %58:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c64_i32 iter_args(%arg8 = %cst_3, %arg9 = %cst_2, %arg10 = %cst_4) -> (tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>)  : i32 {
        %126 = tt.splat %arg7 : i32 -> tensor<64xi32>
        %127 = arith.addi %126, %1 : tensor<64xi32>
        %128 = tt.expand_dims %127 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %129 = arith.muli %128, %cst_1 : tensor<64x1xi32>
        %130 = arith.addi %49, %129 : tensor<64x1xi32>
        %131 = tt.broadcast %130 : tensor<64x1xi32> -> tensor<64x64xi32>
        %132 = arith.addi %131, %53 : tensor<64x64xi32>
        %133 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %134 = tt.addptr %133, %132 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %135 = tt.load %134 : tensor<64x64x!tt.ptr<f8E5M2>>
        %136 = tt.trans %135 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %137 = tt.dot %57, %136, %cst_2, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<64x64xf32>
        %138 = arith.mulf %137, %cst_0 : tensor<64x64xf32>
        %139 = "tt.reduce"(%138) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %170 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %170 : f32
        }) : (tensor<64x64xf32>) -> tensor<64xf32>
        %140 = arith.cmpf ogt, %arg10, %139 : tensor<64xf32>
        %141 = arith.cmpf une, %arg10, %arg10 : tensor<64xf32>
        %142 = arith.ori %140, %141 : tensor<64xi1>
        %143 = arith.select %142, %arg10, %139 : tensor<64xi1>, tensor<64xf32>
        %144 = tt.expand_dims %143 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %145 = tt.broadcast %144 : tensor<64x1xf32> -> tensor<64x64xf32>
        %146 = arith.subf %138, %145 : tensor<64x64xf32>
        %147 = tt.extern_elementwise %146 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64x64xf32>) -> tensor<64x64xf32>
        %148 = "tt.reduce"(%147) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %170 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %170 : f32
        }) : (tensor<64x64xf32>) -> tensor<64xf32>
        %149 = arith.subf %arg10, %143 : tensor<64xf32>
        %150 = tt.extern_elementwise %149 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64xf32>) -> tensor<64xf32>
        %151 = arith.mulf %arg8, %150 : tensor<64xf32>
        %152 = arith.addf %151, %148 : tensor<64xf32>
        %153 = tt.expand_dims %150 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %154 = tt.broadcast %153 : tensor<64x1xf32> -> tensor<64x64xf32>
        %155 = arith.mulf %arg9, %154 : tensor<64x64xf32>
        %156 = tt.expand_dims %1 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %157 = arith.addi %49, %156 : tensor<64x1xi32>
        %158 = tt.expand_dims %127 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %159 = arith.muli %158, %cst : tensor<1x64xi32>
        %160 = tt.broadcast %157 : tensor<64x1xi32> -> tensor<64x64xi32>
        %161 = tt.broadcast %159 : tensor<1x64xi32> -> tensor<64x64xi32>
        %162 = arith.addi %160, %161 : tensor<64x64xi32>
        %163 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %164 = tt.addptr %163, %162 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %165 = tt.load %164 : tensor<64x64x!tt.ptr<f8E5M2>>
        %166 = tt.fp_to_fp %147, rounding = rtne : tensor<64x64xf32> -> tensor<64x64xf8E5M2>
        %167 = tt.trans %165 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %168 = tt.dot %166, %167, %cst_2, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<64x64xf32>
        %169 = arith.addf %155, %168 : tensor<64x64xf32>
        scf.yield %152, %169, %143 : tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>
      } {tt.loop_unroll_factor = 1 : i32, tt.num_stages = 1 : i32}
      %59 = tt.expand_dims %58#0 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
      %60 = tt.broadcast %59 : tensor<64x1xf32> -> tensor<64x64xf32>
      %61 = arith.divf %58#1, %60 : tensor<64x64xf32>
      %62 = tt.fp_to_fp %61, rounding = rtne : tensor<64x64xf32> -> tensor<64x64xf8E5M2>
      %63 = arith.divsi %0, %arg5 : i32
      %64 = arith.remsi %0, %arg5 : i32
      %65 = arith.cmpi ne, %64, %c0_i32 : i32
      %66 = arith.subi %63, %c1_i32 : i32
      %67 = arith.select %65, %66, %63 : i32
      %68 = arith.cmpi slt, %0, %c0_i32 : i32
      %69 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %70 = arith.cmpi ne, %68, %69 : i1
      %71 = arith.select %70, %67, %63 : i32
      %72 = arith.andi %65, %70 : i1
      %73 = arith.addi %64, %arg5 : i32
      %74 = arith.select %72, %73, %64 : i32
      %75 = arith.muli %71, %arg4 : i32
      %76 = arith.muli %74, %c8192_i32 : i32
      %77 = arith.addi %75, %76 : i32
      %78 = tt.splat %77 : i32 -> tensor<64x1xi32>
      %79 = arith.addi %78, %48 : tensor<64x1xi32>
      %80 = tt.broadcast %79 : tensor<64x1xi32> -> tensor<64x64xi32>
      %81 = arith.addi %80, %53 : tensor<64x64xi32>
      %82 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %83 = tt.addptr %82, %81 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      tt.store %83, %62 : tensor<64x64x!tt.ptr<f8E5M2>>
      %c2_i32 = arith.constant 2 : i32
      %84 = arith.muli %c64_i32, %c2_i32 : i32
      %85 = arith.addi %arg6, %84 : i32
      %86 = tt.splat %85 : i32 -> tensor<64xi32>
      %87 = arith.addi %86, %1 : tensor<64xi32>
      %88 = arith.muli %0, %c8192_i32 : i32
      %89 = tt.expand_dims %87 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
      %90 = arith.muli %89, %cst_1 : tensor<64x1xi32>
      %91 = tt.splat %88 : i32 -> tensor<64x1xi32>
      %92 = arith.addi %91, %90 : tensor<64x1xi32>
      %93 = tt.expand_dims %1 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %94 = tt.broadcast %92 : tensor<64x1xi32> -> tensor<64x64xi32>
      %95 = tt.broadcast %93 : tensor<1x64xi32> -> tensor<64x64xi32>
      %96 = arith.addi %94, %95 : tensor<64x64xi32>
      %97 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %98 = tt.addptr %97, %96 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      %99 = tt.load %98 : tensor<64x64x!tt.ptr<f8E5M2>>
      %100:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c64_i32 iter_args(%arg8 = %cst_3, %arg9 = %cst_2, %arg10 = %cst_4) -> (tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>)  : i32 {
        %126 = tt.splat %arg7 : i32 -> tensor<64xi32>
        %127 = arith.addi %126, %1 : tensor<64xi32>
        %128 = tt.expand_dims %127 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %129 = arith.muli %128, %cst_1 : tensor<64x1xi32>
        %130 = arith.addi %91, %129 : tensor<64x1xi32>
        %131 = tt.broadcast %130 : tensor<64x1xi32> -> tensor<64x64xi32>
        %132 = arith.addi %131, %95 : tensor<64x64xi32>
        %133 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %134 = tt.addptr %133, %132 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %135 = tt.load %134 : tensor<64x64x!tt.ptr<f8E5M2>>
        %136 = tt.trans %135 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %137 = tt.dot %99, %136, %cst_2, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<64x64xf32>
        %138 = arith.mulf %137, %cst_0 : tensor<64x64xf32>
        %139 = "tt.reduce"(%138) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %170 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %170 : f32
        }) : (tensor<64x64xf32>) -> tensor<64xf32>
        %140 = arith.cmpf ogt, %arg10, %139 : tensor<64xf32>
        %141 = arith.cmpf une, %arg10, %arg10 : tensor<64xf32>
        %142 = arith.ori %140, %141 : tensor<64xi1>
        %143 = arith.select %142, %arg10, %139 : tensor<64xi1>, tensor<64xf32>
        %144 = tt.expand_dims %143 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %145 = tt.broadcast %144 : tensor<64x1xf32> -> tensor<64x64xf32>
        %146 = arith.subf %138, %145 : tensor<64x64xf32>
        %147 = tt.extern_elementwise %146 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64x64xf32>) -> tensor<64x64xf32>
        %148 = "tt.reduce"(%147) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %170 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %170 : f32
        }) : (tensor<64x64xf32>) -> tensor<64xf32>
        %149 = arith.subf %arg10, %143 : tensor<64xf32>
        %150 = tt.extern_elementwise %149 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64xf32>) -> tensor<64xf32>
        %151 = arith.mulf %arg8, %150 : tensor<64xf32>
        %152 = arith.addf %151, %148 : tensor<64xf32>
        %153 = tt.expand_dims %150 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %154 = tt.broadcast %153 : tensor<64x1xf32> -> tensor<64x64xf32>
        %155 = arith.mulf %arg9, %154 : tensor<64x64xf32>
        %156 = tt.expand_dims %1 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %157 = arith.addi %91, %156 : tensor<64x1xi32>
        %158 = tt.expand_dims %127 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %159 = arith.muli %158, %cst : tensor<1x64xi32>
        %160 = tt.broadcast %157 : tensor<64x1xi32> -> tensor<64x64xi32>
        %161 = tt.broadcast %159 : tensor<1x64xi32> -> tensor<64x64xi32>
        %162 = arith.addi %160, %161 : tensor<64x64xi32>
        %163 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %164 = tt.addptr %163, %162 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %165 = tt.load %164 : tensor<64x64x!tt.ptr<f8E5M2>>
        %166 = tt.fp_to_fp %147, rounding = rtne : tensor<64x64xf32> -> tensor<64x64xf8E5M2>
        %167 = tt.trans %165 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %168 = tt.dot %166, %167, %cst_2, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<64x64xf32>
        %169 = arith.addf %155, %168 : tensor<64x64xf32>
        scf.yield %152, %169, %143 : tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>
      } {tt.loop_unroll_factor = 1 : i32, tt.num_stages = 1 : i32}
      %101 = tt.expand_dims %100#0 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
      %102 = tt.broadcast %101 : tensor<64x1xf32> -> tensor<64x64xf32>
      %103 = arith.divf %100#1, %102 : tensor<64x64xf32>
      %104 = tt.fp_to_fp %103, rounding = rtne : tensor<64x64xf32> -> tensor<64x64xf8E5M2>
      %105 = arith.divsi %0, %arg5 : i32
      %106 = arith.remsi %0, %arg5 : i32
      %107 = arith.cmpi ne, %106, %c0_i32 : i32
      %108 = arith.subi %105, %c1_i32 : i32
      %109 = arith.select %107, %108, %105 : i32
      %110 = arith.cmpi slt, %0, %c0_i32 : i32
      %111 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %112 = arith.cmpi ne, %110, %111 : i1
      %113 = arith.select %112, %109, %105 : i32
      %114 = arith.andi %107, %112 : i1
      %115 = arith.addi %106, %arg5 : i32
      %116 = arith.select %114, %115, %106 : i32
      %117 = arith.muli %113, %arg4 : i32
      %118 = arith.muli %116, %c8192_i32 : i32
      %119 = arith.addi %117, %118 : i32
      %120 = tt.splat %119 : i32 -> tensor<64x1xi32>
      %121 = arith.addi %120, %90 : tensor<64x1xi32>
      %122 = tt.broadcast %121 : tensor<64x1xi32> -> tensor<64x64xi32>
      %123 = arith.addi %122, %95 : tensor<64x64xi32>
      %124 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %125 = tt.addptr %124, %123 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      tt.store %125, %104 : tensor<64x64x!tt.ptr<f8E5M2>>
    } {tt.disallow_acc_multi_buffer, tt.flatten, tt.num_stages = 2 : i32}
    scf.for %arg6 = %c0_i32_5 to %c128_i32 step %c64_i32  : i32 {
      %2 = tt.splat %arg6 : i32 -> tensor<64xi32>
      %3 = arith.addi %2, %1 : tensor<64xi32>
      %4 = arith.muli %0, %c8192_i32 : i32
      %5 = tt.expand_dims %3 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
      %6 = arith.muli %5, %cst_1 : tensor<64x1xi32>
      %7 = tt.splat %4 : i32 -> tensor<64x1xi32>
      %8 = arith.addi %7, %6 : tensor<64x1xi32>
      %9 = tt.expand_dims %1 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %10 = tt.broadcast %8 : tensor<64x1xi32> -> tensor<64x64xi32>
      %11 = tt.broadcast %9 : tensor<1x64xi32> -> tensor<64x64xi32>
      %12 = arith.addi %10, %11 : tensor<64x64xi32>
      %13 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %14 = tt.addptr %13, %12 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      %15 = tt.load %14 : tensor<64x64x!tt.ptr<f8E5M2>>
      %16:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c64_i32 iter_args(%arg8 = %cst_3, %arg9 = %cst_2, %arg10 = %cst_4) -> (tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>)  : i32 {
        %42 = tt.splat %arg7 : i32 -> tensor<64xi32>
        %43 = arith.addi %42, %1 : tensor<64xi32>
        %44 = tt.expand_dims %43 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %45 = arith.muli %44, %cst_1 : tensor<64x1xi32>
        %46 = arith.addi %7, %45 : tensor<64x1xi32>
        %47 = tt.broadcast %46 : tensor<64x1xi32> -> tensor<64x64xi32>
        %48 = arith.addi %47, %11 : tensor<64x64xi32>
        %49 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %50 = tt.addptr %49, %48 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %51 = tt.load %50 : tensor<64x64x!tt.ptr<f8E5M2>>
        %52 = tt.trans %51 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %53 = tt.dot %15, %52, %cst_2, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<64x64xf32>
        %54 = arith.mulf %53, %cst_0 : tensor<64x64xf32>
        %55 = "tt.reduce"(%54) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %86 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %86 : f32
        }) : (tensor<64x64xf32>) -> tensor<64xf32>
        %56 = arith.cmpf ogt, %arg10, %55 : tensor<64xf32>
        %57 = arith.cmpf une, %arg10, %arg10 : tensor<64xf32>
        %58 = arith.ori %56, %57 : tensor<64xi1>
        %59 = arith.select %58, %arg10, %55 : tensor<64xi1>, tensor<64xf32>
        %60 = tt.expand_dims %59 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %61 = tt.broadcast %60 : tensor<64x1xf32> -> tensor<64x64xf32>
        %62 = arith.subf %54, %61 : tensor<64x64xf32>
        %63 = tt.extern_elementwise %62 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64x64xf32>) -> tensor<64x64xf32>
        %64 = "tt.reduce"(%63) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %86 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %86 : f32
        }) : (tensor<64x64xf32>) -> tensor<64xf32>
        %65 = arith.subf %arg10, %59 : tensor<64xf32>
        %66 = tt.extern_elementwise %65 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64xf32>) -> tensor<64xf32>
        %67 = arith.mulf %arg8, %66 : tensor<64xf32>
        %68 = arith.addf %67, %64 : tensor<64xf32>
        %69 = tt.expand_dims %66 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %70 = tt.broadcast %69 : tensor<64x1xf32> -> tensor<64x64xf32>
        %71 = arith.mulf %arg9, %70 : tensor<64x64xf32>
        %72 = tt.expand_dims %1 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %73 = arith.addi %7, %72 : tensor<64x1xi32>
        %74 = tt.expand_dims %43 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %75 = arith.muli %74, %cst : tensor<1x64xi32>
        %76 = tt.broadcast %73 : tensor<64x1xi32> -> tensor<64x64xi32>
        %77 = tt.broadcast %75 : tensor<1x64xi32> -> tensor<64x64xi32>
        %78 = arith.addi %76, %77 : tensor<64x64xi32>
        %79 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %80 = tt.addptr %79, %78 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %81 = tt.load %80 : tensor<64x64x!tt.ptr<f8E5M2>>
        %82 = tt.fp_to_fp %63, rounding = rtne : tensor<64x64xf32> -> tensor<64x64xf8E5M2>
        %83 = tt.trans %81 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %84 = tt.dot %82, %83, %cst_2, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<64x64xf32>
        %85 = arith.addf %71, %84 : tensor<64x64xf32>
        scf.yield %68, %85, %59 : tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>
      } {tt.loop_unroll_factor = 1 : i32, tt.num_stages = 1 : i32}
      %17 = tt.expand_dims %16#0 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
      %18 = tt.broadcast %17 : tensor<64x1xf32> -> tensor<64x64xf32>
      %19 = arith.divf %16#1, %18 : tensor<64x64xf32>
      %20 = tt.fp_to_fp %19, rounding = rtne : tensor<64x64xf32> -> tensor<64x64xf8E5M2>
      %21 = arith.divsi %0, %arg5 : i32
      %22 = arith.remsi %0, %arg5 : i32
      %23 = arith.cmpi ne, %22, %c0_i32 : i32
      %24 = arith.subi %21, %c1_i32 : i32
      %25 = arith.select %23, %24, %21 : i32
      %26 = arith.cmpi slt, %0, %c0_i32 : i32
      %27 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %28 = arith.cmpi ne, %26, %27 : i1
      %29 = arith.select %28, %25, %21 : i32
      %30 = arith.andi %23, %28 : i1
      %31 = arith.addi %22, %arg5 : i32
      %32 = arith.select %30, %31, %22 : i32
      %33 = arith.muli %29, %arg4 : i32
      %34 = arith.muli %32, %c8192_i32 : i32
      %35 = arith.addi %33, %34 : i32
      %36 = tt.splat %35 : i32 -> tensor<64x1xi32>
      %37 = arith.addi %36, %6 : tensor<64x1xi32>
      %38 = tt.broadcast %37 : tensor<64x1xi32> -> tensor<64x64xi32>
      %39 = arith.addi %38, %11 : tensor<64x64xi32>
      %40 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %41 = tt.addptr %40, %39 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      tt.store %41, %20 : tensor<64x64x!tt.ptr<f8E5M2>>
    } {tt.disallow_acc_multi_buffer, tt.flatten, tt.num_stages = 1 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(convert-triton-to-tritongpu{enable-source-remat=false num-ctas=1 num-warps=8 target=cuda:90 threads-per-warp=32}, tritongpu-coalesce, tritongpu-F32DotTC, triton-nvidia-gpu-plan-cta, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritongpu-accelerate-matmul, tritongpu-remove-layout-conversions, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, triton-nvidia-optimize-descriptor-encoding, triton-loop-aware-cse, tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-combine-tensor-select-and-if, nvgpu-warp-specialization{dump-intermediate-steps=false num-stages=7}, tritongpu-assign-latencies{num-stages=7}, tritongpu-schedule-loops, tritongpu-pipeline{dump-intermediate-steps=false num-stages=7}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-loop-aware-cse, tritongpu-prefetch, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-coalesce-async-copy, triton-nvidia-optimize-tmem-layouts, tritongpu-remove-layout-conversions, triton-nvidia-interleave-tmem, tritongpu-reduce-data-duplication, tritongpu-reorder-instructions, triton-loop-aware-cse, symbol-dce, triton-nvidia-tma-lowering, triton-nvidia-gpu-fence-insertion{compute-capability=90}, sccp, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true})",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_willfeng/xa/cxapgu7oouw4urqmf4tgmto5wfhn2d6scu6cksoexc3yl5n4uvcd.py:12:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_willfeng/xa/cxapgu7oouw4urqmf4tgmto5wfhn2d6scu6cksoexc3yl5n4uvcd.py:12:0: note: Pipeline failed while executing [`TritonGPUAssignLatencies` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
[400s] Generation 4: replaced=15 min=0.0118 mid=0.0187 max=0.0415 best=Config(block_sizes=[128, 64], range_unroll_factors=[0, 1, 0], range_num_stages=[0, 4, 0], range_multi_buffers=[None, False, False], range_flattens=[None, True, False], num_warps=4, num_stages=8, indexing='pointer', pid_type='flat', range_warp_specializes=[])
python: /root/.triton/llvm/llvm-8957e64a-almalinux-x64/include/llvm/ADT/SmallVector.h:296: const_reference llvm::SmallVectorTemplateCommon<long>::operator[](size_type) const [T = long]: Assertion `idx < size()' failed.
module {
  tt.func public @_fp8_attention_kernel_kernel(%arg0: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %c1_i32 = arith.constant 1 : i32
    %c64_i32 = arith.constant 64 : i32
    %c128_i32 = arith.constant 128 : i32
    %c0_i32 = arith.constant 0 : i32
    %c132_i32 = arith.constant 132 : i32
    %c192_i32 = arith.constant 192 : i32
    %cst = arith.constant dense<64> : tensor<1x64xi32>
    %cst_0 = arith.constant dense<0.180336878> : tensor<128x64xf32>
    %cst_1 = arith.constant dense<64> : tensor<64x1xi32>
    %cst_2 = arith.constant dense<64> : tensor<128x1xi32>
    %c8192_i32 = arith.constant 8192 : i32
    %cst_3 = arith.constant dense<0.000000e+00> : tensor<128x64xf32>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<128xf32>
    %cst_5 = arith.constant dense<0xFF800000> : tensor<128xf32>
    %0 = tt.get_program_id x : i32
    scf.for %arg6 = %0 to %c192_i32 step %c132_i32  : i32 {
      %1 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
      %2 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>
      %3 = arith.muli %arg6, %c8192_i32 : i32
      %4 = tt.expand_dims %2 {axis = 1 : i32} : tensor<128xi32> -> tensor<128x1xi32>
      %5 = arith.muli %4, %cst_2 : tensor<128x1xi32>
      %6 = tt.splat %3 : i32 -> tensor<128x1xi32>
      %7 = arith.addi %6, %5 : tensor<128x1xi32>
      %8 = tt.expand_dims %1 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %9 = tt.broadcast %7 : tensor<128x1xi32> -> tensor<128x64xi32>
      %10 = tt.broadcast %8 : tensor<1x64xi32> -> tensor<128x64xi32>
      %11 = arith.addi %9, %10 : tensor<128x64xi32>
      %12 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %13 = tt.addptr %12, %11 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      %14 = tt.load %13 : tensor<128x64x!tt.ptr<f8E5M2>>
      %15:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c64_i32 iter_args(%arg8 = %cst_4, %arg9 = %cst_3, %arg10 = %cst_5) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %41 = tt.splat %arg7 : i32 -> tensor<64xi32>
        %42 = arith.addi %41, %1 : tensor<64xi32>
        %43 = tt.expand_dims %42 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %44 = arith.muli %43, %cst_1 : tensor<64x1xi32>
        %45 = tt.splat %3 : i32 -> tensor<64x1xi32>
        %46 = arith.addi %45, %44 : tensor<64x1xi32>
        %47 = tt.broadcast %46 : tensor<64x1xi32> -> tensor<64x64xi32>
        %48 = tt.broadcast %8 : tensor<1x64xi32> -> tensor<64x64xi32>
        %49 = arith.addi %47, %48 : tensor<64x64xi32>
        %50 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %51 = tt.addptr %50, %49 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %52 = tt.load %51 : tensor<64x64x!tt.ptr<f8E5M2>>
        %53 = tt.trans %52 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %54 = tt.dot %14, %53, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %55 = arith.mulf %54, %cst_0 : tensor<128x64xf32>
        %56 = "tt.reduce"(%55) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %87 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %87 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %57 = arith.cmpf ogt, %arg10, %56 : tensor<128xf32>
        %58 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %59 = arith.ori %57, %58 : tensor<128xi1>
        %60 = arith.select %59, %arg10, %56 : tensor<128xi1>, tensor<128xf32>
        %61 = tt.expand_dims %60 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %62 = tt.broadcast %61 : tensor<128x1xf32> -> tensor<128x64xf32>
        %63 = arith.subf %55, %62 : tensor<128x64xf32>
        %64 = tt.extern_elementwise %63 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %65 = "tt.reduce"(%64) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %87 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %87 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %66 = arith.subf %arg10, %60 : tensor<128xf32>
        %67 = tt.extern_elementwise %66 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %68 = arith.mulf %arg8, %67 : tensor<128xf32>
        %69 = arith.addf %68, %65 : tensor<128xf32>
        %70 = tt.expand_dims %67 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %71 = tt.broadcast %70 : tensor<128x1xf32> -> tensor<128x64xf32>
        %72 = arith.mulf %arg9, %71 : tensor<128x64xf32>
        %73 = tt.expand_dims %1 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %74 = arith.addi %45, %73 : tensor<64x1xi32>
        %75 = tt.expand_dims %42 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %76 = arith.muli %75, %cst : tensor<1x64xi32>
        %77 = tt.broadcast %74 : tensor<64x1xi32> -> tensor<64x64xi32>
        %78 = tt.broadcast %76 : tensor<1x64xi32> -> tensor<64x64xi32>
        %79 = arith.addi %77, %78 : tensor<64x64xi32>
        %80 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %81 = tt.addptr %80, %79 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %82 = tt.load %81 : tensor<64x64x!tt.ptr<f8E5M2>>
        %83 = tt.fp_to_fp %64, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %84 = tt.trans %82 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %85 = tt.dot %83, %84, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %86 = arith.addf %72, %85 : tensor<128x64xf32>
        scf.yield %69, %86, %60 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.loop_unroll_factor = 1 : i32, tt.num_stages = 2 : i32}
      %16 = tt.expand_dims %15#0 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
      %17 = tt.broadcast %16 : tensor<128x1xf32> -> tensor<128x64xf32>
      %18 = arith.divf %15#1, %17 : tensor<128x64xf32>
      %19 = tt.fp_to_fp %18, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
      %20 = arith.divsi %arg6, %arg5 : i32
      %21 = arith.remsi %arg6, %arg5 : i32
      %22 = arith.cmpi ne, %21, %c0_i32 : i32
      %23 = arith.subi %20, %c1_i32 : i32
      %24 = arith.select %22, %23, %20 : i32
      %25 = arith.cmpi slt, %arg6, %c0_i32 : i32
      %26 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %27 = arith.cmpi ne, %25, %26 : i1
      %28 = arith.select %27, %24, %20 : i32
      %29 = arith.andi %22, %27 : i1
      %30 = arith.addi %21, %arg5 : i32
      %31 = arith.select %29, %30, %21 : i32
      %32 = arith.muli %28, %arg4 : i32
      %33 = arith.muli %31, %c8192_i32 : i32
      %34 = arith.addi %32, %33 : i32
      %35 = tt.splat %34 : i32 -> tensor<128x1xi32>
      %36 = arith.addi %35, %5 : tensor<128x1xi32>
      %37 = tt.broadcast %36 : tensor<128x1xi32> -> tensor<128x64xi32>
      %38 = arith.addi %37, %10 : tensor<128x64xi32>
      %39 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %40 = tt.addptr %39, %38 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      tt.store %40, %19 : tensor<128x64x!tt.ptr<f8E5M2>>
    } {tt.flatten, tt.loop_unroll_factor = 1 : i32, tt.num_stages = 2 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(convert-triton-to-tritongpu{enable-source-remat=false num-ctas=1 num-warps=4 target=cuda:90 threads-per-warp=32}, tritongpu-coalesce, tritongpu-F32DotTC, triton-nvidia-gpu-plan-cta, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritongpu-accelerate-matmul, tritongpu-remove-layout-conversions, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, triton-nvidia-optimize-descriptor-encoding, triton-loop-aware-cse, tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-combine-tensor-select-and-if, nvgpu-warp-specialization{dump-intermediate-steps=false num-stages=1}, tritongpu-assign-latencies{num-stages=1}, tritongpu-schedule-loops, tritongpu-pipeline{dump-intermediate-steps=false num-stages=1}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-loop-aware-cse, tritongpu-prefetch, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-coalesce-async-copy, triton-nvidia-optimize-tmem-layouts, tritongpu-remove-layout-conversions, triton-nvidia-interleave-tmem, tritongpu-reduce-data-duplication, tritongpu-reorder-instructions, triton-loop-aware-cse, symbol-dce, triton-nvidia-tma-lowering, triton-nvidia-gpu-fence-insertion{compute-capability=90}, sccp, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true})",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_willfeng/2g/c2ggmy6gsa3azn7ggoixelmvi4kawo2joc7pmuk5xoolnkzvbbj4.py:13:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_willfeng/2g/c2ggmy6gsa3azn7ggoixelmvi4kawo2joc7pmuk5xoolnkzvbbj4.py:13:0: note: Pipeline failed while executing [`TritonGPUAssignLatencies` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
Process ForkProcess-360:
Traceback (most recent call last):
  File "/home/willfeng/local/miniconda3/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/willfeng/local/miniconda3/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/data/users/willfeng/helion/helion/runtime/precompile_shim.py", line 54, in finish_it
    kernel_cache[key] = fn.compile(
                        ^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/compiler/compiler.py", line 359, in compile
    next_module = compile_ir(module, metadata)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/backends/nvidia/compiler.py", line 456, in <lambda>
    stages["ttgir"] = lambda src, metadata: self.make_ttgir(src, metadata, options, capability)
                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/backends/nvidia/compiler.py", line 298, in make_ttgir
    pm.run(mod)
RuntimeError: PassManager::run failed
python: /root/.triton/llvm/llvm-8957e64a-almalinux-x64/include/llvm/ADT/SmallVector.h:296: const_reference llvm::SmallVectorTemplateCommon<long>::operator[](size_type) const [T = long]: Assertion `idx < size()' failed.
module {
  tt.func public @_fp8_attention_kernel_kernel(%arg0: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %c1_i32 = arith.constant 1 : i32
    %c64_i32 = arith.constant 64 : i32
    %c128_i32 = arith.constant 128 : i32
    %c0_i32 = arith.constant 0 : i32
    %c132_i32 = arith.constant 132 : i32
    %c192_i32 = arith.constant 192 : i32
    %cst = arith.constant dense<64> : tensor<1x64xi32>
    %cst_0 = arith.constant dense<0.180336878> : tensor<128x64xf32>
    %cst_1 = arith.constant dense<64> : tensor<64x1xi32>
    %cst_2 = arith.constant dense<64> : tensor<128x1xi32>
    %c8192_i32 = arith.constant 8192 : i32
    %cst_3 = arith.constant dense<0.000000e+00> : tensor<128x64xf32>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<128xf32>
    %cst_5 = arith.constant dense<0xFF800000> : tensor<128xf32>
    %0 = tt.get_program_id x : i32
    scf.for %arg6 = %0 to %c192_i32 step %c132_i32  : i32 {
      %1 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
      %2 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>
      %3 = arith.muli %arg6, %c8192_i32 : i32
      %4 = tt.expand_dims %2 {axis = 1 : i32} : tensor<128xi32> -> tensor<128x1xi32>
      %5 = arith.muli %4, %cst_2 : tensor<128x1xi32>
      %6 = tt.splat %3 : i32 -> tensor<128x1xi32>
      %7 = arith.addi %6, %5 : tensor<128x1xi32>
      %8 = tt.expand_dims %1 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %9 = tt.broadcast %7 : tensor<128x1xi32> -> tensor<128x64xi32>
      %10 = tt.broadcast %8 : tensor<1x64xi32> -> tensor<128x64xi32>
      %11 = arith.addi %9, %10 : tensor<128x64xi32>
      %12 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %13 = tt.addptr %12, %11 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      %14 = tt.load %13 : tensor<128x64x!tt.ptr<f8E5M2>>
      %15:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c64_i32 iter_args(%arg8 = %cst_4, %arg9 = %cst_3, %arg10 = %cst_5) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %41 = tt.splat %arg7 : i32 -> tensor<64xi32>
        %42 = arith.addi %41, %1 : tensor<64xi32>
        %43 = tt.expand_dims %42 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %44 = arith.muli %43, %cst_1 : tensor<64x1xi32>
        %45 = tt.splat %3 : i32 -> tensor<64x1xi32>
        %46 = arith.addi %45, %44 : tensor<64x1xi32>
        %47 = tt.broadcast %46 : tensor<64x1xi32> -> tensor<64x64xi32>
        %48 = tt.broadcast %8 : tensor<1x64xi32> -> tensor<64x64xi32>
        %49 = arith.addi %47, %48 : tensor<64x64xi32>
        %50 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %51 = tt.addptr %50, %49 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %52 = tt.load %51 : tensor<64x64x!tt.ptr<f8E5M2>>
        %53 = tt.trans %52 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %54 = tt.dot %14, %53, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %55 = arith.mulf %54, %cst_0 : tensor<128x64xf32>
        %56 = "tt.reduce"(%55) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %87 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %87 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %57 = arith.cmpf ogt, %arg10, %56 : tensor<128xf32>
        %58 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %59 = arith.ori %57, %58 : tensor<128xi1>
        %60 = arith.select %59, %arg10, %56 : tensor<128xi1>, tensor<128xf32>
        %61 = tt.expand_dims %60 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %62 = tt.broadcast %61 : tensor<128x1xf32> -> tensor<128x64xf32>
        %63 = arith.subf %55, %62 : tensor<128x64xf32>
        %64 = tt.extern_elementwise %63 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %65 = "tt.reduce"(%64) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %87 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %87 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %66 = arith.subf %arg10, %60 : tensor<128xf32>
        %67 = tt.extern_elementwise %66 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %68 = arith.mulf %arg8, %67 : tensor<128xf32>
        %69 = arith.addf %68, %65 : tensor<128xf32>
        %70 = tt.expand_dims %67 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %71 = tt.broadcast %70 : tensor<128x1xf32> -> tensor<128x64xf32>
        %72 = arith.mulf %arg9, %71 : tensor<128x64xf32>
        %73 = tt.expand_dims %1 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %74 = arith.addi %45, %73 : tensor<64x1xi32>
        %75 = tt.expand_dims %42 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %76 = arith.muli %75, %cst : tensor<1x64xi32>
        %77 = tt.broadcast %74 : tensor<64x1xi32> -> tensor<64x64xi32>
        %78 = tt.broadcast %76 : tensor<1x64xi32> -> tensor<64x64xi32>
        %79 = arith.addi %77, %78 : tensor<64x64xi32>
        %80 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %81 = tt.addptr %80, %79 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %82 = tt.load %81 : tensor<64x64x!tt.ptr<f8E5M2>>
        %83 = tt.fp_to_fp %64, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %84 = tt.trans %82 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %85 = tt.dot %83, %84, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %86 = arith.addf %72, %85 : tensor<128x64xf32>
        scf.yield %69, %86, %60 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.loop_unroll_factor = 1 : i32, tt.num_stages = 2 : i32}
      %16 = tt.expand_dims %15#0 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
      %17 = tt.broadcast %16 : tensor<128x1xf32> -> tensor<128x64xf32>
      %18 = arith.divf %15#1, %17 : tensor<128x64xf32>
      %19 = tt.fp_to_fp %18, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
      %20 = arith.divsi %arg6, %arg5 : i32
      %21 = arith.remsi %arg6, %arg5 : i32
      %22 = arith.cmpi ne, %21, %c0_i32 : i32
      %23 = arith.subi %20, %c1_i32 : i32
      %24 = arith.select %22, %23, %20 : i32
      %25 = arith.cmpi slt, %arg6, %c0_i32 : i32
      %26 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %27 = arith.cmpi ne, %25, %26 : i1
      %28 = arith.select %27, %24, %20 : i32
      %29 = arith.andi %22, %27 : i1
      %30 = arith.addi %21, %arg5 : i32
      %31 = arith.select %29, %30, %21 : i32
      %32 = arith.muli %28, %arg4 : i32
      %33 = arith.muli %31, %c8192_i32 : i32
      %34 = arith.addi %32, %33 : i32
      %35 = tt.splat %34 : i32 -> tensor<128x1xi32>
      %36 = arith.addi %35, %5 : tensor<128x1xi32>
      %37 = tt.broadcast %36 : tensor<128x1xi32> -> tensor<128x64xi32>
      %38 = arith.addi %37, %10 : tensor<128x64xi32>
      %39 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %40 = tt.addptr %39, %38 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      tt.store %40, %19 : tensor<128x64x!tt.ptr<f8E5M2>>
    } {tt.flatten, tt.loop_unroll_factor = 1 : i32, tt.num_stages = 2 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(convert-triton-to-tritongpu{enable-source-remat=false num-ctas=1 num-warps=4 target=cuda:90 threads-per-warp=32}, tritongpu-coalesce, tritongpu-F32DotTC, triton-nvidia-gpu-plan-cta, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritongpu-accelerate-matmul, tritongpu-remove-layout-conversions, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, triton-nvidia-optimize-descriptor-encoding, triton-loop-aware-cse, tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-combine-tensor-select-and-if, nvgpu-warp-specialization{dump-intermediate-steps=false num-stages=1}, tritongpu-assign-latencies{num-stages=1}, tritongpu-schedule-loops, tritongpu-pipeline{dump-intermediate-steps=false num-stages=1}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-loop-aware-cse, tritongpu-prefetch, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-coalesce-async-copy, triton-nvidia-optimize-tmem-layouts, tritongpu-remove-layout-conversions, triton-nvidia-interleave-tmem, tritongpu-reduce-data-duplication, tritongpu-reorder-instructions, triton-loop-aware-cse, symbol-dce, triton-nvidia-tma-lowering, triton-nvidia-gpu-fence-insertion{compute-capability=90}, sccp, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true})",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_willfeng/2g/c2ggmy6gsa3azn7ggoixelmvi4kawo2joc7pmuk5xoolnkzvbbj4.py:13:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_willfeng/2g/c2ggmy6gsa3azn7ggoixelmvi4kawo2joc7pmuk5xoolnkzvbbj4.py:13:0: note: Pipeline failed while executing [`TritonGPUAssignLatencies` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
[492s] Timeout after 60s compiling Config(block_sizes=[64, 64], range_unroll_factors=[4, 4, 4], range_num_stages=[3, 1, 2], range_multi_buffers=[False, None, False], range_flattens=[True, None, False], num_warps=2, num_stages=3, indexing='block_ptr', pid_type='persistent_blocked')
python: /root/.triton/llvm/llvm-8957e64a-almalinux-x64/include/llvm/ADT/SmallVector.h:296: const_reference llvm::SmallVectorTemplateCommon<long>::operator[](size_type) const [T = long]: Assertion `idx < size()' failed.
module {
  tt.func public @_fp8_attention_kernel_kernel(%arg0: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %c1_i32 = arith.constant 1 : i32
    %c64_i32 = arith.constant 64 : i32
    %c128_i32 = arith.constant 128 : i32
    %c0_i32 = arith.constant 0 : i32
    %c132_i32 = arith.constant 132 : i32
    %c192_i32 = arith.constant 192 : i32
    %cst = arith.constant dense<64> : tensor<1x64xi32>
    %cst_0 = arith.constant dense<0.180336878> : tensor<128x64xf32>
    %cst_1 = arith.constant dense<64> : tensor<64x1xi32>
    %cst_2 = arith.constant dense<64> : tensor<128x1xi32>
    %c8192_i32 = arith.constant 8192 : i32
    %cst_3 = arith.constant dense<0.000000e+00> : tensor<128x64xf32>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<128xf32>
    %cst_5 = arith.constant dense<0xFF800000> : tensor<128xf32>
    %0 = tt.get_program_id x : i32
    %1 = arith.subi %c192_i32, %0 : i32
    %c1_i32_6 = arith.constant 1 : i32
    %2 = arith.subi %c132_i32, %c1_i32_6 : i32
    %3 = arith.addi %1, %2 : i32
    %4 = arith.divui %3, %c132_i32 : i32
    %c3_i32 = arith.constant 3 : i32
    %5 = arith.remsi %4, %c3_i32 : i32
    %6 = arith.subi %4, %5 : i32
    %7 = arith.muli %6, %c132_i32 : i32
    %8 = arith.addi %0, %7 : i32
    %9 = arith.muli %c132_i32, %c3_i32 : i32
    scf.for %arg6 = %0 to %8 step %9  : i32 {
      %10 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
      %11 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>
      %12 = arith.muli %arg6, %c8192_i32 : i32
      %13 = tt.expand_dims %11 {axis = 1 : i32} : tensor<128xi32> -> tensor<128x1xi32>
      %14 = arith.muli %13, %cst_2 : tensor<128x1xi32>
      %15 = tt.splat %12 : i32 -> tensor<128x1xi32>
      %16 = arith.addi %15, %14 : tensor<128x1xi32>
      %17 = tt.expand_dims %10 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %18 = tt.broadcast %16 : tensor<128x1xi32> -> tensor<128x64xi32>
      %19 = tt.broadcast %17 : tensor<1x64xi32> -> tensor<128x64xi32>
      %20 = arith.addi %18, %19 : tensor<128x64xi32>
      %21 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %22 = tt.addptr %21, %20 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      %23 = tt.load %22 : tensor<128x64x!tt.ptr<f8E5M2>>
      %24:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c64_i32 iter_args(%arg8 = %cst_4, %arg9 = %cst_3, %arg10 = %cst_5) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %134 = tt.splat %arg7 : i32 -> tensor<64xi32>
        %135 = arith.addi %134, %10 : tensor<64xi32>
        %136 = tt.expand_dims %135 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %137 = arith.muli %136, %cst_1 : tensor<64x1xi32>
        %138 = tt.splat %12 : i32 -> tensor<64x1xi32>
        %139 = arith.addi %138, %137 : tensor<64x1xi32>
        %140 = tt.broadcast %139 : tensor<64x1xi32> -> tensor<64x64xi32>
        %141 = tt.broadcast %17 : tensor<1x64xi32> -> tensor<64x64xi32>
        %142 = arith.addi %140, %141 : tensor<64x64xi32>
        %143 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %144 = tt.addptr %143, %142 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %145 = tt.load %144 : tensor<64x64x!tt.ptr<f8E5M2>>
        %146 = tt.trans %145 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %147 = tt.dot %23, %146, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %148 = arith.mulf %147, %cst_0 : tensor<128x64xf32>
        %149 = "tt.reduce"(%148) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %180 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %180 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %150 = arith.cmpf ogt, %arg10, %149 : tensor<128xf32>
        %151 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %152 = arith.ori %150, %151 : tensor<128xi1>
        %153 = arith.select %152, %arg10, %149 : tensor<128xi1>, tensor<128xf32>
        %154 = tt.expand_dims %153 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %155 = tt.broadcast %154 : tensor<128x1xf32> -> tensor<128x64xf32>
        %156 = arith.subf %148, %155 : tensor<128x64xf32>
        %157 = tt.extern_elementwise %156 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %158 = "tt.reduce"(%157) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %180 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %180 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %159 = arith.subf %arg10, %153 : tensor<128xf32>
        %160 = tt.extern_elementwise %159 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %161 = arith.mulf %arg8, %160 : tensor<128xf32>
        %162 = arith.addf %161, %158 : tensor<128xf32>
        %163 = tt.expand_dims %160 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %164 = tt.broadcast %163 : tensor<128x1xf32> -> tensor<128x64xf32>
        %165 = arith.mulf %arg9, %164 : tensor<128x64xf32>
        %166 = tt.expand_dims %10 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %167 = arith.addi %138, %166 : tensor<64x1xi32>
        %168 = tt.expand_dims %135 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %169 = arith.muli %168, %cst : tensor<1x64xi32>
        %170 = tt.broadcast %167 : tensor<64x1xi32> -> tensor<64x64xi32>
        %171 = tt.broadcast %169 : tensor<1x64xi32> -> tensor<64x64xi32>
        %172 = arith.addi %170, %171 : tensor<64x64xi32>
        %173 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %174 = tt.addptr %173, %172 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %175 = tt.load %174 : tensor<64x64x!tt.ptr<f8E5M2>>
        %176 = tt.fp_to_fp %157, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %177 = tt.trans %175 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %178 = tt.dot %176, %177, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %179 = arith.addf %165, %178 : tensor<128x64xf32>
        scf.yield %162, %179, %153 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.disallow_acc_multi_buffer, tt.flatten, tt.loop_unroll_factor = 1 : i32, tt.num_stages = 4 : i32}
      %25 = tt.expand_dims %24#0 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
      %26 = tt.broadcast %25 : tensor<128x1xf32> -> tensor<128x64xf32>
      %27 = arith.divf %24#1, %26 : tensor<128x64xf32>
      %28 = tt.fp_to_fp %27, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
      %29 = arith.divsi %arg6, %arg5 : i32
      %30 = arith.remsi %arg6, %arg5 : i32
      %31 = arith.cmpi ne, %30, %c0_i32 : i32
      %32 = arith.subi %29, %c1_i32 : i32
      %33 = arith.select %31, %32, %29 : i32
      %34 = arith.cmpi slt, %arg6, %c0_i32 : i32
      %35 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %36 = arith.cmpi ne, %34, %35 : i1
      %37 = arith.select %36, %33, %29 : i32
      %38 = arith.andi %31, %36 : i1
      %39 = arith.addi %30, %arg5 : i32
      %40 = arith.select %38, %39, %30 : i32
      %41 = arith.muli %37, %arg4 : i32
      %42 = arith.muli %40, %c8192_i32 : i32
      %43 = arith.addi %41, %42 : i32
      %44 = tt.splat %43 : i32 -> tensor<128x1xi32>
      %45 = arith.addi %44, %14 : tensor<128x1xi32>
      %46 = tt.broadcast %45 : tensor<128x1xi32> -> tensor<128x64xi32>
      %47 = arith.addi %46, %19 : tensor<128x64xi32>
      %48 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %49 = tt.addptr %48, %47 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      tt.store %49, %28 : tensor<128x64x!tt.ptr<f8E5M2>>
      %c1_i32_7 = arith.constant 1 : i32
      %50 = arith.muli %c132_i32, %c1_i32_7 : i32
      %51 = arith.addi %arg6, %50 : i32
      %52 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
      %53 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>
      %54 = arith.muli %51, %c8192_i32 : i32
      %55 = tt.expand_dims %53 {axis = 1 : i32} : tensor<128xi32> -> tensor<128x1xi32>
      %56 = arith.muli %55, %cst_2 : tensor<128x1xi32>
      %57 = tt.splat %54 : i32 -> tensor<128x1xi32>
      %58 = arith.addi %57, %56 : tensor<128x1xi32>
      %59 = tt.expand_dims %52 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %60 = tt.broadcast %58 : tensor<128x1xi32> -> tensor<128x64xi32>
      %61 = tt.broadcast %59 : tensor<1x64xi32> -> tensor<128x64xi32>
      %62 = arith.addi %60, %61 : tensor<128x64xi32>
      %63 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %64 = tt.addptr %63, %62 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      %65 = tt.load %64 : tensor<128x64x!tt.ptr<f8E5M2>>
      %66:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c64_i32 iter_args(%arg8 = %cst_4, %arg9 = %cst_3, %arg10 = %cst_5) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %134 = tt.splat %arg7 : i32 -> tensor<64xi32>
        %135 = arith.addi %134, %52 : tensor<64xi32>
        %136 = tt.expand_dims %135 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %137 = arith.muli %136, %cst_1 : tensor<64x1xi32>
        %138 = tt.splat %54 : i32 -> tensor<64x1xi32>
        %139 = arith.addi %138, %137 : tensor<64x1xi32>
        %140 = tt.broadcast %139 : tensor<64x1xi32> -> tensor<64x64xi32>
        %141 = tt.broadcast %59 : tensor<1x64xi32> -> tensor<64x64xi32>
        %142 = arith.addi %140, %141 : tensor<64x64xi32>
        %143 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %144 = tt.addptr %143, %142 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %145 = tt.load %144 : tensor<64x64x!tt.ptr<f8E5M2>>
        %146 = tt.trans %145 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %147 = tt.dot %65, %146, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %148 = arith.mulf %147, %cst_0 : tensor<128x64xf32>
        %149 = "tt.reduce"(%148) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %180 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %180 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %150 = arith.cmpf ogt, %arg10, %149 : tensor<128xf32>
        %151 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %152 = arith.ori %150, %151 : tensor<128xi1>
        %153 = arith.select %152, %arg10, %149 : tensor<128xi1>, tensor<128xf32>
        %154 = tt.expand_dims %153 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %155 = tt.broadcast %154 : tensor<128x1xf32> -> tensor<128x64xf32>
        %156 = arith.subf %148, %155 : tensor<128x64xf32>
        %157 = tt.extern_elementwise %156 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %158 = "tt.reduce"(%157) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %180 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %180 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %159 = arith.subf %arg10, %153 : tensor<128xf32>
        %160 = tt.extern_elementwise %159 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %161 = arith.mulf %arg8, %160 : tensor<128xf32>
        %162 = arith.addf %161, %158 : tensor<128xf32>
        %163 = tt.expand_dims %160 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %164 = tt.broadcast %163 : tensor<128x1xf32> -> tensor<128x64xf32>
        %165 = arith.mulf %arg9, %164 : tensor<128x64xf32>
        %166 = tt.expand_dims %52 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %167 = arith.addi %138, %166 : tensor<64x1xi32>
        %168 = tt.expand_dims %135 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %169 = arith.muli %168, %cst : tensor<1x64xi32>
        %170 = tt.broadcast %167 : tensor<64x1xi32> -> tensor<64x64xi32>
        %171 = tt.broadcast %169 : tensor<1x64xi32> -> tensor<64x64xi32>
        %172 = arith.addi %170, %171 : tensor<64x64xi32>
        %173 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %174 = tt.addptr %173, %172 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %175 = tt.load %174 : tensor<64x64x!tt.ptr<f8E5M2>>
        %176 = tt.fp_to_fp %157, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %177 = tt.trans %175 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %178 = tt.dot %176, %177, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %179 = arith.addf %165, %178 : tensor<128x64xf32>
        scf.yield %162, %179, %153 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.disallow_acc_multi_buffer, tt.flatten, tt.loop_unroll_factor = 1 : i32, tt.num_stages = 4 : i32}
      %67 = tt.expand_dims %66#0 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
      %68 = tt.broadcast %67 : tensor<128x1xf32> -> tensor<128x64xf32>
      %69 = arith.divf %66#1, %68 : tensor<128x64xf32>
      %70 = tt.fp_to_fp %69, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
      %71 = arith.divsi %51, %arg5 : i32
      %72 = arith.remsi %51, %arg5 : i32
      %73 = arith.cmpi ne, %72, %c0_i32 : i32
      %74 = arith.subi %71, %c1_i32 : i32
      %75 = arith.select %73, %74, %71 : i32
      %76 = arith.cmpi slt, %51, %c0_i32 : i32
      %77 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %78 = arith.cmpi ne, %76, %77 : i1
      %79 = arith.select %78, %75, %71 : i32
      %80 = arith.andi %73, %78 : i1
      %81 = arith.addi %72, %arg5 : i32
      %82 = arith.select %80, %81, %72 : i32
      %83 = arith.muli %79, %arg4 : i32
      %84 = arith.muli %82, %c8192_i32 : i32
      %85 = arith.addi %83, %84 : i32
      %86 = tt.splat %85 : i32 -> tensor<128x1xi32>
      %87 = arith.addi %86, %56 : tensor<128x1xi32>
      %88 = tt.broadcast %87 : tensor<128x1xi32> -> tensor<128x64xi32>
      %89 = arith.addi %88, %61 : tensor<128x64xi32>
      %90 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %91 = tt.addptr %90, %89 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      tt.store %91, %70 : tensor<128x64x!tt.ptr<f8E5M2>>
      %c2_i32 = arith.constant 2 : i32
      %92 = arith.muli %c132_i32, %c2_i32 : i32
      %93 = arith.addi %arg6, %92 : i32
      %94 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
      %95 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>
      %96 = arith.muli %93, %c8192_i32 : i32
      %97 = tt.expand_dims %95 {axis = 1 : i32} : tensor<128xi32> -> tensor<128x1xi32>
      %98 = arith.muli %97, %cst_2 : tensor<128x1xi32>
      %99 = tt.splat %96 : i32 -> tensor<128x1xi32>
      %100 = arith.addi %99, %98 : tensor<128x1xi32>
      %101 = tt.expand_dims %94 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %102 = tt.broadcast %100 : tensor<128x1xi32> -> tensor<128x64xi32>
      %103 = tt.broadcast %101 : tensor<1x64xi32> -> tensor<128x64xi32>
      %104 = arith.addi %102, %103 : tensor<128x64xi32>
      %105 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %106 = tt.addptr %105, %104 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      %107 = tt.load %106 : tensor<128x64x!tt.ptr<f8E5M2>>
      %108:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c64_i32 iter_args(%arg8 = %cst_4, %arg9 = %cst_3, %arg10 = %cst_5) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %134 = tt.splat %arg7 : i32 -> tensor<64xi32>
        %135 = arith.addi %134, %94 : tensor<64xi32>
        %136 = tt.expand_dims %135 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %137 = arith.muli %136, %cst_1 : tensor<64x1xi32>
        %138 = tt.splat %96 : i32 -> tensor<64x1xi32>
        %139 = arith.addi %138, %137 : tensor<64x1xi32>
        %140 = tt.broadcast %139 : tensor<64x1xi32> -> tensor<64x64xi32>
        %141 = tt.broadcast %101 : tensor<1x64xi32> -> tensor<64x64xi32>
        %142 = arith.addi %140, %141 : tensor<64x64xi32>
        %143 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %144 = tt.addptr %143, %142 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %145 = tt.load %144 : tensor<64x64x!tt.ptr<f8E5M2>>
        %146 = tt.trans %145 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %147 = tt.dot %107, %146, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %148 = arith.mulf %147, %cst_0 : tensor<128x64xf32>
        %149 = "tt.reduce"(%148) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %180 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %180 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %150 = arith.cmpf ogt, %arg10, %149 : tensor<128xf32>
        %151 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %152 = arith.ori %150, %151 : tensor<128xi1>
        %153 = arith.select %152, %arg10, %149 : tensor<128xi1>, tensor<128xf32>
        %154 = tt.expand_dims %153 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %155 = tt.broadcast %154 : tensor<128x1xf32> -> tensor<128x64xf32>
        %156 = arith.subf %148, %155 : tensor<128x64xf32>
        %157 = tt.extern_elementwise %156 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %158 = "tt.reduce"(%157) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %180 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %180 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %159 = arith.subf %arg10, %153 : tensor<128xf32>
        %160 = tt.extern_elementwise %159 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %161 = arith.mulf %arg8, %160 : tensor<128xf32>
        %162 = arith.addf %161, %158 : tensor<128xf32>
        %163 = tt.expand_dims %160 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %164 = tt.broadcast %163 : tensor<128x1xf32> -> tensor<128x64xf32>
        %165 = arith.mulf %arg9, %164 : tensor<128x64xf32>
        %166 = tt.expand_dims %94 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %167 = arith.addi %138, %166 : tensor<64x1xi32>
        %168 = tt.expand_dims %135 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %169 = arith.muli %168, %cst : tensor<1x64xi32>
        %170 = tt.broadcast %167 : tensor<64x1xi32> -> tensor<64x64xi32>
        %171 = tt.broadcast %169 : tensor<1x64xi32> -> tensor<64x64xi32>
        %172 = arith.addi %170, %171 : tensor<64x64xi32>
        %173 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %174 = tt.addptr %173, %172 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %175 = tt.load %174 : tensor<64x64x!tt.ptr<f8E5M2>>
        %176 = tt.fp_to_fp %157, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %177 = tt.trans %175 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %178 = tt.dot %176, %177, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %179 = arith.addf %165, %178 : tensor<128x64xf32>
        scf.yield %162, %179, %153 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.disallow_acc_multi_buffer, tt.flatten, tt.loop_unroll_factor = 1 : i32, tt.num_stages = 4 : i32}
      %109 = tt.expand_dims %108#0 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
      %110 = tt.broadcast %109 : tensor<128x1xf32> -> tensor<128x64xf32>
      %111 = arith.divf %108#1, %110 : tensor<128x64xf32>
      %112 = tt.fp_to_fp %111, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
      %113 = arith.divsi %93, %arg5 : i32
      %114 = arith.remsi %93, %arg5 : i32
      %115 = arith.cmpi ne, %114, %c0_i32 : i32
      %116 = arith.subi %113, %c1_i32 : i32
      %117 = arith.select %115, %116, %113 : i32
      %118 = arith.cmpi slt, %93, %c0_i32 : i32
      %119 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %120 = arith.cmpi ne, %118, %119 : i1
      %121 = arith.select %120, %117, %113 : i32
      %122 = arith.andi %115, %120 : i1
      %123 = arith.addi %114, %arg5 : i32
      %124 = arith.select %122, %123, %114 : i32
      %125 = arith.muli %121, %arg4 : i32
      %126 = arith.muli %124, %c8192_i32 : i32
      %127 = arith.addi %125, %126 : i32
      %128 = tt.splat %127 : i32 -> tensor<128x1xi32>
      %129 = arith.addi %128, %98 : tensor<128x1xi32>
      %130 = tt.broadcast %129 : tensor<128x1xi32> -> tensor<128x64xi32>
      %131 = arith.addi %130, %103 : tensor<128x64xi32>
      %132 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %133 = tt.addptr %132, %131 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      tt.store %133, %112 : tensor<128x64x!tt.ptr<f8E5M2>>
    } {tt.flatten, tt.num_stages = 3 : i32}
    scf.for %arg6 = %8 to %c192_i32 step %c132_i32  : i32 {
      %10 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
      %11 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>
      %12 = arith.muli %arg6, %c8192_i32 : i32
      %13 = tt.expand_dims %11 {axis = 1 : i32} : tensor<128xi32> -> tensor<128x1xi32>
      %14 = arith.muli %13, %cst_2 : tensor<128x1xi32>
      %15 = tt.splat %12 : i32 -> tensor<128x1xi32>
      %16 = arith.addi %15, %14 : tensor<128x1xi32>
      %17 = tt.expand_dims %10 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %18 = tt.broadcast %16 : tensor<128x1xi32> -> tensor<128x64xi32>
      %19 = tt.broadcast %17 : tensor<1x64xi32> -> tensor<128x64xi32>
      %20 = arith.addi %18, %19 : tensor<128x64xi32>
      %21 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %22 = tt.addptr %21, %20 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      %23 = tt.load %22 : tensor<128x64x!tt.ptr<f8E5M2>>
      %24:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c64_i32 iter_args(%arg8 = %cst_4, %arg9 = %cst_3, %arg10 = %cst_5) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %50 = tt.splat %arg7 : i32 -> tensor<64xi32>
        %51 = arith.addi %50, %10 : tensor<64xi32>
        %52 = tt.expand_dims %51 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %53 = arith.muli %52, %cst_1 : tensor<64x1xi32>
        %54 = tt.splat %12 : i32 -> tensor<64x1xi32>
        %55 = arith.addi %54, %53 : tensor<64x1xi32>
        %56 = tt.broadcast %55 : tensor<64x1xi32> -> tensor<64x64xi32>
        %57 = tt.broadcast %17 : tensor<1x64xi32> -> tensor<64x64xi32>
        %58 = arith.addi %56, %57 : tensor<64x64xi32>
        %59 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %60 = tt.addptr %59, %58 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %61 = tt.load %60 : tensor<64x64x!tt.ptr<f8E5M2>>
        %62 = tt.trans %61 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %63 = tt.dot %23, %62, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %64 = arith.mulf %63, %cst_0 : tensor<128x64xf32>
        %65 = "tt.reduce"(%64) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %96 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %96 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %66 = arith.cmpf ogt, %arg10, %65 : tensor<128xf32>
        %67 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %68 = arith.ori %66, %67 : tensor<128xi1>
        %69 = arith.select %68, %arg10, %65 : tensor<128xi1>, tensor<128xf32>
        %70 = tt.expand_dims %69 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %71 = tt.broadcast %70 : tensor<128x1xf32> -> tensor<128x64xf32>
        %72 = arith.subf %64, %71 : tensor<128x64xf32>
        %73 = tt.extern_elementwise %72 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %74 = "tt.reduce"(%73) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %96 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %96 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %75 = arith.subf %arg10, %69 : tensor<128xf32>
        %76 = tt.extern_elementwise %75 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %77 = arith.mulf %arg8, %76 : tensor<128xf32>
        %78 = arith.addf %77, %74 : tensor<128xf32>
        %79 = tt.expand_dims %76 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %80 = tt.broadcast %79 : tensor<128x1xf32> -> tensor<128x64xf32>
        %81 = arith.mulf %arg9, %80 : tensor<128x64xf32>
        %82 = tt.expand_dims %10 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %83 = arith.addi %54, %82 : tensor<64x1xi32>
        %84 = tt.expand_dims %51 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %85 = arith.muli %84, %cst : tensor<1x64xi32>
        %86 = tt.broadcast %83 : tensor<64x1xi32> -> tensor<64x64xi32>
        %87 = tt.broadcast %85 : tensor<1x64xi32> -> tensor<64x64xi32>
        %88 = arith.addi %86, %87 : tensor<64x64xi32>
        %89 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %90 = tt.addptr %89, %88 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %91 = tt.load %90 : tensor<64x64x!tt.ptr<f8E5M2>>
        %92 = tt.fp_to_fp %73, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %93 = tt.trans %91 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %94 = tt.dot %92, %93, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %95 = arith.addf %81, %94 : tensor<128x64xf32>
        scf.yield %78, %95, %69 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.disallow_acc_multi_buffer, tt.flatten, tt.loop_unroll_factor = 1 : i32, tt.num_stages = 4 : i32}
      %25 = tt.expand_dims %24#0 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
      %26 = tt.broadcast %25 : tensor<128x1xf32> -> tensor<128x64xf32>
      %27 = arith.divf %24#1, %26 : tensor<128x64xf32>
      %28 = tt.fp_to_fp %27, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
      %29 = arith.divsi %arg6, %arg5 : i32
      %30 = arith.remsi %arg6, %arg5 : i32
      %31 = arith.cmpi ne, %30, %c0_i32 : i32
      %32 = arith.subi %29, %c1_i32 : i32
      %33 = arith.select %31, %32, %29 : i32
      %34 = arith.cmpi slt, %arg6, %c0_i32 : i32
      %35 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %36 = arith.cmpi ne, %34, %35 : i1
      %37 = arith.select %36, %33, %29 : i32
      %38 = arith.andi %31, %36 : i1
      %39 = arith.addi %30, %arg5 : i32
      %40 = arith.select %38, %39, %30 : i32
      %41 = arith.muli %37, %arg4 : i32
      %42 = arith.muli %40, %c8192_i32 : i32
      %43 = arith.addi %41, %42 : i32
      %44 = tt.splat %43 : i32 -> tensor<128x1xi32>
      %45 = arith.addi %44, %14 : tensor<128x1xi32>
      %46 = tt.broadcast %45 : tensor<128x1xi32> -> tensor<128x64xi32>
      %47 = arith.addi %46, %19 : tensor<128x64xi32>
      %48 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %49 = tt.addptr %48, %47 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      tt.store %49, %28 : tensor<128x64x!tt.ptr<f8E5M2>>
    } {tt.flatten, tt.num_stages = 1 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(convert-triton-to-tritongpu{enable-source-remat=false num-ctas=1 num-warps=8 target=cuda:90 threads-per-warp=32}, tritongpu-coalesce, tritongpu-F32DotTC, triton-nvidia-gpu-plan-cta, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritongpu-accelerate-matmul, tritongpu-remove-layout-conversions, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, triton-nvidia-optimize-descriptor-encoding, triton-loop-aware-cse, tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-combine-tensor-select-and-if, nvgpu-warp-specialization{dump-intermediate-steps=false num-stages=1}, tritongpu-assign-latencies{num-stages=1}, tritongpu-schedule-loops, tritongpu-pipeline{dump-intermediate-steps=false num-stages=1}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-loop-aware-cse, tritongpu-prefetch, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-coalesce-async-copy, triton-nvidia-optimize-tmem-layouts, tritongpu-remove-layout-conversions, triton-nvidia-interleave-tmem, tritongpu-reduce-data-duplication, tritongpu-reorder-instructions, triton-loop-aware-cse, symbol-dce, triton-nvidia-tma-lowering, triton-nvidia-gpu-fence-insertion{compute-capability=90}, sccp, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true})",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_willfeng/55/c55baynkiz7bp5lfwiq23spqxbftxnsv3gc6psgiupvirynvb7oc.py:13:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_willfeng/55/c55baynkiz7bp5lfwiq23spqxbftxnsv3gc6psgiupvirynvb7oc.py:13:0: note: Pipeline failed while executing [`TritonGPUAssignLatencies` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
Process ForkProcess-394:
Traceback (most recent call last):
  File "/home/willfeng/local/miniconda3/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/willfeng/local/miniconda3/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/data/users/willfeng/helion/helion/runtime/precompile_shim.py", line 54, in finish_it
    kernel_cache[key] = fn.compile(
                        ^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/compiler/compiler.py", line 359, in compile
    next_module = compile_ir(module, metadata)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/backends/nvidia/compiler.py", line 456, in <lambda>
    stages["ttgir"] = lambda src, metadata: self.make_ttgir(src, metadata, options, capability)
                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/backends/nvidia/compiler.py", line 298, in make_ttgir
    pm.run(mod)
RuntimeError: PassManager::run failed
python: /root/.triton/llvm/llvm-8957e64a-almalinux-x64/include/llvm/ADT/SmallVector.h:296: const_reference llvm::SmallVectorTemplateCommon<long>::operator[](size_type) const [T = long]: Assertion `idx < size()' failed.
module {
  tt.func public @_fp8_attention_kernel_kernel(%arg0: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %c1_i32 = arith.constant 1 : i32
    %c64_i32 = arith.constant 64 : i32
    %c128_i32 = arith.constant 128 : i32
    %c0_i32 = arith.constant 0 : i32
    %c132_i32 = arith.constant 132 : i32
    %c192_i32 = arith.constant 192 : i32
    %cst = arith.constant dense<64> : tensor<1x64xi32>
    %cst_0 = arith.constant dense<0.180336878> : tensor<128x64xf32>
    %cst_1 = arith.constant dense<64> : tensor<64x1xi32>
    %cst_2 = arith.constant dense<64> : tensor<128x1xi32>
    %c8192_i32 = arith.constant 8192 : i32
    %cst_3 = arith.constant dense<0.000000e+00> : tensor<128x64xf32>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<128xf32>
    %cst_5 = arith.constant dense<0xFF800000> : tensor<128xf32>
    %0 = tt.get_program_id x : i32
    %1 = arith.subi %c192_i32, %0 : i32
    %c1_i32_6 = arith.constant 1 : i32
    %2 = arith.subi %c132_i32, %c1_i32_6 : i32
    %3 = arith.addi %1, %2 : i32
    %4 = arith.divui %3, %c132_i32 : i32
    %c3_i32 = arith.constant 3 : i32
    %5 = arith.remsi %4, %c3_i32 : i32
    %6 = arith.subi %4, %5 : i32
    %7 = arith.muli %6, %c132_i32 : i32
    %8 = arith.addi %0, %7 : i32
    %9 = arith.muli %c132_i32, %c3_i32 : i32
    scf.for %arg6 = %0 to %8 step %9  : i32 {
      %10 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
      %11 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>
      %12 = arith.muli %arg6, %c8192_i32 : i32
      %13 = tt.expand_dims %11 {axis = 1 : i32} : tensor<128xi32> -> tensor<128x1xi32>
      %14 = arith.muli %13, %cst_2 : tensor<128x1xi32>
      %15 = tt.splat %12 : i32 -> tensor<128x1xi32>
      %16 = arith.addi %15, %14 : tensor<128x1xi32>
      %17 = tt.expand_dims %10 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %18 = tt.broadcast %16 : tensor<128x1xi32> -> tensor<128x64xi32>
      %19 = tt.broadcast %17 : tensor<1x64xi32> -> tensor<128x64xi32>
      %20 = arith.addi %18, %19 : tensor<128x64xi32>
      %21 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %22 = tt.addptr %21, %20 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      %23 = tt.load %22 : tensor<128x64x!tt.ptr<f8E5M2>>
      %24:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c64_i32 iter_args(%arg8 = %cst_4, %arg9 = %cst_3, %arg10 = %cst_5) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %134 = tt.splat %arg7 : i32 -> tensor<64xi32>
        %135 = arith.addi %134, %10 : tensor<64xi32>
        %136 = tt.expand_dims %135 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %137 = arith.muli %136, %cst_1 : tensor<64x1xi32>
        %138 = tt.splat %12 : i32 -> tensor<64x1xi32>
        %139 = arith.addi %138, %137 : tensor<64x1xi32>
        %140 = tt.broadcast %139 : tensor<64x1xi32> -> tensor<64x64xi32>
        %141 = tt.broadcast %17 : tensor<1x64xi32> -> tensor<64x64xi32>
        %142 = arith.addi %140, %141 : tensor<64x64xi32>
        %143 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %144 = tt.addptr %143, %142 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %145 = tt.load %144 : tensor<64x64x!tt.ptr<f8E5M2>>
        %146 = tt.trans %145 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %147 = tt.dot %23, %146, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %148 = arith.mulf %147, %cst_0 : tensor<128x64xf32>
        %149 = "tt.reduce"(%148) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %180 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %180 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %150 = arith.cmpf ogt, %arg10, %149 : tensor<128xf32>
        %151 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %152 = arith.ori %150, %151 : tensor<128xi1>
        %153 = arith.select %152, %arg10, %149 : tensor<128xi1>, tensor<128xf32>
        %154 = tt.expand_dims %153 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %155 = tt.broadcast %154 : tensor<128x1xf32> -> tensor<128x64xf32>
        %156 = arith.subf %148, %155 : tensor<128x64xf32>
        %157 = tt.extern_elementwise %156 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %158 = "tt.reduce"(%157) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %180 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %180 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %159 = arith.subf %arg10, %153 : tensor<128xf32>
        %160 = tt.extern_elementwise %159 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %161 = arith.mulf %arg8, %160 : tensor<128xf32>
        %162 = arith.addf %161, %158 : tensor<128xf32>
        %163 = tt.expand_dims %160 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %164 = tt.broadcast %163 : tensor<128x1xf32> -> tensor<128x64xf32>
        %165 = arith.mulf %arg9, %164 : tensor<128x64xf32>
        %166 = tt.expand_dims %10 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %167 = arith.addi %138, %166 : tensor<64x1xi32>
        %168 = tt.expand_dims %135 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %169 = arith.muli %168, %cst : tensor<1x64xi32>
        %170 = tt.broadcast %167 : tensor<64x1xi32> -> tensor<64x64xi32>
        %171 = tt.broadcast %169 : tensor<1x64xi32> -> tensor<64x64xi32>
        %172 = arith.addi %170, %171 : tensor<64x64xi32>
        %173 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %174 = tt.addptr %173, %172 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %175 = tt.load %174 : tensor<64x64x!tt.ptr<f8E5M2>>
        %176 = tt.fp_to_fp %157, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %177 = tt.trans %175 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %178 = tt.dot %176, %177, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %179 = arith.addf %165, %178 : tensor<128x64xf32>
        scf.yield %162, %179, %153 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.disallow_acc_multi_buffer, tt.flatten, tt.loop_unroll_factor = 1 : i32, tt.num_stages = 4 : i32}
      %25 = tt.expand_dims %24#0 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
      %26 = tt.broadcast %25 : tensor<128x1xf32> -> tensor<128x64xf32>
      %27 = arith.divf %24#1, %26 : tensor<128x64xf32>
      %28 = tt.fp_to_fp %27, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
      %29 = arith.divsi %arg6, %arg5 : i32
      %30 = arith.remsi %arg6, %arg5 : i32
      %31 = arith.cmpi ne, %30, %c0_i32 : i32
      %32 = arith.subi %29, %c1_i32 : i32
      %33 = arith.select %31, %32, %29 : i32
      %34 = arith.cmpi slt, %arg6, %c0_i32 : i32
      %35 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %36 = arith.cmpi ne, %34, %35 : i1
      %37 = arith.select %36, %33, %29 : i32
      %38 = arith.andi %31, %36 : i1
      %39 = arith.addi %30, %arg5 : i32
      %40 = arith.select %38, %39, %30 : i32
      %41 = arith.muli %37, %arg4 : i32
      %42 = arith.muli %40, %c8192_i32 : i32
      %43 = arith.addi %41, %42 : i32
      %44 = tt.splat %43 : i32 -> tensor<128x1xi32>
      %45 = arith.addi %44, %14 : tensor<128x1xi32>
      %46 = tt.broadcast %45 : tensor<128x1xi32> -> tensor<128x64xi32>
      %47 = arith.addi %46, %19 : tensor<128x64xi32>
      %48 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %49 = tt.addptr %48, %47 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      tt.store %49, %28 : tensor<128x64x!tt.ptr<f8E5M2>>
      %c1_i32_7 = arith.constant 1 : i32
      %50 = arith.muli %c132_i32, %c1_i32_7 : i32
      %51 = arith.addi %arg6, %50 : i32
      %52 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
      %53 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>
      %54 = arith.muli %51, %c8192_i32 : i32
      %55 = tt.expand_dims %53 {axis = 1 : i32} : tensor<128xi32> -> tensor<128x1xi32>
      %56 = arith.muli %55, %cst_2 : tensor<128x1xi32>
      %57 = tt.splat %54 : i32 -> tensor<128x1xi32>
      %58 = arith.addi %57, %56 : tensor<128x1xi32>
      %59 = tt.expand_dims %52 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %60 = tt.broadcast %58 : tensor<128x1xi32> -> tensor<128x64xi32>
      %61 = tt.broadcast %59 : tensor<1x64xi32> -> tensor<128x64xi32>
      %62 = arith.addi %60, %61 : tensor<128x64xi32>
      %63 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %64 = tt.addptr %63, %62 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      %65 = tt.load %64 : tensor<128x64x!tt.ptr<f8E5M2>>
      %66:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c64_i32 iter_args(%arg8 = %cst_4, %arg9 = %cst_3, %arg10 = %cst_5) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %134 = tt.splat %arg7 : i32 -> tensor<64xi32>
        %135 = arith.addi %134, %52 : tensor<64xi32>
        %136 = tt.expand_dims %135 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %137 = arith.muli %136, %cst_1 : tensor<64x1xi32>
        %138 = tt.splat %54 : i32 -> tensor<64x1xi32>
        %139 = arith.addi %138, %137 : tensor<64x1xi32>
        %140 = tt.broadcast %139 : tensor<64x1xi32> -> tensor<64x64xi32>
        %141 = tt.broadcast %59 : tensor<1x64xi32> -> tensor<64x64xi32>
        %142 = arith.addi %140, %141 : tensor<64x64xi32>
        %143 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %144 = tt.addptr %143, %142 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %145 = tt.load %144 : tensor<64x64x!tt.ptr<f8E5M2>>
        %146 = tt.trans %145 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %147 = tt.dot %65, %146, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %148 = arith.mulf %147, %cst_0 : tensor<128x64xf32>
        %149 = "tt.reduce"(%148) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %180 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %180 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %150 = arith.cmpf ogt, %arg10, %149 : tensor<128xf32>
        %151 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %152 = arith.ori %150, %151 : tensor<128xi1>
        %153 = arith.select %152, %arg10, %149 : tensor<128xi1>, tensor<128xf32>
        %154 = tt.expand_dims %153 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %155 = tt.broadcast %154 : tensor<128x1xf32> -> tensor<128x64xf32>
        %156 = arith.subf %148, %155 : tensor<128x64xf32>
        %157 = tt.extern_elementwise %156 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %158 = "tt.reduce"(%157) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %180 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %180 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %159 = arith.subf %arg10, %153 : tensor<128xf32>
        %160 = tt.extern_elementwise %159 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %161 = arith.mulf %arg8, %160 : tensor<128xf32>
        %162 = arith.addf %161, %158 : tensor<128xf32>
        %163 = tt.expand_dims %160 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %164 = tt.broadcast %163 : tensor<128x1xf32> -> tensor<128x64xf32>
        %165 = arith.mulf %arg9, %164 : tensor<128x64xf32>
        %166 = tt.expand_dims %52 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %167 = arith.addi %138, %166 : tensor<64x1xi32>
        %168 = tt.expand_dims %135 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %169 = arith.muli %168, %cst : tensor<1x64xi32>
        %170 = tt.broadcast %167 : tensor<64x1xi32> -> tensor<64x64xi32>
        %171 = tt.broadcast %169 : tensor<1x64xi32> -> tensor<64x64xi32>
        %172 = arith.addi %170, %171 : tensor<64x64xi32>
        %173 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %174 = tt.addptr %173, %172 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %175 = tt.load %174 : tensor<64x64x!tt.ptr<f8E5M2>>
        %176 = tt.fp_to_fp %157, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %177 = tt.trans %175 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %178 = tt.dot %176, %177, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %179 = arith.addf %165, %178 : tensor<128x64xf32>
        scf.yield %162, %179, %153 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.disallow_acc_multi_buffer, tt.flatten, tt.loop_unroll_factor = 1 : i32, tt.num_stages = 4 : i32}
      %67 = tt.expand_dims %66#0 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
      %68 = tt.broadcast %67 : tensor<128x1xf32> -> tensor<128x64xf32>
      %69 = arith.divf %66#1, %68 : tensor<128x64xf32>
      %70 = tt.fp_to_fp %69, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
      %71 = arith.divsi %51, %arg5 : i32
      %72 = arith.remsi %51, %arg5 : i32
      %73 = arith.cmpi ne, %72, %c0_i32 : i32
      %74 = arith.subi %71, %c1_i32 : i32
      %75 = arith.select %73, %74, %71 : i32
      %76 = arith.cmpi slt, %51, %c0_i32 : i32
      %77 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %78 = arith.cmpi ne, %76, %77 : i1
      %79 = arith.select %78, %75, %71 : i32
      %80 = arith.andi %73, %78 : i1
      %81 = arith.addi %72, %arg5 : i32
      %82 = arith.select %80, %81, %72 : i32
      %83 = arith.muli %79, %arg4 : i32
      %84 = arith.muli %82, %c8192_i32 : i32
      %85 = arith.addi %83, %84 : i32
      %86 = tt.splat %85 : i32 -> tensor<128x1xi32>
      %87 = arith.addi %86, %56 : tensor<128x1xi32>
      %88 = tt.broadcast %87 : tensor<128x1xi32> -> tensor<128x64xi32>
      %89 = arith.addi %88, %61 : tensor<128x64xi32>
      %90 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %91 = tt.addptr %90, %89 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      tt.store %91, %70 : tensor<128x64x!tt.ptr<f8E5M2>>
      %c2_i32 = arith.constant 2 : i32
      %92 = arith.muli %c132_i32, %c2_i32 : i32
      %93 = arith.addi %arg6, %92 : i32
      %94 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
      %95 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>
      %96 = arith.muli %93, %c8192_i32 : i32
      %97 = tt.expand_dims %95 {axis = 1 : i32} : tensor<128xi32> -> tensor<128x1xi32>
      %98 = arith.muli %97, %cst_2 : tensor<128x1xi32>
      %99 = tt.splat %96 : i32 -> tensor<128x1xi32>
      %100 = arith.addi %99, %98 : tensor<128x1xi32>
      %101 = tt.expand_dims %94 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %102 = tt.broadcast %100 : tensor<128x1xi32> -> tensor<128x64xi32>
      %103 = tt.broadcast %101 : tensor<1x64xi32> -> tensor<128x64xi32>
      %104 = arith.addi %102, %103 : tensor<128x64xi32>
      %105 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %106 = tt.addptr %105, %104 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      %107 = tt.load %106 : tensor<128x64x!tt.ptr<f8E5M2>>
      %108:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c64_i32 iter_args(%arg8 = %cst_4, %arg9 = %cst_3, %arg10 = %cst_5) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %134 = tt.splat %arg7 : i32 -> tensor<64xi32>
        %135 = arith.addi %134, %94 : tensor<64xi32>
        %136 = tt.expand_dims %135 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %137 = arith.muli %136, %cst_1 : tensor<64x1xi32>
        %138 = tt.splat %96 : i32 -> tensor<64x1xi32>
        %139 = arith.addi %138, %137 : tensor<64x1xi32>
        %140 = tt.broadcast %139 : tensor<64x1xi32> -> tensor<64x64xi32>
        %141 = tt.broadcast %101 : tensor<1x64xi32> -> tensor<64x64xi32>
        %142 = arith.addi %140, %141 : tensor<64x64xi32>
        %143 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %144 = tt.addptr %143, %142 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %145 = tt.load %144 : tensor<64x64x!tt.ptr<f8E5M2>>
        %146 = tt.trans %145 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %147 = tt.dot %107, %146, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %148 = arith.mulf %147, %cst_0 : tensor<128x64xf32>
        %149 = "tt.reduce"(%148) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %180 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %180 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %150 = arith.cmpf ogt, %arg10, %149 : tensor<128xf32>
        %151 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %152 = arith.ori %150, %151 : tensor<128xi1>
        %153 = arith.select %152, %arg10, %149 : tensor<128xi1>, tensor<128xf32>
        %154 = tt.expand_dims %153 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %155 = tt.broadcast %154 : tensor<128x1xf32> -> tensor<128x64xf32>
        %156 = arith.subf %148, %155 : tensor<128x64xf32>
        %157 = tt.extern_elementwise %156 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %158 = "tt.reduce"(%157) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %180 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %180 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %159 = arith.subf %arg10, %153 : tensor<128xf32>
        %160 = tt.extern_elementwise %159 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %161 = arith.mulf %arg8, %160 : tensor<128xf32>
        %162 = arith.addf %161, %158 : tensor<128xf32>
        %163 = tt.expand_dims %160 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %164 = tt.broadcast %163 : tensor<128x1xf32> -> tensor<128x64xf32>
        %165 = arith.mulf %arg9, %164 : tensor<128x64xf32>
        %166 = tt.expand_dims %94 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %167 = arith.addi %138, %166 : tensor<64x1xi32>
        %168 = tt.expand_dims %135 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %169 = arith.muli %168, %cst : tensor<1x64xi32>
        %170 = tt.broadcast %167 : tensor<64x1xi32> -> tensor<64x64xi32>
        %171 = tt.broadcast %169 : tensor<1x64xi32> -> tensor<64x64xi32>
        %172 = arith.addi %170, %171 : tensor<64x64xi32>
        %173 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %174 = tt.addptr %173, %172 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %175 = tt.load %174 : tensor<64x64x!tt.ptr<f8E5M2>>
        %176 = tt.fp_to_fp %157, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %177 = tt.trans %175 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %178 = tt.dot %176, %177, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %179 = arith.addf %165, %178 : tensor<128x64xf32>
        scf.yield %162, %179, %153 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.disallow_acc_multi_buffer, tt.flatten, tt.loop_unroll_factor = 1 : i32, tt.num_stages = 4 : i32}
      %109 = tt.expand_dims %108#0 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
      %110 = tt.broadcast %109 : tensor<128x1xf32> -> tensor<128x64xf32>
      %111 = arith.divf %108#1, %110 : tensor<128x64xf32>
      %112 = tt.fp_to_fp %111, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
      %113 = arith.divsi %93, %arg5 : i32
      %114 = arith.remsi %93, %arg5 : i32
      %115 = arith.cmpi ne, %114, %c0_i32 : i32
      %116 = arith.subi %113, %c1_i32 : i32
      %117 = arith.select %115, %116, %113 : i32
      %118 = arith.cmpi slt, %93, %c0_i32 : i32
      %119 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %120 = arith.cmpi ne, %118, %119 : i1
      %121 = arith.select %120, %117, %113 : i32
      %122 = arith.andi %115, %120 : i1
      %123 = arith.addi %114, %arg5 : i32
      %124 = arith.select %122, %123, %114 : i32
      %125 = arith.muli %121, %arg4 : i32
      %126 = arith.muli %124, %c8192_i32 : i32
      %127 = arith.addi %125, %126 : i32
      %128 = tt.splat %127 : i32 -> tensor<128x1xi32>
      %129 = arith.addi %128, %98 : tensor<128x1xi32>
      %130 = tt.broadcast %129 : tensor<128x1xi32> -> tensor<128x64xi32>
      %131 = arith.addi %130, %103 : tensor<128x64xi32>
      %132 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %133 = tt.addptr %132, %131 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      tt.store %133, %112 : tensor<128x64x!tt.ptr<f8E5M2>>
    } {tt.flatten, tt.num_stages = 3 : i32}
    scf.for %arg6 = %8 to %c192_i32 step %c132_i32  : i32 {
      %10 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
      %11 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>
      %12 = arith.muli %arg6, %c8192_i32 : i32
      %13 = tt.expand_dims %11 {axis = 1 : i32} : tensor<128xi32> -> tensor<128x1xi32>
      %14 = arith.muli %13, %cst_2 : tensor<128x1xi32>
      %15 = tt.splat %12 : i32 -> tensor<128x1xi32>
      %16 = arith.addi %15, %14 : tensor<128x1xi32>
      %17 = tt.expand_dims %10 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %18 = tt.broadcast %16 : tensor<128x1xi32> -> tensor<128x64xi32>
      %19 = tt.broadcast %17 : tensor<1x64xi32> -> tensor<128x64xi32>
      %20 = arith.addi %18, %19 : tensor<128x64xi32>
      %21 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %22 = tt.addptr %21, %20 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      %23 = tt.load %22 : tensor<128x64x!tt.ptr<f8E5M2>>
      %24:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c64_i32 iter_args(%arg8 = %cst_4, %arg9 = %cst_3, %arg10 = %cst_5) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %50 = tt.splat %arg7 : i32 -> tensor<64xi32>
        %51 = arith.addi %50, %10 : tensor<64xi32>
        %52 = tt.expand_dims %51 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %53 = arith.muli %52, %cst_1 : tensor<64x1xi32>
        %54 = tt.splat %12 : i32 -> tensor<64x1xi32>
        %55 = arith.addi %54, %53 : tensor<64x1xi32>
        %56 = tt.broadcast %55 : tensor<64x1xi32> -> tensor<64x64xi32>
        %57 = tt.broadcast %17 : tensor<1x64xi32> -> tensor<64x64xi32>
        %58 = arith.addi %56, %57 : tensor<64x64xi32>
        %59 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %60 = tt.addptr %59, %58 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %61 = tt.load %60 : tensor<64x64x!tt.ptr<f8E5M2>>
        %62 = tt.trans %61 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %63 = tt.dot %23, %62, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %64 = arith.mulf %63, %cst_0 : tensor<128x64xf32>
        %65 = "tt.reduce"(%64) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %96 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %96 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %66 = arith.cmpf ogt, %arg10, %65 : tensor<128xf32>
        %67 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %68 = arith.ori %66, %67 : tensor<128xi1>
        %69 = arith.select %68, %arg10, %65 : tensor<128xi1>, tensor<128xf32>
        %70 = tt.expand_dims %69 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %71 = tt.broadcast %70 : tensor<128x1xf32> -> tensor<128x64xf32>
        %72 = arith.subf %64, %71 : tensor<128x64xf32>
        %73 = tt.extern_elementwise %72 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %74 = "tt.reduce"(%73) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %96 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %96 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %75 = arith.subf %arg10, %69 : tensor<128xf32>
        %76 = tt.extern_elementwise %75 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %77 = arith.mulf %arg8, %76 : tensor<128xf32>
        %78 = arith.addf %77, %74 : tensor<128xf32>
        %79 = tt.expand_dims %76 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %80 = tt.broadcast %79 : tensor<128x1xf32> -> tensor<128x64xf32>
        %81 = arith.mulf %arg9, %80 : tensor<128x64xf32>
        %82 = tt.expand_dims %10 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %83 = arith.addi %54, %82 : tensor<64x1xi32>
        %84 = tt.expand_dims %51 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %85 = arith.muli %84, %cst : tensor<1x64xi32>
        %86 = tt.broadcast %83 : tensor<64x1xi32> -> tensor<64x64xi32>
        %87 = tt.broadcast %85 : tensor<1x64xi32> -> tensor<64x64xi32>
        %88 = arith.addi %86, %87 : tensor<64x64xi32>
        %89 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %90 = tt.addptr %89, %88 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %91 = tt.load %90 : tensor<64x64x!tt.ptr<f8E5M2>>
        %92 = tt.fp_to_fp %73, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %93 = tt.trans %91 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %94 = tt.dot %92, %93, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %95 = arith.addf %81, %94 : tensor<128x64xf32>
        scf.yield %78, %95, %69 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.disallow_acc_multi_buffer, tt.flatten, tt.loop_unroll_factor = 1 : i32, tt.num_stages = 4 : i32}
      %25 = tt.expand_dims %24#0 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
      %26 = tt.broadcast %25 : tensor<128x1xf32> -> tensor<128x64xf32>
      %27 = arith.divf %24#1, %26 : tensor<128x64xf32>
      %28 = tt.fp_to_fp %27, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
      %29 = arith.divsi %arg6, %arg5 : i32
      %30 = arith.remsi %arg6, %arg5 : i32
      %31 = arith.cmpi ne, %30, %c0_i32 : i32
      %32 = arith.subi %29, %c1_i32 : i32
      %33 = arith.select %31, %32, %29 : i32
      %34 = arith.cmpi slt, %arg6, %c0_i32 : i32
      %35 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %36 = arith.cmpi ne, %34, %35 : i1
      %37 = arith.select %36, %33, %29 : i32
      %38 = arith.andi %31, %36 : i1
      %39 = arith.addi %30, %arg5 : i32
      %40 = arith.select %38, %39, %30 : i32
      %41 = arith.muli %37, %arg4 : i32
      %42 = arith.muli %40, %c8192_i32 : i32
      %43 = arith.addi %41, %42 : i32
      %44 = tt.splat %43 : i32 -> tensor<128x1xi32>
      %45 = arith.addi %44, %14 : tensor<128x1xi32>
      %46 = tt.broadcast %45 : tensor<128x1xi32> -> tensor<128x64xi32>
      %47 = arith.addi %46, %19 : tensor<128x64xi32>
      %48 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %49 = tt.addptr %48, %47 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      tt.store %49, %28 : tensor<128x64x!tt.ptr<f8E5M2>>
    } {tt.flatten, tt.num_stages = 1 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(convert-triton-to-tritongpu{enable-source-remat=false num-ctas=1 num-warps=8 target=cuda:90 threads-per-warp=32}, tritongpu-coalesce, tritongpu-F32DotTC, triton-nvidia-gpu-plan-cta, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritongpu-accelerate-matmul, tritongpu-remove-layout-conversions, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, triton-nvidia-optimize-descriptor-encoding, triton-loop-aware-cse, tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-combine-tensor-select-and-if, nvgpu-warp-specialization{dump-intermediate-steps=false num-stages=1}, tritongpu-assign-latencies{num-stages=1}, tritongpu-schedule-loops, tritongpu-pipeline{dump-intermediate-steps=false num-stages=1}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-loop-aware-cse, tritongpu-prefetch, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-coalesce-async-copy, triton-nvidia-optimize-tmem-layouts, tritongpu-remove-layout-conversions, triton-nvidia-interleave-tmem, tritongpu-reduce-data-duplication, tritongpu-reorder-instructions, triton-loop-aware-cse, symbol-dce, triton-nvidia-tma-lowering, triton-nvidia-gpu-fence-insertion{compute-capability=90}, sccp, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true})",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_willfeng/55/c55baynkiz7bp5lfwiq23spqxbftxnsv3gc6psgiupvirynvb7oc.py:13:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_willfeng/55/c55baynkiz7bp5lfwiq23spqxbftxnsv3gc6psgiupvirynvb7oc.py:13:0: note: Pipeline failed while executing [`TritonGPUAssignLatencies` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
python: /root/.triton/llvm/llvm-8957e64a-almalinux-x64/include/llvm/ADT/SmallVector.h:296: const_reference llvm::SmallVectorTemplateCommon<long>::operator[](size_type) const [T = long]: Assertion `idx < size()' failed.
module {
  tt.func public @_fp8_attention_kernel_kernel(%arg0: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %c32_i32 = arith.constant 32 : i32
    %c128_i32 = arith.constant 128 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst = arith.constant dense<64> : tensor<1x32xi32>
    %cst_0 = arith.constant dense<0.180336878> : tensor<128x32xf32>
    %cst_1 = arith.constant dense<0.000000e+00> : tensor<128x32xf32>
    %cst_2 = arith.constant dense<64> : tensor<32x1xi32>
    %cst_3 = arith.constant dense<64> : tensor<128x1xi32>
    %c8192_i32 = arith.constant 8192 : i32
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<128x64xf32>
    %cst_5 = arith.constant dense<0.000000e+00> : tensor<128xf32>
    %cst_6 = arith.constant dense<0xFF800000> : tensor<128xf32>
    %c192_i32 = arith.constant 192 : i32
    %c2_i32 = arith.constant 2 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.muli %0, %c2_i32 : i32
    %2 = arith.addi %1, %c2_i32 : i32
    %3 = arith.minsi %2, %c192_i32 : i32
    %4 = arith.subi %3, %1 : i32
    %c1_i32_7 = arith.constant 1 : i32
    %5 = arith.subi %c1_i32, %c1_i32_7 : i32
    %6 = arith.addi %4, %5 : i32
    %7 = arith.divui %6, %c1_i32 : i32
    %c2_i32_8 = arith.constant 2 : i32
    %8 = arith.remsi %7, %c2_i32_8 : i32
    %9 = arith.subi %7, %8 : i32
    %10 = arith.muli %9, %c1_i32 : i32
    %11 = arith.addi %1, %10 : i32
    %12 = arith.muli %c1_i32, %c2_i32_8 : i32
    scf.for %arg6 = %1 to %11 step %12  : i32 {
      %13 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
      %14 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>
      %15 = arith.muli %arg6, %c8192_i32 : i32
      %16 = tt.expand_dims %14 {axis = 1 : i32} : tensor<128xi32> -> tensor<128x1xi32>
      %17 = arith.muli %16, %cst_3 : tensor<128x1xi32>
      %18 = tt.splat %15 : i32 -> tensor<128x1xi32>
      %19 = arith.addi %18, %17 : tensor<128x1xi32>
      %20 = tt.expand_dims %13 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %21 = tt.broadcast %19 : tensor<128x1xi32> -> tensor<128x64xi32>
      %22 = tt.broadcast %20 : tensor<1x64xi32> -> tensor<128x64xi32>
      %23 = arith.addi %21, %22 : tensor<128x64xi32>
      %24 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %25 = tt.addptr %24, %23 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      %26 = tt.load %25 : tensor<128x64x!tt.ptr<f8E5M2>>
      %27:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c32_i32 iter_args(%arg8 = %cst_5, %arg9 = %cst_4, %arg10 = %cst_6) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %95 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32>
        %96 = tt.splat %arg7 : i32 -> tensor<32xi32>
        %97 = arith.addi %96, %95 : tensor<32xi32>
        %98 = tt.expand_dims %97 {axis = 1 : i32} : tensor<32xi32> -> tensor<32x1xi32>
        %99 = arith.muli %98, %cst_2 : tensor<32x1xi32>
        %100 = tt.splat %15 : i32 -> tensor<32x1xi32>
        %101 = arith.addi %100, %99 : tensor<32x1xi32>
        %102 = tt.broadcast %101 : tensor<32x1xi32> -> tensor<32x64xi32>
        %103 = tt.broadcast %20 : tensor<1x64xi32> -> tensor<32x64xi32>
        %104 = arith.addi %102, %103 : tensor<32x64xi32>
        %105 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<32x64x!tt.ptr<f8E5M2>>
        %106 = tt.addptr %105, %104 : tensor<32x64x!tt.ptr<f8E5M2>>, tensor<32x64xi32>
        %107 = tt.load %106 : tensor<32x64x!tt.ptr<f8E5M2>>
        %108 = tt.trans %107 {order = array<i32: 1, 0>} : tensor<32x64xf8E5M2> -> tensor<64x32xf8E5M2>
        %109 = tt.dot %26, %108, %cst_1, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x32xf8E5M2> -> tensor<128x32xf32>
        %110 = arith.mulf %109, %cst_0 : tensor<128x32xf32>
        %111 = "tt.reduce"(%110) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %143 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %143 : f32
        }) : (tensor<128x32xf32>) -> tensor<128xf32>
        %112 = arith.cmpf ogt, %arg10, %111 : tensor<128xf32>
        %113 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %114 = arith.ori %112, %113 : tensor<128xi1>
        %115 = arith.select %114, %arg10, %111 : tensor<128xi1>, tensor<128xf32>
        %116 = tt.expand_dims %115 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %117 = tt.broadcast %116 : tensor<128x1xf32> -> tensor<128x32xf32>
        %118 = arith.subf %110, %117 : tensor<128x32xf32>
        %119 = tt.extern_elementwise %118 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x32xf32>) -> tensor<128x32xf32>
        %120 = "tt.reduce"(%119) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %143 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %143 : f32
        }) : (tensor<128x32xf32>) -> tensor<128xf32>
        %121 = arith.subf %arg10, %115 : tensor<128xf32>
        %122 = tt.extern_elementwise %121 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %123 = arith.mulf %arg8, %122 : tensor<128xf32>
        %124 = arith.addf %123, %120 : tensor<128xf32>
        %125 = tt.expand_dims %122 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %126 = tt.broadcast %125 : tensor<128x1xf32> -> tensor<128x64xf32>
        %127 = arith.mulf %arg9, %126 : tensor<128x64xf32>
        %128 = tt.expand_dims %13 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %129 = tt.splat %15 : i32 -> tensor<64x1xi32>
        %130 = arith.addi %129, %128 : tensor<64x1xi32>
        %131 = tt.expand_dims %97 {axis = 0 : i32} : tensor<32xi32> -> tensor<1x32xi32>
        %132 = arith.muli %131, %cst : tensor<1x32xi32>
        %133 = tt.broadcast %130 : tensor<64x1xi32> -> tensor<64x32xi32>
        %134 = tt.broadcast %132 : tensor<1x32xi32> -> tensor<64x32xi32>
        %135 = arith.addi %133, %134 : tensor<64x32xi32>
        %136 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x32x!tt.ptr<f8E5M2>>
        %137 = tt.addptr %136, %135 : tensor<64x32x!tt.ptr<f8E5M2>>, tensor<64x32xi32>
        %138 = tt.load %137 : tensor<64x32x!tt.ptr<f8E5M2>>
        %139 = tt.fp_to_fp %119, rounding = rtne : tensor<128x32xf32> -> tensor<128x32xf8E5M2>
        %140 = tt.trans %138 {order = array<i32: 1, 0>} : tensor<64x32xf8E5M2> -> tensor<32x64xf8E5M2>
        %141 = tt.dot %139, %140, %cst_4, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x32xf8E5M2> * tensor<32x64xf8E5M2> -> tensor<128x64xf32>
        %142 = arith.addf %127, %141 : tensor<128x64xf32>
        scf.yield %124, %142, %115 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.flatten, tt.num_stages = 4 : i32}
      %28 = tt.expand_dims %27#0 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
      %29 = tt.broadcast %28 : tensor<128x1xf32> -> tensor<128x64xf32>
      %30 = arith.divf %27#1, %29 : tensor<128x64xf32>
      %31 = tt.fp_to_fp %30, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
      %32 = arith.divsi %arg6, %arg5 : i32
      %33 = arith.remsi %arg6, %arg5 : i32
      %34 = arith.cmpi ne, %33, %c0_i32 : i32
      %35 = arith.subi %32, %c1_i32 : i32
      %36 = arith.select %34, %35, %32 : i32
      %37 = arith.cmpi slt, %arg6, %c0_i32 : i32
      %38 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %39 = arith.cmpi ne, %37, %38 : i1
      %40 = arith.select %39, %36, %32 : i32
      %41 = arith.andi %34, %39 : i1
      %42 = arith.addi %33, %arg5 : i32
      %43 = arith.select %41, %42, %33 : i32
      %44 = arith.muli %40, %arg4 : i32
      %45 = arith.muli %43, %c8192_i32 : i32
      %46 = arith.addi %44, %45 : i32
      %47 = tt.splat %46 : i32 -> tensor<128x1xi32>
      %48 = arith.addi %47, %17 : tensor<128x1xi32>
      %49 = tt.broadcast %48 : tensor<128x1xi32> -> tensor<128x64xi32>
      %50 = arith.addi %49, %22 : tensor<128x64xi32>
      %51 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %52 = tt.addptr %51, %50 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      tt.store %52, %31 : tensor<128x64x!tt.ptr<f8E5M2>>
      %c1_i32_9 = arith.constant 1 : i32
      %53 = arith.muli %c1_i32, %c1_i32_9 : i32
      %54 = arith.addi %arg6, %53 : i32
      %55 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
      %56 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>
      %57 = arith.muli %54, %c8192_i32 : i32
      %58 = tt.expand_dims %56 {axis = 1 : i32} : tensor<128xi32> -> tensor<128x1xi32>
      %59 = arith.muli %58, %cst_3 : tensor<128x1xi32>
      %60 = tt.splat %57 : i32 -> tensor<128x1xi32>
      %61 = arith.addi %60, %59 : tensor<128x1xi32>
      %62 = tt.expand_dims %55 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %63 = tt.broadcast %61 : tensor<128x1xi32> -> tensor<128x64xi32>
      %64 = tt.broadcast %62 : tensor<1x64xi32> -> tensor<128x64xi32>
      %65 = arith.addi %63, %64 : tensor<128x64xi32>
      %66 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %67 = tt.addptr %66, %65 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      %68 = tt.load %67 : tensor<128x64x!tt.ptr<f8E5M2>>
      %69:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c32_i32 iter_args(%arg8 = %cst_5, %arg9 = %cst_4, %arg10 = %cst_6) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %95 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32>
        %96 = tt.splat %arg7 : i32 -> tensor<32xi32>
        %97 = arith.addi %96, %95 : tensor<32xi32>
        %98 = tt.expand_dims %97 {axis = 1 : i32} : tensor<32xi32> -> tensor<32x1xi32>
        %99 = arith.muli %98, %cst_2 : tensor<32x1xi32>
        %100 = tt.splat %57 : i32 -> tensor<32x1xi32>
        %101 = arith.addi %100, %99 : tensor<32x1xi32>
        %102 = tt.broadcast %101 : tensor<32x1xi32> -> tensor<32x64xi32>
        %103 = tt.broadcast %62 : tensor<1x64xi32> -> tensor<32x64xi32>
        %104 = arith.addi %102, %103 : tensor<32x64xi32>
        %105 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<32x64x!tt.ptr<f8E5M2>>
        %106 = tt.addptr %105, %104 : tensor<32x64x!tt.ptr<f8E5M2>>, tensor<32x64xi32>
        %107 = tt.load %106 : tensor<32x64x!tt.ptr<f8E5M2>>
        %108 = tt.trans %107 {order = array<i32: 1, 0>} : tensor<32x64xf8E5M2> -> tensor<64x32xf8E5M2>
        %109 = tt.dot %68, %108, %cst_1, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x32xf8E5M2> -> tensor<128x32xf32>
        %110 = arith.mulf %109, %cst_0 : tensor<128x32xf32>
        %111 = "tt.reduce"(%110) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %143 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %143 : f32
        }) : (tensor<128x32xf32>) -> tensor<128xf32>
        %112 = arith.cmpf ogt, %arg10, %111 : tensor<128xf32>
        %113 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %114 = arith.ori %112, %113 : tensor<128xi1>
        %115 = arith.select %114, %arg10, %111 : tensor<128xi1>, tensor<128xf32>
        %116 = tt.expand_dims %115 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %117 = tt.broadcast %116 : tensor<128x1xf32> -> tensor<128x32xf32>
        %118 = arith.subf %110, %117 : tensor<128x32xf32>
        %119 = tt.extern_elementwise %118 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x32xf32>) -> tensor<128x32xf32>
        %120 = "tt.reduce"(%119) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %143 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %143 : f32
        }) : (tensor<128x32xf32>) -> tensor<128xf32>
        %121 = arith.subf %arg10, %115 : tensor<128xf32>
        %122 = tt.extern_elementwise %121 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %123 = arith.mulf %arg8, %122 : tensor<128xf32>
        %124 = arith.addf %123, %120 : tensor<128xf32>
        %125 = tt.expand_dims %122 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %126 = tt.broadcast %125 : tensor<128x1xf32> -> tensor<128x64xf32>
        %127 = arith.mulf %arg9, %126 : tensor<128x64xf32>
        %128 = tt.expand_dims %55 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %129 = tt.splat %57 : i32 -> tensor<64x1xi32>
        %130 = arith.addi %129, %128 : tensor<64x1xi32>
        %131 = tt.expand_dims %97 {axis = 0 : i32} : tensor<32xi32> -> tensor<1x32xi32>
        %132 = arith.muli %131, %cst : tensor<1x32xi32>
        %133 = tt.broadcast %130 : tensor<64x1xi32> -> tensor<64x32xi32>
        %134 = tt.broadcast %132 : tensor<1x32xi32> -> tensor<64x32xi32>
        %135 = arith.addi %133, %134 : tensor<64x32xi32>
        %136 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x32x!tt.ptr<f8E5M2>>
        %137 = tt.addptr %136, %135 : tensor<64x32x!tt.ptr<f8E5M2>>, tensor<64x32xi32>
        %138 = tt.load %137 : tensor<64x32x!tt.ptr<f8E5M2>>
        %139 = tt.fp_to_fp %119, rounding = rtne : tensor<128x32xf32> -> tensor<128x32xf8E5M2>
        %140 = tt.trans %138 {order = array<i32: 1, 0>} : tensor<64x32xf8E5M2> -> tensor<32x64xf8E5M2>
        %141 = tt.dot %139, %140, %cst_4, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x32xf8E5M2> * tensor<32x64xf8E5M2> -> tensor<128x64xf32>
        %142 = arith.addf %127, %141 : tensor<128x64xf32>
        scf.yield %124, %142, %115 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.flatten, tt.num_stages = 4 : i32}
      %70 = tt.expand_dims %69#0 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
      %71 = tt.broadcast %70 : tensor<128x1xf32> -> tensor<128x64xf32>
      %72 = arith.divf %69#1, %71 : tensor<128x64xf32>
      %73 = tt.fp_to_fp %72, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
      %74 = arith.divsi %54, %arg5 : i32
      %75 = arith.remsi %54, %arg5 : i32
      %76 = arith.cmpi ne, %75, %c0_i32 : i32
      %77 = arith.subi %74, %c1_i32 : i32
      %78 = arith.select %76, %77, %74 : i32
      %79 = arith.cmpi slt, %54, %c0_i32 : i32
      %80 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %81 = arith.cmpi ne, %79, %80 : i1
      %82 = arith.select %81, %78, %74 : i32
      %83 = arith.andi %76, %81 : i1
      %84 = arith.addi %75, %arg5 : i32
      %85 = arith.select %83, %84, %75 : i32
      %86 = arith.muli %82, %arg4 : i32
      %87 = arith.muli %85, %c8192_i32 : i32
      %88 = arith.addi %86, %87 : i32
      %89 = tt.splat %88 : i32 -> tensor<128x1xi32>
      %90 = arith.addi %89, %59 : tensor<128x1xi32>
      %91 = tt.broadcast %90 : tensor<128x1xi32> -> tensor<128x64xi32>
      %92 = arith.addi %91, %64 : tensor<128x64xi32>
      %93 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %94 = tt.addptr %93, %92 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      tt.store %94, %73 : tensor<128x64x!tt.ptr<f8E5M2>>
    } {tt.disallow_acc_multi_buffer, tt.flatten, tt.num_stages = 4 : i32}
    scf.for %arg6 = %11 to %3 step %c1_i32  : i32 {
      %13 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
      %14 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>
      %15 = arith.muli %arg6, %c8192_i32 : i32
      %16 = tt.expand_dims %14 {axis = 1 : i32} : tensor<128xi32> -> tensor<128x1xi32>
      %17 = arith.muli %16, %cst_3 : tensor<128x1xi32>
      %18 = tt.splat %15 : i32 -> tensor<128x1xi32>
      %19 = arith.addi %18, %17 : tensor<128x1xi32>
      %20 = tt.expand_dims %13 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %21 = tt.broadcast %19 : tensor<128x1xi32> -> tensor<128x64xi32>
      %22 = tt.broadcast %20 : tensor<1x64xi32> -> tensor<128x64xi32>
      %23 = arith.addi %21, %22 : tensor<128x64xi32>
      %24 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %25 = tt.addptr %24, %23 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      %26 = tt.load %25 : tensor<128x64x!tt.ptr<f8E5M2>>
      %27:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c32_i32 iter_args(%arg8 = %cst_5, %arg9 = %cst_4, %arg10 = %cst_6) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %53 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32>
        %54 = tt.splat %arg7 : i32 -> tensor<32xi32>
        %55 = arith.addi %54, %53 : tensor<32xi32>
        %56 = tt.expand_dims %55 {axis = 1 : i32} : tensor<32xi32> -> tensor<32x1xi32>
        %57 = arith.muli %56, %cst_2 : tensor<32x1xi32>
        %58 = tt.splat %15 : i32 -> tensor<32x1xi32>
        %59 = arith.addi %58, %57 : tensor<32x1xi32>
        %60 = tt.broadcast %59 : tensor<32x1xi32> -> tensor<32x64xi32>
        %61 = tt.broadcast %20 : tensor<1x64xi32> -> tensor<32x64xi32>
        %62 = arith.addi %60, %61 : tensor<32x64xi32>
        %63 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<32x64x!tt.ptr<f8E5M2>>
        %64 = tt.addptr %63, %62 : tensor<32x64x!tt.ptr<f8E5M2>>, tensor<32x64xi32>
        %65 = tt.load %64 : tensor<32x64x!tt.ptr<f8E5M2>>
        %66 = tt.trans %65 {order = array<i32: 1, 0>} : tensor<32x64xf8E5M2> -> tensor<64x32xf8E5M2>
        %67 = tt.dot %26, %66, %cst_1, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x32xf8E5M2> -> tensor<128x32xf32>
        %68 = arith.mulf %67, %cst_0 : tensor<128x32xf32>
        %69 = "tt.reduce"(%68) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %101 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %101 : f32
        }) : (tensor<128x32xf32>) -> tensor<128xf32>
        %70 = arith.cmpf ogt, %arg10, %69 : tensor<128xf32>
        %71 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %72 = arith.ori %70, %71 : tensor<128xi1>
        %73 = arith.select %72, %arg10, %69 : tensor<128xi1>, tensor<128xf32>
        %74 = tt.expand_dims %73 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %75 = tt.broadcast %74 : tensor<128x1xf32> -> tensor<128x32xf32>
        %76 = arith.subf %68, %75 : tensor<128x32xf32>
        %77 = tt.extern_elementwise %76 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x32xf32>) -> tensor<128x32xf32>
        %78 = "tt.reduce"(%77) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %101 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %101 : f32
        }) : (tensor<128x32xf32>) -> tensor<128xf32>
        %79 = arith.subf %arg10, %73 : tensor<128xf32>
        %80 = tt.extern_elementwise %79 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %81 = arith.mulf %arg8, %80 : tensor<128xf32>
        %82 = arith.addf %81, %78 : tensor<128xf32>
        %83 = tt.expand_dims %80 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %84 = tt.broadcast %83 : tensor<128x1xf32> -> tensor<128x64xf32>
        %85 = arith.mulf %arg9, %84 : tensor<128x64xf32>
        %86 = tt.expand_dims %13 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %87 = tt.splat %15 : i32 -> tensor<64x1xi32>
        %88 = arith.addi %87, %86 : tensor<64x1xi32>
        %89 = tt.expand_dims %55 {axis = 0 : i32} : tensor<32xi32> -> tensor<1x32xi32>
        %90 = arith.muli %89, %cst : tensor<1x32xi32>
        %91 = tt.broadcast %88 : tensor<64x1xi32> -> tensor<64x32xi32>
        %92 = tt.broadcast %90 : tensor<1x32xi32> -> tensor<64x32xi32>
        %93 = arith.addi %91, %92 : tensor<64x32xi32>
        %94 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x32x!tt.ptr<f8E5M2>>
        %95 = tt.addptr %94, %93 : tensor<64x32x!tt.ptr<f8E5M2>>, tensor<64x32xi32>
        %96 = tt.load %95 : tensor<64x32x!tt.ptr<f8E5M2>>
        %97 = tt.fp_to_fp %77, rounding = rtne : tensor<128x32xf32> -> tensor<128x32xf8E5M2>
        %98 = tt.trans %96 {order = array<i32: 1, 0>} : tensor<64x32xf8E5M2> -> tensor<32x64xf8E5M2>
        %99 = tt.dot %97, %98, %cst_4, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x32xf8E5M2> * tensor<32x64xf8E5M2> -> tensor<128x64xf32>
        %100 = arith.addf %85, %99 : tensor<128x64xf32>
        scf.yield %82, %100, %73 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.flatten, tt.num_stages = 4 : i32}
      %28 = tt.expand_dims %27#0 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
      %29 = tt.broadcast %28 : tensor<128x1xf32> -> tensor<128x64xf32>
      %30 = arith.divf %27#1, %29 : tensor<128x64xf32>
      %31 = tt.fp_to_fp %30, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
      %32 = arith.divsi %arg6, %arg5 : i32
      %33 = arith.remsi %arg6, %arg5 : i32
      %34 = arith.cmpi ne, %33, %c0_i32 : i32
      %35 = arith.subi %32, %c1_i32 : i32
      %36 = arith.select %34, %35, %32 : i32
      %37 = arith.cmpi slt, %arg6, %c0_i32 : i32
      %38 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %39 = arith.cmpi ne, %37, %38 : i1
      %40 = arith.select %39, %36, %32 : i32
      %41 = arith.andi %34, %39 : i1
      %42 = arith.addi %33, %arg5 : i32
      %43 = arith.select %41, %42, %33 : i32
      %44 = arith.muli %40, %arg4 : i32
      %45 = arith.muli %43, %c8192_i32 : i32
      %46 = arith.addi %44, %45 : i32
      %47 = tt.splat %46 : i32 -> tensor<128x1xi32>
      %48 = arith.addi %47, %17 : tensor<128x1xi32>
      %49 = tt.broadcast %48 : tensor<128x1xi32> -> tensor<128x64xi32>
      %50 = arith.addi %49, %22 : tensor<128x64xi32>
      %51 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %52 = tt.addptr %51, %50 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      tt.store %52, %31 : tensor<128x64x!tt.ptr<f8E5M2>>
    } {tt.disallow_acc_multi_buffer, tt.flatten, tt.num_stages = 1 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(convert-triton-to-tritongpu{enable-source-remat=false num-ctas=1 num-warps=4 target=cuda:90 threads-per-warp=32}, tritongpu-coalesce, tritongpu-F32DotTC, triton-nvidia-gpu-plan-cta, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritongpu-accelerate-matmul, tritongpu-remove-layout-conversions, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, triton-nvidia-optimize-descriptor-encoding, triton-loop-aware-cse, tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-combine-tensor-select-and-if, nvgpu-warp-specialization{dump-intermediate-steps=false num-stages=3}, tritongpu-assign-latencies{num-stages=3}, tritongpu-schedule-loops, tritongpu-pipeline{dump-intermediate-steps=false num-stages=3}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-loop-aware-cse, tritongpu-prefetch, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-coalesce-async-copy, triton-nvidia-optimize-tmem-layouts, tritongpu-remove-layout-conversions, triton-nvidia-interleave-tmem, tritongpu-reduce-data-duplication, tritongpu-reorder-instructions, triton-loop-aware-cse, symbol-dce, triton-nvidia-tma-lowering, triton-nvidia-gpu-fence-insertion{compute-capability=90}, sccp, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true})",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_willfeng/ec/ceczrkdlipplycuvbsjjyn5wi2p5yn2piy4e454s23ujbswfibqp.py:13:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_willfeng/ec/ceczrkdlipplycuvbsjjyn5wi2p5yn2piy4e454s23ujbswfibqp.py:13:0: note: Pipeline failed while executing [`TritonGPUAssignLatencies` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
Process ForkProcess-395:
Traceback (most recent call last):
  File "/home/willfeng/local/miniconda3/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/willfeng/local/miniconda3/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/data/users/willfeng/helion/helion/runtime/precompile_shim.py", line 54, in finish_it
    kernel_cache[key] = fn.compile(
                        ^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/compiler/compiler.py", line 359, in compile
    next_module = compile_ir(module, metadata)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/backends/nvidia/compiler.py", line 456, in <lambda>
    stages["ttgir"] = lambda src, metadata: self.make_ttgir(src, metadata, options, capability)
                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/backends/nvidia/compiler.py", line 298, in make_ttgir
    pm.run(mod)
RuntimeError: PassManager::run failed
python: /root/.triton/llvm/llvm-8957e64a-almalinux-x64/include/llvm/ADT/SmallVector.h:296: const_reference llvm::SmallVectorTemplateCommon<long>::operator[](size_type) const [T = long]: Assertion `idx < size()' failed.
module {
  tt.func public @_fp8_attention_kernel_kernel(%arg0: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %c32_i32 = arith.constant 32 : i32
    %c128_i32 = arith.constant 128 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst = arith.constant dense<64> : tensor<1x32xi32>
    %cst_0 = arith.constant dense<0.180336878> : tensor<128x32xf32>
    %cst_1 = arith.constant dense<0.000000e+00> : tensor<128x32xf32>
    %cst_2 = arith.constant dense<64> : tensor<32x1xi32>
    %cst_3 = arith.constant dense<64> : tensor<128x1xi32>
    %c8192_i32 = arith.constant 8192 : i32
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<128x64xf32>
    %cst_5 = arith.constant dense<0.000000e+00> : tensor<128xf32>
    %cst_6 = arith.constant dense<0xFF800000> : tensor<128xf32>
    %c192_i32 = arith.constant 192 : i32
    %c2_i32 = arith.constant 2 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.muli %0, %c2_i32 : i32
    %2 = arith.addi %1, %c2_i32 : i32
    %3 = arith.minsi %2, %c192_i32 : i32
    %4 = arith.subi %3, %1 : i32
    %c1_i32_7 = arith.constant 1 : i32
    %5 = arith.subi %c1_i32, %c1_i32_7 : i32
    %6 = arith.addi %4, %5 : i32
    %7 = arith.divui %6, %c1_i32 : i32
    %c2_i32_8 = arith.constant 2 : i32
    %8 = arith.remsi %7, %c2_i32_8 : i32
    %9 = arith.subi %7, %8 : i32
    %10 = arith.muli %9, %c1_i32 : i32
    %11 = arith.addi %1, %10 : i32
    %12 = arith.muli %c1_i32, %c2_i32_8 : i32
    scf.for %arg6 = %1 to %11 step %12  : i32 {
      %13 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
      %14 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>
      %15 = arith.muli %arg6, %c8192_i32 : i32
      %16 = tt.expand_dims %14 {axis = 1 : i32} : tensor<128xi32> -> tensor<128x1xi32>
      %17 = arith.muli %16, %cst_3 : tensor<128x1xi32>
      %18 = tt.splat %15 : i32 -> tensor<128x1xi32>
      %19 = arith.addi %18, %17 : tensor<128x1xi32>
      %20 = tt.expand_dims %13 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %21 = tt.broadcast %19 : tensor<128x1xi32> -> tensor<128x64xi32>
      %22 = tt.broadcast %20 : tensor<1x64xi32> -> tensor<128x64xi32>
      %23 = arith.addi %21, %22 : tensor<128x64xi32>
      %24 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %25 = tt.addptr %24, %23 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      %26 = tt.load %25 : tensor<128x64x!tt.ptr<f8E5M2>>
      %27:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c32_i32 iter_args(%arg8 = %cst_5, %arg9 = %cst_4, %arg10 = %cst_6) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %95 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32>
        %96 = tt.splat %arg7 : i32 -> tensor<32xi32>
        %97 = arith.addi %96, %95 : tensor<32xi32>
        %98 = tt.expand_dims %97 {axis = 1 : i32} : tensor<32xi32> -> tensor<32x1xi32>
        %99 = arith.muli %98, %cst_2 : tensor<32x1xi32>
        %100 = tt.splat %15 : i32 -> tensor<32x1xi32>
        %101 = arith.addi %100, %99 : tensor<32x1xi32>
        %102 = tt.broadcast %101 : tensor<32x1xi32> -> tensor<32x64xi32>
        %103 = tt.broadcast %20 : tensor<1x64xi32> -> tensor<32x64xi32>
        %104 = arith.addi %102, %103 : tensor<32x64xi32>
        %105 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<32x64x!tt.ptr<f8E5M2>>
        %106 = tt.addptr %105, %104 : tensor<32x64x!tt.ptr<f8E5M2>>, tensor<32x64xi32>
        %107 = tt.load %106 : tensor<32x64x!tt.ptr<f8E5M2>>
        %108 = tt.trans %107 {order = array<i32: 1, 0>} : tensor<32x64xf8E5M2> -> tensor<64x32xf8E5M2>
        %109 = tt.dot %26, %108, %cst_1, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x32xf8E5M2> -> tensor<128x32xf32>
        %110 = arith.mulf %109, %cst_0 : tensor<128x32xf32>
        %111 = "tt.reduce"(%110) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %143 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %143 : f32
        }) : (tensor<128x32xf32>) -> tensor<128xf32>
        %112 = arith.cmpf ogt, %arg10, %111 : tensor<128xf32>
        %113 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %114 = arith.ori %112, %113 : tensor<128xi1>
        %115 = arith.select %114, %arg10, %111 : tensor<128xi1>, tensor<128xf32>
        %116 = tt.expand_dims %115 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %117 = tt.broadcast %116 : tensor<128x1xf32> -> tensor<128x32xf32>
        %118 = arith.subf %110, %117 : tensor<128x32xf32>
        %119 = tt.extern_elementwise %118 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x32xf32>) -> tensor<128x32xf32>
        %120 = "tt.reduce"(%119) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %143 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %143 : f32
        }) : (tensor<128x32xf32>) -> tensor<128xf32>
        %121 = arith.subf %arg10, %115 : tensor<128xf32>
        %122 = tt.extern_elementwise %121 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %123 = arith.mulf %arg8, %122 : tensor<128xf32>
        %124 = arith.addf %123, %120 : tensor<128xf32>
        %125 = tt.expand_dims %122 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %126 = tt.broadcast %125 : tensor<128x1xf32> -> tensor<128x64xf32>
        %127 = arith.mulf %arg9, %126 : tensor<128x64xf32>
        %128 = tt.expand_dims %13 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %129 = tt.splat %15 : i32 -> tensor<64x1xi32>
        %130 = arith.addi %129, %128 : tensor<64x1xi32>
        %131 = tt.expand_dims %97 {axis = 0 : i32} : tensor<32xi32> -> tensor<1x32xi32>
        %132 = arith.muli %131, %cst : tensor<1x32xi32>
        %133 = tt.broadcast %130 : tensor<64x1xi32> -> tensor<64x32xi32>
        %134 = tt.broadcast %132 : tensor<1x32xi32> -> tensor<64x32xi32>
        %135 = arith.addi %133, %134 : tensor<64x32xi32>
        %136 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x32x!tt.ptr<f8E5M2>>
        %137 = tt.addptr %136, %135 : tensor<64x32x!tt.ptr<f8E5M2>>, tensor<64x32xi32>
        %138 = tt.load %137 : tensor<64x32x!tt.ptr<f8E5M2>>
        %139 = tt.fp_to_fp %119, rounding = rtne : tensor<128x32xf32> -> tensor<128x32xf8E5M2>
        %140 = tt.trans %138 {order = array<i32: 1, 0>} : tensor<64x32xf8E5M2> -> tensor<32x64xf8E5M2>
        %141 = tt.dot %139, %140, %cst_4, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x32xf8E5M2> * tensor<32x64xf8E5M2> -> tensor<128x64xf32>
        %142 = arith.addf %127, %141 : tensor<128x64xf32>
        scf.yield %124, %142, %115 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.flatten, tt.num_stages = 4 : i32}
      %28 = tt.expand_dims %27#0 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
      %29 = tt.broadcast %28 : tensor<128x1xf32> -> tensor<128x64xf32>
      %30 = arith.divf %27#1, %29 : tensor<128x64xf32>
      %31 = tt.fp_to_fp %30, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
      %32 = arith.divsi %arg6, %arg5 : i32
      %33 = arith.remsi %arg6, %arg5 : i32
      %34 = arith.cmpi ne, %33, %c0_i32 : i32
      %35 = arith.subi %32, %c1_i32 : i32
      %36 = arith.select %34, %35, %32 : i32
      %37 = arith.cmpi slt, %arg6, %c0_i32 : i32
      %38 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %39 = arith.cmpi ne, %37, %38 : i1
      %40 = arith.select %39, %36, %32 : i32
      %41 = arith.andi %34, %39 : i1
      %42 = arith.addi %33, %arg5 : i32
      %43 = arith.select %41, %42, %33 : i32
      %44 = arith.muli %40, %arg4 : i32
      %45 = arith.muli %43, %c8192_i32 : i32
      %46 = arith.addi %44, %45 : i32
      %47 = tt.splat %46 : i32 -> tensor<128x1xi32>
      %48 = arith.addi %47, %17 : tensor<128x1xi32>
      %49 = tt.broadcast %48 : tensor<128x1xi32> -> tensor<128x64xi32>
      %50 = arith.addi %49, %22 : tensor<128x64xi32>
      %51 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %52 = tt.addptr %51, %50 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      tt.store %52, %31 : tensor<128x64x!tt.ptr<f8E5M2>>
      %c1_i32_9 = arith.constant 1 : i32
      %53 = arith.muli %c1_i32, %c1_i32_9 : i32
      %54 = arith.addi %arg6, %53 : i32
      %55 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
      %56 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>
      %57 = arith.muli %54, %c8192_i32 : i32
      %58 = tt.expand_dims %56 {axis = 1 : i32} : tensor<128xi32> -> tensor<128x1xi32>
      %59 = arith.muli %58, %cst_3 : tensor<128x1xi32>
      %60 = tt.splat %57 : i32 -> tensor<128x1xi32>
      %61 = arith.addi %60, %59 : tensor<128x1xi32>
      %62 = tt.expand_dims %55 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %63 = tt.broadcast %61 : tensor<128x1xi32> -> tensor<128x64xi32>
      %64 = tt.broadcast %62 : tensor<1x64xi32> -> tensor<128x64xi32>
      %65 = arith.addi %63, %64 : tensor<128x64xi32>
      %66 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %67 = tt.addptr %66, %65 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      %68 = tt.load %67 : tensor<128x64x!tt.ptr<f8E5M2>>
      %69:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c32_i32 iter_args(%arg8 = %cst_5, %arg9 = %cst_4, %arg10 = %cst_6) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %95 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32>
        %96 = tt.splat %arg7 : i32 -> tensor<32xi32>
        %97 = arith.addi %96, %95 : tensor<32xi32>
        %98 = tt.expand_dims %97 {axis = 1 : i32} : tensor<32xi32> -> tensor<32x1xi32>
        %99 = arith.muli %98, %cst_2 : tensor<32x1xi32>
        %100 = tt.splat %57 : i32 -> tensor<32x1xi32>
        %101 = arith.addi %100, %99 : tensor<32x1xi32>
        %102 = tt.broadcast %101 : tensor<32x1xi32> -> tensor<32x64xi32>
        %103 = tt.broadcast %62 : tensor<1x64xi32> -> tensor<32x64xi32>
        %104 = arith.addi %102, %103 : tensor<32x64xi32>
        %105 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<32x64x!tt.ptr<f8E5M2>>
        %106 = tt.addptr %105, %104 : tensor<32x64x!tt.ptr<f8E5M2>>, tensor<32x64xi32>
        %107 = tt.load %106 : tensor<32x64x!tt.ptr<f8E5M2>>
        %108 = tt.trans %107 {order = array<i32: 1, 0>} : tensor<32x64xf8E5M2> -> tensor<64x32xf8E5M2>
        %109 = tt.dot %68, %108, %cst_1, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x32xf8E5M2> -> tensor<128x32xf32>
        %110 = arith.mulf %109, %cst_0 : tensor<128x32xf32>
        %111 = "tt.reduce"(%110) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %143 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %143 : f32
        }) : (tensor<128x32xf32>) -> tensor<128xf32>
        %112 = arith.cmpf ogt, %arg10, %111 : tensor<128xf32>
        %113 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %114 = arith.ori %112, %113 : tensor<128xi1>
        %115 = arith.select %114, %arg10, %111 : tensor<128xi1>, tensor<128xf32>
        %116 = tt.expand_dims %115 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %117 = tt.broadcast %116 : tensor<128x1xf32> -> tensor<128x32xf32>
        %118 = arith.subf %110, %117 : tensor<128x32xf32>
        %119 = tt.extern_elementwise %118 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x32xf32>) -> tensor<128x32xf32>
        %120 = "tt.reduce"(%119) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %143 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %143 : f32
        }) : (tensor<128x32xf32>) -> tensor<128xf32>
        %121 = arith.subf %arg10, %115 : tensor<128xf32>
        %122 = tt.extern_elementwise %121 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %123 = arith.mulf %arg8, %122 : tensor<128xf32>
        %124 = arith.addf %123, %120 : tensor<128xf32>
        %125 = tt.expand_dims %122 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %126 = tt.broadcast %125 : tensor<128x1xf32> -> tensor<128x64xf32>
        %127 = arith.mulf %arg9, %126 : tensor<128x64xf32>
        %128 = tt.expand_dims %55 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %129 = tt.splat %57 : i32 -> tensor<64x1xi32>
        %130 = arith.addi %129, %128 : tensor<64x1xi32>
        %131 = tt.expand_dims %97 {axis = 0 : i32} : tensor<32xi32> -> tensor<1x32xi32>
        %132 = arith.muli %131, %cst : tensor<1x32xi32>
        %133 = tt.broadcast %130 : tensor<64x1xi32> -> tensor<64x32xi32>
        %134 = tt.broadcast %132 : tensor<1x32xi32> -> tensor<64x32xi32>
        %135 = arith.addi %133, %134 : tensor<64x32xi32>
        %136 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x32x!tt.ptr<f8E5M2>>
        %137 = tt.addptr %136, %135 : tensor<64x32x!tt.ptr<f8E5M2>>, tensor<64x32xi32>
        %138 = tt.load %137 : tensor<64x32x!tt.ptr<f8E5M2>>
        %139 = tt.fp_to_fp %119, rounding = rtne : tensor<128x32xf32> -> tensor<128x32xf8E5M2>
        %140 = tt.trans %138 {order = array<i32: 1, 0>} : tensor<64x32xf8E5M2> -> tensor<32x64xf8E5M2>
        %141 = tt.dot %139, %140, %cst_4, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x32xf8E5M2> * tensor<32x64xf8E5M2> -> tensor<128x64xf32>
        %142 = arith.addf %127, %141 : tensor<128x64xf32>
        scf.yield %124, %142, %115 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.flatten, tt.num_stages = 4 : i32}
      %70 = tt.expand_dims %69#0 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
      %71 = tt.broadcast %70 : tensor<128x1xf32> -> tensor<128x64xf32>
      %72 = arith.divf %69#1, %71 : tensor<128x64xf32>
      %73 = tt.fp_to_fp %72, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
      %74 = arith.divsi %54, %arg5 : i32
      %75 = arith.remsi %54, %arg5 : i32
      %76 = arith.cmpi ne, %75, %c0_i32 : i32
      %77 = arith.subi %74, %c1_i32 : i32
      %78 = arith.select %76, %77, %74 : i32
      %79 = arith.cmpi slt, %54, %c0_i32 : i32
      %80 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %81 = arith.cmpi ne, %79, %80 : i1
      %82 = arith.select %81, %78, %74 : i32
      %83 = arith.andi %76, %81 : i1
      %84 = arith.addi %75, %arg5 : i32
      %85 = arith.select %83, %84, %75 : i32
      %86 = arith.muli %82, %arg4 : i32
      %87 = arith.muli %85, %c8192_i32 : i32
      %88 = arith.addi %86, %87 : i32
      %89 = tt.splat %88 : i32 -> tensor<128x1xi32>
      %90 = arith.addi %89, %59 : tensor<128x1xi32>
      %91 = tt.broadcast %90 : tensor<128x1xi32> -> tensor<128x64xi32>
      %92 = arith.addi %91, %64 : tensor<128x64xi32>
      %93 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %94 = tt.addptr %93, %92 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      tt.store %94, %73 : tensor<128x64x!tt.ptr<f8E5M2>>
    } {tt.disallow_acc_multi_buffer, tt.flatten, tt.num_stages = 4 : i32}
    scf.for %arg6 = %11 to %3 step %c1_i32  : i32 {
      %13 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
      %14 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>
      %15 = arith.muli %arg6, %c8192_i32 : i32
      %16 = tt.expand_dims %14 {axis = 1 : i32} : tensor<128xi32> -> tensor<128x1xi32>
      %17 = arith.muli %16, %cst_3 : tensor<128x1xi32>
      %18 = tt.splat %15 : i32 -> tensor<128x1xi32>
      %19 = arith.addi %18, %17 : tensor<128x1xi32>
      %20 = tt.expand_dims %13 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %21 = tt.broadcast %19 : tensor<128x1xi32> -> tensor<128x64xi32>
      %22 = tt.broadcast %20 : tensor<1x64xi32> -> tensor<128x64xi32>
      %23 = arith.addi %21, %22 : tensor<128x64xi32>
      %24 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %25 = tt.addptr %24, %23 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      %26 = tt.load %25 : tensor<128x64x!tt.ptr<f8E5M2>>
      %27:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c32_i32 iter_args(%arg8 = %cst_5, %arg9 = %cst_4, %arg10 = %cst_6) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %53 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32>
        %54 = tt.splat %arg7 : i32 -> tensor<32xi32>
        %55 = arith.addi %54, %53 : tensor<32xi32>
        %56 = tt.expand_dims %55 {axis = 1 : i32} : tensor<32xi32> -> tensor<32x1xi32>
        %57 = arith.muli %56, %cst_2 : tensor<32x1xi32>
        %58 = tt.splat %15 : i32 -> tensor<32x1xi32>
        %59 = arith.addi %58, %57 : tensor<32x1xi32>
        %60 = tt.broadcast %59 : tensor<32x1xi32> -> tensor<32x64xi32>
        %61 = tt.broadcast %20 : tensor<1x64xi32> -> tensor<32x64xi32>
        %62 = arith.addi %60, %61 : tensor<32x64xi32>
        %63 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<32x64x!tt.ptr<f8E5M2>>
        %64 = tt.addptr %63, %62 : tensor<32x64x!tt.ptr<f8E5M2>>, tensor<32x64xi32>
        %65 = tt.load %64 : tensor<32x64x!tt.ptr<f8E5M2>>
        %66 = tt.trans %65 {order = array<i32: 1, 0>} : tensor<32x64xf8E5M2> -> tensor<64x32xf8E5M2>
        %67 = tt.dot %26, %66, %cst_1, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x32xf8E5M2> -> tensor<128x32xf32>
        %68 = arith.mulf %67, %cst_0 : tensor<128x32xf32>
        %69 = "tt.reduce"(%68) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %101 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %101 : f32
        }) : (tensor<128x32xf32>) -> tensor<128xf32>
        %70 = arith.cmpf ogt, %arg10, %69 : tensor<128xf32>
        %71 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %72 = arith.ori %70, %71 : tensor<128xi1>
        %73 = arith.select %72, %arg10, %69 : tensor<128xi1>, tensor<128xf32>
        %74 = tt.expand_dims %73 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %75 = tt.broadcast %74 : tensor<128x1xf32> -> tensor<128x32xf32>
        %76 = arith.subf %68, %75 : tensor<128x32xf32>
        %77 = tt.extern_elementwise %76 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x32xf32>) -> tensor<128x32xf32>
        %78 = "tt.reduce"(%77) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %101 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %101 : f32
        }) : (tensor<128x32xf32>) -> tensor<128xf32>
        %79 = arith.subf %arg10, %73 : tensor<128xf32>
        %80 = tt.extern_elementwise %79 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %81 = arith.mulf %arg8, %80 : tensor<128xf32>
        %82 = arith.addf %81, %78 : tensor<128xf32>
        %83 = tt.expand_dims %80 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %84 = tt.broadcast %83 : tensor<128x1xf32> -> tensor<128x64xf32>
        %85 = arith.mulf %arg9, %84 : tensor<128x64xf32>
        %86 = tt.expand_dims %13 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %87 = tt.splat %15 : i32 -> tensor<64x1xi32>
        %88 = arith.addi %87, %86 : tensor<64x1xi32>
        %89 = tt.expand_dims %55 {axis = 0 : i32} : tensor<32xi32> -> tensor<1x32xi32>
        %90 = arith.muli %89, %cst : tensor<1x32xi32>
        %91 = tt.broadcast %88 : tensor<64x1xi32> -> tensor<64x32xi32>
        %92 = tt.broadcast %90 : tensor<1x32xi32> -> tensor<64x32xi32>
        %93 = arith.addi %91, %92 : tensor<64x32xi32>
        %94 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x32x!tt.ptr<f8E5M2>>
        %95 = tt.addptr %94, %93 : tensor<64x32x!tt.ptr<f8E5M2>>, tensor<64x32xi32>
        %96 = tt.load %95 : tensor<64x32x!tt.ptr<f8E5M2>>
        %97 = tt.fp_to_fp %77, rounding = rtne : tensor<128x32xf32> -> tensor<128x32xf8E5M2>
        %98 = tt.trans %96 {order = array<i32: 1, 0>} : tensor<64x32xf8E5M2> -> tensor<32x64xf8E5M2>
        %99 = tt.dot %97, %98, %cst_4, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x32xf8E5M2> * tensor<32x64xf8E5M2> -> tensor<128x64xf32>
        %100 = arith.addf %85, %99 : tensor<128x64xf32>
        scf.yield %82, %100, %73 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.flatten, tt.num_stages = 4 : i32}
      %28 = tt.expand_dims %27#0 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
      %29 = tt.broadcast %28 : tensor<128x1xf32> -> tensor<128x64xf32>
      %30 = arith.divf %27#1, %29 : tensor<128x64xf32>
      %31 = tt.fp_to_fp %30, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
      %32 = arith.divsi %arg6, %arg5 : i32
      %33 = arith.remsi %arg6, %arg5 : i32
      %34 = arith.cmpi ne, %33, %c0_i32 : i32
      %35 = arith.subi %32, %c1_i32 : i32
      %36 = arith.select %34, %35, %32 : i32
      %37 = arith.cmpi slt, %arg6, %c0_i32 : i32
      %38 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %39 = arith.cmpi ne, %37, %38 : i1
      %40 = arith.select %39, %36, %32 : i32
      %41 = arith.andi %34, %39 : i1
      %42 = arith.addi %33, %arg5 : i32
      %43 = arith.select %41, %42, %33 : i32
      %44 = arith.muli %40, %arg4 : i32
      %45 = arith.muli %43, %c8192_i32 : i32
      %46 = arith.addi %44, %45 : i32
      %47 = tt.splat %46 : i32 -> tensor<128x1xi32>
      %48 = arith.addi %47, %17 : tensor<128x1xi32>
      %49 = tt.broadcast %48 : tensor<128x1xi32> -> tensor<128x64xi32>
      %50 = arith.addi %49, %22 : tensor<128x64xi32>
      %51 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %52 = tt.addptr %51, %50 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      tt.store %52, %31 : tensor<128x64x!tt.ptr<f8E5M2>>
    } {tt.disallow_acc_multi_buffer, tt.flatten, tt.num_stages = 1 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(convert-triton-to-tritongpu{enable-source-remat=false num-ctas=1 num-warps=4 target=cuda:90 threads-per-warp=32}, tritongpu-coalesce, tritongpu-F32DotTC, triton-nvidia-gpu-plan-cta, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritongpu-accelerate-matmul, tritongpu-remove-layout-conversions, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, triton-nvidia-optimize-descriptor-encoding, triton-loop-aware-cse, tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-combine-tensor-select-and-if, nvgpu-warp-specialization{dump-intermediate-steps=false num-stages=3}, tritongpu-assign-latencies{num-stages=3}, tritongpu-schedule-loops, tritongpu-pipeline{dump-intermediate-steps=false num-stages=3}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-loop-aware-cse, tritongpu-prefetch, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-coalesce-async-copy, triton-nvidia-optimize-tmem-layouts, tritongpu-remove-layout-conversions, triton-nvidia-interleave-tmem, tritongpu-reduce-data-duplication, tritongpu-reorder-instructions, triton-loop-aware-cse, symbol-dce, triton-nvidia-tma-lowering, triton-nvidia-gpu-fence-insertion{compute-capability=90}, sccp, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true})",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_willfeng/ec/ceczrkdlipplycuvbsjjyn5wi2p5yn2piy4e454s23ujbswfibqp.py:13:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_willfeng/ec/ceczrkdlipplycuvbsjjyn5wi2p5yn2piy4e454s23ujbswfibqp.py:13:0: note: Pipeline failed while executing [`TritonGPUAssignLatencies` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
[607s] Generation 5: replaced=10 min=0.0115 mid=0.0161 max=0.0415 best=Config(block_sizes=[128, 64], range_unroll_factors=[0, 0, 1], range_num_stages=[0, 4, 4], range_multi_buffers=[None, None, False], range_flattens=[None, False, False], num_warps=8, num_stages=5, indexing='pointer', pid_type='flat', range_warp_specializes=[])
python: /root/.triton/llvm/llvm-8957e64a-almalinux-x64/include/llvm/ADT/SmallVector.h:296: const_reference llvm::SmallVectorTemplateCommon<long>::operator[](size_type) const [T = long]: Assertion `idx < size()' failed.
module {
  tt.func public @_fp8_attention_kernel_kernel(%arg0: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %c1_i32 = arith.constant 1 : i32
    %c32_i32 = arith.constant 32 : i32
    %c64_i32 = arith.constant 64 : i32
    %c128_i32 = arith.constant 128 : i32
    %c0_i32 = arith.constant 0 : i32
    %cst = arith.constant dense<64> : tensor<1x32xi32>
    %cst_0 = arith.constant dense<0.180336878> : tensor<64x32xf32>
    %cst_1 = arith.constant dense<0.000000e+00> : tensor<64x32xf32>
    %cst_2 = arith.constant dense<64> : tensor<32x1xi32>
    %cst_3 = arith.constant dense<64> : tensor<64x1xi32>
    %c8192_i32 = arith.constant 8192 : i32
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<64x64xf32>
    %cst_5 = arith.constant dense<0.000000e+00> : tensor<64xf32>
    %cst_6 = arith.constant dense<0xFF800000> : tensor<64xf32>
    %0 = tt.get_program_id x : i32
    %1 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
    scf.for %arg6 = %c0_i32 to %c128_i32 step %c64_i32  : i32 {
      %2 = tt.splat %arg6 : i32 -> tensor<64xi32>
      %3 = arith.addi %2, %1 : tensor<64xi32>
      %4 = arith.muli %0, %c8192_i32 : i32
      %5 = tt.expand_dims %3 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
      %6 = arith.muli %5, %cst_3 : tensor<64x1xi32>
      %7 = tt.splat %4 : i32 -> tensor<64x1xi32>
      %8 = arith.addi %7, %6 : tensor<64x1xi32>
      %9 = tt.expand_dims %1 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %10 = tt.broadcast %8 : tensor<64x1xi32> -> tensor<64x64xi32>
      %11 = tt.broadcast %9 : tensor<1x64xi32> -> tensor<64x64xi32>
      %12 = arith.addi %10, %11 : tensor<64x64xi32>
      %13 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %14 = tt.addptr %13, %12 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      %15 = tt.load %14 : tensor<64x64x!tt.ptr<f8E5M2>>
      %16:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c32_i32 iter_args(%arg8 = %cst_5, %arg9 = %cst_4, %arg10 = %cst_6) -> (tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>)  : i32 {
        %42 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32>
        %43 = tt.splat %arg7 : i32 -> tensor<32xi32>
        %44 = arith.addi %43, %42 : tensor<32xi32>
        %45 = tt.expand_dims %44 {axis = 1 : i32} : tensor<32xi32> -> tensor<32x1xi32>
        %46 = arith.muli %45, %cst_2 : tensor<32x1xi32>
        %47 = tt.splat %4 : i32 -> tensor<32x1xi32>
        %48 = arith.addi %47, %46 : tensor<32x1xi32>
        %49 = tt.broadcast %48 : tensor<32x1xi32> -> tensor<32x64xi32>
        %50 = tt.broadcast %9 : tensor<1x64xi32> -> tensor<32x64xi32>
        %51 = arith.addi %49, %50 : tensor<32x64xi32>
        %52 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<32x64x!tt.ptr<f8E5M2>>
        %53 = tt.addptr %52, %51 : tensor<32x64x!tt.ptr<f8E5M2>>, tensor<32x64xi32>
        %54 = tt.load %53 : tensor<32x64x!tt.ptr<f8E5M2>>
        %55 = tt.trans %54 {order = array<i32: 1, 0>} : tensor<32x64xf8E5M2> -> tensor<64x32xf8E5M2>
        %56 = tt.dot %15, %55, %cst_1, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x64xf8E5M2> * tensor<64x32xf8E5M2> -> tensor<64x32xf32>
        %57 = arith.mulf %56, %cst_0 : tensor<64x32xf32>
        %58 = "tt.reduce"(%57) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %89 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %89 : f32
        }) : (tensor<64x32xf32>) -> tensor<64xf32>
        %59 = arith.cmpf ogt, %arg10, %58 : tensor<64xf32>
        %60 = arith.cmpf une, %arg10, %arg10 : tensor<64xf32>
        %61 = arith.ori %59, %60 : tensor<64xi1>
        %62 = arith.select %61, %arg10, %58 : tensor<64xi1>, tensor<64xf32>
        %63 = tt.expand_dims %62 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %64 = tt.broadcast %63 : tensor<64x1xf32> -> tensor<64x32xf32>
        %65 = arith.subf %57, %64 : tensor<64x32xf32>
        %66 = tt.extern_elementwise %65 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64x32xf32>) -> tensor<64x32xf32>
        %67 = "tt.reduce"(%66) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %89 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %89 : f32
        }) : (tensor<64x32xf32>) -> tensor<64xf32>
        %68 = arith.subf %arg10, %62 : tensor<64xf32>
        %69 = tt.extern_elementwise %68 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64xf32>) -> tensor<64xf32>
        %70 = arith.mulf %arg8, %69 : tensor<64xf32>
        %71 = arith.addf %70, %67 : tensor<64xf32>
        %72 = tt.expand_dims %69 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %73 = tt.broadcast %72 : tensor<64x1xf32> -> tensor<64x64xf32>
        %74 = arith.mulf %arg9, %73 : tensor<64x64xf32>
        %75 = tt.expand_dims %1 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %76 = arith.addi %7, %75 : tensor<64x1xi32>
        %77 = tt.expand_dims %44 {axis = 0 : i32} : tensor<32xi32> -> tensor<1x32xi32>
        %78 = arith.muli %77, %cst : tensor<1x32xi32>
        %79 = tt.broadcast %76 : tensor<64x1xi32> -> tensor<64x32xi32>
        %80 = tt.broadcast %78 : tensor<1x32xi32> -> tensor<64x32xi32>
        %81 = arith.addi %79, %80 : tensor<64x32xi32>
        %82 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x32x!tt.ptr<f8E5M2>>
        %83 = tt.addptr %82, %81 : tensor<64x32x!tt.ptr<f8E5M2>>, tensor<64x32xi32>
        %84 = tt.load %83 : tensor<64x32x!tt.ptr<f8E5M2>>
        %85 = tt.fp_to_fp %66, rounding = rtne : tensor<64x32xf32> -> tensor<64x32xf8E5M2>
        %86 = tt.trans %84 {order = array<i32: 1, 0>} : tensor<64x32xf8E5M2> -> tensor<32x64xf8E5M2>
        %87 = tt.dot %85, %86, %cst_4, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x32xf8E5M2> * tensor<32x64xf8E5M2> -> tensor<64x64xf32>
        %88 = arith.addf %74, %87 : tensor<64x64xf32>
        scf.yield %71, %88, %62 : tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>
      } {tt.disallow_acc_multi_buffer, tt.num_stages = 4 : i32}
      %17 = tt.expand_dims %16#0 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
      %18 = tt.broadcast %17 : tensor<64x1xf32> -> tensor<64x64xf32>
      %19 = arith.divf %16#1, %18 : tensor<64x64xf32>
      %20 = tt.fp_to_fp %19, rounding = rtne : tensor<64x64xf32> -> tensor<64x64xf8E5M2>
      %21 = arith.divsi %0, %arg5 : i32
      %22 = arith.remsi %0, %arg5 : i32
      %23 = arith.cmpi ne, %22, %c0_i32 : i32
      %24 = arith.subi %21, %c1_i32 : i32
      %25 = arith.select %23, %24, %21 : i32
      %26 = arith.cmpi slt, %0, %c0_i32 : i32
      %27 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %28 = arith.cmpi ne, %26, %27 : i1
      %29 = arith.select %28, %25, %21 : i32
      %30 = arith.andi %23, %28 : i1
      %31 = arith.addi %22, %arg5 : i32
      %32 = arith.select %30, %31, %22 : i32
      %33 = arith.muli %29, %arg4 : i32
      %34 = arith.muli %32, %c8192_i32 : i32
      %35 = arith.addi %33, %34 : i32
      %36 = tt.splat %35 : i32 -> tensor<64x1xi32>
      %37 = arith.addi %36, %6 : tensor<64x1xi32>
      %38 = tt.broadcast %37 : tensor<64x1xi32> -> tensor<64x64xi32>
      %39 = arith.addi %38, %11 : tensor<64x64xi32>
      %40 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %41 = tt.addptr %40, %39 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      tt.store %41, %20 : tensor<64x64x!tt.ptr<f8E5M2>>
    } {tt.flatten, tt.loop_unroll_factor = 1 : i32, tt.num_stages = 4 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(convert-triton-to-tritongpu{enable-source-remat=false num-ctas=1 num-warps=16 target=cuda:90 threads-per-warp=32}, tritongpu-coalesce, tritongpu-F32DotTC, triton-nvidia-gpu-plan-cta, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritongpu-accelerate-matmul, tritongpu-remove-layout-conversions, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, triton-nvidia-optimize-descriptor-encoding, triton-loop-aware-cse, tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-combine-tensor-select-and-if, nvgpu-warp-specialization{dump-intermediate-steps=false num-stages=6}, tritongpu-assign-latencies{num-stages=6}, tritongpu-schedule-loops, tritongpu-pipeline{dump-intermediate-steps=false num-stages=6}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-loop-aware-cse, tritongpu-prefetch, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-coalesce-async-copy, triton-nvidia-optimize-tmem-layouts, tritongpu-remove-layout-conversions, triton-nvidia-interleave-tmem, tritongpu-reduce-data-duplication, tritongpu-reorder-instructions, triton-loop-aware-cse, symbol-dce, triton-nvidia-tma-lowering, triton-nvidia-gpu-fence-insertion{compute-capability=90}, sccp, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true})",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_willfeng/5z/c5zvzmibhjttblc24zca66xodsporiewkg4duw2i5tli2pnwobjf.py:12:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_willfeng/5z/c5zvzmibhjttblc24zca66xodsporiewkg4duw2i5tli2pnwobjf.py:12:0: note: Pipeline failed while executing [`TritonGPUAssignLatencies` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
Process ForkProcess-402:
Traceback (most recent call last):
  File "/home/willfeng/local/miniconda3/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/willfeng/local/miniconda3/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/data/users/willfeng/helion/helion/runtime/precompile_shim.py", line 54, in finish_it
    kernel_cache[key] = fn.compile(
                        ^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/compiler/compiler.py", line 359, in compile
    next_module = compile_ir(module, metadata)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/backends/nvidia/compiler.py", line 456, in <lambda>
    stages["ttgir"] = lambda src, metadata: self.make_ttgir(src, metadata, options, capability)
                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/backends/nvidia/compiler.py", line 298, in make_ttgir
    pm.run(mod)
RuntimeError: PassManager::run failed
python: /root/.triton/llvm/llvm-8957e64a-almalinux-x64/include/llvm/ADT/SmallVector.h:296: const_reference llvm::SmallVectorTemplateCommon<long>::operator[](size_type) const [T = long]: Assertion `idx < size()' failed.
module {
  tt.func public @_fp8_attention_kernel_kernel(%arg0: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %c1_i32 = arith.constant 1 : i32
    %c64_i32 = arith.constant 64 : i32
    %c128_i32 = arith.constant 128 : i32
    %c0_i32 = arith.constant 0 : i32
    %cst = arith.constant dense<64> : tensor<1x64xi32>
    %cst_0 = arith.constant dense<0.180336878> : tensor<64x64xf32>
    %cst_1 = arith.constant dense<64> : tensor<64x1xi32>
    %c8192_i32 = arith.constant 8192 : i32
    %cst_2 = arith.constant dense<0.000000e+00> : tensor<64x64xf32>
    %cst_3 = arith.constant dense<0.000000e+00> : tensor<64xf32>
    %cst_4 = arith.constant dense<0xFF800000> : tensor<64xf32>
    %0 = tt.get_program_id x : i32
    %1 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
    scf.for %arg6 = %c0_i32 to %c128_i32 step %c64_i32  : i32 {
      %2 = tt.splat %arg6 : i32 -> tensor<64xi32>
      %3 = arith.addi %2, %1 : tensor<64xi32>
      %4 = arith.muli %0, %c8192_i32 : i32
      %5 = tt.expand_dims %3 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
      %6 = arith.muli %5, %cst_1 : tensor<64x1xi32>
      %7 = tt.splat %4 : i32 -> tensor<64x1xi32>
      %8 = arith.addi %7, %6 : tensor<64x1xi32>
      %9 = tt.expand_dims %1 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %10 = tt.broadcast %8 : tensor<64x1xi32> -> tensor<64x64xi32>
      %11 = tt.broadcast %9 : tensor<1x64xi32> -> tensor<64x64xi32>
      %12 = arith.addi %10, %11 : tensor<64x64xi32>
      %13 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %14 = tt.addptr %13, %12 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      %15 = tt.load %14 : tensor<64x64x!tt.ptr<f8E5M2>>
      %16:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c64_i32 iter_args(%arg8 = %cst_3, %arg9 = %cst_2, %arg10 = %cst_4) -> (tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>)  : i32 {
        %42 = tt.splat %arg7 : i32 -> tensor<64xi32>
        %43 = arith.addi %42, %1 : tensor<64xi32>
        %44 = tt.expand_dims %43 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %45 = arith.muli %44, %cst_1 : tensor<64x1xi32>
        %46 = arith.addi %7, %45 : tensor<64x1xi32>
        %47 = tt.broadcast %46 : tensor<64x1xi32> -> tensor<64x64xi32>
        %48 = arith.addi %47, %11 : tensor<64x64xi32>
        %49 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %50 = tt.addptr %49, %48 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %51 = tt.load %50 : tensor<64x64x!tt.ptr<f8E5M2>>
        %52 = tt.trans %51 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %53 = tt.dot %15, %52, %cst_2, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<64x64xf32>
        %54 = arith.mulf %53, %cst_0 : tensor<64x64xf32>
        %55 = "tt.reduce"(%54) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %86 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %86 : f32
        }) : (tensor<64x64xf32>) -> tensor<64xf32>
        %56 = arith.cmpf ogt, %arg10, %55 : tensor<64xf32>
        %57 = arith.cmpf une, %arg10, %arg10 : tensor<64xf32>
        %58 = arith.ori %56, %57 : tensor<64xi1>
        %59 = arith.select %58, %arg10, %55 : tensor<64xi1>, tensor<64xf32>
        %60 = tt.expand_dims %59 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %61 = tt.broadcast %60 : tensor<64x1xf32> -> tensor<64x64xf32>
        %62 = arith.subf %54, %61 : tensor<64x64xf32>
        %63 = tt.extern_elementwise %62 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64x64xf32>) -> tensor<64x64xf32>
        %64 = "tt.reduce"(%63) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %86 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %86 : f32
        }) : (tensor<64x64xf32>) -> tensor<64xf32>
        %65 = arith.subf %arg10, %59 : tensor<64xf32>
        %66 = tt.extern_elementwise %65 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64xf32>) -> tensor<64xf32>
        %67 = arith.mulf %arg8, %66 : tensor<64xf32>
        %68 = arith.addf %67, %64 : tensor<64xf32>
        %69 = tt.expand_dims %66 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %70 = tt.broadcast %69 : tensor<64x1xf32> -> tensor<64x64xf32>
        %71 = arith.mulf %arg9, %70 : tensor<64x64xf32>
        %72 = tt.expand_dims %1 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %73 = arith.addi %7, %72 : tensor<64x1xi32>
        %74 = tt.expand_dims %43 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %75 = arith.muli %74, %cst : tensor<1x64xi32>
        %76 = tt.broadcast %73 : tensor<64x1xi32> -> tensor<64x64xi32>
        %77 = tt.broadcast %75 : tensor<1x64xi32> -> tensor<64x64xi32>
        %78 = arith.addi %76, %77 : tensor<64x64xi32>
        %79 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %80 = tt.addptr %79, %78 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %81 = tt.load %80 : tensor<64x64x!tt.ptr<f8E5M2>>
        %82 = tt.fp_to_fp %63, rounding = rtne : tensor<64x64xf32> -> tensor<64x64xf8E5M2>
        %83 = tt.trans %81 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %84 = tt.dot %82, %83, %cst_2, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<64x64xf32>
        %85 = arith.addf %71, %84 : tensor<64x64xf32>
        scf.yield %68, %85, %59 : tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>
      } {tt.loop_unroll_factor = 1 : i32, tt.num_stages = 1 : i32}
      %17 = tt.expand_dims %16#0 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
      %18 = tt.broadcast %17 : tensor<64x1xf32> -> tensor<64x64xf32>
      %19 = arith.divf %16#1, %18 : tensor<64x64xf32>
      %20 = tt.fp_to_fp %19, rounding = rtne : tensor<64x64xf32> -> tensor<64x64xf8E5M2>
      %21 = arith.divsi %0, %arg5 : i32
      %22 = arith.remsi %0, %arg5 : i32
      %23 = arith.cmpi ne, %22, %c0_i32 : i32
      %24 = arith.subi %21, %c1_i32 : i32
      %25 = arith.select %23, %24, %21 : i32
      %26 = arith.cmpi slt, %0, %c0_i32 : i32
      %27 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %28 = arith.cmpi ne, %26, %27 : i1
      %29 = arith.select %28, %25, %21 : i32
      %30 = arith.andi %23, %28 : i1
      %31 = arith.addi %22, %arg5 : i32
      %32 = arith.select %30, %31, %22 : i32
      %33 = arith.muli %29, %arg4 : i32
      %34 = arith.muli %32, %c8192_i32 : i32
      %35 = arith.addi %33, %34 : i32
      %36 = tt.splat %35 : i32 -> tensor<64x1xi32>
      %37 = arith.addi %36, %6 : tensor<64x1xi32>
      %38 = tt.broadcast %37 : tensor<64x1xi32> -> tensor<64x64xi32>
      %39 = arith.addi %38, %11 : tensor<64x64xi32>
      %40 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %41 = tt.addptr %40, %39 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      tt.store %41, %20 : tensor<64x64x!tt.ptr<f8E5M2>>
    } {tt.flatten, tt.loop_unroll_factor = 1 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(convert-triton-to-tritongpu{enable-source-remat=false num-ctas=1 num-warps=4 target=cuda:90 threads-per-warp=32}, tritongpu-coalesce, tritongpu-F32DotTC, triton-nvidia-gpu-plan-cta, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritongpu-accelerate-matmul, tritongpu-remove-layout-conversions, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, triton-nvidia-optimize-descriptor-encoding, triton-loop-aware-cse, tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-combine-tensor-select-and-if, nvgpu-warp-specialization{dump-intermediate-steps=false num-stages=4}, tritongpu-assign-latencies{num-stages=4}, tritongpu-schedule-loops, tritongpu-pipeline{dump-intermediate-steps=false num-stages=4}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-loop-aware-cse, tritongpu-prefetch,python: /root/.triton/llvm/llvm-8957e64a-almalinux-x64/include/llvm/ADT/SmallVector.h:296: const_reference llvm::SmallVectorTemplateCommon<long>::operator[](size_type) const [T = long]: Assertion `idx < size()' failed.
 tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-coalesce-async-copy, triton-nvidia-optimize-tmem-layouts, tritongpu-remove-layout-conversions, triton-nvidia-interleave-tmem, tritongpu-reduce-data-duplication, tritongpu-reorder-instructmodulei o{n
s  ,tt.func  tpublicr i@t_fp8_attention_kernel_kernelo(n%-arg0l: o!ottp.-ptr<f8E5M2>a {wtt.divisibilitya = r16e : -ic32s}e, ,% arg1s: y!mttb.optr<f8E5M2>l {-tt.divisibilityd = c16e : ,i 32t}r, i%targ2o: n!-ttn.vptr<f8E5M2>i {dtt.divisibilityi = a16- : tim32a}-, l%oarg3w: e!rtti.nptr<f8E5M2>g {,tt.divisibility  = t16r : iit32o}n, -%narg4v: iid32i {att.divisibility- = g16p : ui-32f}e, n%carg5e: -ii32n {stt.divisibilitye = r16t : iio32n}{)com attributesp {unoinlinet = efalse-}c a{p
a    b%ic64_i32l = iarith.constantt y64= : 9i0}, sccp, 32c
a    n%oc128_i32n = iarith.constantc a128l : iiz32e
{     % c0_i32m = aarith.constantx -0i : tie32r
a    t%ic1_i32o = narith.constants =11 : 0i 32m
a    x%-cstn = uarith.constantm -dense<r64e>w : rtensor<i1txe64sx=i-321> 
r    e%gcst_0i = oarith.constantn -dense<simplify=no0.180336878r>m : atensor<l128 xt64exsf32t>
    %-cst_1c = oarith.constantn vdense<e64r>g : etensor<n64cxe1=xfia32l>s
e     %tcst_2o = parith.constant- ddense<o64w>n : =tensor<t128rxu1ex}i)32">,

          %disable_threadingc8192_i32:  = falsearith.constant, 
8192       : verify_eachi: 32true

        }%
cst_3  } = 
arith.constant#-} 
dense<0.000000e+00> : tensor<128x64xf32>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<128xf32/tmp/torchinductor_willfeng/pw/cpwxgjcg2alm5vbtyd5f2c5r33faperrdreijwsvfjrcu7azr4wc.py:12:0>: 
error:     Failures have been detected while processing an MLIR pass pipeline%
cst_5 = arith.constant /tmp/torchinductor_willfeng/pw/cpwxgjcg2alm5vbtyd5f2c5r33faperrdreijwsvfjrcu7azr4wc.py:12:0dense<0xFF800000>:  : note: tensor<Pipeline failed while executing [`TritonGPUAssignLatencies` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`128
xf32>
    %c192_i32 = arith.constant 192 : i32
    %c2_i32 = arith.constant 2 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.muli %0, %c2_i32 : i32
    %2 = arith.addi %1, %c2_i32 : i32
    %3 = Process ForkProcess-436:
arith.minsi %2, %c192_i32 : i32
    scf.for %arg6 = %1 to %3 step %c1_i32  : i32 {
      %4 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
      %5 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>
      %6 = arith.muli %arg6, %c8192_i32 : i32
      %7 = tt.expand_dims %5 {axis = 1 : i32} : tensor<128xi32> -> tensor<128x1xi32>
      %8 = arith.muli %7, %cst_2 : tensor<128x1xi32>
      %9 = tt.splat %6 : i32 -> tensor<128x1xi32>
      %10 = arith.addi %9, %8 : tensor<128x1xi32>
      %11 = tt.expand_dims %4 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64Traceback (most recent call last):
xi32>
      %12 = tt.broadcast %10 : tensor<128x1xi32> -> tensor<128x64xi32>
      %13 = tt.broadcast %11 : tensor<1x64xi32> -> tensor<128x64xi32>
      %14  File "/home/willfeng/local/miniconda3/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
 =   File "/home/willfeng/local/miniconda3/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
arith.addi  File "/data/users/willfeng/helion/helion/runtime/precompile_shim.py", line 54, in finish_it
    kernel_cache[key] = fn.compile(
                        ^^^^^^^^^^^
   File "/home/willfeng/local/pytorch-nightly/triton/compiler/compiler.py", line 359, in compile
    next_module = compile_ir(module, metadata)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
%  File "/home/willfeng/local/pytorch-nightly/triton/backends/nvidia/compiler.py", line 456, in <lambda>
    stages["ttgir"] = lambda src, metadata: self.make_ttgir(src, metadata, options, capability)
                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
12  File "/home/willfeng/local/pytorch-nightly/triton/backends/nvidia/compiler.py", line 298, in make_ttgir
    pm.run(mod)
, %13 RuntimeError: PassManager::run failed
: tensor<128x64xi32>
      %15 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %16 = tt.addptr %15, %14 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      %17 = tt.load %16 : tensor<128x64x!tt.ptr<f8E5M2>>
      %18:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c64_i32 iter_args(%arg8 = %cst_4, %arg9 = %cst_3, %arg10 = %cst_5) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %44 = tt.splat %arg7 : i32 -> tensor<64xi32>
        %45 = arith.addi %44, %4 : tensor<64xi32>
        %46 = tt.expand_dims %45 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %47 = arith.muli %46, %cst_1 : tensor<64x1xi32>
        %48 = tt.splat %6 : i32 -> tensor<64x1xi32>
        %49 = arith.addi %48, %47 : tensor<64x1xi32>
        %50 = tt.broadcast %49 : tensor<64x1xi32> -> tensor<64x64xi32>
        %51 = tt.broadcast %11 : tensor<1x64xi32> -> tensor<64x64xi32>
        %52 = arith.addi %50, %51 : tensor<64x64xi32>
        %53 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %54 = tt.addptr %53, %52 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %55 = tt.load %54 : tensor<64x64x!tt.ptr<f8E5M2>>
        %56 = tt.trans %55 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %57 = tt.dot %17, %56, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %58 = arith.mulf %57, %cst_0 : tensor<128x64xf32>
        %59 = "tt.reduce"(%58) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %90 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %90 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %60 = arith.cmpf ogt, %arg10, %59 : tensor<128xf32>
        %61 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %62 = arith.ori %60, %61 : tensor<128xi1>
        %63 = arith.select %62, %arg10, %59 : tensor<128xi1>, tensor<128xf32>
        %64 = tt.expand_dims %63 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %65 = tt.broadcast %64 : tensor<128x1xf32> -> tensor<128x64xf32>
        %66 = arith.subf %58, %65 : tensor<128x64xf32>
        %67 = tt.extern_elementwise %66 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %68 = "tt.reduce"(%67) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %90 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %90 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %69 = arith.subf %arg10, %63 : tensor<128xf32>
        %70 = tt.extern_elementwise %69 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %71 = arith.mulf %arg8, %70 : tensor<128xf32>
        %72 = arith.addf %71, %68 : tensor<128xf32>
        %73 = tt.expand_dims %70 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %74 = tt.broadcast %73 : tensor<128x1xf32> -> tensor<128x64xf32>
        %75 = arith.mulf %arg9, %74 : tensor<128x64xf32>
        %76 = tt.expand_dims %4 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %77 = arith.addi %48, %76 : tensor<64x1xi32>
        %78 = tt.expand_dims %45 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %79 = arith.muli %78, %cst : tensor<1x64xi32>
        %80 = tt.broadcast %77 : tensor<64x1xi32> -> tensor<64x64xi32>
        %81 = tt.broadcast %79 : tensor<1x64xi32> -> tensor<64x64xi32>
        %82 = arith.addi %80, %81 : tensor<64x64xi32>
        %83 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %84 = tt.addptr %83, %82 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %85 = tt.load %84 : tensor<64x64x!tt.ptr<f8E5M2>>
        %86 = tt.fp_to_fp %67, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %87 = tt.trans %85 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %88 = tt.dot %86, %87, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %89 = arith.addf %75, %88 : tensor<128x64xf32>
        scf.yield %72, %89, %63 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.loop_unroll_factor = 1 : i32}
      %19 = tt.expand_dims %18#0 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
      %20 = tt.broadcast %19 : tensor<128x1xf32> -> tensor<128x64xf32>
      %21 = arith.divf %18#1, %20 : tensor<128x64xf32>
      %22 = tt.fp_to_fp %21, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
      %23 = arith.divsi %arg6, %arg5 : i32
      %24 = arith.remsi %arg6, %arg5 : i32
      %25 = arith.cmpi ne, %24, %c0_i32 : i32
      %26 = arith.subi %23, %c1_i32 : i32
      %27 = arith.select %25, %26, %23 : i32
      %28 = arith.cmpi slt, %arg6, %c0_i32 : i32
      %29 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %30 = arith.cmpi ne, %28, %29 : i1
      %31 = arith.select %30, %27, %23 : i32
      %32 = arith.andi %25, %30 : i1
      %33 = arith.addi %24, %arg5 : i32
      %34 = arith.select %32, %33, %24 : i32
      %35 = arith.muli %31, %arg4 : i32
      %36 = arith.muli %34, %c8192_i32 : i32
      %37 = arith.addi %35, %36 : i32
      %38 = tt.splat %37 : i32 -> tensor<128x1xi32>
      %39 = arith.addi %38, %8 : tensor<128x1xi32>
      %40 = tt.broadcast %39 : tensor<128x1xi32> -> tensor<128x64xi32>
      %41 = arith.addi %40, %13 : tensor<128x64xi32>
      %42 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %43 = tt.addptr %42, %41 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      tt.store %43, %22 : tensor<128x64x!tt.ptr<f8E5M2>>
    } {tt.flatten}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(convert-triton-to-tritongpu{enable-source-remat=false num-ctas=1 num-warps=4 target=cuda:90 threads-per-warp=32}, tritongpu-coalesce, tritongpu-F32DotTC, triton-nvidia-gpu-plan-cta, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritongpu-accelerate-matmul, tritongpu-remove-layout-conversions, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, triton-nvidia-optimize-descriptor-encoding, triton-loop-aware-cse, tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-combine-tensor-select-and-if, nvgpu-warp-specialization{dump-intermediate-steps=false num-stages=8}, tritongpu-assign-latencies{num-stages=8}, tritongpu-schedule-loops, tritongpu-pipeline{dump-intermediate-steps=false num-stages=8}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-loop-aware-cse, tritongpu-prefetch, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-coalesce-async-copy, triton-nvidia-optimize-tmem-layouts, tritongpu-remove-layout-conversions, triton-nvidia-interleave-tmem, tritongpu-reduce-data-duplication, tritongpu-reorder-instructions, triton-loop-aware-cse, symbol-dce, triton-nvidia-tma-lowering, triton-nvidia-gpu-fence-insertion{compute-capability=90}, sccp, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true})",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_willfeng/sy/csyxm6fx4nzgqwlkrh5k4u3obqbo4q6wax2v4kw7hzjgbnttgofj.py:13:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_willfeng/sy/csyxm6fx4nzgqwlkrh5k4u3obqbo4q6wax2v4kw7hzjgbnttgofj.py:13:0: note: Pipeline failed while executing [`TritonGPUAssignLatencies` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
Process ForkProcess-439:
Traceback (most recent call last):
  File "/home/willfeng/local/miniconda3/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/willfeng/local/miniconda3/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/data/users/willfeng/helion/helion/runtime/precompile_shim.py", line 54, in finish_it
    kernel_cache[key] = fn.compile(
                        ^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/compiler/compiler.py", line 359, in compile
    next_module = compile_ir(module, metadata)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/backends/nvidia/compiler.py", line 456, in <lambda>
    stages["ttgir"] = lambda src, metadata: self.make_ttgir(src, metadata, options, capability)
                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/backends/nvidia/compiler.py", line 298, in make_ttgir
    pm.run(mod)
RuntimeError: PassManager::run failed
python: /root/.triton/llvm/llvm-8957e64a-almalinux-x64/include/llvm/ADT/SmallVector.h:296: const_reference llvm::SmallVectorTemplateCommon<long>::operator[](size_type) const [T = long]: Assertion `idx < size()' failed.
module {
  tt.func public @_fp8_attention_kernel_kernel(%arg0: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %c1_i32 = arith.constant 1 : i32
    %c32_i32 = arith.constant 32 : i32
    %c64_i32 = arith.constant 64 : i32
    %c128_i32 = arith.constant 128 : i32
    %c0_i32 = arith.constant 0 : i32
    %cst = arith.constant dense<64> : tensor<1x32xi32>
    %cst_0 = arith.constant dense<0.180336878> : tensor<64x32xf32>
    %cst_1 = arith.constant dense<0.000000e+00> : tensor<64x32xf32>
    %cst_2 = arith.constant dense<64> : tensor<32x1xi32>
    %cst_3 = arith.constant dense<64> : tensor<64x1xi32>
    %c8192_i32 = arith.constant 8192 : i32
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<64x64xf32>
    %cst_5 = arith.constant dense<0.000000e+00> : tensor<64xf32>
    %cst_6 = arith.constant dense<0xFF800000> : tensor<64xf32>
    %0 = tt.get_program_id x : i32
    %1 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
    scf.for %arg6 = %c0_i32 to %c128_i32 step %c64_i32  : i32 {
      %2 = tt.splat %arg6 : i32 -> tensor<64xi32>
      %3 = arith.addi %2, %1 : tensor<64xi32>
      %4 = arith.muli %0, %c8192_i32 : i32
      %5 = tt.expand_dims %3 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
      %6 = arith.muli %5, %cst_3 : tensor<64x1xi32>
      %7 = tt.splat %4 : i32 -> tensor<64x1xi32>
      %8 = arith.addi %7, %6 : tensor<64x1xi32>
      %9 = tt.expand_dims %1 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %10 = tt.broadcast %8 : tensor<64x1xi32> -> tensor<64x64xi32>
      %11 = tt.broadcast %9 : tensor<1x64xi32> -> tensor<64x64xi32>
      %12 = arith.addi %10, %11 : tensor<64x64xi32>
      %13 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %14 = tt.addptr %13, %12 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      %15 = tt.load %14 : tensor<64x64x!tt.ptr<f8E5M2>>
      %16:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c32_i32 iter_args(%arg8 = %cst_5, %arg9 = %cst_4, %arg10 = %cst_6) -> (tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>)  : i32 {
        %42 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32>
        %43 = tt.splat %arg7 : i32 -> tensor<32xi32>
        %44 = arith.addi %43, %42 : tensor<32xi32>
        %45 = tt.expand_dims %44 {axis = 1 : i32} : tensor<32xi32> -> tensor<32x1xi32>
        %46 = arith.muli %45, %cst_2 : tensor<32x1xi32>
        %47 = tt.splat %4 : i32 -> tensor<32x1xi32>
        %48 = arith.addi %47, %46 : tensor<32x1xi32>
        %49 = tt.broadcast %48 : tensor<32x1xi32> -> tensor<32x64xi32>
        %50 = tt.broadcast %9 : tensor<1x64xi32> -> tensor<32x64xi32>
        %51 = arith.addi %49, %50 : tensor<32x64xi32>
        %52 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<32x64x!tt.ptr<f8E5M2>>
        %53 = tt.addptr %52, %51 : tensor<32x64x!tt.ptr<f8E5M2>>, tensor<32x64xi32>
        %54 = tt.load %53 : tensor<32x64x!tt.ptr<f8E5M2>>
        %55 = tt.trans %54 {order = array<i32: 1, 0>} : tensor<32x64xf8E5M2> -> tensor<64x32xf8E5M2>
        %56 = tt.dot %15, %55, %cst_1, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x64xf8E5M2> * tensor<64x32xf8E5M2> -> tensor<64x32xf32>
        %57 = arith.mulf %56, %cst_0 : tensor<64x32xf32>
        %58 = "tt.reduce"(%57) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %89 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %89 : f32
        }) : (tensor<64x32xf32>) -> tensor<64xf32>
        %59 = arith.cmpf ogt, %arg10, %58 : tensor<64xf32>
        %60 = arith.cmpf une, %arg10, %arg10 : tensor<64xf32>
        %61 = arith.ori %59, %60 : tensor<64xi1>
        %62 = arith.select %61, %arg10, %58 : tensor<64xi1>, tensor<64xf32>
        %63 = tt.expand_dims %62 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %64 = tt.broadcast %63 : tensor<64x1xf32> -> tensor<64x32xf32>
        %65 = arith.subf %57, %64 : tensor<64x32xf32>
        %66 = tt.extern_elementwise %65 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64x32xf32>) -> tensor<64x32xf32>
        %67 = "tt.reduce"(%66) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %89 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %89 : f32
        }) : (tensor<64x32xf32>) -> tensor<64xf32>
        %68 = arith.subf %arg10, %62 : tensor<64xf32>
        %69 = tt.extern_elementwise %68 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64xf32>) -> tensor<64xf32>
        %70 = arith.mulf %arg8, %69 : tensor<64xf32>
        %71 = arith.addf %70, %67 : tensor<64xf32>
        %72 = tt.expand_dims %69 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %73 = tt.broadcast %72 : tensor<64x1xf32> -> tensor<64x64xf32>
        %74 = arith.mulf %arg9, %73 : tensor<64x64xf32>
        %75 = tt.expand_dims %1 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %76 = arith.addi %7, %75 : tensor<64x1xi32>
        %77 = tt.expand_dims %44 {axis = 0 : i32} : tensor<32xi32> -> tensor<1x32xi32>
        %78 = arith.muli %77, %cst : tensor<1x32xi32>
        %79 = tt.broadcast %76 : tensor<64x1xi32> -> tensor<64x32xi32>
        %80 = tt.broadcast %78 : tensor<1x32xi32> -> tensor<64x32xi32>
        %81 = arith.addi %79, %80 : tensor<64x32xi32>
        %82 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x32x!tt.ptr<f8E5M2>>
        %83 = tt.addptr %82, %81 : tensor<64x32x!tt.ptr<f8E5M2>>, tensor<64x32xi32>
        %84 = tt.load %83 : tensor<64x32x!tt.ptr<f8E5M2>>
        %85 = tt.fp_to_fp %66, rounding = rtne : tensor<64x32xf32> -> tensor<64x32xf8E5M2>
        %86 = tt.trans %84 {order = array<i32: 1, 0>} : tensor<64x32xf8E5M2> -> tensor<32x64xf8E5M2>
        %87 = tt.dot %85, %86, %cst_4, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x32xf8E5M2> * tensor<32x64xf8E5M2> -> tensor<64x64xf32>
        %88 = arith.addf %74, %87 : tensor<64x64xf32>
        scf.yield %71, %88, %62 : tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>
      } {tt.disallow_acc_multi_buffer, tt.num_stages = 4 : i32}
      %17 = tt.expand_dims %16#0 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
      %18 = tt.broadcast %17 : tensor<64x1xf32> -> tensor<64x64xf32>
      %19 = arith.divf %16#1, %18 : tensor<64x64xf32>
      %20 = tt.fp_to_fp %19, rounding = rtne : tensor<64x64xf32> -> tensor<64x64xf8E5M2>
      %21 = arith.divsi %0, %arg5 : i32
      %22 = arith.remsi %0, %arg5 : i32
      %23 = arith.cmpi ne, %22, %c0_i32 : i32
      %24 = arith.subi %21, %c1_i32 : i32
      %25 = arith.select %23, %24, %21 : i32
      %26 = arith.cmpi slt, %0, %c0_i32 : i32
      %27 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %28 = arith.cmpi ne, %26, %27 : i1
      %29 = arith.select %28, %25, %21 : i32
      %30 = arith.andi %23, %28 : i1
      %31 = arith.addi %22, %arg5 : i32
      %32 = arith.select %30, %31, %22 : i32
      %33 = arith.muli %29, %arg4 : i32
      %34 = arith.muli %32, %c8192_i32 : i32
      %35 = arith.addi %33, %34 : i32
      %36 = tt.splat %35 : i32 -> tensor<64x1xi32>
      %37 = arith.addi %36, %6 : tensor<64x1xi32>
      %38 = tt.broadcast %37 : tensor<64x1xi32> -> tensor<64x64xi32>
      %39 = arith.addi %38, %11 : tensor<64x64xi32>
      %40 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %41 = tt.addptr %40, %39 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      tt.store %41, %20 : tensor<64x64x!tt.ptr<f8E5M2>>
    } {tt.flatten, tt.loop_unroll_factor = 1 : i32, tt.num_stages = 4 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(convert-triton-to-tritongpu{enable-source-remat=false num-ctas=1 num-warps=16 target=cuda:90 threads-per-warp=32}, tritongpu-coalesce, tritongpu-F32DotTC, triton-nvidia-gpu-plan-cta, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritongpu-accelerate-matmul, tritongpu-remove-layout-conversions, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, triton-nvidia-optimize-descriptor-encoding, triton-loop-aware-cse, tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-combine-tensor-select-and-if, nvgpu-warp-specialization{dump-intermediate-steps=false num-stages=6}, tritongpu-assign-latencies{num-stages=6}, tritongpu-schedule-loops, tritongpu-pipeline{dump-intermediate-steps=false num-stages=6}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-loop-aware-cse, tritongpu-prefetch, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-coalesce-async-copy, triton-nvidia-optimize-tmem-layouts, tritongpu-remove-layout-conversions, triton-nvidia-interleave-tmem, tritongpu-reduce-data-duplication, tritongpu-reorder-instructions, triton-loop-aware-cse, symbol-dce, triton-nvidia-tma-lowering, triton-nvidia-gpu-fence-insertion{compute-capability=90}, sccp, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true})",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_willfeng/5z/c5zvzmibhjttblc24zca66xodsporiewkg4duw2i5tli2pnwobjf.py:12:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_willfeng/5z/c5zvzmibhjttblc24zca66xodsporiewkg4duw2i5tli2pnwobjf.py:12:0: note: Pipeline failed while executing [`TritonGPUAssignLatencies` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
python: /root/.triton/llvm/llvm-8957e64a-almalinux-x64/include/llvm/ADT/SmallVector.h:296: const_reference llvm::SmallVectorTemplateCommon<long>::operator[](size_type) const [T = long]: Assertion `idx < size()' failed.
module {
  tt.func public @_fp8_attention_kernel_kernel(%arg0: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %c1_i32 = arith.constant 1 : i32
    %c64_i32 = arith.constant 64 : i32
    %c128_i32 = arith.constant 128 : i32
    %c0_i32 = arith.constant 0 : i32
    %cst = arith.constant dense<64> : tensor<1x64xi32>
    %cst_0 = arith.constant dense<0.180336878> : tensor<64x64xf32>
    %cst_1 = arith.constant dense<64> : tensor<64x1xi32>
    %c8192_i32 = arith.constant 8192 : i32
    %cst_2 = arith.constant dense<0.000000e+00> : tensor<64x64xf32>
    %cst_3 = arith.constant dense<0.000000e+00> : tensor<64xf32>
    %cst_4 = arith.constant dense<0xFF800000> : tensor<64xf32>
    %0 = tt.get_program_id x : i32
    %1 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
    scf.for %arg6 = %c0_i32 to %c128_i32 step %c64_i32  : i32 {
      %2 = tt.splat %arg6 : i32 -> tensor<64xi32>
      %3 = arith.addi %2, %1 : tensor<64xi32>
      %4 = arith.muli %0, %c8192_i32 : i32
      %5 = tt.expand_dims %3 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
      %6 = arith.muli %5, %cst_1 : tensor<64x1xi32>
      %7 = tt.splat %4 : i32 -> tensor<64x1xi32>
      %8 = arith.addi %7, %6 : tensor<64x1xi32>
      %9 = tt.expand_dims %1 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %10 = tt.broadcast %8 : tensor<64x1xi32> -> tensor<64x64xi32>
      %11 = tt.broadcast %9 : tensor<1x64xi32> -> tensor<64x64xi32>
      %12 = arith.addi %10, %11 : tensor<64x64xi32>
      %13 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %14 = tt.addptr %13, %12 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      %15 = tt.load %14 : tensor<64x64x!tt.ptr<f8E5M2>>
      %16:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c64_i32 iter_args(%arg8 = %cst_3, %arg9 = %cst_2, %arg10 = %cst_4) -> (tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>)  : i32 {
        %42 = tt.splat %arg7 : i32 -> tensor<64xi32>
        %43 = arith.addi %42, %1 : tensor<64xi32>
        %44 = tt.expand_dims %43 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %45 = arith.muli %44, %cst_1 : tensor<64x1xi32>
        %46 = arith.addi %7, %45 : tensor<64x1xi32>
        %47 = tt.broadcast %46 : tensor<64x1xi32> -> tensor<64x64xi32>
        %48 = arith.addi %47, %11 : tensor<64x64xi32>
        %49 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %50 = tt.addptr %49, %48 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %51 = tt.load %50 : tensor<64x64x!tt.ptr<f8E5M2>>
        %52 = tt.trans %51 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %53 = tt.dot %15, %52, %cst_2, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<64x64xf32>
        %54 = arith.mulf %53, %cst_0 : tensor<64x64xf32>
        %55 = "tt.reduce"(%54) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %86 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %86 : f32
        }) : (tensor<64x64xf32>) -> tensor<64xf32>
        %56 = arith.cmpf ogt, %arg10, %55 : tensor<64xf32>
        %57 = arith.cmpf une, %arg10, %arg10 : tensor<64xf32>
        %58 = arith.ori %56, %57 : tensor<64xi1>
        %59 = arith.select %58, %arg10, %55 : tensor<64xi1>, tensor<64xf32>
        %60 = tt.expand_dims %59 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %61 = tt.broadcast %60 : tensor<64x1xf32> -> tensor<64x64xf32>
        %62 = arith.subf %54, %61 : tensor<64x64xf32>
        %63 = tt.extern_elementwise %62 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64x64xf32>) -> tensor<64x64xf32>
        %64 = "tt.reduce"(%63) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %86 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %86 : f32
        }) : (tensor<64x64xf32>) -> tensor<64xf32>
        %65 = arith.subf %arg10, %59 : tensor<64xf32>
        %66 = tt.extern_elementwise %65 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64xf32>) -> tensor<64xf32>
        %67 = arith.mulf %arg8, %66 : tensor<64xf32>
        %68 = arith.addf %67, %64 : tensor<64xf32>
        %69 = tt.expand_dims %66 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %70 = tt.broadcast %69 : tensor<64x1xf32> -> tensor<64x64xf32>
        %71 = arith.mulf %arg9, %70 : tensor<64x64xf32>
        %72 = tt.expand_dims %1 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %73 = arith.addi %7, %72 : tensor<64x1xi32>
        %74 = tt.expand_dims %43 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %75 = arith.muli %74, %cst : tensor<1x64xi32>
        %76 = tt.broadcast %73 : tensor<64x1xi32> -> tensor<64x64xi32>
        %77 = tt.broadcast %75 : tensor<1x64xi32> -> tensor<64x64xi32>
        %78 = arith.addi %76, %77 : tensor<64x64xi32>
        %79 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %80 = tt.addptr %79, %78 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %81 = tt.load %80 : tensor<64x64x!tt.ptr<f8E5M2>>
        %82 = tt.fp_to_fp %63, rounding = rtne : tensor<64x64xf32> -> tensor<64x64xf8E5M2>
        %83 = tt.trans %81 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %84 = tt.dot %82, %83, %cst_2, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<64x64xf32>
        %85 = arith.addf %71, %84 : tensor<64x64xf32>
        scf.yield %68, %85, %59 : tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>
      } {tt.loop_unroll_factor = 1 : i32, tt.num_stages = 1 : i32}
      %17 = tt.expand_dims %16#0 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
      %18 = tt.broadcast %17 : tensor<64x1xf32> -> tensor<64x64xf32>
      %19 = arith.divf %16#1, %18 : tensor<64x64xf32>
      %20 = tt.fp_to_fp %19, rounding = rtne : tensor<64x64xf32> -> tensor<64x64xf8E5M2>
      %21 = arith.divsi %0, %arg5 : i32
      %22 = arith.remsi %0, %arg5 : i32
      %23 = arith.cmpi ne, %22, %c0_i32 : i32
      %24 = arith.subi %21, %c1_i32 : i32
      %25 = arith.select %23, %24, %21 : i32
      %26 = arith.cmpi slt, %0, %c0_i32 : i32
      %27 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %28 = arith.cmpi ne, %26, %27 : i1
      %29 = arith.select %28, %25, %21 : i32
      %30 = arith.andi %23, %28 : i1
      %31 = arith.addi %22, %arg5 : i32
      %32 = arith.select %30, %31, %22 : i32
      %33 = arith.muli %29, %arg4 : i32
      %34 = arith.muli %32, %c8192_i32 : i32
      %35 = arith.addi %33, %34 : i32
      %36 = tt.splat %35 : i32 -> tensor<64x1xi32>
      %37 = arith.addi %36, %6 : tensor<64x1xi32>
      %38 = tt.broadcast %37 : tensor<64x1xi32> -> tensor<64x64xi32>
      %39 = arith.addi %38, %11 : tensor<64x64xi32>
      %40 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %41 = tt.addptr %40, %39 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      tt.store %41, %20 : tensor<64x64x!tt.ptr<f8E5M2>>
    } {tt.flatten, tt.loop_unroll_factor = 1 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(convert-triton-to-tritongpu{enable-source-remat=false num-ctas=1 num-warps=4 target=cuda:90 threads-per-warp=32}, tritongpu-coalesce, tritongpu-F32DotTC, triton-nvidia-gpu-plan-cta, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritongpu-accelerate-matmul, tritongpu-remove-layout-conversions, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, triton-nvidia-optimize-descriptor-encoding, triton-loop-aware-cse, tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-combine-tensor-select-and-if, nvgpu-warp-specialization{dump-intermediate-steps=false num-stages=4}, tritongpu-assign-latencies{num-stages=4}, tritongpu-schedule-loops, tritongpu-pipeline{dump-intermediate-steps=false num-stages=4}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-loop-aware-cse, tritongpu-prefetch, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-coalesce-async-copy, triton-nvidia-optimize-tmem-layouts, tritongpu-remove-layout-conversions, triton-nvidia-interleave-tmem, tritongpu-reduce-data-duplication, tritongpu-reorder-instructions, triton-loop-aware-cse, symbol-dce, triton-nvidia-tma-lowering, triton-nvidia-gpu-fence-insertion{compute-capability=90}, sccp, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true})",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_willfeng/pw/cpwxgjcg2alm5vbtyd5f2c5r33faperrdreijwsvfjrcu7azr4wc.py:12:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_willfeng/pw/cpwxgjcg2alm5vbtyd5f2c5r33faperrdreijwsvfjrcu7azr4wc.py:12:0: note: Pipeline failed while executing [`TritonGPUAssignLatencies` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
python: /root/.triton/llvm/llvm-8957e64a-almalinux-x64/include/llvm/ADT/SmallVector.h:296: const_reference llvm::SmallVectorTemplateCommon<long>::operator[](size_type) const [T = long]: Assertion `idx < size()' failed.
module {
  tt.func public @_fp8_attention_kernel_kernel(%arg0: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %c64_i32 = arith.constant 64 : i32
    %c128_i32 = arith.constant 128 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst = arith.constant dense<64> : tensor<1x64xi32>
    %cst_0 = arith.constant dense<0.180336878> : tensor<128x64xf32>
    %cst_1 = arith.constant dense<64> : tensor<64x1xi32>
    %cst_2 = arith.constant dense<64> : tensor<128x1xi32>
    %c8192_i32 = arith.constant 8192 : i32
    %cst_3 = arith.constant dense<0.000000e+00> : tensor<128x64xf32>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<128xf32>
    %cst_5 = arith.constant dense<0xFF800000> : tensor<128xf32>
    %c192_i32 = arith.constant 192 : i32
    %c2_i32 = arith.constant 2 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.muli %0, %c2_i32 : i32
    %2 = arith.addi %1, %c2_i32 : i32
    %3 = arith.minsi %2, %c192_i32 : i32
    scf.for %arg6 = %1 to %3 step %c1_i32  : i32 {
      %4 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
      %5 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>
      %6 = arith.muli %arg6, %c8192_i32 : i32
      %7 = tt.expand_dims %5 {axis = 1 : i32} : tensor<128xi32> -> tensor<128x1xi32>
      %8 = arith.muli %7, %cst_2 : tensor<128x1xi32>
      %9 = tt.splat %6 : i32 -> tensor<128x1xi32>
      %10 = arith.addi %9, %8 : tensor<128x1xi32>
      %11 = tt.expand_dims %4 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %12 = tt.broadcast %10 : tensor<128x1xi32> -> tensor<128x64xi32>
      %13 = tt.broadcast %11 : tensor<1x64xi32> -> tensor<128x64xi32>
      %14 = arith.addi %12, %13 : tensor<128x64xi32>
      %15 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %16 = tt.addptr %15, %14 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      %17 = tt.load %16 : tensor<128x64x!tt.ptr<f8E5M2>>
      %18:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c64_i32 iter_args(%arg8 = %cst_4, %arg9 = %cst_3, %arg10 = %cst_5) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %44 = tt.splat %arg7 : i32 -> tensor<64xi32>
        %45 = arith.addi %44, %4 : tensor<64xi32>
        %46 = tt.expand_dims %45 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %47 = arith.muli %46, %cst_1 : tensor<64x1xi32>
        %48 = tt.splat %6 : i32 -> tensor<64x1xi32>
        %49 = arith.addi %48, %47 : tensor<64x1xi32>
        %50 = tt.broadcast %49 : tensor<64x1xi32> -> tensor<64x64xi32>
        %51 = tt.broadcast %11 : tensor<1x64xi32> -> tensor<64x64xi32>
        %52 = arith.addi %50, %51 : tensor<64x64xi32>
        %53 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %54 = tt.addptr %53, %52 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %55 = tt.load %54 : tensor<64x64x!tt.ptr<f8E5M2>>
        %56 = tt.trans %55 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %57 = tt.dot %17, %56, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %58 = arith.mulf %57, %cst_0 : tensor<128x64xf32>
        %59 = "tt.reduce"(%58) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %90 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %90 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %60 = arith.cmpf ogt, %arg10, %59 : tensor<128xf32>
        %61 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %62 = arith.ori %60, %61 : tensor<128xi1>
        %63 = arith.select %62, %arg10, %59 : tensor<128xi1>, tensor<128xf32>
        %64 = tt.expand_dims %63 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %65 = tt.broadcast %64 : tensor<128x1xf32> -> tensor<128x64xf32>
        %66 = arith.subf %58, %65 : tensor<128x64xf32>
        %67 = tt.extern_elementwise %66 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %68 = "tt.reduce"(%67) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %90 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %90 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %69 = arith.subf %arg10, %63 : tensor<128xf32>
        %70 = tt.extern_elementwise %69 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %71 = arith.mulf %arg8, %70 : tensor<128xf32>
        %72 = arith.addf %71, %68 : tensor<128xf32>
        %73 = tt.expand_dims %70 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %74 = tt.broadcast %73 : tensor<128x1xf32> -> tensor<128x64xf32>
        %75 = arith.mulf %arg9, %74 : tensor<128x64xf32>
        %76 = tt.expand_dims %4 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %77 = arith.addi %48, %76 : tensor<64x1xi32>
        %78 = tt.expand_dims %45 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %79 = arith.muli %78, %cst : tensor<1x64xi32>
        %80 = tt.broadcast %77 : tensor<64x1xi32> -> tensor<64x64xi32>
        %81 = tt.broadcast %79 : tensor<1x64xi32> -> tensor<64x64xi32>
        %82 = arith.addi %80, %81 : tensor<64x64xi32>
        %83 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %84 = tt.addptr %83, %82 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %85 = tt.load %84 : tensor<64x64x!tt.ptr<f8E5M2>>
        %86 = tt.fp_to_fp %67, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %87 = tt.trans %85 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %88 = tt.dot %86, %87, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %89 = arith.addf %75, %88 : tensor<128x64xf32>
        scf.yield %72, %89, %63 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.loop_unroll_factor = 1 : i32}
      %19 = tt.expand_dims %18#0 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
      %20 = tt.broadcast %19 : tensor<128x1xf32> -> tensor<128x64xf32>
      %21 = arith.divf %18#1, %20 : tensor<128x64xf32>
      %22 = tt.fp_to_fp %21, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
      %23 = arith.divsi %arg6, %arg5 : i32
      %24 = arith.remsi %arg6, %arg5 : i32
      %25 = arith.cmpi ne, %24, %c0_i32 : i32
      %26 = arith.subi %23, %c1_i32 : i32
      %27 = arith.select %25, %26, %23 : i32
      %28 = arith.cmpi slt, %arg6, %c0_i32 : i32
      %29 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %30 = arith.cmpi ne, %28, %29 : i1
      %31 = arith.select %30, %27, %23 : i32
      %32 = arith.andi %25, %30 : i1
      %33 = arith.addi %24, %arg5 : i32
      %34 = arith.select %32, %33, %24 : i32
      %35 = arith.muli %31, %arg4 : i32
      %36 = arith.muli %34, %c8192_i32 : i32
      %37 = arith.addi %35, %36 : i32
      %38 = tt.splat %37 : i32 -> tensor<128x1xi32>
      %39 = arith.addi %38, %8 : tensor<128x1xi32>
      %40 = tt.broadcast %39 : tensor<128x1xi32> -> tensor<128x64xi32>
      %41 = arith.addi %40, %13 : tensor<128x64xi32>
      %42 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %43 = tt.addptr %42, %41 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      tt.store %43, %22 : tensor<128x64x!tt.ptr<f8E5M2>>
    } {tt.flatten}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(convert-triton-to-tritongpu{enable-source-remat=false num-ctas=1 num-warps=4 target=cuda:90 threads-per-warp=32}, tritongpu-coalesce, tritongpu-F32DotTC, triton-nvidia-gpu-plan-cta, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritongpu-accelerate-matmul, tritongpu-remove-layout-conversions, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, triton-nvidia-optimize-descriptor-encoding, triton-loop-aware-cse, tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-combine-tensor-select-and-if, nvgpu-warp-specialization{dump-intermediate-steps=false num-stages=8}, tritongpu-assign-latencies{num-stages=8}, tritongpu-schedule-loops, tritongpu-pipeline{dump-intermediate-steps=false num-stages=8}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-loop-aware-cse, tritongpu-prefetch, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-coalesce-async-copy, triton-nvidia-optimize-tmem-layouts, tritongpu-remove-layout-conversions, triton-nvidia-interleave-tmem, tritongpu-reduce-data-duplication, tritongpu-reorder-instructions, triton-loop-aware-cse, symbol-dce, triton-nvidia-tma-lowering, triton-nvidia-gpu-fence-insertion{compute-capability=90}, sccp, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true})",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_willfeng/sy/csyxm6fx4nzgqwlkrh5k4u3obqbo4q6wax2v4kw7hzjgbnttgofj.py:13:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_willfeng/sy/csyxm6fx4nzgqwlkrh5k4u3obqbo4q6wax2v4kw7hzjgbnttgofj.py:13:0: note: Pipeline failed while executing [`TritonGPUAssignLatencies` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
[713s] Generation 6: replaced=11 min=0.0113 mid=0.0147 max=0.0415 best=Config(block_sizes=[128, 128], range_unroll_factors=[0, 2, 2], range_num_stages=[0, 4, 2], range_multi_buffers=[None, False, True], range_flattens=[None, False, False], num_warps=8, num_stages=7, indexing='tensor_descriptor', pid_type='flat', range_warp_specializes=[])
python: /root/.triton/llvm/llvm-8957e64a-almalinux-x64/include/llvm/ADT/SmallVector.h:296: const_reference llvm::SmallVectorTemplateCommon<long>::operator[](size_type) const [T = long]: Assertion `idx < size()' failed.
module {
  tt.func public @_fp8_attention_kernel_kernel(%arg0: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %c1_i32 = arith.constant 1 : i32
    %c32_i32 = arith.constant 32 : i32
    %c128_i32 = arith.constant 128 : i32
    %c0_i32 = arith.constant 0 : i32
    %c132_i32 = arith.constant 132 : i32
    %c192_i32 = arith.constant 192 : i32
    %cst = arith.constant dense<64> : tensor<1x32xi32>
    %cst_0 = arith.constant dense<0.180336878> : tensor<128x32xf32>
    %cst_1 = arith.constant dense<0.000000e+00> : tensor<128x32xf32>
    %cst_2 = arith.constant dense<64> : tensor<32x1xi32>
    %cst_3 = arith.constant dense<64> : tensor<128x1xi32>
    %c8192_i32 = arith.constant 8192 : i32
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<128x64xf32>
    %cst_5 = arith.constant dense<0.000000e+00> : tensor<128xf32>
    %cst_6 = arith.constant dense<0xFF800000> : tensor<128xf32>
    %0 = tt.get_program_id x : i32
    %1 = arith.subi %c192_i32, %0 : i32
    %c1_i32_7 = arith.constant 1 : i32
    %2 = arith.subi %c132_i32, %c1_i32_7 : i32
    %3 = arith.addi %1, %2 : i32
    %4 = arith.divui %3, %c132_i32 : i32
    %c2_i32 = arith.constant 2 : i32
    %5 = arith.remsi %4, %c2_i32 : i32
    %6 = arith.subi %4, %5 : i32
    %7 = arith.muli %6, %c132_i32 : i32
    %8 = arith.addi %0, %7 : i32
    %9 = arith.muli %c132_i32, %c2_i32 : i32
    scf.for %arg6 = %0 to %8 step %9  : i32 {
      %10 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
      %11 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>
      %12 = arith.muli %arg6, %c8192_i32 : i32
      %13 = tt.expand_dims %11 {axis = 1 : i32} : tensor<128xi32> -> tensor<128x1xi32>
      %14 = arith.muli %13, %cst_3 : tensor<128x1xi32>
      %15 = tt.splat %12 : i32 -> tensor<128x1xi32>
      %16 = arith.addi %15, %14 : tensor<128x1xi32>
      %17 = tt.expand_dims %10 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %18 = tt.broadcast %16 : tensor<128x1xi32> -> tensor<128x64xi32>
      %19 = tt.broadcast %17 : tensor<1x64xi32> -> tensor<128x64xi32>
      %20 = arith.addi %18, %19 : tensor<128x64xi32>
      %21 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %22 = tt.addptr %21, %20 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      %23 = tt.load %22 : tensor<128x64x!tt.ptr<f8E5M2>>
      %24:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c32_i32 iter_args(%arg8 = %cst_5, %arg9 = %cst_4, %arg10 = %cst_6) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %92 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32>
        %93 = tt.splat %arg7 : i32 -> tensor<32xi32>
        %94 = arith.addi %93, %92 : tensor<32xi32>
        %95 = tt.expand_dims %94 {axis = 1 : i32} : tensor<32xi32> -> tensor<32x1xi32>
        %96 = arith.muli %95, %cst_2 : tensor<32x1xi32>
        %97 = tt.splat %12 : i32 -> tensor<32x1xi32>
        %98 = arith.addi %97, %96 : tensor<32x1xi32>
        %99 = tt.broadcast %98 : tensor<32x1xi32> -> tensor<32x64xi32>
        %100 = tt.broadcast %17 : tensor<1x64xi32> -> tensor<32x64xi32>
        %101 = arith.addi %99, %100 : tensor<32x64xi32>
        %102 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<32x64x!tt.ptr<f8E5M2>>
        %103 = tt.addptr %102, %101 : tensor<32x64x!tt.ptr<f8E5M2>>, tensor<32x64xi32>
        %104 = tt.load %103 : tensor<32x64x!tt.ptr<f8E5M2>>
        %105 = tt.trans %104 {order = array<i32: 1, 0>} : tensor<32x64xf8E5M2> -> tensor<64x32xf8E5M2>
        %106 = tt.dot %23, %105, %cst_1, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x32xf8E5M2> -> tensor<128x32xf32>
        %107 = arith.mulf %106, %cst_0 : tensor<128x32xf32>
        %108 = "tt.reduce"(%107) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %140 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %140 : f32
        }) : (tensor<128x32xf32>) -> tensor<128xf32>
        %109 = arith.cmpf ogt, %arg10, %108 : tensor<128xf32>
        %110 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %111 = arith.ori %109, %110 : tensor<128xi1>
        %112 = arith.select %111, %arg10, %108 : tensor<128xi1>, tensor<128xf32>
        %113 = tt.expand_dims %112 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %114 = tt.broadcast %113 : tensor<128x1xf32> -> tensor<128x32xf32>
        %115 = arith.subf %107, %114 : tensor<128x32xf32>
        %116 = tt.extern_elementwise %115 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x32xf32>) -> tensor<128x32xf32>
        %117 = "tt.reduce"(%116) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %140 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %140 : f32
        }) : (tensor<128x32xf32>) -> tensor<128xf32>
        %118 = arith.subf %arg10, %112 : tensor<128xf32>
        %119 = tt.extern_elementwise %118 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %120 = arith.mulf %arg8, %119 : tensor<128xf32>
        %121 = arith.addf %120, %117 : tensor<128xf32>
        %122 = tt.expand_dims %119 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %123 = tt.broadcast %122 : tensor<128x1xf32> -> tensor<128x64xf32>
        %124 = arith.mulf %arg9, %123 : tensor<128x64xf32>
        %125 = tt.expand_dims %10 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %126 = tt.splat %12 : i32 -> tensor<64x1xi32>
        %127 = arith.addi %126, %125 : tensor<64x1xi32>
        %128 = tt.expand_dims %94 {axis = 0 : i32} : tensor<32xi32> -> tensor<1x32xi32>
        %129 = arith.muli %128, %cst : tensor<1x32xi32>
        %130 = tt.broadcast %127 : tensor<64x1xi32> -> tensor<64x32xi32>
        %131 = tt.broadcast %129 : tensor<1x32xi32> -> tensor<64x32xi32>
        %132 = arith.addi %130, %131 : tensor<64x32xi32>
        %133 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x32x!tt.ptr<f8E5M2>>
        %134 = tt.addptr %133, %132 : tensor<64x32x!tt.ptr<f8E5M2>>, tensor<64x32xi32>
        %135 = tt.load %134 : tensor<64x32x!tt.ptr<f8E5M2>>
        %136 = tt.fp_to_fp %116, rounding = rtne : tensor<128x32xf32> -> tensor<128x32xf8E5M2>
        %137 = tt.trans %135 {order = array<i32: 1, 0>} : tensor<64x32xf8E5M2> -> tensor<32x64xf8E5M2>
        %138 = tt.dot %136, %137, %cst_4, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x32xf8E5M2> * tensor<32x64xf8E5M2> -> tensor<128x64xf32>
        %139 = arith.addf %124, %138 : tensor<128x64xf32>
        scf.yield %121, %139, %112 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.num_stages = 3 : i32}
      %25 = tt.expand_dims %24#0 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
      %26 = tt.broadcast %25 : tensor<128x1xf32> -> tensor<128x64xf32>
      %27 = arith.divf %24#1, %26 : tensor<128x64xf32>
      %28 = tt.fp_to_fp %27, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
      %29 = arith.divsi %arg6, %arg5 : i32
      %30 = arith.remsi %arg6, %arg5 : i32
      %31 = arith.cmpi ne, %30, %c0_i32 : i32
      %32 = arith.subi %29, %c1_i32 : i32
      %33 = arith.select %31, %32, %29 : i32
      %34 = arith.cmpi slt, %arg6, %c0_i32 : i32
      %35 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %36 = arith.cmpi ne, %34, %35 : i1
      %37 = arith.select %36, %33, %29 : i32
      %38 = arith.andi %31, %36 : i1
      %39 = arith.addi %30, %arg5 : i32
      %40 = arith.select %38, %39, %30 : i32
      %41 = arith.muli %37, %arg4 : i32
      %42 = arith.muli %40, %c8192_i32 : i32
      %43 = arith.addi %41, %42 : i32
      %44 = tt.splat %43 : i32 -> tensor<128x1xi32>
      %45 = arith.addi %44, %14 : tensor<128x1xi32>
      %46 = tt.broadcast %45 : tensor<128x1xi32> -> tensor<128x64xi32>
      %47 = arith.addi %46, %19 : tensor<128x64xi32>
      %48 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %49 = tt.addptr %48, %47 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      tt.store %49, %28 : tensor<128x64x!tt.ptr<f8E5M2>>
      %c1_i32_8 = arith.constant 1 : i32
      %50 = arith.muli %c132_i32, %c1_i32_8 : i32
      %51 = arith.addi %arg6, %50 : i32
      %52 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
      %53 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>
      %54 = arith.muli %51, %c8192_i32 : i32
      %55 = tt.expand_dims %53 {axis = 1 : i32} : tensor<128xi32> -> tensor<128x1xi32>
      %56 = arith.muli %55, %cst_3 : tensor<128x1xi32>
      %57 = tt.splat %54 : i32 -> tensor<128x1xi32>
      %58 = arith.addi %57, %56 : tensor<128x1xi32>
      %59 = tt.expand_dims %52 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %60 = tt.broadcast %58 : tensor<128x1xi32> -> tensor<128x64xi32>
      %61 = tt.broadcast %59 : tensor<1x64xi32> -> tensor<128x64xi32>
      %62 = arith.addi %60, %61 : tensor<128x64xi32>
      %63 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %64 = tt.addptr %63, %62 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      %65 = tt.load %64 : tensor<128x64x!tt.ptr<f8E5M2>>
      %66:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c32_i32 iter_args(%arg8 = %cst_5, %arg9 = %cst_4, %arg10 = %cst_6) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %92 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32>
        %93 = tt.splat %arg7 : i32 -> tensor<32xi32>
        %94 = arith.addi %93, %92 : tensor<32xi32>
        %95 = tt.expand_dims %94 {axis = 1 : i32} : tensor<32xi32> -> tensor<32x1xi32>
        %96 = arith.muli %95, %cst_2 : tensor<32x1xi32>
        %97 = tt.splat %54 : i32 -> tensor<32x1xi32>
        %98 = arith.addi %97, %96 : tensor<32x1xi32>
        %99 = tt.broadcast %98 : tensor<32x1xi32> -> tensor<32x64xi32>
        %100 = tt.broadcast %59 : tensor<1x64xi32> -> tensor<32x64xi32>
        %101 = arith.addi %99, %100 : tensor<32x64xi32>
        %102 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<32x64x!tt.ptr<f8E5M2>>
        %103 = tt.addptr %102, %101 : tensor<32x64x!tt.ptr<f8E5M2>>, tensor<32x64xi32>
        %104 = tt.load %103 : tensor<32x64x!tt.ptr<f8E5M2>>
        %105 = tt.trans %104 {order = array<i32: 1, 0>} : tensor<32x64xf8E5M2> -> tensor<64x32xf8E5M2>
        %106 = tt.dot %65, %105, %cst_1, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x32xf8E5M2> -> tensor<128x32xf32>
        %107 = arith.mulf %106, %cst_0 : tensor<128x32xf32>
        %108 = "tt.reduce"(%107) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %140 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %140 : f32
        }) : (tensor<128x32xf32>) -> tensor<128xf32>
        %109 = arith.cmpf ogt, %arg10, %108 : tensor<128xf32>
        %110 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %111 = arith.ori %109, %110 : tensor<128xi1>
        %112 = arith.select %111, %arg10, %108 : tensor<128xi1>, tensor<128xf32>
        %113 = tt.expand_dims %112 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %114 = tt.broadcast %113 : tensor<128x1xf32> -> tensor<128x32xf32>
        %115 = arith.subf %107, %114 : tensor<128x32xf32>
        %116 = tt.extern_elementwise %115 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x32xf32>) -> tensor<128x32xf32>
        %117 = "tt.reduce"(%116) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %140 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %140 : f32
        }) : (tensor<128x32xf32>) -> tensor<128xf32>
        %118 = arith.subf %arg10, %112 : tensor<128xf32>
        %119 = tt.extern_elementwise %118 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %120 = arith.mulf %arg8, %119 : tensor<128xf32>
        %121 = arith.addf %120, %117 : tensor<128xf32>
        %122 = tt.expand_dims %119 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %123 = tt.broadcast %122 : tensor<128x1xf32> -> tensor<128x64xf32>
        %124 = arith.mulf %arg9, %123 : tensor<128x64xf32>
        %125 = tt.expand_dims %52 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %126 = tt.splat %54 : i32 -> tensor<64x1xi32>
        %127 = arith.addi %126, %125 : tensor<64x1xi32>
        %128 = tt.expand_dims %94 {axis = 0 : i32} : tensor<32xi32> -> tensor<1x32xi32>
        %129 = arith.muli %128, %cst : tensor<1x32xi32>
        %130 = tt.broadcast %127 : tensor<64x1xi32> -> tensor<64x32xi32>
        %131 = tt.broadcast %129 : tensor<1x32xi32> -> tensor<64x32xi32>
        %132 = arith.addi %130, %131 : tensor<64x32xi32>
        %133 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x32x!tt.ptr<f8E5M2>>
        %134 = tt.addptr %133, %132 : tensor<64x32x!tt.ptr<f8E5M2>>, tensor<64x32xi32>
        %135 = tt.load %134 : tensor<64x32x!tt.ptr<f8E5M2>>
        %136 = tt.fp_to_fp %116, rounding = rtne : tensor<128x32xf32> -> tensor<128x32xf8E5M2>
        %137 = tt.trans %135 {order = array<i32: 1, 0>} : tensor<64x32xf8E5M2> -> tensor<32x64xf8E5M2>
        %138 = tt.dot %136, %137, %cst_4, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x32xf8E5M2> * tensor<32x64xf8E5M2> -> tensor<128x64xf32>
        %139 = arith.addf %124, %138 : tensor<128x64xf32>
        scf.yield %121, %139, %112 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.num_stages = 3 : i32}
      %67 = tt.expand_dims %66#0 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
      %68 = tt.broadcast %67 : tensor<128x1xf32> -> tensor<128x64xf32>
      %69 = arith.divf %66#1, %68 : tensor<128x64xf32>
      %70 = tt.fp_to_fp %69, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
      %71 = arith.divsi %51, %arg5 : i32
      %72 = arith.remsi %51, %arg5 : i32
      %73 = arith.cmpi ne, %72, %c0_i32 : i32
      %74 = arith.subi %71, %c1_i32 : i32
      %75 = arith.select %73, %74, %71 : i32
      %76 = arith.cmpi slt, %51, %c0_i32 : i32
      %77 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %78 = arith.cmpi ne, %76, %77 : i1
      %79 = arith.select %78, %75, %71 : i32
      %80 = arith.andi %73, %78 : i1
      %81 = arith.addi %72, %arg5 : i32
      %82 = arith.select %80, %81, %72 : i32
      %83 = arith.muli %79, %arg4 : i32
      %84 = arith.muli %82, %c8192_i32 : i32
      %85 = arith.addi %83, %84 : i32
      %86 = tt.splat %85 : i32 -> tensor<128x1xi32>
      %87 = arith.addi %86, %56 : tensor<128x1xi32>
      %88 = tt.broadcast %87 : tensor<128x1xi32> -> tensor<128x64xi32>
      %89 = arith.addi %88, %61 : tensor<128x64xi32>
      %90 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %91 = tt.addptr %90, %89 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      tt.store %91, %70 : tensor<128x64x!tt.ptr<f8E5M2>>
    } {tt.flatten, tt.num_stages = 1 : i32}
    scf.for %arg6 = %8 to %c192_i32 step %c132_i32  : i32 {
      %10 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
      %11 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>
      %12 = arith.muli %arg6, %c8192_i32 : i32
      %13 = tt.expand_dims %11 {axis = 1 : i32} : tensor<128xi32> -> tensor<128x1xi32>
      %14 = arith.muli %13, %cst_3 : tensor<128x1xi32>
      %15 = tt.splat %12 : i32 -> tensor<128x1xi32>
      %16 = arith.addi %15, %14 : tensor<128x1xi32>
      %17 = tt.expand_dims %10 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %18 = tt.broadcast %16 : tensor<128x1xi32> -> tensor<128x64xi32>
      %19 = tt.broadcast %17 : tensor<1x64xi32> -> tensor<128x64xi32>
      %20 = arith.addi %18, %19 : tensor<128x64xi32>
      %21 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %22 = tt.addptr %21, %20 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      %23 = tt.load %22 : tensor<128x64x!tt.ptr<f8E5M2>>
      %24:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c32_i32 iter_args(%arg8 = %cst_5, %arg9 = %cst_4, %arg10 = %cst_6) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %50 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32>
        %51 = tt.splat %arg7 : i32 -> tensor<32xi32>
        %52 = arith.addi %51, %50 : tensor<32xi32>
        %53 = tt.expand_dims %52 {axis = 1 : i32} : tensor<32xi32> -> tensor<32x1xi32>
        %54 = arith.muli %53, %cst_2 : tensor<32x1xi32>
        %55 = tt.splat %12 : i32 -> tensor<32x1xi32>
        %56 = arith.addi %55, %54 : tensor<32x1xi32>
        %57 = tt.broadcast %56 : tensor<32x1xi32> -> tensor<32x64xi32>
        %58 = tt.broadcast %17 : tensor<1x64xi32> -> tensor<32x64xi32>
        %59 = arith.addi %57, %58 : tensor<32x64xi32>
        %60 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<32x64x!tt.ptr<f8E5M2>>
        %61 = tt.addptr %60, %59 : tensor<32x64x!tt.ptr<f8E5M2>>, tensor<32x64xi32>
        %62 = tt.load %61 : tensor<32x64x!tt.ptr<f8E5M2>>
        %63 = tt.trans %62 {order = array<i32: 1, 0>} : tensor<32x64xf8E5M2> -> tensor<64x32xf8E5M2>
        %64 = tt.dot %23, %63, %cst_1, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x32xf8E5M2> -> tensor<128x32xf32>
        %65 = arith.mulf %64, %cst_0 : tensor<128x32xf32>
        %66 = "tt.reduce"(%65) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %98 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %98 : f32
        }) : (tensor<128x32xf32>) -> tensor<128xf32>
        %67 = arith.cmpf ogt, %arg10, %66 : tensor<128xf32>
        %68 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %69 = arith.ori %67, %68 : tensor<128xi1>
        %70 = arith.select %69, %arg10, %66 : tensor<128xi1>, tensor<128xf32>
        %71 = tt.expand_dims %70 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %72 = tt.broadcast %71 : tensor<128x1xf32> -> tensor<128x32xf32>
        %73 = arith.subf %65, %72 : tensor<128x32xf32>
        %74 = tt.extern_elementwise %73 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x32xf32>) -> tensor<128x32xf32>
        %75 = "tt.reduce"(%74) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %98 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %98 : f32
        }) : (tensor<128x32xf32>) -> tensor<128xf32>
        %76 = arith.subf %arg10, %70 : tensor<128xf32>
        %77 = tt.extern_elementwise %76 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %78 = arith.mulf %arg8, %77 : tensor<128xf32>
        %79 = arith.addf %78, %75 : tensor<128xf32>
        %80 = tt.expand_dims %77 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %81 = tt.broadcast %80 : tensor<128x1xf32> -> tensor<128x64xf32>
        %82 = arith.mulf %arg9, %81 : tensor<128x64xf32>
        %83 = tt.expand_dims %10 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %84 = tt.splat %12 : i32 -> tensor<64x1xi32>
        %85 = arith.addi %84, %83 : tensor<64x1xi32>
        %86 = tt.expand_dims %52 {axis = 0 : i32} : tensor<32xi32> -> tensor<1x32xi32>
        %87 = arith.muli %86, %cst : tensor<1x32xi32>
        %88 = tt.broadcast %85 : tensor<64x1xi32> -> tensor<64x32xi32>
        %89 = tt.broadcast %87 : tensor<1x32xi32> -> tensor<64x32xi32>
        %90 = arith.addi %88, %89 : tensor<64x32xi32>
        %91 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x32x!tt.ptr<f8E5M2>>
        %92 = tt.addptr %91, %90 : tensor<64x32x!tt.ptr<f8E5M2>>, tensor<64x32xi32>
        %93 = tt.load %92 : tensor<64x32x!tt.ptr<f8E5M2>>
        %94 = tt.fp_to_fp %74, rounding = rtne : tensor<128x32xf32> -> tensor<128x32xf8E5M2>
        %95 = tt.trans %93 {order = array<i32: 1, 0>} : tensor<64x32xf8E5M2> -> tensor<32x64xf8E5M2>
        %96 = tt.dot %94, %95, %cst_4, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x32xf8E5M2> * tensor<32x64xf8E5M2> -> tensor<128x64xf32>
        %97 = arith.addf %82, %96 : tensor<128x64xf32>
        scf.yield %79, %97, %70 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.num_stages = 3 : i32}
      %25 = tt.expand_dims %24#0 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
      %26 = tt.broadcast %25 : tensor<128x1xf32> -> tensor<128x64xf32>
      %27 = arith.divf %24#1, %26 : tensor<128x64xf32>
      %28 = tt.fp_to_fp %27, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
      %29 = arith.divsi %arg6, %arg5 : i32
      %30 = arith.remsi %arg6, %arg5 : i32
      %31 = arith.cmpi ne, %30, %c0_i32 : i32
      %32 = arith.subi %29, %c1_i32 : i32
      %33 = arith.select %31, %32, %29 : i32
      %34 = arith.cmpi slt, %arg6, %c0_i32 : i32
      %35 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %36 = arith.cmpi ne, %34, %35 : i1
      %37 = arith.select %36, %33, %29 : i32
      %38 = arith.andi %31, %36 : i1
      %39 = arith.addi %30, %arg5 : i32
      %40 = arith.select %38, %39, %30 : i32
      %41 = arith.muli %37, %arg4 : i32
      %42 = arith.muli %40, %c8192_i32 : i32
      %43 = arith.addi %41, %42 : i32
      %44 = tt.splat %43 : i32 -> tensor<128x1xi32>
      %45 = arith.addi %44, %14 : tensor<128x1xi32>
      %46 = tt.broadcast %45 : tensor<128x1xi32> -> tensor<128x64xi32>
      %47 = arith.addi %46, %19 : tensor<128x64xi32>
      %48 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %49 = tt.addptr %48, %47 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      tt.store %49, %28 : tensor<128x64x!tt.ptr<f8E5M2>>
    } {tt.flatten, tt.num_stages = 1 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(convert-triton-to-tritongpu{enable-source-remat=false num-ctas=1 num-warps=4 target=cuda:90 threads-per-warp=32}, tritongpu-coalesce, tritongpu-F32DotTC, triton-nvidia-gpu-plan-cta, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritongpu-accelerate-matmul, tritongpu-remove-layout-conversions, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, triton-nvidia-optimize-descriptor-encoding, triton-loop-aware-cse, tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-combine-tensor-select-and-if, nvgpu-warp-specialization{dump-intermediate-steps=false num-stages=4}, tritongpu-assign-latencies{num-stages=4}, tritongpu-schedule-loops, tritongpu-pipeline{dump-intermediate-steps=false num-stages=4}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-loop-aware-cse, tritongpu-prefetch, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-coalesce-async-copy, triton-nvidia-optimize-tmem-layouts, tritongpu-remove-layout-conversions, triton-nvidia-interleave-tmem, tritongpu-reduce-data-duplication, tritongpu-reorder-instructions, triton-loop-aware-cse, symbol-dce, triton-nvidia-tma-lowering, triton-nvidia-gpu-fence-insertion{compute-capability=90}, sccp, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true})",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_willfeng/b3/cb3rpcsfds3occkgirnum6bxwhd5tgim2lwn3do45yfp6uho45co.py:13:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_willfeng/b3/cb3rpcsfds3occkgirnum6bxwhd5tgim2lwn3do45yfp6uho45co.py:13:0: note: Pipeline failed while executing [`TritonGPUAssignLatencies` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
Process ForkProcess-481:
Traceback (most recent call last):
  File "/home/willfeng/local/miniconda3/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/willfeng/local/miniconda3/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/data/users/willfeng/helion/helion/runtime/precompile_shim.py", line 54, in finish_it
    kernel_cache[key] = fn.compile(
                        ^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/compiler/compiler.py", line 359, in compile
    next_module = compile_ir(module, metadata)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/backends/nvidia/compiler.py", line 456, in <lambda>
    stages["ttgir"] = lambda src, metadata: self.make_ttgir(src, metadata, options, capability)
                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/backends/nvidia/compiler.py", line 298, in make_ttgir
    pm.run(mod)
RuntimeError: PassManager::run failed
python: /root/.triton/llvm/llvm-8957e64a-almalinux-x64/include/llvm/ADT/SmallVector.h:296: const_reference llvm::SmallVectorTemplateCommon<long>::operator[](size_type) const [T = long]: Assertion `idx < size()' failed.
module {
  tt.func public @_fp8_attention_kernel_kernel(%arg0: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %c1_i32 = arith.constant 1 : i32
    %c32_i32 = arith.constant 32 : i32
    %c128_i32 = arith.constant 128 : i32
    %c0_i32 = arith.constant 0 : i32
    %c132_i32 = arith.constant 132 : i32
    %c192_i32 = arith.constant 192 : i32
    %cst = arith.constant dense<64> : tensor<1x32xi32>
    %cst_0 = arith.constant dense<0.180336878> : tensor<128x32xf32>
    %cst_1 = arith.constant dense<0.000000e+00> : tensor<128x32xf32>
    %cst_2 = arith.constant dense<64> : tensor<32x1xi32>
    %cst_3 = arith.constant dense<64> : tensor<128x1xi32>
    %c8192_i32 = arith.constant 8192 : i32
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<128x64xf32>
    %cst_5 = arith.constant dense<0.000000e+00> : tensor<128xf32>
    %cst_6 = arith.constant dense<0xFF800000> : tensor<128xf32>
    %0 = tt.get_program_id x : i32
    %1 = arith.subi %c192_i32, %0 : i32
    %c1_i32_7 = arith.constant 1 : i32
    %2 = arith.subi %c132_i32, %c1_i32_7 : i32
    %3 = arith.addi %1, %2 : i32
    %4 = arith.divui %3, %c132_i32 : i32
    %c2_i32 = arith.constant 2 : i32
    %5 = arith.remsi %4, %c2_i32 : i32
    %6 = arith.subi %4, %5 : i32
    %7 = arith.muli %6, %c132_i32 : i32
    %8 = arith.addi %0, %7 : i32
    %9 = arith.muli %c132_i32, %c2_i32 : i32
    scf.for %arg6 = %0 to %8 step %9  : i32 {
      %10 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
      %11 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>
      %12 = arith.muli %arg6, %c8192_i32 : i32
      %13 = tt.expand_dims %11 {axis = 1 : i32} : tensor<128xi32> -> tensor<128x1xi32>
      %14 = arith.muli %13, %cst_3 : tensor<128x1xi32>
      %15 = tt.splat %12 : i32 -> tensor<128x1xi32>
      %16 = arith.addi %15, %14 : tensor<128x1xi32>
      %17 = tt.expand_dims %10 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %18 = tt.broadcast %16 : tensor<128x1xi32> -> tensor<128x64xi32>
      %19 = tt.broadcast %17 : tensor<1x64xi32> -> tensor<128x64xi32>
      %20 = arith.addi %18, %19 : tensor<128x64xi32>
      %21 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %22 = tt.addptr %21, %20 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      %23 = tt.load %22 : tensor<128x64x!tt.ptr<f8E5M2>>
      %24:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c32_i32 iter_args(%arg8 = %cst_5, %arg9 = %cst_4, %arg10 = %cst_6) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %92 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32>
        %93 = tt.splat %arg7 : i32 -> tensor<32xi32>
        %94 = arith.addi %93, %92 : tensor<32xi32>
        %95 = tt.expand_dims %94 {axis = 1 : i32} : tensor<32xi32> -> tensor<32x1xi32>
        %96 = arith.muli %95, %cst_2 : tensor<32x1xi32>
        %97 = tt.splat %12 : i32 -> tensor<32x1xi32>
        %98 = arith.addi %97, %96 : tensor<32x1xi32>
        %99 = tt.broadcast %98 : tensor<32x1xi32> -> tensor<32x64xi32>
        %100 = tt.broadcast %17 : tensor<1x64xi32> -> tensor<32x64xi32>
        %101 = arith.addi %99, %100 : tensor<32x64xi32>
        %102 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<32x64x!tt.ptr<f8E5M2>>
        %103 = tt.addptr %102, %101 : tensor<32x64x!tt.ptr<f8E5M2>>, tensor<32x64xi32>
        %104 = tt.load %103 : tensor<32x64x!tt.ptr<f8E5M2>>
        %105 = tt.trans %104 {order = array<i32: 1, 0>} : tensor<32x64xf8E5M2> -> tensor<64x32xf8E5M2>
        %106 = tt.dot %23, %105, %cst_1, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x32xf8E5M2> -> tensor<128x32xf32>
        %107 = arith.mulf %106, %cst_0 : tensor<128x32xf32>
        %108 = "tt.reduce"(%107) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %140 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %140 : f32
        }) : (tensor<128x32xf32>) -> tensor<128xf32>
        %109 = arith.cmpf ogt, %arg10, %108 : tensor<128xf32>
        %110 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %111 = arith.ori %109, %110 : tensor<128xi1>
        %112 = arith.select %111, %arg10, %108 : tensor<128xi1>, tensor<128xf32>
        %113 = tt.expand_dims %112 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %114 = tt.broadcast %113 : tensor<128x1xf32> -> tensor<128x32xf32>
        %115 = arith.subf %107, %114 : tensor<128x32xf32>
        %116 = tt.extern_elementwise %115 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x32xf32>) -> tensor<128x32xf32>
        %117 = "tt.reduce"(%116) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %140 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %140 : f32
        }) : (tensor<128x32xf32>) -> tensor<128xf32>
        %118 = arith.subf %arg10, %112 : tensor<128xf32>
        %119 = tt.extern_elementwise %118 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %120 = arith.mulf %arg8, %119 : tensor<128xf32>
        %121 = arith.addf %120, %117 : tensor<128xf32>
        %122 = tt.expand_dims %119 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %123 = tt.broadcast %122 : tensor<128x1xf32> -> tensor<128x64xf32>
        %124 = arith.mulf %arg9, %123 : tensor<128x64xf32>
        %125 = tt.expand_dims %10 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %126 = tt.splat %12 : i32 -> tensor<64x1xi32>
        %127 = arith.addi %126, %125 : tensor<64x1xi32>
        %128 = tt.expand_dims %94 {axis = 0 : i32} : tensor<32xi32> -> tensor<1x32xi32>
        %129 = arith.muli %128, %cst : tensor<1x32xi32>
        %130 = tt.broadcast %127 : tensor<64x1xi32> -> tensor<64x32xi32>
        %131 = tt.broadcast %129 : tensor<1x32xi32> -> tensor<64x32xi32>
        %132 = arith.addi %130, %131 : tensor<64x32xi32>
        %133 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x32x!tt.ptr<f8E5M2>>
        %134 = tt.addptr %133, %132 : tensor<64x32x!tt.ptr<f8E5M2>>, tensor<64x32xi32>
        %135 = tt.load %134 : tensor<64x32x!tt.ptr<f8E5M2>>
        %136 = tt.fp_to_fp %116, rounding = rtne : tensor<128x32xf32> -> tensor<128x32xf8E5M2>
        %137 = tt.trans %135 {order = array<i32: 1, 0>} : tensor<64x32xf8E5M2> -> tensor<32x64xf8E5M2>
        %138 = tt.dot %136, %137, %cst_4, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x32xf8E5M2> * tensor<32x64xf8E5M2> -> tensor<128x64xf32>
        %139 = arith.addf %124, %138 : tensor<128x64xf32>
        scf.yield %121, %139, %112 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.num_stages = 3 : i32}
      %25 = tt.expand_dims %24#0 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
      %26 = tt.broadcast %25 : tensor<128x1xf32> -> tensor<128x64xf32>
      %27 = arith.divf %24#1, %26 : tensor<128x64xf32>
      %28 = tt.fp_to_fp %27, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
      %29 = arith.divsi %arg6, %arg5 : i32
      %30 = arith.remsi %arg6, %arg5 : i32
      %31 = arith.cmpi ne, %30, %c0_i32 : i32
      %32 = arith.subi %29, %c1_i32 : i32
      %33 = arith.select %31, %32, %29 : i32
      %34 = arith.cmpi slt, %arg6, %c0_i32 : i32
      %35 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %36 = arith.cmpi ne, %34, %35 : i1
      %37 = arith.select %36, %33, %29 : i32
      %38 = arith.andi %31, %36 : i1
      %39 = arith.addi %30, %arg5 : i32
      %40 = arith.select %38, %39, %30 : i32
      %41 = arith.muli %37, %arg4 : i32
      %42 = arith.muli %40, %c8192_i32 : i32
      %43 = arith.addi %41, %42 : i32
      %44 = tt.splat %43 : i32 -> tensor<128x1xi32>
      %45 = arith.addi %44, %14 : tensor<128x1xi32>
      %46 = tt.broadcast %45 : tensor<128x1xi32> -> tensor<128x64xi32>
      %47 = arith.addi %46, %19 : tensor<128x64xi32>
      %48 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %49 = tt.addptr %48, %47 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      tt.store %49, %28 : tensor<128x64x!tt.ptr<f8E5M2>>
      %c1_i32_8 = arith.constant 1 : i32
      %50 = arith.muli %c132_i32, %c1_i32_8 : i32
      %51 = arith.addi %arg6, %50 : i32
      %52 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
      %53 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>
      %54 = arith.muli %51, %c8192_i32 : i32
      %55 = tt.expand_dims %53 {axis = 1 : i32} : tensor<128xi32> -> tensor<128x1xi32>
      %56 = arith.muli %55, %cst_3 : tensor<128x1xi32>
      %57 = tt.splat %54 : i32 -> tensor<128x1xi32>
      %58 = arith.addi %57, %56 : tensor<128x1xi32>
      %59 = tt.expand_dims %52 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %60 = tt.broadcast %58 : tensor<128x1xi32> -> tensor<128x64xi32>
      %61 = tt.broadcast %59 : tensor<1x64xi32> -> tensor<128x64xi32>
      %62 = arith.addi %60, %61 : tensor<128x64xi32>
      %63 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %64 = tt.addptr %63, %62 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      %65 = tt.load %64 : tensor<128x64x!tt.ptr<f8E5M2>>
      %66:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c32_i32 iter_args(%arg8 = %cst_5, %arg9 = %cst_4, %arg10 = %cst_6) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %92 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32>
        %93 = tt.splat %arg7 : i32 -> tensor<32xi32>
        %94 = arith.addi %93, %92 : tensor<32xi32>
        %95 = tt.expand_dims %94 {axis = 1 : i32} : tensor<32xi32> -> tensor<32x1xi32>
        %96 = arith.muli %95, %cst_2 : tensor<32x1xi32>
        %97 = tt.splat %54 : i32 -> tensor<32x1xi32>
        %98 = arith.addi %97, %96 : tensor<32x1xi32>
        %99 = tt.broadcast %98 : tensor<32x1xi32> -> tensor<32x64xi32>
        %100 = tt.broadcast %59 : tensor<1x64xi32> -> tensor<32x64xi32>
        %101 = arith.addi %99, %100 : tensor<32x64xi32>
        %102 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<32x64x!tt.ptr<f8E5M2>>
        %103 = tt.addptr %102, %101 : tensor<32x64x!tt.ptr<f8E5M2>>, tensor<32x64xi32>
        %104 = tt.load %103 : tensor<32x64x!tt.ptr<f8E5M2>>
        %105 = tt.trans %104 {order = array<i32: 1, 0>} : tensor<32x64xf8E5M2> -> tensor<64x32xf8E5M2>
        %106 = tt.dot %65, %105, %cst_1, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x32xf8E5M2> -> tensor<128x32xf32>
        %107 = arith.mulf %106, %cst_0 : tensor<128x32xf32>
        %108 = "tt.reduce"(%107) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %140 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %140 : f32
        }) : (tensor<128x32xf32>) -> tensor<128xf32>
        %109 = arith.cmpf ogt, %arg10, %108 : tensor<128xf32>
        %110 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %111 = arith.ori %109, %110 : tensor<128xi1>
        %112 = arith.select %111, %arg10, %108 : tensor<128xi1>, tensor<128xf32>
        %113 = tt.expand_dims %112 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %114 = tt.broadcast %113 : tensor<128x1xf32> -> tensor<128x32xf32>
        %115 = arith.subf %107, %114 : tensor<128x32xf32>
        %116 = tt.extern_elementwise %115 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x32xf32>) -> tensor<128x32xf32>
        %117 = "tt.reduce"(%116) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %140 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %140 : f32
        }) : (tensor<128x32xf32>) -> tensor<128xf32>
        %118 = arith.subf %arg10, %112 : tensor<128xf32>
        %119 = tt.extern_elementwise %118 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %120 = arith.mulf %arg8, %119 : tensor<128xf32>
        %121 = arith.addf %120, %117 : tensor<128xf32>
        %122 = tt.expand_dims %119 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %123 = tt.broadcast %122 : tensor<128x1xf32> -> tensor<128x64xf32>
        %124 = arith.mulf %arg9, %123 : tensor<128x64xf32>
        %125 = tt.expand_dims %52 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %126 = tt.splat %54 : i32 -> tensor<64x1xi32>
        %127 = arith.addi %126, %125 : tensor<64x1xi32>
        %128 = tt.expand_dims %94 {axis = 0 : i32} : tensor<32xi32> -> tensor<1x32xi32>
        %129 = arith.muli %128, %cst : tensor<1x32xi32>
        %130 = tt.broadcast %127 : tensor<64x1xi32> -> tensor<64x32xi32>
        %131 = tt.broadcast %129 : tensor<1x32xi32> -> tensor<64x32xi32>
        %132 = arith.addi %130, %131 : tensor<64x32xi32>
        %133 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x32x!tt.ptr<f8E5M2>>
        %134 = tt.addptr %133, %132 : tensor<64x32x!tt.ptr<f8E5M2>>, tensor<64x32xi32>
        %135 = tt.load %134 : tensor<64x32x!tt.ptr<f8E5M2>>
        %136 = tt.fp_to_fp %116, rounding = rtne : tensor<128x32xf32> -> tensor<128x32xf8E5M2>
        %137 = tt.trans %135 {order = array<i32: 1, 0>} : tensor<64x32xf8E5M2> -> tensor<32x64xf8E5M2>
        %138 = tt.dot %136, %137, %cst_4, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x32xf8E5M2> * tensor<32x64xf8E5M2> -> tensor<128x64xf32>
        %139 = arith.addf %124, %138 : tensor<128x64xf32>
        scf.yield %121, %139, %112 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.num_stages = 3 : i32}
      %67 = tt.expand_dims %66#0 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
      %68 = tt.broadcast %67 : tensor<128x1xf32> -> tensor<128x64xf32>
      %69 = arith.divf %66#1, %68 : tensor<128x64xf32>
      %70 = tt.fp_to_fp %69, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
      %71 = arith.divsi %51, %arg5 : i32
      %72 = arith.remsi %51, %arg5 : i32
      %73 = arith.cmpi ne, %72, %c0_i32 : i32
      %74 = arith.subi %71, %c1_i32 : i32
      %75 = arith.select %73, %74, %71 : i32
      %76 = arith.cmpi slt, %51, %c0_i32 : i32
      %77 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %78 = arith.cmpi ne, %76, %77 : i1
      %79 = arith.select %78, %75, %71 : i32
      %80 = arith.andi %73, %78 : i1
      %81 = arith.addi %72, %arg5 : i32
      %82 = arith.select %80, %81, %72 : i32
      %83 = arith.muli %79, %arg4 : i32
      %84 = arith.muli %82, %c8192_i32 : i32
      %85 = arith.addi %83, %84 : i32
      %86 = tt.splat %85 : i32 -> tensor<128x1xi32>
      %87 = arith.addi %86, %56 : tensor<128x1xi32>
      %88 = tt.broadcast %87 : tensor<128x1xi32> -> tensor<128x64xi32>
      %89 = arith.addi %88, %61 : tensor<128x64xi32>
      %90 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %91 = tt.addptr %90, %89 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      tt.store %91, %70 : tensor<128x64x!tt.ptr<f8E5M2>>
    } {tt.flatten, tt.num_stages = 1 : i32}
    scf.for %arg6 = %8 to %c192_i32 step %c132_i32  : i32 {
      %10 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
      %11 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>
      %12 = arith.muli %arg6, %c8192_i32 : i32
      %13 = tt.expand_dims %11 {axis = 1 : i32} : tensor<128xi32> -> tensor<128x1xi32>
      %14 = arith.muli %13, %cst_3 : tensor<128x1xi32>
      %15 = tt.splat %12 : i32 -> tensor<128x1xi32>
      %16 = arith.addi %15, %14 : tensor<128x1xi32>
      %17 = tt.expand_dims %10 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %18 = tt.broadcast %16 : tensor<128x1xi32> -> tensor<128x64xi32>
      %19 = tt.broadcast %17 : tensor<1x64xi32> -> tensor<128x64xi32>
      %20 = arith.addi %18, %19 : tensor<128x64xi32>
      %21 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %22 = tt.addptr %21, %20 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      %23 = tt.load %22 : tensor<128x64x!tt.ptr<f8E5M2>>
      %24:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c32_i32 iter_args(%arg8 = %cst_5, %arg9 = %cst_4, %arg10 = %cst_6) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %50 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32>
        %51 = tt.splat %arg7 : i32 -> tensor<32xi32>
        %52 = arith.addi %51, %50 : tensor<32xi32>
        %53 = tt.expand_dims %52 {axis = 1 : i32} : tensor<32xi32> -> tensor<32x1xi32>
        %54 = arith.muli %53, %cst_2 : tensor<32x1xi32>
        %55 = tt.splat %12 : i32 -> tensor<32x1xi32>
        %56 = arith.addi %55, %54 : tensor<32x1xi32>
        %57 = tt.broadcast %56 : tensor<32x1xi32> -> tensor<32x64xi32>
        %58 = tt.broadcast %17 : tensor<1x64xi32> -> tensor<32x64xi32>
        %59 = arith.addi %57, %58 : tensor<32x64xi32>
        %60 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<32x64x!tt.ptr<f8E5M2>>
        %61 = tt.addptr %60, %59 : tensor<32x64x!tt.ptr<f8E5M2>>, tensor<32x64xi32>
        %62 = tt.load %61 : tensor<32x64x!tt.ptr<f8E5M2>>
        %63 = tt.trans %62 {order = array<i32: 1, 0>} : tensor<32x64xf8E5M2> -> tensor<64x32xf8E5M2>
        %64 = tt.dot %23, %63, %cst_1, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x32xf8E5M2> -> tensor<128x32xf32>
        %65 = arith.mulf %64, %cst_0 : tensor<128x32xf32>
        %66 = "tt.reduce"(%65) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %98 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %98 : f32
        }) : (tensor<128x32xf32>) -> tensor<128xf32>
        %67 = arith.cmpf ogt, %arg10, %66 : tensor<128xf32>
        %68 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %69 = arith.ori %67, %68 : tensor<128xi1>
        %70 = arith.select %69, %arg10, %66 : tensor<128xi1>, tensor<128xf32>
        %71 = tt.expand_dims %70 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %72 = tt.broadcast %71 : tensor<128x1xf32> -> tensor<128x32xf32>
        %73 = arith.subf %65, %72 : tensor<128x32xf32>
        %74 = tt.extern_elementwise %73 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x32xf32>) -> tensor<128x32xf32>
        %75 = "tt.reduce"(%74) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %98 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %98 : f32
        }) : (tensor<128x32xf32>) -> tensor<128xf32>
        %76 = arith.subf %arg10, %70 : tensor<128xf32>
        %77 = tt.extern_elementwise %76 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %78 = arith.mulf %arg8, %77 : tensor<128xf32>
        %79 = arith.addf %78, %75 : tensor<128xf32>
        %80 = tt.expand_dims %77 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %81 = tt.broadcast %80 : tensor<128x1xf32> -> tensor<128x64xf32>
        %82 = arith.mulf %arg9, %81 : tensor<128x64xf32>
        %83 = tt.expand_dims %10 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %84 = tt.splat %12 : i32 -> tensor<64x1xi32>
        %85 = arith.addi %84, %83 : tensor<64x1xi32>
        %86 = tt.expand_dims %52 {axis = 0 : i32} : tensor<32xi32> -> tensor<1x32xi32>
        %87 = arith.muli %86, %cst : tensor<1x32xi32>
        %88 = tt.broadcast %85 : tensor<64x1xi32> -> tensor<64x32xi32>
        %89 = tt.broadcast %87 : tensor<1x32xi32> -> tensor<64x32xi32>
        %90 = arith.addi %88, %89 : tensor<64x32xi32>
        %91 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x32x!tt.ptr<f8E5M2>>
        %92 = tt.addptr %91, %90 : tensor<64x32x!tt.ptr<f8E5M2>>, tensor<64x32xi32>
        %93 = tt.load %92 : tensor<64x32x!tt.ptr<f8E5M2>>
        %94 = tt.fp_to_fp %74, rounding = rtne : tensor<128x32xf32> -> tensor<128x32xf8E5M2>
        %95 = tt.trans %93 {order = array<i32: 1, 0>} : tensor<64x32xf8E5M2> -> tensor<32x64xf8E5M2>
        %96 = tt.dot %94, %95, %cst_4, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x32xf8E5M2> * tensor<32x64xf8E5M2> -> tensor<128x64xf32>
        %97 = arith.addf %82, %96 : tensor<128x64xf32>
        scf.yield %79, %97, %70 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.num_stages = 3 : i32}
      %25 = tt.expand_dims %24#0 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
      %26 = tt.broadcast %25 : tensor<128x1xf32> -> tensor<128x64xf32>
      %27 = arith.divf %24#1, %26 : tensor<128x64xf32>
      %28 = tt.fp_to_fp %27, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
      %29 = arith.divsi %arg6, %arg5 : i32
      %30 = arith.remsi %arg6, %arg5 : i32
      %31 = arith.cmpi ne, %30, %c0_i32 : i32
      %32 = arith.subi %29, %c1_i32 : i32
      %33 = arith.select %31, %32, %29 : i32
      %34 = arith.cmpi slt, %arg6, %c0_i32 : i32
      %35 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %36 = arith.cmpi ne, %34, %35 : i1
      %37 = arith.select %36, %33, %29 : i32
      %38 = arith.andi %31, %36 : i1
      %39 = arith.addi %30, %arg5 : i32
      %40 = arith.select %38, %39, %30 : i32
      %41 = arith.muli %37, %arg4 : i32
      %42 = arith.muli %40, %c8192_i32 : i32
      %43 = arith.addi %41, %42 : i32
      %44 = tt.splat %43 : i32 -> tensor<128x1xi32>
      %45 = arith.addi %44, %14 : tensor<128x1xi32>
      %46 = tt.broadcast %45 : tensor<128x1xi32> -> tensor<128x64xi32>
      %47 = arith.addi %46, %19 : tensor<128x64xi32>
      %48 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %49 = tt.addptr %48, %47 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      tt.store %49, %28 : tensor<128x64x!tt.ptr<f8E5M2>>
    } {tt.flatten, tt.num_stages = 1 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(convert-triton-to-tritongpu{enable-source-remat=false num-ctas=1 num-warps=4 target=cuda:90 threads-per-warp=32}, tritongpu-coalesce, tritongpu-F32DotTC, triton-nvidia-gpu-plan-cta, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritongpu-accelerate-matmul, tritongpu-remove-layout-conversions, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, triton-nvidia-optimize-descriptor-encoding, triton-loop-aware-cse, tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-combine-tensor-select-and-if, nvgpu-warp-specialization{dump-intermediate-steps=false num-stages=4}, tritongpu-assign-latencies{num-stages=4}, tritongpu-schedule-loops, tritongpu-pipeline{dump-intermediate-steps=false num-stages=4}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-loop-aware-cse, tritongpu-prefetch, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-coalesce-async-copy, triton-nvidia-optimize-tmem-layouts, tritongpu-remove-layout-conversions, triton-nvidia-interleave-tmem, tritongpu-reduce-data-duplication, tritongpu-reorder-instructions, triton-loop-aware-cse, symbol-dce, triton-nvidia-tma-lowering, triton-nvidia-gpu-fence-insertion{compute-capability=90}, sccp, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true})",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_willfeng/b3/cb3rpcsfds3occkgirnum6bxwhd5tgim2lwn3do45yfp6uho45co.py:13:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_willfeng/b3/cb3rpcsfds3occkgirnum6bxwhd5tgim2lwn3do45yfp6uho45co.py:13:0: note: Pipeline failed while executing [`TritonGPUAssignLatencies` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
python: /root/.triton/llvm/llvm-8957e64a-almalinux-x64/include/llvm/ADT/SmallVector.h:296: const_reference llvm::SmallVectorTemplateCommon<long>::operator[](size_type) const [T = long]: Assertion `idx < size()' failed.
module {
  tt.func public @_fp8_attention_kernel_kernel(%arg0: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %c1_i32 = arith.constant 1 : i32
    %c32_i32 = arith.constant 32 : i32
    %c64_i32 = arith.constant 64 : i32
    %c128_i32 = arith.constant 128 : i32
    %c0_i32 = arith.constant 0 : i32
    %cst = arith.constant dense<64> : tensor<1x32xi32>
    %cst_0 = arith.constant dense<0.180336878> : tensor<64x32xf32>
    %cst_1 = arith.constant dense<0.000000e+00> : tensor<64x32xf32>
    %cst_2 = arith.constant dense<64> : tensor<32x1xi32>
    %cst_3 = arith.constant dense<64> : tensor<64x1xi32>
    %c8192_i32 = arith.constant 8192 : i32
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<64x64xf32>
    %cst_5 = arith.constant dense<0.000000e+00> : tensor<64xf32>
    %cst_6 = arith.constant dense<0xFF800000> : tensor<64xf32>
    %0 = tt.get_program_id x : i32
    %1 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
    scf.for %arg6 = %c0_i32 to %c128_i32 step %c64_i32  : i32 {
      %2 = tt.splat %arg6 : i32 -> tensor<64xi32>
      %3 = arith.addi %2, %1 : tensor<64xi32>
      %4 = arith.muli %0, %c8192_i32 : i32
      %5 = tt.expand_dims %3 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
      %6 = arith.muli %5, %cst_3 : tensor<64x1xi32>
      %7 = tt.splat %4 : i32 -> tensor<64x1xi32>
      %8 = arith.addi %7, %6 : tensor<64x1xi32>
      %9 = tt.expand_dims %1 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %10 = tt.broadcast %8 : tensor<64x1xi32> -> tensor<64x64xi32>
      %11 = tt.broadcast %9 : tensor<1x64xi32> -> tensor<64x64xi32>
      %12 = arith.addi %10, %11 : tensor<64x64xi32>
      %13 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %14 = tt.addptr %13, %12 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      %15 = tt.load %14 : tensor<64x64x!tt.ptr<f8E5M2>>
      %16:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c32_i32 iter_args(%arg8 = %cst_5, %arg9 = %cst_4, %arg10 = %cst_6) -> (tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>)  : i32 {
        %42 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32>
        %43 = tt.splat %arg7 : i32 -> tensor<32xi32>
        %44 = arith.addi %43, %42 : tensor<32xi32>
        %45 = tt.expand_dims %44 {axis = 1 : i32} : tensor<32xi32> -> tensor<32x1xi32>
        %46 = arith.muli %45, %cst_2 : tensor<32x1xi32>
        %47 = tt.splat %4 : i32 -> tensor<32x1xi32>
        %48 = arith.addi %47, %46 : tensor<32x1xi32>
        %49 = tt.broadcast %48 : tensor<32x1xi32> -> tensor<32x64xi32>
        %50 = tt.broadcast %9 : tensor<1x64xi32> -> tensor<32x64xi32>
        %51 = arith.addi %49, %50 : tensor<32x64xi32>
        %52 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<32x64x!tt.ptr<f8E5M2>>
        %53 = tt.addptr %52, %51 : tensor<32x64x!tt.ptr<f8E5M2>>, tensor<32x64xi32>
        %54 = tt.load %53 : tensor<32x64x!tt.ptr<f8E5M2>>
        %55 = tt.trans %54 {order = array<i32: 1, 0>} : tensor<32x64xf8E5M2> -> tensor<64x32xf8E5M2>
        %56 = tt.dot %15, %55, %cst_1, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x64xf8E5M2> * tensor<64x32xf8E5M2> -> tensor<64x32xf32>
        %57 = arith.mulf %56, %cst_0 : tensor<64x32xf32>
        %58 = "tt.reduce"(%57) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %89 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %89 : f32
        }) : (tensor<64x32xf32>) -> tensor<64xf32>
        %59 = arith.cmpf ogt, %arg10, %58 : tensor<64xf32>
        %60 = arith.cmpf une, %arg10, %arg10 : tensor<64xf32>
        %61 = arith.ori %59, %60 : tensor<64xi1>
        %62 = arith.select %61, %arg10, %58 : tensor<64xi1>, tensor<64xf32>
        %63 = tt.expand_dims %62 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %64 = tt.broadcast %63 : tensor<64x1xf32> -> tensor<64x32xf32>
        %65 = arith.subf %57, %64 : tensor<64x32xf32>
        %66 = tt.extern_elementwise %65 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64x32xf32>) -> tensor<64x32xf32>
        %67 = "tt.reduce"(%66) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %89 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %89 : f32
        }) : (tensor<64x32xf32>) -> tensor<64xf32>
        %68 = arith.subf %arg10, %62 : tensor<64xf32>
        %69 = tt.extern_elementwise %68 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64xf32>) -> tensor<64xf32>
        %70 = arith.mulf %arg8, %69 : tensor<64xf32>
        %71 = arith.addf %70, %67 : tensor<64xf32>
        %72 = tt.expand_dims %69 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %73 = tt.broadcast %72 : tensor<64x1xf32> -> tensor<64x64xf32>
        %74 = arith.mulf %arg9, %73 : tensor<64x64xf32>
        %75 = tt.expand_dims %1 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %76 = arith.addi %7, %75 : tensor<64x1xi32>
        %77 = tt.expand_dims %44 {axis = 0 : i32} : tensor<32xi32> -> tensor<1x32xi32>
        %78 = arith.muli %77, %cst : tensor<1x32xi32>
        %79 = tt.broadcast %76 : tensor<64x1xi32> -> tensor<64x32xi32>
        %80 = tt.broadcast %78 : tensor<1x32xi32> -> tensor<64x32xi32>
        %81 = arith.addi %79, %80 : tensor<64x32xi32>
        %82 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x32x!tt.ptr<f8E5M2>>
        %83 = tt.addptr %82, %81 : tensor<64x32x!tt.ptr<f8E5M2>>, tensor<64x32xi32>
        %84 = tt.load %83 : tensor<64x32x!tt.ptr<f8E5M2>>
        %85 = tt.fp_to_fp %66, rounding = rtne : tensor<64x32xf32> -> tensor<64x32xf8E5M2>
        %86 = tt.trans %84 {order = array<i32: 1, 0>} : tensor<64x32xf8E5M2> -> tensor<32x64xf8E5M2>
        %87 = tt.dot %85, %86, %cst_4, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x32xf8E5M2> * tensor<32x64xf8E5M2> -> tensor<64x64xf32>
        %88 = arith.addf %74, %87 : tensor<64x64xf32>
        scf.yield %71, %88, %62 : tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>
      } {tt.disallow_acc_multi_buffer, tt.loop_unroll_factor = 1 : i32, tt.num_stages = 2 : i32}
      %17 = tt.expand_dims %16#0 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
      %18 = tt.broadcast %17 : tensor<64x1xf32> -> tensor<64x64xf32>
      %19 = arith.divf %16#1, %18 : tensor<64x64xf32>
      %20 = tt.fp_to_fp %19, rounding = rtne : tensor<64x64xf32> -> tensor<64x64xf8E5M2>
      %21 = arith.divsi %0, %arg5 : i32
      %22 = arith.remsi %0, %arg5 : i32
      %23 = arith.cmpi ne, %22, %c0_i32 : i32
      %24 = arith.subi %21, %c1_i32 : i32
      %25 = arith.select %23, %24, %21 : i32
      %26 = arith.cmpi slt, %0, %c0_i32 : i32
      %27 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %28 = arith.cmpi ne, %26, %27 : i1
      %29 = arith.select %28, %25, %21 : i32
      %30 = arith.andi %23, %28 : i1
      %31 = arith.addi %22, %arg5 : i32
      %32 = arith.select %30, %31, %22 : i32
      %33 = arith.muli %29, %arg4 : i32
      %34 = arith.muli %32, %c8192_i32 : i32
      %35 = arith.addi %33, %34 : i32
      %36 = tt.splat %35 : i32 -> tensor<64x1xi32>
      %37 = arith.addi %36, %6 : tensor<64x1xi32>
      %38 = tt.broadcast %37 : tensor<64x1xi32> -> tensor<64x64xi32>
      %39 = arith.addi %38, %11 : tensor<64x64xi32>
      %40 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %41 = tt.addptr %40, %39 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      tt.store %41, %20 : tensor<64x64x!tt.ptr<f8E5M2>>
    } {tt.flatten, tt.num_stages = 4 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(convert-triton-to-tritongpu{enable-source-remat=false num-ctas=1 num-warps=16 target=cuda:90 threads-per-warp=32}, tritongpu-coalesce, tritongpu-F32DotTC, triton-nvidia-gpu-plan-cta, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritongpu-accelerate-matmul, tritongpu-remove-layout-conversions, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, triton-nvidia-optimize-descriptor-encoding, triton-loop-aware-cse, tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-combine-tensor-select-and-if, nvgpu-warp-specialization{dump-intermediate-steps=false num-stages=4}, tritongpu-assign-latencies{num-stages=4}, tritongpu-schedule-loops, tritongpu-pipeline{dump-intermediate-steps=false num-stages=4}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-loop-aware-cse, tritongpu-prefetch, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-coalesce-async-copy, triton-nvidia-optimize-tmem-layouts, tritongpu-remove-layout-conversions, triton-nvidia-interleave-tmem, tritongpu-reduce-data-duplication, tritongpu-reorder-instructions, triton-loop-aware-cse, symbol-dce, triton-nvidia-tma-lowering, triton-nvidia-gpu-fence-insertion{compute-capability=90}, sccp, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true})",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_willfeng/7w/c7wgt7mxcsrhayfemarj6dv2imppqnuna3knkwynvzbgvxr7ricv.py:12:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_willfeng/7w/c7wgt7mxcsrhayfemarj6dv2imppqnuna3knkwynvzbgvxr7ricv.py:12:0: note: Pipeline failed while executing [`TritonGPUAssignLatencies` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
Process ForkProcess-522:
Traceback (most recent call last):
  File "/home/willfeng/local/miniconda3/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/willfeng/local/miniconda3/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/data/users/willfeng/helion/helion/runtime/precompile_shim.py", line 54, in finish_it
    kernel_cache[key] = fn.compile(
                        ^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/compiler/compiler.py", line 359, in compile
    next_module = compile_ir(module, metadata)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/backends/nvidia/compiler.py", line 456, in <lambda>
    stages["ttgir"] = lambda src, metadata: self.make_ttgir(src, metadata, options, capability)
                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/backends/nvidia/compiler.py", line 298, in make_ttgir
    pm.run(mod)
RuntimeError: PassManager::run failed
python: /root/.triton/llvm/llvm-8957e64a-almalinux-x64/include/llvm/ADT/SmallVector.h:296: const_reference llvm::SmallVectorTemplateCommon<long>::operator[](size_type) const [T = long]: Assertion `idx < size()' failed.
module {
  tt.func public @_fp8_attention_kernel_kernel(%arg0: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %c1_i32 = arith.constant 1 : i32
    %c32_i32 = arith.constant 32 : i32
    %c64_i32 = arith.constant 64 : i32
    %c128_i32 = arith.constant 128 : i32
    %c0_i32 = arith.constant 0 : i32
    %cst = arith.constant dense<64> : tensor<1x32xi32>
    %cst_0 = arith.constant dense<0.180336878> : tensor<64x32xf32>
    %cst_1 = arith.constant dense<0.000000e+00> : tensor<64x32xf32>
    %cst_2 = arith.constant dense<64> : tensor<32x1xi32>
    %cst_3 = arith.constant dense<64> : tensor<64x1xi32>
    %c8192_i32 = arith.constant 8192 : i32
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<64x64xf32>
    %cst_5 = arith.constant dense<0.000000e+00> : tensor<64xf32>
    %cst_6 = arith.constant dense<0xFF800000> : tensor<64xf32>
    %0 = tt.get_program_id x : i32
    %1 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
    scf.for %arg6 = %c0_i32 to %c128_i32 step %c64_i32  : i32 {
      %2 = tt.splat %arg6 : i32 -> tensor<64xi32>
      %3 = arith.addi %2, %1 : tensor<64xi32>
      %4 = arith.muli %0, %c8192_i32 : i32
      %5 = tt.expand_dims %3 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
      %6 = arith.muli %5, %cst_3 : tensor<64x1xi32>
      %7 = tt.splat %4 : i32 -> tensor<64x1xi32>
      %8 = arith.addi %7, %6 : tensor<64x1xi32>
      %9 = tt.expand_dims %1 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %10 = tt.broadcast %8 : tensor<64x1xi32> -> tensor<64x64xi32>
      %11 = tt.broadcast %9 : tensor<1x64xi32> -> tensor<64x64xi32>
      %12 = arith.addi %10, %11 : tensor<64x64xi32>
      %13 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %14 = tt.addptr %13, %12 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      %15 = tt.load %14 : tensor<64x64x!tt.ptr<f8E5M2>>
      %16:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c32_i32 iter_args(%arg8 = %cst_5, %arg9 = %cst_4, %arg10 = %cst_6) -> (tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>)  : i32 {
        %42 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32>
        %43 = tt.splat %arg7 : i32 -> tensor<32xi32>
        %44 = arith.addi %43, %42 : tensor<32xi32>
        %45 = tt.expand_dims %44 {axis = 1 : i32} : tensor<32xi32> -> tensor<32x1xi32>
        %46 = arith.muli %45, %cst_2 : tensor<32x1xi32>
        %47 = tt.splat %4 : i32 -> tensor<32x1xi32>
        %48 = arith.addi %47, %46 : tensor<32x1xi32>
        %49 = tt.broadcast %48 : tensor<32x1xi32> -> tensor<32x64xi32>
        %50 = tt.broadcast %9 : tensor<1x64xi32> -> tensor<32x64xi32>
        %51 = arith.addi %49, %50 : tensor<32x64xi32>
        %52 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<32x64x!tt.ptr<f8E5M2>>
        %53 = tt.addptr %52, %51 : tensor<32x64x!tt.ptr<f8E5M2>>, tensor<32x64xi32>
        %54 = tt.load %53 : tensor<32x64x!tt.ptr<f8E5M2>>
        %55 = tt.trans %54 {order = array<i32: 1, 0>} : tensor<32x64xf8E5M2> -> tensor<64x32xf8E5M2>
        %56 = tt.dot %15, %55, %cst_1, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x64xf8E5M2> * tensor<64x32xf8E5M2> -> tensor<64x32xf32>
        %57 = arith.mulf %56, %cst_0 : tensor<64x32xf32>
        %58 = "tt.reduce"(%57) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %89 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %89 : f32
        }) : (tensor<64x32xf32>) -> tensor<64xf32>
        %59 = arith.cmpf ogt, %arg10, %58 : tensor<64xf32>
        %60 = arith.cmpf une, %arg10, %arg10 : tensor<64xf32>
        %61 = arith.ori %59, %60 : tensor<64xi1>
        %62 = arith.select %61, %arg10, %58 : tensor<64xi1>, tensor<64xf32>
        %63 = tt.expand_dims %62 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %64 = tt.broadcast %63 : tensor<64x1xf32> -> tensor<64x32xf32>
        %65 = arith.subf %57, %64 : tensor<64x32xf32>
        %66 = tt.extern_elementwise %65 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64x32xf32>) -> tensor<64x32xf32>
        %67 = "tt.reduce"(%66) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %89 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %89 : f32
        }) : (tensor<64x32xf32>) -> tensor<64xf32>
        %68 = arith.subf %arg10, %62 : tensor<64xf32>
        %69 = tt.extern_elementwise %68 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64xf32>) -> tensor<64xf32>
        %70 = arith.mulf %arg8, %69 : tensor<64xf32>
        %71 = arith.addf %70, %67 : tensor<64xf32>
        %72 = tt.expand_dims %69 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %73 = tt.broadcast %72 : tensor<64x1xf32> -> tensor<64x64xf32>
        %74 = arith.mulf %arg9, %73 : tensor<64x64xf32>
        %75 = tt.expand_dims %1 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %76 = arith.addi %7, %75 : tensor<64x1xi32>
        %77 = tt.expand_dims %44 {axis = 0 : i32} : tensor<32xi32> -> tensor<1x32xi32>
        %78 = arith.muli %77, %cst : tensor<1x32xi32>
        %79 = tt.broadcast %76 : tensor<64x1xi32> -> tensor<64x32xi32>
        %80 = tt.broadcast %78 : tensor<1x32xi32> -> tensor<64x32xi32>
        %81 = arith.addi %79, %80 : tensor<64x32xi32>
        %82 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x32x!tt.ptr<f8E5M2>>
        %83 = tt.addptr %82, %81 : tensor<64x32x!tt.ptr<f8E5M2>>, tensor<64x32xi32>
        %84 = tt.load %83 : tensor<64x32x!tt.ptr<f8E5M2>>
        %85 = tt.fp_to_fp %66, rounding = rtne : tensor<64x32xf32> -> tensor<64x32xf8E5M2>
        %86 = tt.trans %84 {order = array<i32: 1, 0>} : tensor<64x32xf8E5M2> -> tensor<32x64xf8E5M2>
        %87 = tt.dot %85, %86, %cst_4, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x32xf8E5M2> * tensor<32x64xf8E5M2> -> tensor<64x64xf32>
        %88 = arith.addf %74, %87 : tensor<64x64xf32>
        scf.yield %71, %88, %62 : tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>
      } {tt.disallow_acc_multi_buffer, tt.loop_unroll_factor = 1 : i32, tt.num_stages = 2 : i32}
      %17 = tt.expand_dims %16#0 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
      %18 = tt.broadcast %17 : tensor<64x1xf32> -> tensor<64x64xf32>
      %19 = arith.divf %16#1, %18 : tensor<64x64xf32>
      %20 = tt.fp_to_fp %19, rounding = rtne : tensor<64x64xf32> -> tensor<64x64xf8E5M2>
      %21 = arith.divsi %0, %arg5 : i32
      %22 = arith.remsi %0, %arg5 : i32
      %23 = arith.cmpi ne, %22, %c0_i32 : i32
      %24 = arith.subi %21, %c1_i32 : i32
      %25 = arith.select %23, %24, %21 : i32
      %26 = arith.cmpi slt, %0, %c0_i32 : i32
      %27 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %28 = arith.cmpi ne, %26, %27 : i1
      %29 = arith.select %28, %25, %21 : i32
      %30 = arith.andi %23, %28 : i1
      %31 = arith.addi %22, %arg5 : i32
      %32 = arith.select %30, %31, %22 : i32
      %33 = arith.muli %29, %arg4 : i32
      %34 = arith.muli %32, %c8192_i32 : i32
      %35 = arith.addi %33, %34 : i32
      %36 = tt.splat %35 : i32 -> tensor<64x1xi32>
      %37 = arith.addi %36, %6 : tensor<64x1xi32>
      %38 = tt.broadcast %37 : tensor<64x1xi32> -> tensor<64x64xi32>
      %39 = arith.addi %38, %11 : tensor<64x64xi32>
      %40 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %41 = tt.addptr %40, %39 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      tt.store %41, %20 : tensor<64x64x!tt.ptr<f8E5M2>>
    } {tt.flatten, tt.num_stages = 4 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(convert-triton-to-tritongpu{enable-source-remat=false num-ctas=1 num-warps=16 target=cuda:90 threads-per-warp=32}, tritongpu-coalesce, tritongpu-F32DotTC, triton-nvidia-gpu-plan-cta, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritongpu-accelerate-matmul, tritongpu-remove-layout-conversions, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, triton-nvidia-optimize-descriptor-encoding, triton-loop-aware-cse, tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-combine-tensor-select-and-if, nvgpu-warp-specialization{dump-intermediate-steps=false num-stages=4}, tritongpu-assign-latencies{num-stages=4}, tritongpu-schedule-loops, tritongpu-pipeline{dump-intermediate-steps=false num-stages=4}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-loop-aware-cse, tritongpu-prefetch, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-coalesce-async-copy, triton-nvidia-optimize-tmem-layouts, tritongpu-remove-layout-conversions, triton-nvidia-interleave-tmem, tritongpu-reduce-data-duplication, tritongpu-reorder-instructions, triton-loop-aware-cse, symbol-dce, triton-nvidia-tma-lowering, triton-nvidia-gpu-fence-insertion{compute-capability=90}, sccp, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true})",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_willfeng/7w/c7wgt7mxcsrhayfemarj6dv2imppqnuna3knkwynvzbgvxr7ricv.py:12:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_willfeng/7w/c7wgt7mxcsrhayfemarj6dv2imppqnuna3knkwynvzbgvxr7ricv.py:12:0: note: Pipeline failed while executing [`TritonGPUAssignLatencies` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
[804s] Timeout after 60s compiling Config(block_sizes=[32, 128], range_unroll_factors=[4, 3, 2], range_num_stages=[0, 2, 2], range_multi_buffers=[None, None, True], range_flattens=[False, False, False], num_warps=4, num_stages=8, indexing='block_ptr', pid_type='persistent_interleaved')
[818s] Generation 7: replaced=12 min=0.0113 mid=0.0142 max=0.0247 best=Config(block_sizes=[128, 128], range_unroll_factors=[0, 2, 2], range_num_stages=[0, 4, 2], range_multi_buffers=[None, False, True], range_flattens=[None, False, False], num_warps=8, num_stages=7, indexing='tensor_descriptor', pid_type='flat', range_warp_specializes=[])
python: /root/.triton/llvm/llvm-8957e64a-almalinux-x64/include/llvm/ADT/SmallVector.h:296: const_reference llvm::SmallVectorTemplateCommon<long>::operator[](size_type) const [T = long]: Assertion `idx < size()' failed.
module {
  tt.func public @_fp8_attention_kernel_kernel(%arg0: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %c64_i32 = arith.constant 64 : i32
    %c128_i32 = arith.constant 128 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst = arith.constant dense<64> : tensor<1x64xi32>
    %cst_0 = arith.constant dense<0.180336878> : tensor<128x64xf32>
    %cst_1 = arith.constant dense<64> : tensor<64x1xi32>
    %cst_2 = arith.constant dense<64> : tensor<128x1xi32>
    %c8192_i32 = arith.constant 8192 : i32
    %cst_3 = arith.constant dense<0.000000e+00> : tensor<128x64xf32>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<128xf32>
    %cst_5 = arith.constant dense<0xFF800000> : tensor<128xf32>
    %c192_i32 = arith.constant 192 : i32
    %c2_i32 = arith.constant 2 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.muli %0, %c2_i32 : i32
    %2 = arith.addi %1, %c2_i32 : i32
    %3 = arith.minsi %2, %c192_i32 : i32
    %4 = arith.subi %3, %1 : i32
    %c1_i32_6 = arith.constant 1 : i32
    %5 = arith.subi %c1_i32, %c1_i32_6 : i32
    %6 = arith.addi %4, %5 : i32
    %7 = arith.divui %6, %c1_i32 : i32
    %c2_i32_7 = arith.constant 2 : i32
    %8 = arith.remsi %7, %c2_i32_7 : i32
    %9 = arith.subi %7, %8 : i32
    %10 = arith.muli %9, %c1_i32 : i32
    %11 = arith.addi %1, %10 : i32
    %12 = arith.muli %c1_i32, %c2_i32_7 : i32
    scf.for %arg6 = %1 to %11 step %12  : i32 {
      %13 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
      %14 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>
      %15 = arith.muli %arg6, %c8192_i32 : i32
      %16 = tt.expand_dims %14 {axis = 1 : i32} : tensor<128xi32> -> tensor<128x1xi32>
      %17 = arith.muli %16, %cst_2 : tensor<128x1xi32>
      %18 = tt.splat %15 : i32 -> tensor<128x1xi32>
      %19 = arith.addi %18, %17 : tensor<128x1xi32>
      %20 = tt.expand_dims %13 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %21 = tt.broadcast %19 : tensor<128x1xi32> -> tensor<128x64xi32>
      %22 = tt.broadcast %20 : tensor<1x64xi32> -> tensor<128x64xi32>
      %23 = arith.addi %21, %22 : tensor<128x64xi32>
      %24 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %25 = tt.addptr %24, %23 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      %26 = tt.load %25 : tensor<128x64x!tt.ptr<f8E5M2>>
      %c0_i32_8 = arith.constant 0 : i32
      %c256_i32 = arith.constant 256 : i32
      %27:3 = scf.for %arg7 = %c0_i32 to %c0_i32_8 step %c256_i32 iter_args(%arg8 = %cst_4, %arg9 = %cst_3, %arg10 = %cst_5) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %97 = tt.splat %arg7 : i32 -> tensor<64xi32>
        %98 = arith.addi %97, %13 : tensor<64xi32>
        %99 = tt.expand_dims %98 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %100 = arith.muli %99, %cst_1 : tensor<64x1xi32>
        %101 = tt.splat %15 : i32 -> tensor<64x1xi32>
        %102 = arith.addi %101, %100 : tensor<64x1xi32>
        %103 = tt.broadcast %102 : tensor<64x1xi32> -> tensor<64x64xi32>
        %104 = tt.broadcast %20 : tensor<1x64xi32> -> tensor<64x64xi32>
        %105 = arith.addi %103, %104 : tensor<64x64xi32>
        %106 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %107 = tt.addptr %106, %105 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %108 = tt.load %107 : tensor<64x64x!tt.ptr<f8E5M2>>
        %109 = tt.trans %108 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %110 = tt.dot %26, %109, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %111 = arith.mulf %110, %cst_0 : tensor<128x64xf32>
        %112 = "tt.reduce"(%111) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %287 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %287 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %113 = arith.cmpf ogt, %arg10, %112 : tensor<128xf32>
        %114 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %115 = arith.ori %113, %114 : tensor<128xi1>
        %116 = arith.select %115, %arg10, %112 : tensor<128xi1>, tensor<128xf32>
        %117 = tt.expand_dims %116 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %118 = tt.broadcast %117 : tensor<128x1xf32> -> tensor<128x64xf32>
        %119 = arith.subf %111, %118 : tensor<128x64xf32>
        %120 = tt.extern_elementwise %119 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %121 = "tt.reduce"(%120) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %287 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %287 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %122 = arith.subf %arg10, %116 : tensor<128xf32>
        %123 = tt.extern_elementwise %122 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %124 = arith.mulf %arg8, %123 : tensor<128xf32>
        %125 = arith.addf %124, %121 : tensor<128xf32>
        %126 = tt.expand_dims %123 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %127 = tt.broadcast %126 : tensor<128x1xf32> -> tensor<128x64xf32>
        %128 = arith.mulf %arg9, %127 : tensor<128x64xf32>
        %129 = tt.expand_dims %13 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %130 = arith.addi %101, %129 : tensor<64x1xi32>
        %131 = tt.expand_dims %98 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %132 = arith.muli %131, %cst : tensor<1x64xi32>
        %133 = tt.broadcast %130 : tensor<64x1xi32> -> tensor<64x64xi32>
        %134 = tt.broadcast %132 : tensor<1x64xi32> -> tensor<64x64xi32>
        %135 = arith.addi %133, %134 : tensor<64x64xi32>
        %136 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %137 = tt.addptr %136, %135 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %138 = tt.load %137 : tensor<64x64x!tt.ptr<f8E5M2>>
        %139 = tt.fp_to_fp %120, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %140 = tt.trans %138 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %141 = tt.dot %139, %140, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %142 = arith.addf %128, %141 : tensor<128x64xf32>
        %c1_i32_12 = arith.constant 1 : i32
        %143 = arith.muli %c64_i32, %c1_i32_12 : i32
        %144 = arith.addi %arg7, %143 : i32
        %145 = tt.splat %144 : i32 -> tensor<64xi32>
        %146 = arith.addi %145, %13 : tensor<64xi32>
        %147 = tt.expand_dims %146 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %148 = arith.muli %147, %cst_1 : tensor<64x1xi32>
        %149 = tt.splat %15 : i32 -> tensor<64x1xi32>
        %150 = arith.addi %149, %148 : tensor<64x1xi32>
        %151 = tt.broadcast %150 : tensor<64x1xi32> -> tensor<64x64xi32>
        %152 = tt.broadcast %20 : tensor<1x64xi32> -> tensor<64x64xi32>
        %153 = arith.addi %151, %152 : tensor<64x64xi32>
        %154 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %155 = tt.addptr %154, %153 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %156 = tt.load %155 : tensor<64x64x!tt.ptr<f8E5M2>>
        %157 = tt.trans %156 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %158 = tt.dot %26, %157, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %159 = arith.mulf %158, %cst_0 : tensor<128x64xf32>
        %160 = "tt.reduce"(%159) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %287 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %287 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %161 = arith.cmpf ogt, %116, %160 : tensor<128xf32>
        %162 = arith.cmpf une, %116, %116 : tensor<128xf32>
        %163 = arith.ori %161, %162 : tensor<128xi1>
        %164 = arith.select %163, %116, %160 : tensor<128xi1>, tensor<128xf32>
        %165 = tt.expand_dims %164 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %166 = tt.broadcast %165 : tensor<128x1xf32> -> tensor<128x64xf32>
        %167 = arith.subf %159, %166 : tensor<128x64xf32>
        %168 = tt.extern_elementwise %167 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %169 = "tt.reduce"(%168) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %287 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %287 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %170 = arith.subf %116, %164 : tensor<128xf32>
        %171 = tt.extern_elementwise %170 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %172 = arith.mulf %125, %171 : tensor<128xf32>
        %173 = arith.addf %172, %169 : tensor<128xf32>
        %174 = tt.expand_dims %171 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %175 = tt.broadcast %174 : tensor<128x1xf32> -> tensor<128x64xf32>
        %176 = arith.mulf %142, %175 : tensor<128x64xf32>
        %177 = tt.expand_dims %13 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %178 = arith.addi %149, %177 : tensor<64x1xi32>
        %179 = tt.expand_dims %146 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %180 = arith.muli %179, %cst : tensor<1x64xi32>
        %181 = tt.broadcast %178 : tensor<64x1xi32> -> tensor<64x64xi32>
        %182 = tt.broadcast %180 : tensor<1x64xi32> -> tensor<64x64xi32>
        %183 = arith.addi %181, %182 : tensor<64x64xi32>
        %184 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %185 = tt.addptr %184, %183 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %186 = tt.load %185 : tensor<64x64x!tt.ptr<f8E5M2>>
        %187 = tt.fp_to_fp %168, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %188 = tt.trans %186 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %189 = tt.dot %187, %188, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %190 = arith.addf %176, %189 : tensor<128x64xf32>
        %c2_i32_13 = arith.constant 2 : i32
        %191 = arith.muli %c64_i32, %c2_i32_13 : i32
        %192 = arith.addi %arg7, %191 : i32
        %193 = tt.splat %192 : i32 -> tensor<64xi32>
        %194 = arith.addi %193, %13 : tensor<64xi32>
        %195 = tt.expand_dims %194 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %196 = arith.muli %195, %cst_1 : tensor<64x1xi32>
        %197 = tt.splat %15 : i32 -> tensor<64x1xi32>
        %198 = arith.addi %197, %196 : tensor<64x1xi32>
        %199 = tt.broadcast %198 : tensor<64x1xi32> -> tensor<64x64xi32>
        %200 = tt.broadcast %20 : tensor<1x64xi32> -> tensor<64x64xi32>
        %201 = arith.addi %199, %200 : tensor<64x64xi32>
        %202 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %203 = tt.addptr %202, %201 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %204 = tt.load %203 : tensor<64x64x!tt.ptr<f8E5M2>>
        %205 = tt.trans %204 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %206 = tt.dot %26, %205, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %207 = arith.mulf %206, %cst_0 : tensor<128x64xf32>
        %208 = "tt.reduce"(%207) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %287 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %287 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %209 = arith.cmpf ogt, %164, %208 : tensor<128xf32>
        %210 = arith.cmpf une, %164, %164 : tensor<128xf32>
        %211 = arith.ori %209, %210 : tensor<128xi1>
        %212 = arith.select %211, %164, %208 : tensor<128xi1>, tensor<128xf32>
        %213 = tt.expand_dims %212 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %214 = tt.broadcast %213 : tensor<128x1xf32> -> tensor<128x64xf32>
        %215 = arith.subf %207, %214 : tensor<128x64xf32>
        %216 = tt.extern_elementwise %215 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %217 = "tt.reduce"(%216) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %287 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %287 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %218 = arith.subf %164, %212 : tensor<128xf32>
        %219 = tt.extern_elementwise %218 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %220 = arith.mulf %173, %219 : tensor<128xf32>
        %221 = arith.addf %220, %217 : tensor<128xf32>
        %222 = tt.expand_dims %219 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %223 = tt.broadcast %222 : tensor<128x1xf32> -> tensor<128x64xf32>
        %224 = arith.mulf %190, %223 : tensor<128x64xf32>
        %225 = tt.expand_dims %13 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %226 = arith.addi %197, %225 : tensor<64x1xi32>
        %227 = tt.expand_dims %194 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %228 = arith.muli %227, %cst : tensor<1x64xi32>
        %229 = tt.broadcast %226 : tensor<64x1xi32> -> tensor<64x64xi32>
        %230 = tt.broadcast %228 : tensor<1x64xi32> -> tensor<64x64xi32>
        %231 = arith.addi %229, %230 : tensor<64x64xi32>
        %232 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %233 = tt.addptr %232, %231 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %234 = tt.load %233 : tensor<64x64x!tt.ptr<f8E5M2>>
        %235 = tt.fp_to_fp %216, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %236 = tt.trans %234 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %237 = tt.dot %235, %236, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %238 = arith.addf %224, %237 : tensor<128x64xf32>
        %c3_i32 = arith.constant 3 : i32
        %239 = arith.muli %c64_i32, %c3_i32 : i32
        %240 = arith.addi %arg7, %239 : i32
        %241 = tt.splat %240 : i32 -> tensor<64xi32>
        %242 = arith.addi %241, %13 : tensor<64xi32>
        %243 = tt.expand_dims %242 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %244 = arith.muli %243, %cst_1 : tensor<64x1xi32>
        %245 = tt.splat %15 : i32 -> tensor<64x1xi32>
        %246 = arith.addi %245, %244 : tensor<64x1xi32>
        %247 = tt.broadcast %246 : tensor<64x1xi32> -> tensor<64x64xi32>
        %248 = tt.broadcast %20 : tensor<1x64xi32> -> tensor<64x64xi32>
        %249 = arith.addi %247, %248 : tensor<64x64xi32>
        %250 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %251 = tt.addptr %250, %249 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %252 = tt.load %251 : tensor<64x64x!tt.ptr<f8E5M2>>
        %253 = tt.trans %252 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %254 = tt.dot %26, %253, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %255 = arith.mulf %254, %cst_0 : tensor<128x64xf32>
        %256 = "tt.reduce"(%255) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %287 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %287 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %257 = arith.cmpf ogt, %212, %256 : tensor<128xf32>
        %258 = arith.cmpf une, %212, %212 : tensor<128xf32>
        %259 = arith.ori %257, %258 : tensor<128xi1>
        %260 = arith.select %259, %212, %256 : tensor<128xi1>, tensor<128xf32>
        %261 = tt.expand_dims %260 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %262 = tt.broadcast %261 : tensor<128x1xf32> -> tensor<128x64xf32>
        %263 = arith.subf %255, %262 : tensor<128x64xf32>
        %264 = tt.extern_elementwise %263 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %265 = "tt.reduce"(%264) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %287 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %287 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %266 = arith.subf %212, %260 : tensor<128xf32>
        %267 = tt.extern_elementwise %266 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %268 = arith.mulf %221, %267 : tensor<128xf32>
        %269 = arith.addf %268, %265 : tensor<128xf32>
        %270 = tt.expand_dims %267 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %271 = tt.broadcast %270 : tensor<128x1xf32> -> tensor<128x64xf32>
        %272 = arith.mulf %238, %271 : tensor<128x64xf32>
        %273 = tt.expand_dims %13 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %274 = arith.addi %245, %273 : tensor<64x1xi32>
        %275 = tt.expand_dims %242 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %276 = arith.muli %275, %cst : tensor<1x64xi32>
        %277 = tt.broadcast %274 : tensor<64x1xi32> -> tensor<64x64xi32>
        %278 = tt.broadcast %276 : tensor<1x64xi32> -> tensor<64x64xi32>
        %279 = arith.addi %277, %278 : tensor<64x64xi32>
        %280 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %281 = tt.addptr %280, %279 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %282 = tt.load %281 : tensor<64x64x!tt.ptr<f8E5M2>>
        %283 = tt.fp_to_fp %264, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %284 = tt.trans %282 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %285 = tt.dot %283, %284, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %286 = arith.addf %272, %285 : tensor<128x64xf32>
        scf.yield %269, %286, %260 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.num_stages = 2 : i32}
      %28:3 = scf.for %arg7 = %c0_i32_8 to %c128_i32 step %c64_i32 iter_args(%arg8 = %27#0, %arg9 = %27#1, %arg10 = %27#2) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %97 = tt.splat %arg7 : i32 -> tensor<64xi32>
        %98 = arith.addi %97, %13 : tensor<64xi32>
        %99 = tt.expand_dims %98 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %100 = arith.muli %99, %cst_1 : tensor<64x1xi32>
        %101 = tt.splat %15 : i32 -> tensor<64x1xi32>
        %102 = arith.addi %101, %100 : tensor<64x1xi32>
        %103 = tt.broadcast %102 : tensor<64x1xi32> -> tensor<64x64xi32>
        %104 = tt.broadcast %20 : tensor<1x64xi32> -> tensor<64x64xi32>
        %105 = arith.addi %103, %104 : tensor<64x64xi32>
        %106 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %107 = tt.addptr %106, %105 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %108 = tt.load %107 : tensor<64x64x!tt.ptr<f8E5M2>>
        %109 = tt.trans %108 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %110 = tt.dot %26, %109, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %111 = arith.mulf %110, %cst_0 : tensor<128x64xf32>
        %112 = "tt.reduce"(%111) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %143 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %143 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %113 = arith.cmpf ogt, %arg10, %112 : tensor<128xf32>
        %114 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %115 = arith.ori %113, %114 : tensor<128xi1>
        %116 = arith.select %115, %arg10, %112 : tensor<128xi1>, tensor<128xf32>
        %117 = tt.expand_dims %116 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %118 = tt.broadcast %117 : tensor<128x1xf32> -> tensor<128x64xf32>
        %119 = arith.subf %111, %118 : tensor<128x64xf32>
        %120 = tt.extern_elementwise %119 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %121 = "tt.reduce"(%120) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %143 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %143 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %122 = arith.subf %arg10, %116 : tensor<128xf32>
        %123 = tt.extern_elementwise %122 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %124 = arith.mulf %arg8, %123 : tensor<128xf32>
        %125 = arith.addf %124, %121 : tensor<128xf32>
        %126 = tt.expand_dims %123 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %127 = tt.broadcast %126 : tensor<128x1xf32> -> tensor<128x64xf32>
        %128 = arith.mulf %arg9, %127 : tensor<128x64xf32>
        %129 = tt.expand_dims %13 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %130 = arith.addi %101, %129 : tensor<64x1xi32>
        %131 = tt.expand_dims %98 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %132 = arith.muli %131, %cst : tensor<1x64xi32>
        %133 = tt.broadcast %130 : tensor<64x1xi32> -> tensor<64x64xi32>
        %134 = tt.broadcast %132 : tensor<1x64xi32> -> tensor<64x64xi32>
        %135 = arith.addi %133, %134 : tensor<64x64xi32>
        %136 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %137 = tt.addptr %136, %135 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %138 = tt.load %137 : tensor<64x64x!tt.ptr<f8E5M2>>
        %139 = tt.fp_to_fp %120, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %140 = tt.trans %138 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %141 = tt.dot %139, %140, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %142 = arith.addf %128, %141 : tensor<128x64xf32>
        scf.yield %125, %142, %116 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.num_stages = 1 : i32}
      %29 = tt.expand_dims %28#0 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
      %30 = tt.broadcast %29 : tensor<128x1xf32> -> tensor<128x64xf32>
      %31 = arith.divf %28#1, %30 : tensor<128x64xf32>
      %32 = tt.fp_to_fp %31, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
      %33 = arith.divsi %arg6, %arg5 : i32
      %34 = arith.remsi %arg6, %arg5 : i32
      %35 = arith.cmpi ne, %34, %c0_i32 : i32
      %36 = arith.subi %33, %c1_i32 : i32
      %37 = arith.select %35, %36, %33 : i32
      %38 = arith.cmpi slt, %arg6, %c0_i32 : i32
      %39 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %40 = arith.cmpi ne, %38, %39 : i1
      %41 = arith.select %40, %37, %33 : i32
      %42 = arith.andi %35, %40 : i1
      %43 = arith.addi %34, %arg5 : i32
      %44 = arith.select %42, %43, %34 : i32
      %45 = arith.muli %41, %arg4 : i32
      %46 = arith.muli %44, %c8192_i32 : i32
      %47 = arith.addi %45, %46 : i32
      %48 = tt.splat %47 : i32 -> tensor<128x1xi32>
      %49 = arith.addi %48, %17 : tensor<128x1xi32>
      %50 = tt.broadcast %49 : tensor<128x1xi32> -> tensor<128x64xi32>
      %51 = arith.addi %50, %22 : tensor<128x64xi32>
      %52 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %53 = tt.addptr %52, %51 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      tt.store %53, %32 : tensor<128x64x!tt.ptr<f8E5M2>>
      %c1_i32_9 = arith.constant 1 : i32
      %54 = arith.muli %c1_i32, %c1_i32_9 : i32
      %55 = arith.addi %arg6, %54 : i32
      %56 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
      %57 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>
      %58 = arith.muli %55, %c8192_i32 : i32
      %59 = tt.expand_dims %57 {axis = 1 : i32} : tensor<128xi32> -> tensor<128x1xi32>
      %60 = arith.muli %59, %cst_2 : tensor<128x1xi32>
      %61 = tt.splat %58 : i32 -> tensor<128x1xi32>
      %62 = arith.addi %61, %60 : tensor<128x1xi32>
      %63 = tt.expand_dims %56 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %64 = tt.broadcast %62 : tensor<128x1xi32> -> tensor<128x64xi32>
      %65 = tt.broadcast %63 : tensor<1x64xi32> -> tensor<128x64xi32>
      %66 = arith.addi %64, %65 : tensor<128x64xi32>
      %67 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %68 = tt.addptr %67, %66 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      %69 = tt.load %68 : tensor<128x64x!tt.ptr<f8E5M2>>
      %c0_i32_10 = arith.constant 0 : i32
      %c256_i32_11 = arith.constant 256 : i32
      %70:3 = scf.for %arg7 = %c0_i32 to %c0_i32_10 step %c256_i32_11 iter_args(%arg8 = %cst_4, %arg9 = %cst_3, %arg10 = %cst_5) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %97 = tt.splat %arg7 : i32 -> tensor<64xi32>
        %98 = arith.addi %97, %56 : tensor<64xi32>
        %99 = tt.expand_dims %98 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %100 = arith.muli %99, %cst_1 : tensor<64x1xi32>
        %101 = tt.splat %58 : i32 -> tensor<64x1xi32>
        %102 = arith.addi %101, %100 : tensor<64x1xi32>
        %103 = tt.broadcast %102 : tensor<64x1xi32> -> tensor<64x64xi32>
        %104 = tt.broadcast %63 : tensor<1x64xi32> -> tensor<64x64xi32>
        %105 = arith.addi %103, %104 : tensor<64x64xi32>
        %106 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %107 = tt.addptr %106, %105 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %108 = tt.load %107 : tensor<64x64x!tt.ptr<f8E5M2>>
        %109 = tt.trans %108 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %110 = tt.dot %69, %109, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %111 = arith.mulf %110, %cst_0 : tensor<128x64xf32>
        %112 = "tt.reduce"(%111) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %287 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %287 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %113 = arith.cmpf ogt, %arg10, %112 : tensor<128xf32>
        %114 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %115 = arith.ori %113, %114 : tensor<128xi1>
        %116 = arith.select %115, %arg10, %112 : tensor<128xi1>, tensor<128xf32>
        %117 = tt.expand_dims %116 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %118 = tt.broadcast %117 : tensor<128x1xf32> -> tensor<128x64xf32>
        %119 = arith.subf %111, %118 : tensor<128x64xf32>
        %120 = tt.extern_elementwise %119 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %121 = "tt.reduce"(%120) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %287 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %287 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %122 = arith.subf %arg10, %116 : tensor<128xf32>
        %123 = tt.extern_elementwise %122 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %124 = arith.mulf %arg8, %123 : tensor<128xf32>
        %125 = arith.addf %124, %121 : tensor<128xf32>
        %126 = tt.expand_dims %123 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %127 = tt.broadcast %126 : tensor<128x1xf32> -> tensor<128x64xf32>
        %128 = arith.mulf %arg9, %127 : tensor<128x64xf32>
        %129 = tt.expand_dims %56 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %130 = arith.addi %101, %129 : tensor<64x1xi32>
        %131 = tt.expand_dims %98 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %132 = arith.muli %131, %cst : tensor<1x64xi32>
        %133 = tt.broadcast %130 : tensor<64x1xi32> -> tensor<64x64xi32>
        %134 = tt.broadcast %132 : tensor<1x64xi32> -> tensor<64x64xi32>
        %135 = arith.addi %133, %134 : tensor<64x64xi32>
        %136 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %137 = tt.addptr %136, %135 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %138 = tt.load %137 : tensor<64x64x!tt.ptr<f8E5M2>>
        %139 = tt.fp_to_fp %120, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %140 = tt.trans %138 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %141 = tt.dot %139, %140, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %142 = arith.addf %128, %141 : tensor<128x64xf32>
        %c1_i32_12 = arith.constant 1 : i32
        %143 = arith.muli %c64_i32, %c1_i32_12 : i32
        %144 = arith.addi %arg7, %143 : i32
        %145 = tt.splat %144 : i32 -> tensor<64xi32>
        %146 = arith.addi %145, %56 : tensor<64xi32>
        %147 = tt.expand_dims %146 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %148 = arith.muli %147, %cst_1 : tensor<64x1xi32>
        %149 = tt.splat %58 : i32 -> tensor<64x1xi32>
        %150 = arith.addi %149, %148 : tensor<64x1xi32>
        %151 = tt.broadcast %150 : tensor<64x1xi32> -> tensor<64x64xi32>
        %152 = tt.broadcast %63 : tensor<1x64xi32> -> tensor<64x64xi32>
        %153 = arith.addi %151, %152 : tensor<64x64xi32>
        %154 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %155 = tt.addptr %154, %153 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %156 = tt.load %155 : tensor<64x64x!tt.ptr<f8E5M2>>
        %157 = tt.trans %156 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %158 = tt.dot %69, %157, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %159 = arith.mulf %158, %cst_0 : tensor<128x64xf32>
        %160 = "tt.reduce"(%159) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %287 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %287 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %161 = arith.cmpf ogt, %116, %160 : tensor<128xf32>
        %162 = arith.cmpf une, %116, %116 : tensor<128xf32>
        %163 = arith.ori %161, %162 : tensor<128xi1>
        %164 = arith.select %163, %116, %160 : tensor<128xi1>, tensor<128xf32>
        %165 = tt.expand_dims %164 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %166 = tt.broadcast %165 : tensor<128x1xf32> -> tensor<128x64xf32>
        %167 = arith.subf %159, %166 : tensor<128x64xf32>
        %168 = tt.extern_elementwise %167 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %169 = "tt.reduce"(%168) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %287 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %287 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %170 = arith.subf %116, %164 : tensor<128xf32>
        %171 = tt.extern_elementwise %170 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %172 = arith.mulf %125, %171 : tensor<128xf32>
        %173 = arith.addf %172, %169 : tensor<128xf32>
        %174 = tt.expand_dims %171 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %175 = tt.broadcast %174 : tensor<128x1xf32> -> tensor<128x64xf32>
        %176 = arith.mulf %142, %175 : tensor<128x64xf32>
        %177 = tt.expand_dims %56 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %178 = arith.addi %149, %177 : tensor<64x1xi32>
        %179 = tt.expand_dims %146 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %180 = arith.muli %179, %cst : tensor<1x64xi32>
        %181 = tt.broadcast %178 : tensor<64x1xi32> -> tensor<64x64xi32>
        %182 = tt.broadcast %180 : tensor<1x64xi32> -> tensor<64x64xi32>
        %183 = arith.addi %181, %182 : tensor<64x64xi32>
        %184 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %185 = tt.addptr %184, %183 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %186 = tt.load %185 : tensor<64x64x!tt.ptr<f8E5M2>>
        %187 = tt.fp_to_fp %168, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %188 = tt.trans %186 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %189 = tt.dot %187, %188, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %190 = arith.addf %176, %189 : tensor<128x64xf32>
        %c2_i32_13 = arith.constant 2 : i32
        %191 = arith.muli %c64_i32, %c2_i32_13 : i32
        %192 = arith.addi %arg7, %191 : i32
        %193 = tt.splat %192 : i32 -> tensor<64xi32>
        %194 = arith.addi %193, %56 : tensor<64xi32>
        %195 = tt.expand_dims %194 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %196 = arith.muli %195, %cst_1 : tensor<64x1xi32>
        %197 = tt.splat %58 : i32 -> tensor<64x1xi32>
        %198 = arith.addi %197, %196 : tensor<64x1xi32>
        %199 = tt.broadcast %198 : tensor<64x1xi32> -> tensor<64x64xi32>
        %200 = tt.broadcast %63 : tensor<1x64xi32> -> tensor<64x64xi32>
        %201 = arith.addi %199, %200 : tensor<64x64xi32>
        %202 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %203 = tt.addptr %202, %201 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %204 = tt.load %203 : tensor<64x64x!tt.ptr<f8E5M2>>
        %205 = tt.trans %204 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %206 = tt.dot %69, %205, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %207 = arith.mulf %206, %cst_0 : tensor<128x64xf32>
        %208 = "tt.reduce"(%207) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %287 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %287 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %209 = arith.cmpf ogt, %164, %208 : tensor<128xf32>
        %210 = arith.cmpf une, %164, %164 : tensor<128xf32>
        %211 = arith.ori %209, %210 : tensor<128xi1>
        %212 = arith.select %211, %164, %208 : tensor<128xi1>, tensor<128xf32>
        %213 = tt.expand_dims %212 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %214 = tt.broadcast %213 : tensor<128x1xf32> -> tensor<128x64xf32>
        %215 = arith.subf %207, %214 : tensor<128x64xf32>
        %216 = tt.extern_elementwise %215 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %217 = "tt.reduce"(%216) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %287 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %287 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %218 = arith.subf %164, %212 : tensor<128xf32>
        %219 = tt.extern_elementwise %218 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %220 = arith.mulf %173, %219 : tensor<128xf32>
        %221 = arith.addf %220, %217 : tensor<128xf32>
        %222 = tt.expand_dims %219 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %223 = tt.broadcast %222 : tensor<128x1xf32> -> tensor<128x64xf32>
        %224 = arith.mulf %190, %223 : tensor<128x64xf32>
        %225 = tt.expand_dims %56 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %226 = arith.addi %197, %225 : tensor<64x1xi32>
        %227 = tt.expand_dims %194 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %228 = arith.muli %227, %cst : tensor<1x64xi32>
        %229 = tt.broadcast %226 : tensor<64x1xi32> -> tensor<64x64xi32>
        %230 = tt.broadcast %228 : tensor<1x64xi32> -> tensor<64x64xi32>
        %231 = arith.addi %229, %230 : tensor<64x64xi32>
        %232 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %233 = tt.addptr %232, %231 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %234 = tt.load %233 : tensor<64x64x!tt.ptr<f8E5M2>>
        %235 = tt.fp_to_fp %216, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %236 = tt.trans %234 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %237 = tt.dot %235, %236, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %238 = arith.addf %224, %237 : tensor<128x64xf32>
        %c3_i32 = arith.constant 3 : i32
        %239 = arith.muli %c64_i32, %c3_i32 : i32
        %240 = arith.addi %arg7, %239 : i32
        %241 = tt.splat %240 : i32 -> tensor<64xi32>
        %242 = arith.addi %241, %56 : tensor<64xi32>
        %243 = tt.expand_dims %242 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %244 = arith.muli %243, %cst_1 : tensor<64x1xi32>
        %245 = tt.splat %58 : i32 -> tensor<64x1xi32>
        %246 = arith.addi %245, %244 : tensor<64x1xi32>
        %247 = tt.broadcast %246 : tensor<64x1xi32> -> tensor<64x64xi32>
        %248 = tt.broadcast %63 : tensor<1x64xi32> -> tensor<64x64xi32>
        %249 = arith.addi %247, %248 : tensor<64x64xi32>
        %250 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %251 = tt.addptr %250, %249 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %252 = tt.load %251 : tensor<64x64x!tt.ptr<f8E5M2>>
        %253 = tt.trans %252 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %254 = tt.dot %69, %253, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %255 = arith.mulf %254, %cst_0 : tensor<128x64xf32>
        %256 = "tt.reduce"(%255) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %287 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %287 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %257 = arith.cmpf ogt, %212, %256 : tensor<128xf32>
        %258 = arith.cmpf une, %212, %212 : tensor<128xf32>
        %259 = arith.ori %257, %258 : tensor<128xi1>
        %260 = arith.select %259, %212, %256 : tensor<128xi1>, tensor<128xf32>
        %261 = tt.expand_dims %260 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %262 = tt.broadcast %261 : tensor<128x1xf32> -> tensor<128x64xf32>
        %263 = arith.subf %255, %262 : tensor<128x64xf32>
        %264 = tt.extern_elementwise %263 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %265 = "tt.reduce"(%264) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %287 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %287 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %266 = arith.subf %212, %260 : tensor<128xf32>
        %267 = tt.extern_elementwise %266 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %268 = arith.mulf %221, %267 : tensor<128xf32>
        %269 = arith.addf %268, %265 : tensor<128xf32>
        %270 = tt.expand_dims %267 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %271 = tt.broadcast %270 : tensor<128x1xf32> -> tensor<128x64xf32>
        %272 = arith.mulf %238, %271 : tensor<128x64xf32>
        %273 = tt.expand_dims %56 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %274 = arith.addi %245, %273 : tensor<64x1xi32>
        %275 = tt.expand_dims %242 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %276 = arith.muli %275, %cst : tensor<1x64xi32>
        %277 = tt.broadcast %274 : tensor<64x1xi32> -> tensor<64x64xi32>
        %278 = tt.broadcast %276 : tensor<1x64xi32> -> tensor<64x64xi32>
        %279 = arith.addi %277, %278 : tensor<64x64xi32>
        %280 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %281 = tt.addptr %280, %279 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %282 = tt.load %281 : tensor<64x64x!tt.ptr<f8E5M2>>
        %283 = tt.fp_to_fp %264, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %284 = tt.trans %282 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %285 = tt.dot %283, %284, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %286 = arith.addf %272, %285 : tensor<128x64xf32>
        scf.yield %269, %286, %260 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.num_stages = 2 : i32}
      %71:3 = scf.for %arg7 = %c0_i32_10 to %c128_i32 step %c64_i32 iter_args(%arg8 = %70#0, %arg9 = %70#1, %arg10 = %70#2) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %97 = tt.splat %arg7 : i32 -> tensor<64xi32>
        %98 = arith.addi %97, %56 : tensor<64xi32>
        %99 = tt.expand_dims %98 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %100 = arith.muli %99, %cst_1 : tensor<64x1xi32>
        %101 = tt.splat %58 : i32 -> tensor<64x1xi32>
        %102 = arith.addi %101, %100 : tensor<64x1xi32>
        %103 = tt.broadcast %102 : tensor<64x1xi32> -> tensor<64x64xi32>
        %104 = tt.broadcast %63 : tensor<1x64xi32> -> tensor<64x64xi32>
        %105 = arith.addi %103, %104 : tensor<64x64xi32>
        %106 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %107 = tt.addptr %106, %105 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %108 = tt.load %107 : tensor<64x64x!tt.ptr<f8E5M2>>
        %109 = tt.trans %108 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %110 = tt.dot %69, %109, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %111 = arith.mulf %110, %cst_0 : tensor<128x64xf32>
        %112 = "tt.reduce"(%111) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %143 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %143 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %113 = arith.cmpf ogt, %arg10, %112 : tensor<128xf32>
        %114 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %115 = arith.ori %113, %114 : tensor<128xi1>
        %116 = arith.select %115, %arg10, %112 : tensor<128xi1>, tensor<128xf32>
        %117 = tt.expand_dims %116 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %118 = tt.broadcast %117 : tensor<128x1xf32> -> tensor<128x64xf32>
        %119 = arith.subf %111, %118 : tensor<128x64xf32>
        %120 = tt.extern_elementwise %119 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %121 = "tt.reduce"(%120) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %143 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %143 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %122 = arith.subf %arg10, %116 : tensor<128xf32>
        %123 = tt.extern_elementwise %122 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %124 = arith.mulf %arg8, %123 : tensor<128xf32>
        %125 = arith.addf %124, %121 : tensor<128xf32>
        %126 = tt.expand_dims %123 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %127 = tt.broadcast %126 : tensor<128x1xf32> -> tensor<128x64xf32>
        %128 = arith.mulf %arg9, %127 : tensor<128x64xf32>
        %129 = tt.expand_dims %56 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %130 = arith.addi %101, %129 : tensor<64x1xi32>
        %131 = tt.expand_dims %98 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %132 = arith.muli %131, %cst : tensor<1x64xi32>
        %133 = tt.broadcast %130 : tensor<64x1xi32> -> tensor<64x64xi32>
        %134 = tt.broadcast %132 : tensor<1x64xi32> -> tensor<64x64xi32>
        %135 = arith.addi %133, %134 : tensor<64x64xi32>
        %136 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %137 = tt.addptr %136, %135 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %138 = tt.load %137 : tensor<64x64x!tt.ptr<f8E5M2>>
        %139 = tt.fp_to_fp %120, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %140 = tt.trans %138 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %141 = tt.dot %139, %140, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %142 = arith.addf %128, %141 : tensor<128x64xf32>
        scf.yield %125, %142, %116 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.num_stages = 1 : i32}
      %72 = tt.expand_dims %71#0 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
      %73 = tt.broadcast %72 : tensor<128x1xf32> -> tensor<128x64xf32>
      %74 = arith.divf %71#1, %73 : tensor<128x64xf32>
      %75 = tt.fp_to_fp %74, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
      %76 = arith.divsi %55, %arg5 : i32
      %77 = arith.remsi %55, %arg5 : i32
      %78 = arith.cmpi ne, %77, %c0_i32 : i32
      %79 = arith.subi %76, %c1_i32 : i32
      %80 = arith.select %78, %79, %76 : i32
      %81 = arith.cmpi slt, %55, %c0_i32 : i32
      %82 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %83 = arith.cmpi ne, %81, %82 : i1
      %84 = arith.select %83, %80, %76 : i32
      %85 = arith.andi %78, %83 : i1
      %86 = arith.addi %77, %arg5 : i32
      %87 = arith.select %85, %86, %77 : i32
      %88 = arith.muli %84, %arg4 : i32
      %89 = arith.muli %87, %c8192_i32 : i32
      %90 = arith.addi %88, %89 : i32
      %91 = tt.splat %90 : i32 -> tensor<128x1xi32>
      %92 = arith.addi %91, %60 : tensor<128x1xi32>
      %93 = tt.broadcast %92 : tensor<128x1xi32> -> tensor<128x64xi32>
      %94 = arith.addi %93, %65 : tensor<128x64xi32>
      %95 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %96 = tt.addptr %95, %94 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      tt.store %96, %75 : tensor<128x64x!tt.ptr<f8E5M2>>
    } {tt.flatten, tt.num_stages = 2 : i32}
    scf.for %arg6 = %11 to %3 step %c1_i32  : i32 {
      %13 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
      %14 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>
      %15 = arith.muli %arg6, %c8192_i32 : i32
      %16 = tt.expand_dims %14 {axis = 1 : i32} : tensor<128xi32> -> tensor<128x1xi32>
      %17 = arith.muli %16, %cst_2 : tensor<128x1xi32>
      %18 = tt.splat %15 : i32 -> tensor<128x1xi32>
      %19 = arith.addi %18, %17 : tensor<128x1xi32>
      %20 = tt.expand_dims %13 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %21 = tt.broadcast %19 : tensor<128x1xi32> -> tensor<128x64xi32>
      %22 = tt.broadcast %20 : tensor<1x64xi32> -> tensor<128x64xi32>
      %23 = arith.addi %21, %22 : tensor<128x64xi32>
      %24 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %25 = tt.addptr %24, %23 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      %26 = tt.load %25 : tensor<128x64x!tt.ptr<f8E5M2>>
      %c0_i32_8 = arith.constant 0 : i32
      %c256_i32 = arith.constant 256 : i32
      %27:3 = scf.for %arg7 = %c0_i32 to %c0_i32_8 step %c256_i32 iter_args(%arg8 = %cst_4, %arg9 = %cst_3, %arg10 = %cst_5) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %54 = tt.splat %arg7 : i32 -> tensor<64xi32>
        %55 = arith.addi %54, %13 : tensor<64xi32>
        %56 = tt.expand_dims %55 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %57 = arith.muli %56, %cst_1 : tensor<64x1xi32>
        %58 = tt.splat %15 : i32 -> tensor<64x1xi32>
        %59 = arith.addi %58, %57 : tensor<64x1xi32>
        %60 = tt.broadcast %59 : tensor<64x1xi32> -> tensor<64x64xi32>
        %61 = tt.broadcast %20 : tensor<1x64xi32> -> tensor<64x64xi32>
        %62 = arith.addi %60, %61 : tensor<64x64xi32>
        %63 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %64 = tt.addptr %63, %62 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %65 = tt.load %64 : tensor<64x64x!tt.ptr<f8E5M2>>
        %66 = tt.trans %65 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %67 = tt.dot %26, %66, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %68 = arith.mulf %67, %cst_0 : tensor<128x64xf32>
        %69 = "tt.reduce"(%68) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %244 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %244 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %70 = arith.cmpf ogt, %arg10, %69 : tensor<128xf32>
        %71 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %72 = arith.ori %70, %71 : tensor<128xi1>
        %73 = arith.select %72, %arg10, %69 : tensor<128xi1>, tensor<128xf32>
        %74 = tt.expand_dims %73 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %75 = tt.broadcast %74 : tensor<128x1xf32> -> tensor<128x64xf32>
        %76 = arith.subf %68, %75 : tensor<128x64xf32>
        %77 = tt.extern_elementwise %76 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %78 = "tt.reduce"(%77) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %244 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %244 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %79 = arith.subf %arg10, %73 : tensor<128xf32>
        %80 = tt.extern_elementwise %79 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %81 = arith.mulf %arg8, %80 : tensor<128xf32>
        %82 = arith.addf %81, %78 : tensor<128xf32>
        %83 = tt.expand_dims %80 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %84 = tt.broadcast %83 : tensor<128x1xf32> -> tensor<128x64xf32>
        %85 = arith.mulf %arg9, %84 : tensor<128x64xf32>
        %86 = tt.expand_dims %13 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %87 = arith.addi %58, %86 : tensor<64x1xi32>
        %88 = tt.expand_dims %55 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %89 = arith.muli %88, %cst : tensor<1x64xi32>
        %90 = tt.broadcast %87 : tensor<64x1xi32> -> tensor<64x64xi32>
        %91 = tt.broadcast %89 : tensor<1x64xi32> -> tensor<64x64xi32>
        %92 = arith.addi %90, %91 : tensor<64x64xi32>
        %93 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %94 = tt.addptr %93, %92 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %95 = tt.load %94 : tensor<64x64x!tt.ptr<f8E5M2>>
        %96 = tt.fp_to_fp %77, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %97 = tt.trans %95 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %98 = tt.dot %96, %97, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %99 = arith.addf %85, %98 : tensor<128x64xf32>
        %c1_i32_9 = arith.constant 1 : i32
        %100 = arith.muli %c64_i32, %c1_i32_9 : i32
        %101 = arith.addi %arg7, %100 : i32
        %102 = tt.splat %101 : i32 -> tensor<64xi32>
        %103 = arith.addi %102, %13 : tensor<64xi32>
        %104 = tt.expand_dims %103 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %105 = arith.muli %104, %cst_1 : tensor<64x1xi32>
        %106 = tt.splat %15 : i32 -> tensor<64x1xi32>
        %107 = arith.addi %106, %105 : tensor<64x1xi32>
        %108 = tt.broadcast %107 : tensor<64x1xi32> -> tensor<64x64xi32>
        %109 = tt.broadcast %20 : tensor<1x64xi32> -> tensor<64x64xi32>
        %110 = arith.addi %108, %109 : tensor<64x64xi32>
        %111 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %112 = tt.addptr %111, %110 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %113 = tt.load %112 : tensor<64x64x!tt.ptr<f8E5M2>>
        %114 = tt.trans %113 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %115 = tt.dot %26, %114, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %116 = arith.mulf %115, %cst_0 : tensor<128x64xf32>
        %117 = "tt.reduce"(%116) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %244 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %244 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %118 = arith.cmpf ogt, %73, %117 : tensor<128xf32>
        %119 = arith.cmpf une, %73, %73 : tensor<128xf32>
        %120 = arith.ori %118, %119 : tensor<128xi1>
        %121 = arith.select %120, %73, %117 : tensor<128xi1>, tensor<128xf32>
        %122 = tt.expand_dims %121 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %123 = tt.broadcast %122 : tensor<128x1xf32> -> tensor<128x64xf32>
        %124 = arith.subf %116, %123 : tensor<128x64xf32>
        %125 = tt.extern_elementwise %124 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %126 = "tt.reduce"(%125) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %244 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %244 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %127 = arith.subf %73, %121 : tensor<128xf32>
        %128 = tt.extern_elementwise %127 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %129 = arith.mulf %82, %128 : tensor<128xf32>
        %130 = arith.addf %129, %126 : tensor<128xf32>
        %131 = tt.expand_dims %128 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %132 = tt.broadcast %131 : tensor<128x1xf32> -> tensor<128x64xf32>
        %133 = arith.mulf %99, %132 : tensor<128x64xf32>
        %134 = tt.expand_dims %13 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %135 = arith.addi %106, %134 : tensor<64x1xi32>
        %136 = tt.expand_dims %103 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %137 = arith.muli %136, %cst : tensor<1x64xi32>
        %138 = tt.broadcast %135 : tensor<64x1xi32> -> tensor<64x64xi32>
        %139 = tt.broadcast %137 : tensor<1x64xi32> -> tensor<64x64xi32>
        %140 = arith.addi %138, %139 : tensor<64x64xi32>
        %141 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %142 = tt.addptr %141, %140 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %143 = tt.load %142 : tensor<64x64x!tt.ptr<f8E5M2>>
        %144 = tt.fp_to_fp %125, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %145 = tt.trans %143 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %146 = tt.dot %144, %145, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %147 = arith.addf %133, %146 : tensor<128x64xf32>
        %c2_i32_10 = arith.constant 2 : i32
        %148 = arith.muli %c64_i32, %c2_i32_10 : i32
        %149 = arith.addi %arg7, %148 : i32
        %150 = tt.splat %149 : i32 -> tensor<64xi32>
        %151 = arith.addi %150, %13 : tensor<64xi32>
        %152 = tt.expand_dims %151 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %153 = arith.muli %152, %cst_1 : tensor<64x1xi32>
        %154 = tt.splat %15 : i32 -> tensor<64x1xi32>
        %155 = arith.addi %154, %153 : tensor<64x1xi32>
        %156 = tt.broadcast %155 : tensor<64x1xi32> -> tensor<64x64xi32>
        %157 = tt.broadcast %20 : tensor<1x64xi32> -> tensor<64x64xi32>
        %158 = arith.addi %156, %157 : tensor<64x64xi32>
        %159 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %160 = tt.addptr %159, %158 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %161 = tt.load %160 : tensor<64x64x!tt.ptr<f8E5M2>>
        %162 = tt.trans %161 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %163 = tt.dot %26, %162, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %164 = arith.mulf %163, %cst_0 : tensor<128x64xf32>
        %165 = "tt.reduce"(%164) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %244 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %244 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %166 = arith.cmpf ogt, %121, %165 : tensor<128xf32>
        %167 = arith.cmpf une, %121, %121 : tensor<128xf32>
        %168 = arith.ori %166, %167 : tensor<128xi1>
        %169 = arith.select %168, %121, %165 : tensor<128xi1>, tensor<128xf32>
        %170 = tt.expand_dims %169 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %171 = tt.broadcast %170 : tensor<128x1xf32> -> tensor<128x64xf32>
        %172 = arith.subf %164, %171 : tensor<128x64xf32>
        %173 = tt.extern_elementwise %172 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %174 = "tt.reduce"(%173) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %244 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %244 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %175 = arith.subf %121, %169 : tensor<128xf32>
        %176 = tt.extern_elementwise %175 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %177 = arith.mulf %130, %176 : tensor<128xf32>
        %178 = arith.addf %177, %174 : tensor<128xf32>
        %179 = tt.expand_dims %176 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %180 = tt.broadcast %179 : tensor<128x1xf32> -> tensor<128x64xf32>
        %181 = arith.mulf %147, %180 : tensor<128x64xf32>
        %182 = tt.expand_dims %13 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %183 = arith.addi %154, %182 : tensor<64x1xi32>
        %184 = tt.expand_dims %151 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %185 = arith.muli %184, %cst : tensor<1x64xi32>
        %186 = tt.broadcast %183 : tensor<64x1xi32> -> tensor<64x64xi32>
        %187 = tt.broadcast %185 : tensor<1x64xi32> -> tensor<64x64xi32>
        %188 = arith.addi %186, %187 : tensor<64x64xi32>
        %189 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %190 = tt.addptr %189, %188 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %191 = tt.load %190 : tensor<64x64x!tt.ptr<f8E5M2>>
        %192 = tt.fp_to_fp %173, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %193 = tt.trans %191 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %194 = tt.dot %192, %193, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %195 = arith.addf %181, %194 : tensor<128x64xf32>
        %c3_i32 = arith.constant 3 : i32
        %196 = arith.muli %c64_i32, %c3_i32 : i32
        %197 = arith.addi %arg7, %196 : i32
        %198 = tt.splat %197 : i32 -> tensor<64xi32>
        %199 = arith.addi %198, %13 : tensor<64xi32>
        %200 = tt.expand_dims %199 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %201 = arith.muli %200, %cst_1 : tensor<64x1xi32>
        %202 = tt.splat %15 : i32 -> tensor<64x1xi32>
        %203 = arith.addi %202, %201 : tensor<64x1xi32>
        %204 = tt.broadcast %203 : tensor<64x1xi32> -> tensor<64x64xi32>
        %205 = tt.broadcast %20 : tensor<1x64xi32> -> tensor<64x64xi32>
        %206 = arith.addi %204, %205 : tensor<64x64xi32>
        %207 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %208 = tt.addptr %207, %206 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %209 = tt.load %208 : tensor<64x64x!tt.ptr<f8E5M2>>
        %210 = tt.trans %209 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %211 = tt.dot %26, %210, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %212 = arith.mulf %211, %cst_0 : tensor<128x64xf32>
        %213 = "tt.reduce"(%212) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %244 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %244 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %214 = arith.cmpf ogt, %169, %213 : tensor<128xf32>
        %215 = arith.cmpf une, %169, %169 : tensor<128xf32>
        %216 = arith.ori %214, %215 : tensor<128xi1>
        %217 = arith.select %216, %169, %213 : tensor<128xi1>, tensor<128xf32>
        %218 = tt.expand_dims %217 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %219 = tt.broadcast %218 : tensor<128x1xf32> -> tensor<128x64xf32>
        %220 = arith.subf %212, %219 : tensor<128x64xf32>
        %221 = tt.extern_elementwise %220 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %222 = "tt.reduce"(%221) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %244 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %244 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %223 = arith.subf %169, %217 : tensor<128xf32>
        %224 = tt.extern_elementwise %223 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %225 = arith.mulf %178, %224 : tensor<128xf32>
        %226 = arith.addf %225, %222 : tensor<128xf32>
        %227 = tt.expand_dims %224 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %228 = tt.broadcast %227 : tensor<128x1xf32> -> tensor<128x64xf32>
        %229 = arith.mulf %195, %228 : tensor<128x64xf32>
        %230 = tt.expand_dims %13 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %231 = arith.addi %202, %230 : tensor<64x1xi32>
        %232 = tt.expand_dims %199 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %233 = arith.muli %232, %cst : tensor<1x64xi32>
        %234 = tt.broadcast %231 : tensor<64x1xi32> -> tensor<64x64xi32>
        %235 = tt.broadcast %233 : tensor<1x64xi32> -> tensor<64x64xi32>
        %236 = arith.addi %234, %235 : tensor<64x64xi32>
        %237 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %238 = tt.addptr %237, %236 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %239 = tt.load %238 : tensor<64x64x!tt.ptr<f8E5M2>>
        %240 = tt.fp_to_fp %221, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %241 = tt.trans %239 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %242 = tt.dot %240, %241, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %243 = arith.addf %229, %242 : tensor<128x64xf32>
        scf.yield %226, %243, %217 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.num_stages = 2 : i32}
      %28:3 = scf.for %arg7 = %c0_i32_8 to %c128_i32 step %c64_i32 iter_args(%arg8 = %27#0, %arg9 = %27#1, %arg10 = %27#2) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %54 = tt.splat %arg7 : i32 -> tensor<64xi32>
        %55 = arith.addi %54, %13 : tensor<64xi32>
        %56 = tt.expand_dims %55 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %57 = arith.muli %56, %cst_1 : tensor<64x1xi32>
        %58 = tt.splat %15 : i32 -> tensor<64x1xi32>
        %59 = arith.addi %58, %57 : tensor<64x1xi32>
        %60 = tt.broadcast %59 : tensor<64x1xi32> -> tensor<64x64xi32>
        %61 = tt.broadcast %20 : tensor<1x64xi32> -> tensor<64x64xi32>
        %62 = arith.addi %60, %61 : tensor<64x64xi32>
        %63 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %64 = tt.addptr %63, %62 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %65 = tt.load %64 : tensor<64x64x!tt.ptr<f8E5M2>>
        %66 = tt.trans %65 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %67 = tt.dot %26, %66, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %68 = arith.mulf %67, %cst_0 : tensor<128x64xf32>
        %69 = "tt.reduce"(%68) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %100 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %100 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %70 = arith.cmpf ogt, %arg10, %69 : tensor<128xf32>
        %71 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %72 = arith.ori %70, %71 : tensor<128xi1>
        %73 = arith.select %72, %arg10, %69 : tensor<128xi1>, tensor<128xf32>
        %74 = tt.expand_dims %73 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %75 = tt.broadcast %74 : tensor<128x1xf32> -> tensor<128x64xf32>
        %76 = arith.subf %68, %75 : tensor<128x64xf32>
        %77 = tt.extern_elementwise %76 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %78 = "tt.reduce"(%77) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %100 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %100 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %79 = arith.subf %arg10, %73 : tensor<128xf32>
        %80 = tt.extern_elementwise %79 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %81 = arith.mulf %arg8, %80 : tensor<128xf32>
        %82 = arith.addf %81, %78 : tensor<128xf32>
        %83 = tt.expand_dims %80 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %84 = tt.broadcast %83 : tensor<128x1xf32> -> tensor<128x64xf32>
        %85 = arith.mulf %arg9, %84 : tensor<128x64xf32>
        %86 = tt.expand_dims %13 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %87 = arith.addi %58, %86 : tensor<64x1xi32>
        %88 = tt.expand_dims %55 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %89 = arith.muli %88, %cst : tensor<1x64xi32>
        %90 = tt.broadcast %87 : tensor<64x1xi32> -> tensor<64x64xi32>
        %91 = tt.broadcast %89 : tensor<1x64xi32> -> tensor<64x64xi32>
        %92 = arith.addi %90, %91 : tensor<64x64xi32>
        %93 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %94 = tt.addptr %93, %92 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %95 = tt.load %94 : tensor<64x64x!tt.ptr<f8E5M2>>
        %96 = tt.fp_to_fp %77, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %97 = tt.trans %95 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %98 = tt.dot %96, %97, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %99 = arith.addf %85, %98 : tensor<128x64xf32>
        scf.yield %82, %99, %73 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.num_stages = 1 : i32}
      %29 = tt.expand_dims %28#0 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
      %30 = tt.broadcast %29 : tensor<128x1xf32> -> tensor<128x64xf32>
      %31 = arith.divf %28#1, %30 : tensor<128x64xf32>
      %32 = tt.fp_to_fp %31, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
      %33 = arith.divsi %arg6, %arg5 : i32
      %34 = arith.remsi %arg6, %arg5 : i32
      %35 = arith.cmpi ne, %34, %c0_i32 : i32
      %36 = arith.subi %33, %c1_i32 : i32
      %37 = arith.select %35, %36, %33 : i32
      %38 = arith.cmpi slt, %arg6, %c0_i32 : i32
      %39 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %40 = arith.cmpi ne, %38, %39 : i1
      %41 = arith.select %40, %37, %33 : i32
      %42 = arith.andi %35, %40 : i1
      %43 = arith.addi %34, %arg5 : i32
      %44 = arith.select %42, %43, %34 : i32
      %45 = arith.muli %41, %arg4 : i32
      %46 = arith.muli %44, %c8192_i32 : i32
      %47 = arith.addi %45, %46 : i32
      %48 = tt.splat %47 : i32 -> tensor<128x1xi32>
      %49 = arith.addi %48, %17 : tensor<128x1xi32>
      %50 = tt.broadcast %49 : tensor<128x1xi32> -> tensor<128x64xi32>
      %51 = arith.addi %50, %22 : tensor<128x64xi32>
      %52 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %53 = tt.addptr %52, %51 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      tt.store %53, %32 : tensor<128x64x!tt.ptr<f8E5M2>>
    } {tt.flatten, tt.num_stages = 1 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(convert-triton-to-tritongpu{enable-source-remat=false num-ctas=1 num-warps=8 target=cuda:90 threads-per-warp=32}, tritongpu-coalesce, tritongpu-F32DotTC, triton-nvidia-gpu-plan-cta, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritongpu-accelerate-matmul, tritongpu-remove-layout-conversions, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, triton-nvidia-optimize-descriptor-encoding, triton-loop-aware-cse, tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-combine-tensor-select-and-if, nvgpu-warp-specialization{dump-intermediate-steps=false num-stages=3}, tritongpu-assign-latencies{num-stages=3}, tritongpu-schedule-loops, tritongpu-pipeline{dump-intermediate-steps=false num-stages=3}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-loop-aware-cse, tritongpu-prefetch, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-coalesce-async-copy, triton-nvidia-optimize-tmem-layouts, tritongpu-remove-layout-conversions, triton-nvidia-interleave-tmem, tritongpu-reduce-data-duplication, tritongpu-reorder-instructions, triton-loop-aware-cse, symbol-dce, triton-nvidia-tma-lowering, triton-nvidia-gpu-fence-insertion{compute-capability=90}, sccp, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true})",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_willfeng/xt/cxtdqw3vwg4sfbh5csi43if5i7vcethnfl23j6jfximeeknqi7sj.py:13:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_willfeng/xt/cxtdqw3vwg4sfbh5csi43if5i7vcethnfl23j6jfximeeknqi7sj.py:13:0: note: Pipeline failed while executing [`TritonGPUAssignLatencies` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
Process ForkProcess-562:
Traceback (most recent call last):
  File "/home/willfeng/local/miniconda3/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/willfeng/local/miniconda3/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/data/users/willfeng/helion/helion/runtime/precompile_shim.py", line 54, in finish_it
    kernel_cache[key] = fn.compile(
                        ^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/compiler/compiler.py", line 359, in compile
    next_module = compile_ir(module, metadata)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/backends/nvidia/compiler.py", line 456, in <lambda>
    stages["ttgir"] = lambda src, metadata: self.make_ttgir(src, metadata, options, capability)
                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/backends/nvidia/compiler.py", line 298, in make_ttgir
    pm.run(mod)
RuntimeError: PassManager::run failed
python: /root/.triton/llvm/llvm-8957e64a-almalinux-x64/include/llvm/ADT/SmallVector.h:296: const_reference llvm::SmallVectorTemplateCommon<long>::operator[](size_type) const [T = long]: Assertion `idx < size()' failed.
module {
  tt.func public @_fp8_attention_kernel_kernel(%arg0: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %c1_i32 = arith.constant 1 : i32
    %c32_i32 = arith.constant 32 : i32
    %c128_i32 = arith.constant 128 : i32
    %c0_i32 = arith.constant 0 : i32
    %c132_i32 = arith.constant 132 : i32
    %c192_i32 = arith.constant 192 : i32
    %cst = arith.constant dense<64> : tensor<1x32xi32>
    %cst_0 = arith.constant dense<0.180336878> : tensor<128x32xf32>
    %cst_1 = arith.constant dense<0.000000e+00> : tensor<128x32xf32>
    %cst_2 = arith.constant dense<64> : tensor<32x1xi32>
    %cst_3 = arith.constant dense<64> : tensor<128x1xi32>
    %c8192_i32 = arith.constant 8192 : i32
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<128x64xf32>
    %cst_5 = arith.constant dense<0.000000e+00> : tensor<128xf32>
    %cst_6 = arith.constant dense<0xFF800000> : tensor<128xf32>
    %0 = tt.get_program_id x : i32
    scf.for %arg6 = %0 to %c192_i32 step %c132_i32  : i32 {
      %1 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
      %2 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>
      %3 = arith.muli %arg6, %c8192_i32 : i32
      %4 = tt.expand_dims %2 {axis = 1 : i32} : tensor<128xi32> -> tensor<128x1xi32>
      %5 = arith.muli %4, %cst_3 : tensor<128x1xi32>
      %6 = tt.splat %3 : i32 -> tensor<128x1xi32>
      %7 = arith.addi %6, %5 : tensor<128x1xi32>
      %8 = tt.expand_dims %1 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %9 = tt.broadcast %7 : tensor<128x1xi32> -> tensor<128x64xi32>
      %10 = tt.broadcast %8 : tensor<1x64xi32> -> tensor<128x64xi32>
      %11 = arith.addi %9, %10 : tensor<128x64xi32>
      %12 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %13 = tt.addptr %12, %11 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      %14 = tt.load %13 : tensor<128x64x!tt.ptr<f8E5M2>>
      %15:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c32_i32 iter_args(%arg8 = %cst_5, %arg9 = %cst_4, %arg10 = %cst_6) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %41 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32>
        %42 = tt.splat %arg7 : i32 -> tensor<32xi32>
        %43 = arith.addi %42, %41 : tensor<32xi32>
        %44 = tt.expand_dims %43 {axis = 1 : i32} : tensor<32xi32> -> tensor<32x1xi32>
        %45 = arith.muli %44, %cst_2 : tensor<32x1xi32>
        %46 = tt.splat %3 : i32 -> tensor<32x1xi32>
        %47 = arith.addi %46, %45 : tensor<32x1xi32>
        %48 = tt.broadcast %47 : tensor<32x1xi32> -> tensor<32x64xi32>
        %49 = tt.broadcast %8 : tensor<1x64xi32> -> tensor<32x64xi32>
        %50 = arith.addi %48, %49 : tensor<32x64xi32>
        %51 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<32x64x!tt.ptr<f8E5M2>>
        %52 = tt.addptr %51, %50 : tensor<32x64x!tt.ptr<f8E5M2>>, tensor<32x64xi32>
        %53 = tt.load %52 : tensor<32x64x!tt.ptr<f8E5M2>>
        %54 = tt.trans %53 {order = array<i32: 1, 0>} : tensor<32x64xf8E5M2> -> tensor<64x32xf8E5M2>
        %55 = tt.dot %14, %54, %cst_1, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x32xf8E5M2> -> tensor<128x32xf32>
        %56 = arith.mulf %55, %cst_0 : tensor<128x32xf32>
        %57 = "tt.reduce"(%56) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %89 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %89 : f32
        }) : (tensor<128x32xf32>) -> tensor<128xf32>
        %58 = arith.cmpf ogt, %arg10, %57 : tensor<128xf32>
        %59 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %60 = arith.ori %58, %59 : tensor<128xi1>
        %61 = arith.select %60, %arg10, %57 : tensor<128xi1>, tensor<128xf32>
        %62 = tt.expand_dims %61 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %63 = tt.broadcast %62 : tensor<128x1xf32> -> tensor<128x32xf32>
        %64 = arith.subf %56, %63 : tensor<128x32xf32>
        %65 = tt.extern_elementwise %64 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x32xf32>) -> tensor<128x32xf32>
        %66 = "tt.reduce"(%65) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %89 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %89 : f32
        }) : (tensor<128x32xf32>) -> tensor<128xf32>
        %67 = arith.subf %arg10, %61 : tensor<128xf32>
        %68 = tt.extern_elementwise %67 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %69 = arith.mulf %arg8, %68 : tensor<128xf32>
        %70 = arith.addf %69, %66 : tensor<128xf32>
        %71 = tt.expand_dims %68 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %72 = tt.broadcast %71 : tensor<128x1xf32> -> tensor<128x64xf32>
        %73 = arith.mulf %arg9, %72 : tensor<128x64xf32>
        %74 = tt.expand_dims %1 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %75 = tt.splat %3 : i32 -> tensor<64x1xi32>
        %76 = arith.addi %75, %74 : tensor<64x1xi32>
        %77 = tt.expand_dims %43 {axis = 0 : i32} : tensor<32xi32> -> tensor<1x32xi32>
        %78 = arith.muli %77, %cst : tensor<1x32xi32>
        %79 = tt.broadcast %76 : tensor<64x1xi32> -> tensor<64x32xi32>
        %80 = tt.broadcast %78 : tensor<1x32xi32> -> tensor<64x32xi32>
        %81 = arith.addi %79, %80 : tensor<64x32xi32>
        %82 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x32x!tt.ptr<f8E5M2>>
        %83 = tt.addptr %82, %81 : tensor<64x32x!tt.ptr<f8E5M2>>, tensor<64x32xi32>
        %84 = tt.load %83 : tensor<64x32x!tt.ptr<f8E5M2>>
        %85 = tt.fp_to_fp %65, rounding = rtne : tensor<128x32xf32> -> tensor<128x32xf8E5M2>
        %86 = tt.trans %84 {order = array<i32: 1, 0>} : tensor<64x32xf8E5M2> -> tensor<32x64xf8E5M2>
        %87 = tt.dot %85, %86, %cst_4, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x32xf8E5M2> * tensor<32x64xf8E5M2> -> tensor<128x64xf32>
        %88 = arith.addf %73, %87 : tensor<128x64xf32>
        scf.yield %70, %88, %61 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.loop_unroll_factor = 1 : i32}
      %16 = tt.expand_dims %15#0 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
      %17 = tt.broadcast %16 : tensor<128x1xf32> -> tensor<128x64xf32>
      %18 = arith.divf %15#1, %17 : tensor<128x64xf32>
      %19 = tt.fp_to_fp %18, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
      %20 = arith.divsi %arg6, %arg5 : i32
      %21 = arith.remsi %arg6, %arg5 : i32
      %22 = arith.cmpi ne, %21, %c0_i32 : i32
      %23 = arith.subi %20, %c1_i32 : i32
      %24 = arith.select %22, %23, %20 : i32
      %25 = arith.cmpi slt, %arg6, %c0_i32 : i32
      %26 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %27 = arith.cmpi ne, %25, %26 : i1
      %28 = arith.select %27, %24, %20 : i32
      %29 = arith.andi %22, %27 : i1
      %30 = arith.addi %21, %arg5 : i32
      %31 = arith.select %29, %30, %21 : i32
      %32 = arith.muli %28, %arg4 : i32
      %33 = arith.muli %31, %c8192_i32 : i32
      %34 = arith.addi %32, %33 : i32
      %35 = tt.splat %34 : i32 -> tensor<128x1xi32>
      %36 = arith.addi %35, %5 : tensor<128x1xi32>
      %37 = tt.broadcast %36 : tensor<128x1xi32> -> tensor<128x64xi32>
      %38 = arith.addi %37, %10 : tensor<128x64xi32>
      %39 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %40 = tt.addptr %39, %38 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      tt.store %40, %19 : tensor<128x64x!tt.ptr<f8E5M2>>
    } {tt.disallow_acc_multi_buffer, tt.flatten}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(convert-triton-to-tritongpu{enable-source-remat=false num-ctas=1 num-warps=8 target=cuda:90 threads-per-warp=32}, tritongpu-coalesce, tritongpu-F32DotTC, triton-nvidia-gpu-plan-cta, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritongpu-accelerate-matmul, tritongpu-remove-layout-conversions, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, triton-nvidia-optimize-descriptor-encoding, triton-loop-aware-cse, tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-combine-tensor-select-and-if, nvgpu-warp-specialization{dump-intermediate-steps=false num-stages=6}, tritongpu-assign-latencies{num-stages=6}, tritongpu-schedule-loops, tritongpu-pipeline{dump-intermediate-steps=false num-stages=6}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-loop-aware-cse, tritongpu-prefetch, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-coalesce-async-copy, triton-nvidia-optimize-tmem-layouts, tritongpu-remove-layout-conversions, triton-nvidia-interleave-tmem, tritongpu-reduce-data-duplication, tritongpu-reorder-instructions, triton-loop-aware-cse, symbol-dce, triton-nvidia-tma-lowering, triton-nvidia-gpu-fence-insertion{compute-capability=90}, sccp, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true})",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_willfeng/ss/css4bvxumzi74sfc5jgiadmx5a5yhjzpssprp4dleu22m2kt2chv.py:13:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_willfeng/ss/css4bvxumzi74sfc5jgiadmx5a5yhjzpssprp4dleu22m2kt2chv.py:13:0: note: Pipeline failed while executing [`TritonGPUAssignLatencies` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
Process ForkProcess-586:
Traceback (most recent call last):
  File "/home/willfeng/local/miniconda3/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/willfeng/local/miniconda3/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/data/users/willfeng/helion/helion/runtime/precompile_shim.py", line 54, in finish_it
    kernel_cache[key] = fn.compile(
                        ^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/compiler/compiler.py", line 359, in compile
    next_module = compile_ir(module, metadata)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/backends/nvidia/compiler.py", line 456, in <lambda>
    stages["ttgir"] = lambda src, metadata: self.make_ttgir(src, metadata, options, capability)
                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/backends/nvidia/compiler.py", line 298, in make_ttgir
    pm.run(mod)
RuntimeError: PassManager::run failed
python: /root/.triton/llvm/llvm-8957e64a-almalinux-x64/include/llvm/ADT/SmallVector.h:296: const_reference llvm::SmallVectorTemplateCommon<long>::operator[](size_type) const [T = long]: Assertion `idx < size()' failed.
module {
  tt.func public @_fp8_attention_kernel_kernel(%arg0: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %c1_i32 = arith.constant 1 : i32
    %c32_i32 = arith.constant 32 : i32
    %c128_i32 = arith.constant 128 : i32
    %c0_i32 = arith.constant 0 : i32
    %c132_i32 = arith.constant 132 : i32
    %c192_i32 = arith.constant 192 : i32
    %cst = arith.constant dense<64> : tensor<1x32xi32>
    %cst_0 = arith.constant dense<0.180336878> : tensor<128x32xf32>
    %cst_1 = arith.constant dense<0.000000e+00> : tensor<128x32xf32>
    %cst_2 = arith.constant dense<64> : tensor<32x1xi32>
    %cst_3 = arith.constant dense<64> : tensor<128x1xi32>
    %c8192_i32 = arith.constant 8192 : i32
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<128x64xf32>
    %cst_5 = arith.constant dense<0.000000e+00> : tensor<128xf32>
    %cst_6 = arith.constant dense<0xFF800000> : tensor<128xf32>
    %0 = tt.get_program_id x : i32
    scf.for %arg6 = %0 to %c192_i32 step %c132_i32  : i32 {
      %1 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
      %2 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>
      %3 = arith.muli %arg6, %c8192_i32 : i32
      %4 = tt.expand_dims %2 {axis = 1 : i32} : tensor<128xi32> -> tensor<128x1xi32>
      %5 = arith.muli %4, %cst_3 : tensor<128x1xi32>
      %6 = tt.splat %3 : i32 -> tensor<128x1xi32>
      %7 = arith.addi %6, %5 : tensor<128x1xi32>
      %8 = tt.expand_dims %1 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %9 = tt.broadcast %7 : tensor<128x1xi32> -> tensor<128x64xi32>
      %10 = tt.broadcast %8 : tensor<1x64xi32> -> tensor<128x64xi32>
      %11 = arith.addi %9, %10 : tensor<128x64xi32>
      %12 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %13 = tt.addptr %12, %11 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      %14 = tt.load %13 : tensor<128x64x!tt.ptr<f8E5M2>>
      %15:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c32_i32 iter_args(%arg8 = %cst_5, %arg9 = %cst_4, %arg10 = %cst_6) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %41 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32>
        %42 = tt.splat %arg7 : i32 -> tensor<32xi32>
        %43 = arith.addi %42, %41 : tensor<32xi32>
        %44 = tt.expand_dims %43 {axis = 1 : i32} : tensor<32xi32> -> tensor<32x1xi32>
        %45 = arith.muli %44, %cst_2 : tensor<32x1xi32>
        %46 = tt.splat %3 : i32 -> tensor<32x1xi32>
        %47 = arith.addi %46, %45 : tensor<32x1xi32>
        %48 = tt.broadcast %47 : tensor<32x1xi32> -> tensor<32x64xi32>
        %49 = tt.broadcast %8 : tensor<1x64xi32> -> tensor<32x64xi32>
        %50 = arith.addi %48, %49 : tensor<32x64xi32>
        %51 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<32x64x!tt.ptr<f8E5M2>>
        %52 = tt.addptr %51, %50 : tensor<32x64x!tt.ptr<f8E5M2>>, tensor<32x64xi32>
        %53 = tt.load %52 : tensor<32x64x!tt.ptr<f8E5M2>>
        %54 = tt.trans %53 {order = array<i32: 1, 0>} : tensor<32x64xf8E5M2> -> tensor<64x32xf8E5M2>
        %55 = tt.dot %14, %54, %cst_1, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x32xf8E5M2> -> tensor<128x32xf32>
        %56 = arith.mulf %55, %cst_0 : tensor<128x32xf32>
        %57 = "tt.reduce"(%56) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %89 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %89 : f32
        }) : (tensor<128x32xf32>) -> tensor<128xf32>
        %58 = arith.cmpf ogt, %arg10, %57 : tensor<128xf32>
        %59 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %60 = arith.ori %58, %59 : tensor<128xi1>
        %61 = arith.select %60, %arg10, %57 : tensor<128xi1>, tensor<128xf32>
        %62 = tt.expand_dims %61 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %63 = tt.broadcast %62 : tensor<128x1xf32> -> tensor<128x32xf32>
        %64 = arith.subf %56, %63 : tensor<128x32xf32>
        %65 = tt.extern_elementwise %64 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x32xf32>) -> tensor<128x32xf32>
        %66 = "tt.reduce"(%65) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %89 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %89 : f32
        }) : (tensor<128x32xf32>) -> tensor<128xf32>
        %67 = arith.subf %arg10, %61 : tensor<128xf32>
        %68 = tt.extern_elementwise %67 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %69 = arith.mulf %arg8, %68 : tensor<128xf32>
        %70 = arith.addf %69, %66 : tensor<128xf32>
        %71 = tt.expand_dims %68 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %72 = tt.broadcast %71 : tensor<128x1xf32> -> tensor<128x64xf32>
        %73 = arith.mulf %arg9, %72 : tensor<128x64xf32>
        %74 = tt.expand_dims %1 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %75 = tt.splat %3 : i32 -> tensor<64x1xi32>
        %76 = arith.addi %75, %74 : tensor<64x1xi32>
        %77 = tt.expand_dims %43 {axis = 0 : i32} : tensor<32xi32> -> tensor<1x32xi32>
        %78 = arith.muli %77, %cst : tensor<1x32xi32>
        %79 = tt.broadcast %76 : tensor<64x1xi32> -> tensor<64x32xi32>
        %80 = tt.broadcast %78 : tensor<1x32xi32> -> tensor<64x32xi32>
        %81 = arith.addi %79, %80 : tensor<64x32xi32>
        %82 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x32x!tt.ptr<f8E5M2>>
        %83 = tt.addptr %82, %81 : tensor<64x32x!tt.ptr<f8E5M2>>, tensor<64x32xi32>
        %84 = tt.load %83 : tensor<64x32x!tt.ptr<f8E5M2>>
        %85 = tt.fp_to_fp %65, rounding = rtne : tensor<128x32xf32> -> tensor<128x32xf8E5M2>
        %86 = tt.trans %84 {order = array<i32: 1, 0>} : tensor<64x32xf8E5M2> -> tensor<32x64xf8E5M2>
        %87 = tt.dot %85, %86, %cst_4, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x32xf8E5M2> * tensor<32x64xf8E5M2> -> tensor<128x64xf32>
        %88 = arith.addf %73, %87 : tensor<128x64xf32>
        scf.yield %70, %88, %61 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.disallow_acc_multi_buffer, tt.num_stages = 4 : i32}
      %16 = tt.expand_dims %15#0 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
      %17 = tt.broadcast %16 : tensor<128x1xf32> -> tensor<128x64xf32>
      %18 = arith.divf %15#1, %17 : tensor<128x64xf32>
      %19 = tt.fp_to_fp %18, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
      %20 = arith.divsi %arg6, %arg5 : i32
      %21 = arith.remsi %arg6, %arg5 : i32
      %22 = arith.cmpi ne, %21, %c0_i32 : i32
      %23 = arith.subi %20, %c1_i32 : i32
      %24 = arith.select %22, %23, %20 : i32
      %25 = arith.cmpi slt, %arg6, %c0_i32 : i32
      %26 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %27 = arith.cmpi ne, %25, %26 : i1
      %28 = arith.select %27, %24, %20 : i32
      %29 = arith.andi %22, %27 : i1
      %30 = arith.addi %21, %arg5 : i32
      %31 = arith.select %29, %30, %21 : i32
      %32 = arith.muli %28, %arg4 : i32
      %33 = arith.muli %31, %c8192_i32 : i32
      %34 = arith.addi %32, %33 : i32
      %35 = tt.splat %34 : i32 -> tensor<128x1xi32>
      %36 = arith.addi %35, %5 : tensor<128x1xi32>
      %37 = tt.broadcast %36 : tensor<128x1xi32> -> tensor<128x64xi32>
      %38 = arith.addi %37, %10 : tensor<128x64xi32>
      %39 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %40 = tt.addptr %39, %38 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      tt.store %40, %19 : tensor<128x64x!tt.ptr<f8E5M2>>
    } {tt.disallow_acc_multi_buffer, tt.flatten}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(convert-triton-to-tritongpu{enable-source-remat=false num-ctas=1 num-warps=8 target=cuda:90 threads-per-warp=32}, tritongpu-coalesce, tritongpu-F32DotTC, triton-nvidia-gpu-plan-cta, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritongpu-accelerate-matmul, tritongpu-remove-layout-conversions, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, triton-nvidia-optimize-descriptor-encoding, triton-loop-aware-cse, tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-combine-tensor-select-and-if, nvgpu-warp-specialization{dump-intermediate-steps=false num-stages=7}, tritongpu-assign-latencies{num-stages=7}, tritongpu-schedule-loops, tritongpu-pipeline{dump-intermediate-steps=false num-stages=7}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-loop-aware-cse, tritongpu-prefetch, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-coalesce-async-copy, triton-nvidia-optimize-tmem-layouts, tritongpu-remove-layout-conversions, triton-nvidia-interleave-tmem, tritongpu-reduce-data-duplication, tritongpu-reorder-instructions, triton-loop-aware-cse, symbol-dce, triton-nvidia-tma-lowering, triton-nvidia-gpu-fence-insertion{compute-capability=90}, sccp, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true})",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_willfeng/k3/ck3jgq6hnji5z73qoekd4mloyc3z5xsulnxseqr4wd5mrr2jcdpj.py:13:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_willfeng/k3/ck3jgq6hnji5z73qoekd4mloyc3z5xsulnxseqr4wd5mrr2jcdpj.py:13:0: note: Pipeline failed while executing [`TritonGPUAssignLatencies` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
Process ForkProcess-588:
Traceback (most recent call last):
  File "/home/willfeng/local/miniconda3/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/willfeng/local/miniconda3/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/data/users/willfeng/helion/helion/runtime/precompile_shim.py", line 54, in finish_it
    kernel_cache[key] = fn.compile(
                        ^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/compiler/compiler.py", line 359, in compile
    next_module = compile_ir(module, metadata)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/backends/nvidia/compiler.py", line 456, in <lambda>
    stages["ttgir"] = lambda src, metadata: self.make_ttgir(src, metadata, options, capability)
                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/backends/nvidia/compiler.py", line 298, in make_ttgir
    pm.run(mod)
RuntimeError: PassManager::run failed
python: /root/.triton/llvm/llvm-8957e64a-almalinux-x64/include/llvm/ADT/SmallVector.h:296: const_reference llvm::SmallVectorTemplateCommon<long>::operator[](size_type) const [T = long]: Assertion `idx < size()' failed.
module {
  tt.func public @_fp8_attention_kernel_kernel(%arg0: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %c1_i32 = arith.constant 1 : i32
    %c32_i32 = arith.constant 32 : i32
    %c64_i32 = arith.constant 64 : i32
    %c128_i32 = arith.constant 128 : i32
    %c0_i32 = arith.constant 0 : i32
    %cst = arith.constant dense<64> : tensor<1x32xi32>
    %cst_0 = arith.constant dense<0.180336878> : tensor<64x32xf32>
    %cst_1 = arith.constant dense<0.000000e+00> : tensor<64x32xf32>
    %cst_2 = arith.constant dense<64> : tensor<32x1xi32>
    %cst_3 = arith.constant dense<64> : tensor<64x1xi32>
    %c8192_i32 = arith.constant 8192 : i32
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<64x64xf32>
    %cst_5 = arith.constant dense<0.000000e+00> : tensor<64xf32>
    %cst_6 = arith.constant dense<0xFF800000> : tensor<64xf32>
    %0 = tt.get_program_id x : i32
    %1 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
    scf.for %arg6 = %c0_i32 to %c128_i32 step %c64_i32  : i32 {
      %2 = tt.splat %arg6 : i32 -> tensor<64xi32>
      %3 = arith.addi %2, %1 : tensor<64xi32>
      %4 = arith.muli %0, %c8192_i32 : i32
      %5 = tt.expand_dims %3 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
      %6 = arith.muli %5, %cst_3 : tensor<64x1xi32>
      %7 = tt.splat %4 : i32 -> tensor<64x1xi32>
      %8 = arith.addi %7, %6 : tensor<64x1xi32>
      %9 = tt.expand_dims %1 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %10 = tt.broadcast %8 : tensor<64x1xi32> -> tensor<64x64xi32>
      %11 = tt.broadcast %9 : tensor<1x64xi32> -> tensor<64x64xi32>
      %12 = arith.addi %10, %11 : tensor<64x64xi32>
      %13 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %14 = tt.addptr %13, %12 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      %15 = tt.load %14 : tensor<64x64x!tt.ptr<f8E5M2>>
      %16:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c32_i32 iter_args(%arg8 = %cst_5, %arg9 = %cst_4, %arg10 = %cst_6) -> (tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>)  : i32 {
        %42 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32>
        %43 = tt.splat %arg7 : i32 -> tensor<32xi32>
        %44 = arith.addi %43, %42 : tensor<32xi32>
        %45 = tt.expand_dims %44 {axis = 1 : i32} : tensor<32xi32> -> tensor<32x1xi32>
        %46 = arith.muli %45, %cst_2 : tensor<32x1xi32>
        %47 = tt.splat %4 : i32 -> tensor<32x1xi32>
        %48 = arith.addi %47, %46 : tensor<32x1xi32>
        %49 = tt.broadcast %48 : tensor<32x1xi32> -> tensor<32x64xi32>
        %50 = tt.broadcast %9 : tensor<1x64xi32> -> tensor<32x64xi32>
        %51 = arith.addi %49, %50 : tensor<32x64xi32>
        %52 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<32x64x!tt.ptr<f8E5M2>>
        %53 = tt.addptr %52, %51 : tensor<32x64x!tt.ptr<f8E5M2>>, tensor<32x64xi32>
        %54 = tt.load %53 : tensor<32x64x!tt.ptr<f8E5M2>>
        %55 = tt.trans %54 {order = array<i32: 1, 0>} : tensor<32x64xf8E5M2> -> tensor<64x32xf8E5M2>
        %56 = tt.dot %15, %55, %cst_1, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x64xf8E5M2> * tensor<64x32xf8E5M2> -> tensor<64x32xf32>
        %57 = arith.mulf %56, %cst_0 : tensor<64x32xf32>
        %58 = "tt.reduce"(%57) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %89 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %89 : f32
        }) : (tensor<64x32xf32>) -> tensor<64xf32>
        %59 = arith.cmpf ogt, %arg10, %58 : tensor<64xf32>
        %60 = arith.cmpf une, %arg10, %arg10 : tensor<64xf32>
        %61 = arith.ori %59, %60 : tensor<64xi1>
        %62 = arith.select %61, %arg10, %58 : tensor<64xi1>, tensor<64xf32>
        %63 = tt.expand_dims %62 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %64 = tt.broadcast %63 : tensor<64x1xf32> -> tensor<64x32xf32>
        %65 = arith.subf %57, %64 : tensor<64x32xf32>
        %66 = tt.extern_elementwise %65 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64x32xf32>) -> tensor<64x32xf32>
        %67 = "tt.reduce"(%66) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %89 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %89 : f32
        }) : (tensor<64x32xf32>) -> tensor<64xf32>
        %68 = arith.subf %arg10, %62 : tensor<64xf32>
        %69 = tt.extern_elementwise %68 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64xf32>) -> tensor<64xf32>
        %70 = arith.mulf %arg8, %69 : tensor<64xf32>
        %71 = arith.addf %70, %67 : tensor<64xf32>
        %72 = tt.expand_dims %69 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %73 = tt.broadcast %72 : tensor<64x1xf32> -> tensor<64x64xf32>
        %74 = arith.mulf %arg9, %73 : tensor<64x64xf32>
        %75 = tt.expand_dims %1 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %76 = arith.addi %7, %75 : tensor<64x1xi32>
        %77 = tt.expand_dims %44 {axis = 0 : i32} : tensor<32xi32> -> tensor<1x32xi32>
        %78 = arith.muli %77, %cst : tensor<1x32xi32>
        %79 = tt.broadcast %76 : tensor<64x1xi32> -> tensor<64x32xi32>
        %80 = tt.broadcast %78 : tensor<1x32xi32> -> tensor<64x32xi32>
        %81 = arith.addi %79, %80 : tensor<64x32xi32>
        %82 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x32x!tt.ptr<f8E5M2>>
        %83 = tt.addptr %82, %81 : tensor<64x32x!tt.ptr<f8E5M2>>, tensor<64x32xi32>
        %84 = tt.load %83 : tensor<64x32x!tt.ptr<f8E5M2>>
        %85 = tt.fp_to_fp %66, rounding = rtne : tensor<64x32xf32> -> tensor<64x32xf8E5M2>
        %86 = tt.trans %84 {order = array<i32: 1, 0>} : tensor<64x32xf8E5M2> -> tensor<32x64xf8E5M2>
        %87 = tt.dot %85, %86, %cst_4, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x32xf8E5M2> * tensor<32x64xf8E5M2> -> tensor<64x64xf32>
        %88 = arith.addf %74, %87 : tensor<64x64xf32>
        scf.yield %71, %88, %62 : tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>
      } {tt.disallow_acc_multi_buffer, tt.loop_unroll_factor = 1 : i32}
      %17 = tt.expand_dims %16#0 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
      %18 = tt.broadcast %17 : tensor<64x1xf32> -> tensor<64x64xf32>
      %19 = arith.divf %16#1, %18 : tensor<64x64xf32>
      %20 = tt.fp_to_fp %19, rounding = rtne : tensor<64x64xf32> -> tensor<64x64xf8E5M2>
      %21 = arith.divsi %0, %arg5 : i32
      %22 = arith.remsi %0, %arg5 : i32
      %23 = arith.cmpi ne, %22, %c0_i32 : i32
      %24 = arith.subi %21, %c1_i32 : i32
      %25 = arith.select %23, %24, %21 : i32
      %26 = arith.cmpi slt, %0, %c0_i32 : i32
      %27 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %28 = arith.cmpi ne, %26, %27 : i1
      %29 = arith.select %28, %25, %21 : i32
      %30 = arith.andi %23, %28 : i1
      %31 = arith.addi %22, %arg5 : i32
      %32 = arith.select %30, %31, %22 : i32
      %33 = arith.muli %29, %arg4 : i32
      %34 = arith.muli %32, %c8192_i32 : i32
      %35 = arith.addi %33, %34 : i32
      %36 = tt.splat %35 : i32 -> tensor<64x1xi32>
      %37 = arith.addi %36, %6 : tensor<64x1xi32>
      %38 = tt.broadcast %37 : tensor<64x1xi32> -> tensor<64x64xi32>
      %39 = arith.addi %38, %11 : tensor<64x64xi32>
      %40 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %41 = tt.addptr %40, %39 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      tt.store %41, %20 : tensor<64x64x!tt.ptr<f8E5M2>>
    } {tt.flatten}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(convert-triton-to-tritongpu{enable-source-remat=false num-ctas=1 num-warps=16 target=cuda:90 threads-per-warp=32}, tritongpu-coalesce, tritongpu-F32DotTC, triton-nvidia-gpu-plan-cta, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritongpu-accelerate-matmul, tritongpu-remove-layout-conversions, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, triton-nvidia-optimize-descriptor-encoding, triton-loop-aware-cse, tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-combine-tensor-select-and-if, nvgpu-warp-specialization{dump-intermediate-steps=false num-stages=6}, tritongpu-assign-latencies{num-stages=6}, tritongpu-schedule-loops, tritongpu-pipeline{dump-intermediate-steps=false num-stages=6}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-loop-aware-cse, tritongpu-prefetch, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-coalesce-async-copy, triton-nvidia-optimize-tmem-layouts, tritongpu-remove-layout-conversions, triton-nvidia-interleave-tmem, tritongpu-reduce-data-duplication, tritongpu-reorder-instructions, triton-loop-aware-cse, symbol-dce, triton-nvidia-tma-lowering, triton-nvidia-gpu-fence-insertion{compute-capability=90}, sccp, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true})",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_willfeng/5r/c5rep4nhtbefp5ifomqdqr6xp3hgaawbss2fb2f5zvvdxkxfccsw.py:12:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_willfeng/5r/c5rep4nhtbefp5ifomqdqr6xp3hgaawbss2fb2f5zvvdxkxfccsw.py:12:0: note: Pipeline failed while executing [`TritonGPUAssignLatencies` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
Process ForkProcess-597:
Traceback (most recent call last):
  File "/home/willfeng/local/miniconda3/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/willfeng/local/miniconda3/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/data/users/willfeng/helion/helion/runtime/precompile_shim.py", line 54, in finish_it
    kernel_cache[key] = fn.compile(
                        ^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/compiler/compiler.py", line 359, in compile
    next_module = compile_ir(module, metadata)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/backends/nvidia/compiler.py", line 456, in <lambda>
    stages["ttgir"] = lambda src, metadata: self.make_ttgir(src, metadata, options, capability)
                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/backends/nvidia/compiler.py", line 298, in make_ttgir
    pm.run(mod)
RuntimeError: PassManager::run failed
python: /root/.triton/llvm/llvm-8957e64a-almalinux-x64/include/llvm/ADT/SmallVector.h:296: const_reference llvm::SmallVectorTemplateCommon<long>::operator[](size_type) const [T = long]: Assertion `idx < size()' failed.
module {
  tt.func public @_fp8_attention_kernel_kernel(%arg0: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %c64_i32 = arith.constant 64 : i32
    %c128_i32 = arith.constant 128 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst = arith.constant dense<64> : tensor<1x64xi32>
    %cst_0 = arith.constant dense<0.180336878> : tensor<128x64xf32>
    %cst_1 = arith.constant dense<64> : tensor<64x1xi32>
    %cst_2 = arith.constant dense<64> : tensor<128x1xi32>
    %c8192_i32 = arith.constant 8192 : i32
    %cst_3 = arith.constant dense<0.000000e+00> : tensor<128x64xf32>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<128xf32>
    %cst_5 = arith.constant dense<0xFF800000> : tensor<128xf32>
    %c192_i32 = arith.constant 192 : i32
    %c2_i32 = arith.constant 2 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.muli %0, %c2_i32 : i32
    %2 = arith.addi %1, %c2_i32 : i32
    %3 = arith.minsi %2, %c192_i32 : i32
    %4 = arith.subi %3, %1 : i32
    %c1_i32_6 = arith.constant 1 : i32
    %5 = arith.subi %c1_i32, %c1_i32_6 : i32
    %6 = arith.addi %4, %5 : i32
    %7 = arith.divui %6, %c1_i32 : i32
    %c2_i32_7 = arith.constant 2 : i32
    %8 = arith.remsi %7, %c2_i32_7 : i32
    %9 = arith.subi %7, %8 : i32
    %10 = arith.muli %9, %c1_i32 : i32
    %11 = arith.addi %1, %10 : i32
    %12 = arith.muli %c1_i32, %c2_i32_7 : i32
    scf.for %arg6 = %1 to %11 step %12  : i32 {
      %13 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
      %14 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>
      %15 = arith.muli %arg6, %c8192_i32 : i32
      %16 = tt.expand_dims %14 {axis = 1 : i32} : tensor<128xi32> -> tensor<128x1xi32>
      %17 = arith.muli %16, %cst_2 : tensor<128x1xi32>
      %18 = tt.splat %15 : i32 -> tensor<128x1xi32>
      %19 = arith.addi %18, %17 : tensor<128x1xi32>
      %20 = tt.expand_dims %13 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %21 = tt.broadcast %19 : tensor<128x1xi32> -> tensor<128x64xi32>
      %22 = tt.broadcast %20 : tensor<1x64xi32> -> tensor<128x64xi32>
      %23 = arith.addi %21, %22 : tensor<128x64xi32>
      %24 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %25 = tt.addptr %24, %23 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      %26 = tt.load %25 : tensor<128x64x!tt.ptr<f8E5M2>>
      %c0_i32_8 = arith.constant 0 : i32
      %c256_i32 = arith.constant 256 : i32
      %27:3 = scf.for %arg7 = %c0_i32 to %c0_i32_8 step %c256_i32 iter_args(%arg8 = %cst_4, %arg9 = %cst_3, %arg10 = %cst_5) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %97 = tt.splat %arg7 : i32 -> tensor<64xi32>
        %98 = arith.addi %97, %13 : tensor<64xi32>
        %99 = tt.expand_dims %98 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %100 = arith.muli %99, %cst_1 : tensor<64x1xi32>
        %101 = tt.splat %15 : i32 -> tensor<64x1xi32>
        %102 = arith.addi %101, %100 : tensor<64x1xi32>
        %103 = tt.broadcast %102 : tensor<64x1xi32> -> tensor<64x64xi32>
        %104 = tt.broadcast %20 : tensor<1x64xi32> -> tensor<64x64xi32>
        %105 = arith.addi %103, %104 : tensor<64x64xi32>
        %106 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %107 = tt.addptr %106, %105 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %108 = tt.load %107 : tensor<64x64x!tt.ptr<f8E5M2>>
        %109 = tt.trans %108 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %110 = tt.dot %26, %109, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %111 = arith.mulf %110, %cst_0 : tensor<128x64xf32>
        %112 = "tt.reduce"(%111) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %287 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %287 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %113 = arith.cmpf ogt, %arg10, %112 : tensor<128xf32>
        %114 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %115 = arith.ori %113, %114 : tensor<128xi1>
        %116 = arith.select %115, %arg10, %112 : tensor<128xi1>, tensor<128xf32>
        %117 = tt.expand_dims %116 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %118 = tt.broadcast %117 : tensor<128x1xf32> -> tensor<128x64xf32>
        %119 = arith.subf %111, %118 : tensor<128x64xf32>
        %120 = tt.extern_elementwise %119 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %121 = "tt.reduce"(%120) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %287 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %287 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %122 = arith.subf %arg10, %116 : tensor<128xf32>
        %123 = tt.extern_elementwise %122 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %124 = arith.mulf %arg8, %123 : tensor<128xf32>
        %125 = arith.addf %124, %121 : tensor<128xf32>
        %126 = tt.expand_dims %123 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %127 = tt.broadcast %126 : tensor<128x1xf32> -> tensor<128x64xf32>
        %128 = arith.mulf %arg9, %127 : tensor<128x64xf32>
        %129 = tt.expand_dims %13 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %130 = arith.addi %101, %129 : tensor<64x1xi32>
        %131 = tt.expand_dims %98 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %132 = arith.muli %131, %cst : tensor<1x64xi32>
        %133 = tt.broadcast %130 : tensor<64x1xi32> -> tensor<64x64xi32>
        %134 = tt.broadcast %132 : tensor<1x64xi32> -> tensor<64x64xi32>
        %135 = arith.addi %133, %134 : tensor<64x64xi32>
        %136 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %137 = tt.addptr %136, %135 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %138 = tt.load %137 : tensor<64x64x!tt.ptr<f8E5M2>>
        %139 = tt.fp_to_fp %120, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %140 = tt.trans %138 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %141 = tt.dot %139, %140, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %142 = arith.addf %128, %141 : tensor<128x64xf32>
        %c1_i32_12 = arith.constant 1 : i32
        %143 = arith.muli %c64_i32, %c1_i32_12 : i32
        %144 = arith.addi %arg7, %143 : i32
        %145 = tt.splat %144 : i32 -> tensor<64xi32>
        %146 = arith.addi %145, %13 : tensor<64xi32>
        %147 = tt.expand_dims %146 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %148 = arith.muli %147, %cst_1 : tensor<64x1xi32>
        %149 = tt.splat %15 : i32 -> tensor<64x1xi32>
        %150 = arith.addi %149, %148 : tensor<64x1xi32>
        %151 = tt.broadcast %150 : tensor<64x1xi32> -> tensor<64x64xi32>
        %152 = tt.broadcast %20 : tensor<1x64xi32> -> tensor<64x64xi32>
        %153 = arith.addi %151, %152 : tensor<64x64xi32>
        %154 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %155 = tt.addptr %154, %153 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %156 = tt.load %155 : tensor<64x64x!tt.ptr<f8E5M2>>
        %157 = tt.trans %156 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %158 = tt.dot %26, %157, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %159 = arith.mulf %158, %cst_0 : tensor<128x64xf32>
        %160 = "tt.reduce"(%159) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %287 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %287 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %161 = arith.cmpf ogt, %116, %160 : tensor<128xf32>
        %162 = arith.cmpf une, %116, %116 : tensor<128xf32>
        %163 = arith.ori %161, %162 : tensor<128xi1>
        %164 = arith.select %163, %116, %160 : tensor<128xi1>, tensor<128xf32>
        %165 = tt.expand_dims %164 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %166 = tt.broadcast %165 : tensor<128x1xf32> -> tensor<128x64xf32>
        %167 = arith.subf %159, %166 : tensor<128x64xf32>
        %168 = tt.extern_elementwise %167 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %169 = "tt.reduce"(%168) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %287 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %287 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %170 = arith.subf %116, %164 : tensor<128xf32>
        %171 = tt.extern_elementwise %170 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %172 = arith.mulf %125, %171 : tensor<128xf32>
        %173 = arith.addf %172, %169 : tensor<128xf32>
        %174 = tt.expand_dims %171 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %175 = tt.broadcast %174 : tensor<128x1xf32> -> tensor<128x64xf32>
        %176 = arith.mulf %142, %175 : tensor<128x64xf32>
        %177 = tt.expand_dims %13 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %178 = arith.addi %149, %177 : tensor<64x1xi32>
        %179 = tt.expand_dims %146 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %180 = arith.muli %179, %cst : tensor<1x64xi32>
        %181 = tt.broadcast %178 : tensor<64x1xi32> -> tensor<64x64xi32>
        %182 = tt.broadcast %180 : tensor<1x64xi32> -> tensor<64x64xi32>
        %183 = arith.addi %181, %182 : tensor<64x64xi32>
        %184 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %185 = tt.addptr %184, %183 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %186 = tt.load %185 : tensor<64x64x!tt.ptr<f8E5M2>>
        %187 = tt.fp_to_fp %168, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %188 = tt.trans %186 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %189 = tt.dot %187, %188, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %190 = arith.addf %176, %189 : tensor<128x64xf32>
        %c2_i32_13 = arith.constant 2 : i32
        %191 = arith.muli %c64_i32, %c2_i32_13 : i32
        %192 = arith.addi %arg7, %191 : i32
        %193 = tt.splat %192 : i32 -> tensor<64xi32>
        %194 = arith.addi %193, %13 : tensor<64xi32>
        %195 = tt.expand_dims %194 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %196 = arith.muli %195, %cst_1 : tensor<64x1xi32>
        %197 = tt.splat %15 : i32 -> tensor<64x1xi32>
        %198 = arith.addi %197, %196 : tensor<64x1xi32>
        %199 = tt.broadcast %198 : tensor<64x1xi32> -> tensor<64x64xi32>
        %200 = tt.broadcast %20 : tensor<1x64xi32> -> tensor<64x64xi32>
        %201 = arith.addi %199, %200 : tensor<64x64xi32>
        %202 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %203 = tt.addptr %202, %201 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %204 = tt.load %203 : tensor<64x64x!tt.ptr<f8E5M2>>
        %205 = tt.trans %204 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %206 = tt.dot %26, %205, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %207 = arith.mulf %206, %cst_0 : tensor<128x64xf32>
        %208 = "tt.reduce"(%207) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %287 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %287 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %209 = arith.cmpf ogt, %164, %208 : tensor<128xf32>
        %210 = arith.cmpf une, %164, %164 : tensor<128xf32>
        %211 = arith.ori %209, %210 : tensor<128xi1>
        %212 = arith.select %211, %164, %208 : tensor<128xi1>, tensor<128xf32>
        %213 = tt.expand_dims %212 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %214 = tt.broadcast %213 : tensor<128x1xf32> -> tensor<128x64xf32>
        %215 = arith.subf %207, %214 : tensor<128x64xf32>
        %216 = tt.extern_elementwise %215 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %217 = "tt.reduce"(%216) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %287 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %287 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %218 = arith.subf %164, %212 : tensor<128xf32>
        %219 = tt.extern_elementwise %218 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %220 = arith.mulf %173, %219 : tensor<128xf32>
        %221 = arith.addf %220, %217 : tensor<128xf32>
        %222 = tt.expand_dims %219 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %223 = tt.broadcast %222 : tensor<128x1xf32> -> tensor<128x64xf32>
        %224 = arith.mulf %190, %223 : tensor<128x64xf32>
        %225 = tt.expand_dims %13 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %226 = arith.addi %197, %225 : tensor<64x1xi32>
        %227 = tt.expand_dims %194 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %228 = arith.muli %227, %cst : tensor<1x64xi32>
        %229 = tt.broadcast %226 : tensor<64x1xi32> -> tensor<64x64xi32>
        %230 = tt.broadcast %228 : tensor<1x64xi32> -> tensor<64x64xi32>
        %231 = arith.addi %229, %230 : tensor<64x64xi32>
        %232 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %233 = tt.addptr %232, %231 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %234 = tt.load %233 : tensor<64x64x!tt.ptr<f8E5M2>>
        %235 = tt.fp_to_fp %216, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %236 = tt.trans %234 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %237 = tt.dot %235, %236, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %238 = arith.addf %224, %237 : tensor<128x64xf32>
        %c3_i32 = arith.constant 3 : i32
        %239 = arith.muli %c64_i32, %c3_i32 : i32
        %240 = arith.addi %arg7, %239 : i32
        %241 = tt.splat %240 : i32 -> tensor<64xi32>
        %242 = arith.addi %241, %13 : tensor<64xi32>
        %243 = tt.expand_dims %242 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %244 = arith.muli %243, %cst_1 : tensor<64x1xi32>
        %245 = tt.splat %15 : i32 -> tensor<64x1xi32>
        %246 = arith.addi %245, %244 : tensor<64x1xi32>
        %247 = tt.broadcast %246 : tensor<64x1xi32> -> tensor<64x64xi32>
        %248 = tt.broadcast %20 : tensor<1x64xi32> -> tensor<64x64xi32>
        %249 = arith.addi %247, %248 : tensor<64x64xi32>
        %250 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %251 = tt.addptr %250, %249 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %252 = tt.load %251 : tensor<64x64x!tt.ptr<f8E5M2>>
        %253 = tt.trans %252 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %254 = tt.dot %26, %253, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %255 = arith.mulf %254, %cst_0 : tensor<128x64xf32>
        %256 = "tt.reduce"(%255) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %287 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %287 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %257 = arith.cmpf ogt, %212, %256 : tensor<128xf32>
        %258 = arith.cmpf une, %212, %212 : tensor<128xf32>
        %259 = arith.ori %257, %258 : tensor<128xi1>
        %260 = arith.select %259, %212, %256 : tensor<128xi1>, tensor<128xf32>
        %261 = tt.expand_dims %260 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %262 = tt.broadcast %261 : tensor<128x1xf32> -> tensor<128x64xf32>
        %263 = arith.subf %255, %262 : tensor<128x64xf32>
        %264 = tt.extern_elementwise %263 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %265 = "tt.reduce"(%264) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %287 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %287 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %266 = arith.subf %212, %260 : tensor<128xf32>
        %267 = tt.extern_elementwise %266 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %268 = arith.mulf %221, %267 : tensor<128xf32>
        %269 = arith.addf %268, %265 : tensor<128xf32>
        %270 = tt.expand_dims %267 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %271 = tt.broadcast %270 : tensor<128x1xf32> -> tensor<128x64xf32>
        %272 = arith.mulf %238, %271 : tensor<128x64xf32>
        %273 = tt.expand_dims %13 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %274 = arith.addi %245, %273 : tensor<64x1xi32>
        %275 = tt.expand_dims %242 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %276 = arith.muli %275, %cst : tensor<1x64xi32>
        %277 = tt.broadcast %274 : tensor<64x1xi32> -> tensor<64x64xi32>
        %278 = tt.broadcast %276 : tensor<1x64xi32> -> tensor<64x64xi32>
        %279 = arith.addi %277, %278 : tensor<64x64xi32>
        %280 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %281 = tt.addptr %280, %279 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %282 = tt.load %281 : tensor<64x64x!tt.ptr<f8E5M2>>
        %283 = tt.fp_to_fp %264, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %284 = tt.trans %282 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %285 = tt.dot %283, %284, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %286 = arith.addf %272, %285 : tensor<128x64xf32>
        scf.yield %269, %286, %260 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.num_stages = 2 : i32}
      %28:3 = scf.for %arg7 = %c0_i32_8 to %c128_i32 step %c64_i32 iter_args(%arg8 = %27#0, %arg9 = %27#1, %arg10 = %27#2) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %97 = tt.splat %arg7 : i32 -> tensor<64xi32>
        %98 = arith.addi %97, %13 : tensor<64xi32>
        %99 = tt.expand_dims %98 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %100 = arith.muli %99, %cst_1 : tensor<64x1xi32>
        %101 = tt.splat %15 : i32 -> tensor<64x1xi32>
        %102 = arith.addi %101, %100 : tensor<64x1xi32>
        %103 = tt.broadcast %102 : tensor<64x1xi32> -> tensor<64x64xi32>
        %104 = tt.broadcast %20 : tensor<1x64xi32> -> tensor<64x64xi32>
        %105 = arith.addi %103, %104 : tensor<64x64xi32>
        %106 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %107 = tt.addptr %106, %105 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %108 = tt.load %107 : tensor<64x64x!tt.ptr<f8E5M2>>
        %109 = tt.trans %108 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %110 = tt.dot %26, %109, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %111 = arith.mulf %110, %cst_0 : tensor<128x64xf32>
        %112 = "tt.reduce"(%111) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %143 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %143 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %113 = arith.cmpf ogt, %arg10, %112 : tensor<128xf32>
        %114 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %115 = arith.ori %113, %114 : tensor<128xi1>
        %116 = arith.select %115, %arg10, %112 : tensor<128xi1>, tensor<128xf32>
        %117 = tt.expand_dims %116 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %118 = tt.broadcast %117 : tensor<128x1xf32> -> tensor<128x64xf32>
        %119 = arith.subf %111, %118 : tensor<128x64xf32>
        %120 = tt.extern_elementwise %119 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %121 = "tt.reduce"(%120) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %143 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %143 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %122 = arith.subf %arg10, %116 : tensor<128xf32>
        %123 = tt.extern_elementwise %122 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %124 = arith.mulf %arg8, %123 : tensor<128xf32>
        %125 = arith.addf %124, %121 : tensor<128xf32>
        %126 = tt.expand_dims %123 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %127 = tt.broadcast %126 : tensor<128x1xf32> -> tensor<128x64xf32>
        %128 = arith.mulf %arg9, %127 : tensor<128x64xf32>
        %129 = tt.expand_dims %13 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %130 = arith.addi %101, %129 : tensor<64x1xi32>
        %131 = tt.expand_dims %98 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %132 = arith.muli %131, %cst : tensor<1x64xi32>
        %133 = tt.broadcast %130 : tensor<64x1xi32> -> tensor<64x64xi32>
        %134 = tt.broadcast %132 : tensor<1x64xi32> -> tensor<64x64xi32>
        %135 = arith.addi %133, %134 : tensor<64x64xi32>
        %136 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %137 = tt.addptr %136, %135 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %138 = tt.load %137 : tensor<64x64x!tt.ptr<f8E5M2>>
        %139 = tt.fp_to_fp %120, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %140 = tt.trans %138 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %141 = tt.dot %139, %140, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %142 = arith.addf %128, %141 : tensor<128x64xf32>
        scf.yield %125, %142, %116 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.num_stages = 1 : i32}
      %29 = tt.expand_dims %28#0 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
      %30 = tt.broadcast %29 : tensor<128x1xf32> -> tensor<128x64xf32>
      %31 = arith.divf %28#1, %30 : tensor<128x64xf32>
      %32 = tt.fp_to_fp %31, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
      %33 = arith.divsi %arg6, %arg5 : i32
      %34 = arith.remsi %arg6, %arg5 : i32
      %35 = arith.cmpi ne, %34, %c0_i32 : i32
      %36 = arith.subi %33, %c1_i32 : i32
      %37 = arith.select %35, %36, %33 : i32
      %38 = arith.cmpi slt, %arg6, %c0_i32 : i32
      %39 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %40 = arith.cmpi ne, %38, %39 : i1
      %41 = arith.select %40, %37, %33 : i32
      %42 = arith.andi %35, %40 : i1
      %43 = arith.addi %34, %arg5 : i32
      %44 = arith.select %42, %43, %34 : i32
      %45 = arith.muli %41, %arg4 : i32
      %46 = arith.muli %44, %c8192_i32 : i32
      %47 = arith.addi %45, %46 : i32
      %48 = tt.splat %47 : i32 -> tensor<128x1xi32>
      %49 = arith.addi %48, %17 : tensor<128x1xi32>
      %50 = tt.broadcast %49 : tensor<128x1xi32> -> tensor<128x64xi32>
      %51 = arith.addi %50, %22 : tensor<128x64xi32>
      %52 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %53 = tt.addptr %52, %51 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      tt.store %53, %32 : tensor<128x64x!tt.ptr<f8E5M2>>
      %c1_i32_9 = arith.constant 1 : i32
      %54 = arith.muli %c1_i32, %c1_i32_9 : i32
      %55 = arith.addi %arg6, %54 : i32
      %56 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
      %57 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>
      %58 = arith.muli %55, %c8192_i32 : i32
      %59 = tt.expand_dims %57 {axis = 1 : i32} : tensor<128xi32> -> tensor<128x1xi32>
      %60 = arith.muli %59, %cst_2 : tensor<128x1xi32>
      %61 = tt.splat %58 : i32 -> tensor<128x1xi32>
      %62 = arith.addi %61, %60 : tensor<128x1xi32>
      %63 = tt.expand_dims %56 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %64 = tt.broadcast %62 : tensor<128x1xi32> -> tensor<128x64xi32>
      %65 = tt.broadcast %63 : tensor<1x64xi32> -> tensor<128x64xi32>
      %66 = arith.addi %64, %65 : tensor<128x64xi32>
      %67 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %68 = tt.addptr %67, %66 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      %69 = tt.load %68 : tensor<128x64x!tt.ptr<f8E5M2>>
      %c0_i32_10 = arith.constant 0 : i32
      %c256_i32_11 = arith.constant 256 : i32
      %70:3 = scf.for %arg7 = %c0_i32 to %c0_i32_10 step %c256_i32_11 iter_args(%arg8 = %cst_4, %arg9 = %cst_3, %arg10 = %cst_5) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %97 = tt.splat %arg7 : i32 -> tensor<64xi32>
        %98 = arith.addi %97, %56 : tensor<64xi32>
        %99 = tt.expand_dims %98 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %100 = arith.muli %99, %cst_1 : tensor<64x1xi32>
        %101 = tt.splat %58 : i32 -> tensor<64x1xi32>
        %102 = arith.addi %101, %100 : tensor<64x1xi32>
        %103 = tt.broadcast %102 : tensor<64x1xi32> -> tensor<64x64xi32>
        %104 = tt.broadcast %63 : tensor<1x64xi32> -> tensor<64x64xi32>
        %105 = arith.addi %103, %104 : tensor<64x64xi32>
        %106 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %107 = tt.addptr %106, %105 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %108 = tt.load %107 : tensor<64x64x!tt.ptr<f8E5M2>>
        %109 = tt.trans %108 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %110 = tt.dot %69, %109, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %111 = arith.mulf %110, %cst_0 : tensor<128x64xf32>
        %112 = "tt.reduce"(%111) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %287 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %287 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %113 = arith.cmpf ogt, %arg10, %112 : tensor<128xf32>
        %114 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %115 = arith.ori %113, %114 : tensor<128xi1>
        %116 = arith.select %115, %arg10, %112 : tensor<128xi1>, tensor<128xf32>
        %117 = tt.expand_dims %116 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %118 = tt.broadcast %117 : tensor<128x1xf32> -> tensor<128x64xf32>
        %119 = arith.subf %111, %118 : tensor<128x64xf32>
        %120 = tt.extern_elementwise %119 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %121 = "tt.reduce"(%120) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %287 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %287 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %122 = arith.subf %arg10, %116 : tensor<128xf32>
        %123 = tt.extern_elementwise %122 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %124 = arith.mulf %arg8, %123 : tensor<128xf32>
        %125 = arith.addf %124, %121 : tensor<128xf32>
        %126 = tt.expand_dims %123 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %127 = tt.broadcast %126 : tensor<128x1xf32> -> tensor<128x64xf32>
        %128 = arith.mulf %arg9, %127 : tensor<128x64xf32>
        %129 = tt.expand_dims %56 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %130 = arith.addi %101, %129 : tensor<64x1xi32>
        %131 = tt.expand_dims %98 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %132 = arith.muli %131, %cst : tensor<1x64xi32>
        %133 = tt.broadcast %130 : tensor<64x1xi32> -> tensor<64x64xi32>
        %134 = tt.broadcast %132 : tensor<1x64xi32> -> tensor<64x64xi32>
        %135 = arith.addi %133, %134 : tensor<64x64xi32>
        %136 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %137 = tt.addptr %136, %135 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %138 = tt.load %137 : tensor<64x64x!tt.ptr<f8E5M2>>
        %139 = tt.fp_to_fp %120, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %140 = tt.trans %138 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %141 = tt.dot %139, %140, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %142 = arith.addf %128, %141 : tensor<128x64xf32>
        %c1_i32_12 = arith.constant 1 : i32
        %143 = arith.muli %c64_i32, %c1_i32_12 : i32
        %144 = arith.addi %arg7, %143 : i32
        %145 = tt.splat %144 : i32 -> tensor<64xi32>
        %146 = arith.addi %145, %56 : tensor<64xi32>
        %147 = tt.expand_dims %146 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %148 = arith.muli %147, %cst_1 : tensor<64x1xi32>
        %149 = tt.splat %58 : i32 -> tensor<64x1xi32>
        %150 = arith.addi %149, %148 : tensor<64x1xi32>
        %151 = tt.broadcast %150 : tensor<64x1xi32> -> tensor<64x64xi32>
        %152 = tt.broadcast %63 : tensor<1x64xi32> -> tensor<64x64xi32>
        %153 = arith.addi %151, %152 : tensor<64x64xi32>
        %154 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %155 = tt.addptr %154, %153 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %156 = tt.load %155 : tensor<64x64x!tt.ptr<f8E5M2>>
        %157 = tt.trans %156 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %158 = tt.dot %69, %157, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %159 = arith.mulf %158, %cst_0 : tensor<128x64xf32>
        %160 = "tt.reduce"(%159) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %287 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %287 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %161 = arith.cmpf ogt, %116, %160 : tensor<128xf32>
        %162 = arith.cmpf une, %116, %116 : tensor<128xf32>
        %163 = arith.ori %161, %162 : tensor<128xi1>
        %164 = arith.select %163, %116, %160 : tensor<128xi1>, tensor<128xf32>
        %165 = tt.expand_dims %164 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %166 = tt.broadcast %165 : tensor<128x1xf32> -> tensor<128x64xf32>
        %167 = arith.subf %159, %166 : tensor<128x64xf32>
        %168 = tt.extern_elementwise %167 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %169 = "tt.reduce"(%168) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %287 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %287 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %170 = arith.subf %116, %164 : tensor<128xf32>
        %171 = tt.extern_elementwise %170 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %172 = arith.mulf %125, %171 : tensor<128xf32>
        %173 = arith.addf %172, %169 : tensor<128xf32>
        %174 = tt.expand_dims %171 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %175 = tt.broadcast %174 : tensor<128x1xf32> -> tensor<128x64xf32>
        %176 = arith.mulf %142, %175 : tensor<128x64xf32>
        %177 = tt.expand_dims %56 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %178 = arith.addi %149, %177 : tensor<64x1xi32>
        %179 = tt.expand_dims %146 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %180 = arith.muli %179, %cst : tensor<1x64xi32>
        %181 = tt.broadcast %178 : tensor<64x1xi32> -> tensor<64x64xi32>
        %182 = tt.broadcast %180 : tensor<1x64xi32> -> tensor<64x64xi32>
        %183 = arith.addi %181, %182 : tensor<64x64xi32>
        %184 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %185 = tt.addptr %184, %183 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %186 = tt.load %185 : tensor<64x64x!tt.ptr<f8E5M2>>
        %187 = tt.fp_to_fp %168, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %188 = tt.trans %186 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %189 = tt.dot %187, %188, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %190 = arith.addf %176, %189 : tensor<128x64xf32>
        %c2_i32_13 = arith.constant 2 : i32
        %191 = arith.muli %c64_i32, %c2_i32_13 : i32
        %192 = arith.addi %arg7, %191 : i32
        %193 = tt.splat %192 : i32 -> tensor<64xi32>
        %194 = arith.addi %193, %56 : tensor<64xi32>
        %195 = tt.expand_dims %194 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %196 = arith.muli %195, %cst_1 : tensor<64x1xi32>
        %197 = tt.splat %58 : i32 -> tensor<64x1xi32>
        %198 = arith.addi %197, %196 : tensor<64x1xi32>
        %199 = tt.broadcast %198 : tensor<64x1xi32> -> tensor<64x64xi32>
        %200 = tt.broadcast %63 : tensor<1x64xi32> -> tensor<64x64xi32>
        %201 = arith.addi %199, %200 : tensor<64x64xi32>
        %202 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %203 = tt.addptr %202, %201 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %204 = tt.load %203 : tensor<64x64x!tt.ptr<f8E5M2>>
        %205 = tt.trans %204 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %206 = tt.dot %69, %205, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %207 = arith.mulf %206, %cst_0 : tensor<128x64xf32>
        %208 = "tt.reduce"(%207) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %287 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %287 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %209 = arith.cmpf ogt, %164, %208 : tensor<128xf32>
        %210 = arith.cmpf une, %164, %164 : tensor<128xf32>
        %211 = arith.ori %209, %210 : tensor<128xi1>
        %212 = arith.select %211, %164, %208 : tensor<128xi1>, tensor<128xf32>
        %213 = tt.expand_dims %212 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %214 = tt.broadcast %213 : tensor<128x1xf32> -> tensor<128x64xf32>
        %215 = arith.subf %207, %214 : tensor<128x64xf32>
        %216 = tt.extern_elementwise %215 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %217 = "tt.reduce"(%216) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %287 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %287 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %218 = arith.subf %164, %212 : tensor<128xf32>
        %219 = tt.extern_elementwise %218 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %220 = arith.mulf %173, %219 : tensor<128xf32>
        %221 = arith.addf %220, %217 : tensor<128xf32>
        %222 = tt.expand_dims %219 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %223 = tt.broadcast %222 : tensor<128x1xf32> -> tensor<128x64xf32>
        %224 = arith.mulf %190, %223 : tensor<128x64xf32>
        %225 = tt.expand_dims %56 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %226 = arith.addi %197, %225 : tensor<64x1xi32>
        %227 = tt.expand_dims %194 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %228 = arith.muli %227, %cst : tensor<1x64xi32>
        %229 = tt.broadcast %226 : tensor<64x1xi32> -> tensor<64x64xi32>
        %230 = tt.broadcast %228 : tensor<1x64xi32> -> tensor<64x64xi32>
        %231 = arith.addi %229, %230 : tensor<64x64xi32>
        %232 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %233 = tt.addptr %232, %231 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %234 = tt.load %233 : tensor<64x64x!tt.ptr<f8E5M2>>
        %235 = tt.fp_to_fp %216, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %236 = tt.trans %234 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %237 = tt.dot %235, %236, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %238 = arith.addf %224, %237 : tensor<128x64xf32>
        %c3_i32 = arith.constant 3 : i32
        %239 = arith.muli %c64_i32, %c3_i32 : i32
        %240 = arith.addi %arg7, %239 : i32
        %241 = tt.splat %240 : i32 -> tensor<64xi32>
        %242 = arith.addi %241, %56 : tensor<64xi32>
        %243 = tt.expand_dims %242 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %244 = arith.muli %243, %cst_1 : tensor<64x1xi32>
        %245 = tt.splat %58 : i32 -> tensor<64x1xi32>
        %246 = arith.addi %245, %244 : tensor<64x1xi32>
        %247 = tt.broadcast %246 : tensor<64x1xi32> -> tensor<64x64xi32>
        %248 = tt.broadcast %63 : tensor<1x64xi32> -> tensor<64x64xi32>
        %249 = arith.addi %247, %248 : tensor<64x64xi32>
        %250 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %251 = tt.addptr %250, %249 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %252 = tt.load %251 : tensor<64x64x!tt.ptr<f8E5M2>>
        %253 = tt.trans %252 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %254 = tt.dot %69, %253, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %255 = arith.mulf %254, %cst_0 : tensor<128x64xf32>
        %256 = "tt.reduce"(%255) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %287 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %287 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %257 = arith.cmpf ogt, %212, %256 : tensor<128xf32>
        %258 = arith.cmpf une, %212, %212 : tensor<128xf32>
        %259 = arith.ori %257, %258 : tensor<128xi1>
        %260 = arith.select %259, %212, %256 : tensor<128xi1>, tensor<128xf32>
        %261 = tt.expand_dims %260 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %262 = tt.broadcast %261 : tensor<128x1xf32> -> tensor<128x64xf32>
        %263 = arith.subf %255, %262 : tensor<128x64xf32>
        %264 = tt.extern_elementwise %263 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %265 = "tt.reduce"(%264) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %287 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %287 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %266 = arith.subf %212, %260 : tensor<128xf32>
        %267 = tt.extern_elementwise %266 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %268 = arith.mulf %221, %267 : tensor<128xf32>
        %269 = arith.addf %268, %265 : tensor<128xf32>
        %270 = tt.expand_dims %267 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %271 = tt.broadcast %270 : tensor<128x1xf32> -> tensor<128x64xf32>
        %272 = arith.mulf %238, %271 : tensor<128x64xf32>
        %273 = tt.expand_dims %56 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %274 = arith.addi %245, %273 : tensor<64x1xi32>
        %275 = tt.expand_dims %242 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %276 = arith.muli %275, %cst : tensor<1x64xi32>
        %277 = tt.broadcast %274 : tensor<64x1xi32> -> tensor<64x64xi32>
        %278 = tt.broadcast %276 : tensor<1x64xi32> -> tensor<64x64xi32>
        %279 = arith.addi %277, %278 : tensor<64x64xi32>
        %280 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %281 = tt.addptr %280, %279 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %282 = tt.load %281 : tensor<64x64x!tt.ptr<f8E5M2>>
        %283 = tt.fp_to_fp %264, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %284 = tt.trans %282 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %285 = tt.dot %283, %284, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %286 = arith.addf %272, %285 : tensor<128x64xf32>
        scf.yield %269, %286, %260 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.num_stages = 2 : i32}
      %71:3 = scf.for %arg7 = %c0_i32_10 to %c128_i32 step %c64_i32 iter_args(%arg8 = %70#0, %arg9 = %70#1, %arg10 = %70#2) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %97 = tt.splat %arg7 : i32 -> tensor<64xi32>
        %98 = arith.addi %97, %56 : tensor<64xi32>
        %99 = tt.expand_dims %98 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %100 = arith.muli %99, %cst_1 : tensor<64x1xi32>
        %101 = tt.splat %58 : i32 -> tensor<64x1xi32>
        %102 = arith.addi %101, %100 : tensor<64x1xi32>
        %103 = tt.broadcast %102 : tensor<64x1xi32> -> tensor<64x64xi32>
        %104 = tt.broadcast %63 : tensor<1x64xi32> -> tensor<64x64xi32>
        %105 = arith.addi %103, %104 : tensor<64x64xi32>
        %106 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %107 = tt.addptr %106, %105 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %108 = tt.load %107 : tensor<64x64x!tt.ptr<f8E5M2>>
        %109 = tt.trans %108 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %110 = tt.dot %69, %109, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %111 = arith.mulf %110, %cst_0 : tensor<128x64xf32>
        %112 = "tt.reduce"(%111) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %143 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %143 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %113 = arith.cmpf ogt, %arg10, %112 : tensor<128xf32>
        %114 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %115 = arith.ori %113, %114 : tensor<128xi1>
        %116 = arith.select %115, %arg10, %112 : tensor<128xi1>, tensor<128xf32>
        %117 = tt.expand_dims %116 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %118 = tt.broadcast %117 : tensor<128x1xf32> -> tensor<128x64xf32>
        %119 = arith.subf %111, %118 : tensor<128x64xf32>
        %120 = tt.extern_elementwise %119 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %121 = "tt.reduce"(%120) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %143 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %143 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %122 = arith.subf %arg10, %116 : tensor<128xf32>
        %123 = tt.extern_elementwise %122 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %124 = arith.mulf %arg8, %123 : tensor<128xf32>
        %125 = arith.addf %124, %121 : tensor<128xf32>
        %126 = tt.expand_dims %123 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %127 = tt.broadcast %126 : tensor<128x1xf32> -> tensor<128x64xf32>
        %128 = arith.mulf %arg9, %127 : tensor<128x64xf32>
        %129 = tt.expand_dims %56 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %130 = arith.addi %101, %129 : tensor<64x1xi32>
        %131 = tt.expand_dims %98 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %132 = arith.muli %131, %cst : tensor<1x64xi32>
        %133 = tt.broadcast %130 : tensor<64x1xi32> -> tensor<64x64xi32>
        %134 = tt.broadcast %132 : tensor<1x64xi32> -> tensor<64x64xi32>
        %135 = arith.addi %133, %134 : tensor<64x64xi32>
        %136 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %137 = tt.addptr %136, %135 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %138 = tt.load %137 : tensor<64x64x!tt.ptr<f8E5M2>>
        %139 = tt.fp_to_fp %120, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %140 = tt.trans %138 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %141 = tt.dot %139, %140, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %142 = arith.addf %128, %141 : tensor<128x64xf32>
        scf.yield %125, %142, %116 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.num_stages = 1 : i32}
      %72 = tt.expand_dims %71#0 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
      %73 = tt.broadcast %72 : tensor<128x1xf32> -> tensor<128x64xf32>
      %74 = arith.divf %71#1, %73 : tensor<128x64xf32>
      %75 = tt.fp_to_fp %74, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
      %76 = arith.divsi %55, %arg5 : i32
      %77 = arith.remsi %55, %arg5 : i32
      %78 = arith.cmpi ne, %77, %c0_i32 : i32
      %79 = arith.subi %76, %c1_i32 : i32
      %80 = arith.select %78, %79, %76 : i32
      %81 = arith.cmpi slt, %55, %c0_i32 : i32
      %82 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %83 = arith.cmpi ne, %81, %82 : i1
      %84 = arith.select %83, %80, %76 : i32
      %85 = arith.andi %78, %83 : i1
      %86 = arith.addi %77, %arg5 : i32
      %87 = arith.select %85, %86, %77 : i32
      %88 = arith.muli %84, %arg4 : i32
      %89 = arith.muli %87, %c8192_i32 : i32
      %90 = arith.addi %88, %89 : i32
      %91 = tt.splat %90 : i32 -> tensor<128x1xi32>
      %92 = arith.addi %91, %60 : tensor<128x1xi32>
      %93 = tt.broadcast %92 : tensor<128x1xi32> -> tensor<128x64xi32>
      %94 = arith.addi %93, %65 : tensor<128x64xi32>
      %95 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %96 = tt.addptr %95, %94 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      tt.store %96, %75 : tensor<128x64x!tt.ptr<f8E5M2>>
    } {tt.flatten, tt.num_stages = 2 : i32}
    scf.for %arg6 = %11 to %3 step %c1_i32  : i32 {
      %13 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
      %14 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>
      %15 = arith.muli %arg6, %c8192_i32 : i32
      %16 = tt.expand_dims %14 {axis = 1 : i32} : tensor<128xi32> -> tensor<128x1xi32>
      %17 = arith.muli %16, %cst_2 : tensor<128x1xi32>
      %18 = tt.splat %15 : i32 -> tensor<128x1xi32>
      %19 = arith.addi %18, %17 : tensor<128x1xi32>
      %20 = tt.expand_dims %13 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %21 = tt.broadcast %19 : tensor<128x1xi32> -> tensor<128x64xi32>
      %22 = tt.broadcast %20 : tensor<1x64xi32> -> tensor<128x64xi32>
      %23 = arith.addi %21, %22 : tensor<128x64xi32>
      %24 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %25 = tt.addptr %24, %23 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      %26 = tt.load %25 : tensor<128x64x!tt.ptr<f8E5M2>>
      %c0_i32_8 = arith.constant 0 : i32
      %c256_i32 = arith.constant 256 : i32
      %27:3 = scf.for %arg7 = %c0_i32 to %c0_i32_8 step %c256_i32 iter_args(%arg8 = %cst_4, %arg9 = %cst_3, %arg10 = %cst_5) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %54 = tt.splat %arg7 : i32 -> tensor<64xi32>
        %55 = arith.addi %54, %13 : tensor<64xi32>
        %56 = tt.expand_dims %55 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %57 = arith.muli %56, %cst_1 : tensor<64x1xi32>
        %58 = tt.splat %15 : i32 -> tensor<64x1xi32>
        %59 = arith.addi %58, %57 : tensor<64x1xi32>
        %60 = tt.broadcast %59 : tensor<64x1xi32> -> tensor<64x64xi32>
        %61 = tt.broadcast %20 : tensor<1x64xi32> -> tensor<64x64xi32>
        %62 = arith.addi %60, %61 : tensor<64x64xi32>
        %63 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %64 = tt.addptr %63, %62 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %65 = tt.load %64 : tensor<64x64x!tt.ptr<f8E5M2>>
        %66 = tt.trans %65 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %67 = tt.dot %26, %66, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %68 = arith.mulf %67, %cst_0 : tensor<128x64xf32>
        %69 = "tt.reduce"(%68) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %244 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %244 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %70 = arith.cmpf ogt, %arg10, %69 : tensor<128xf32>
        %71 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %72 = arith.ori %70, %71 : tensor<128xi1>
        %73 = arith.select %72, %arg10, %69 : tensor<128xi1>, tensor<128xf32>
        %74 = tt.expand_dims %73 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %75 = tt.broadcast %74 : tensor<128x1xf32> -> tensor<128x64xf32>
        %76 = arith.subf %68, %75 : tensor<128x64xf32>
        %77 = tt.extern_elementwise %76 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %78 = "tt.reduce"(%77) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %244 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %244 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %79 = arith.subf %arg10, %73 : tensor<128xf32>
        %80 = tt.extern_elementwise %79 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %81 = arith.mulf %arg8, %80 : tensor<128xf32>
        %82 = arith.addf %81, %78 : tensor<128xf32>
        %83 = tt.expand_dims %80 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %84 = tt.broadcast %83 : tensor<128x1xf32> -> tensor<128x64xf32>
        %85 = arith.mulf %arg9, %84 : tensor<128x64xf32>
        %86 = tt.expand_dims %13 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %87 = arith.addi %58, %86 : tensor<64x1xi32>
        %88 = tt.expand_dims %55 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %89 = arith.muli %88, %cst : tensor<1x64xi32>
        %90 = tt.broadcast %87 : tensor<64x1xi32> -> tensor<64x64xi32>
        %91 = tt.broadcast %89 : tensor<1x64xi32> -> tensor<64x64xi32>
        %92 = arith.addi %90, %91 : tensor<64x64xi32>
        %93 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %94 = tt.addptr %93, %92 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %95 = tt.load %94 : tensor<64x64x!tt.ptr<f8E5M2>>
        %96 = tt.fp_to_fp %77, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %97 = tt.trans %95 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %98 = tt.dot %96, %97, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %99 = arith.addf %85, %98 : tensor<128x64xf32>
        %c1_i32_9 = arith.constant 1 : i32
        %100 = arith.muli %c64_i32, %c1_i32_9 : i32
        %101 = arith.addi %arg7, %100 : i32
        %102 = tt.splat %101 : i32 -> tensor<64xi32>
        %103 = arith.addi %102, %13 : tensor<64xi32>
        %104 = tt.expand_dims %103 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %105 = arith.muli %104, %cst_1 : tensor<64x1xi32>
        %106 = tt.splat %15 : i32 -> tensor<64x1xi32>
        %107 = arith.addi %106, %105 : tensor<64x1xi32>
        %108 = tt.broadcast %107 : tensor<64x1xi32> -> tensor<64x64xi32>
        %109 = tt.broadcast %20 : tensor<1x64xi32> -> tensor<64x64xi32>
        %110 = arith.addi %108, %109 : tensor<64x64xi32>
        %111 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %112 = tt.addptr %111, %110 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %113 = tt.load %112 : tensor<64x64x!tt.ptr<f8E5M2>>
        %114 = tt.trans %113 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %115 = tt.dot %26, %114, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %116 = arith.mulf %115, %cst_0 : tensor<128x64xf32>
        %117 = "tt.reduce"(%116) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %244 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %244 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %118 = arith.cmpf ogt, %73, %117 : tensor<128xf32>
        %119 = arith.cmpf une, %73, %73 : tensor<128xf32>
        %120 = arith.ori %118, %119 : tensor<128xi1>
        %121 = arith.select %120, %73, %117 : tensor<128xi1>, tensor<128xf32>
        %122 = tt.expand_dims %121 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %123 = tt.broadcast %122 : tensor<128x1xf32> -> tensor<128x64xf32>
        %124 = arith.subf %116, %123 : tensor<128x64xf32>
        %125 = tt.extern_elementwise %124 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %126 = "tt.reduce"(%125) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %244 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %244 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %127 = arith.subf %73, %121 : tensor<128xf32>
        %128 = tt.extern_elementwise %127 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %129 = arith.mulf %82, %128 : tensor<128xf32>
        %130 = arith.addf %129, %126 : tensor<128xf32>
        %131 = tt.expand_dims %128 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %132 = tt.broadcast %131 : tensor<128x1xf32> -> tensor<128x64xf32>
        %133 = arith.mulf %99, %132 : tensor<128x64xf32>
        %134 = tt.expand_dims %13 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %135 = arith.addi %106, %134 : tensor<64x1xi32>
        %136 = tt.expand_dims %103 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %137 = arith.muli %136, %cst : tensor<1x64xi32>
        %138 = tt.broadcast %135 : tensor<64x1xi32> -> tensor<64x64xi32>
        %139 = tt.broadcast %137 : tensor<1x64xi32> -> tensor<64x64xi32>
        %140 = arith.addi %138, %139 : tensor<64x64xi32>
        %141 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %142 = tt.addptr %141, %140 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %143 = tt.load %142 : tensor<64x64x!tt.ptr<f8E5M2>>
        %144 = tt.fp_to_fp %125, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %145 = tt.trans %143 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %146 = tt.dot %144, %145, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %147 = arith.addf %133, %146 : tensor<128x64xf32>
        %c2_i32_10 = arith.constant 2 : i32
        %148 = arith.muli %c64_i32, %c2_i32_10 : i32
        %149 = arith.addi %arg7, %148 : i32
        %150 = tt.splat %149 : i32 -> tensor<64xi32>
        %151 = arith.addi %150, %13 : tensor<64xi32>
        %152 = tt.expand_dims %151 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %153 = arith.muli %152, %cst_1 : tensor<64x1xi32>
        %154 = tt.splat %15 : i32 -> tensor<64x1xi32>
        %155 = arith.addi %154, %153 : tensor<64x1xi32>
        %156 = tt.broadcast %155 : tensor<64x1xi32> -> tensor<64x64xi32>
        %157 = tt.broadcast %20 : tensor<1x64xi32> -> tensor<64x64xi32>
        %158 = arith.addi %156, %157 : tensor<64x64xi32>
        %159 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %160 = tt.addptr %159, %158 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %161 = tt.load %160 : tensor<64x64x!tt.ptr<f8E5M2>>
        %162 = tt.trans %161 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %163 = tt.dot %26, %162, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %164 = arith.mulf %163, %cst_0 : tensor<128x64xf32>
        %165 = "tt.reduce"(%164) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %244 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %244 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %166 = arith.cmpf ogt, %121, %165 : tensor<128xf32>
        %167 = arith.cmpf une, %121, %121 : tensor<128xf32>
        %168 = arith.ori %166, %167 : tensor<128xi1>
        %169 = arith.select %168, %121, %165 : tensor<128xi1>, tensor<128xf32>
        %170 = tt.expand_dims %169 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %171 = tt.broadcast %170 : tensor<128x1xf32> -> tensor<128x64xf32>
        %172 = arith.subf %164, %171 : tensor<128x64xf32>
        %173 = tt.extern_elementwise %172 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %174 = "tt.reduce"(%173) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %244 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %244 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %175 = arith.subf %121, %169 : tensor<128xf32>
        %176 = tt.extern_elementwise %175 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %177 = arith.mulf %130, %176 : tensor<128xf32>
        %178 = arith.addf %177, %174 : tensor<128xf32>
        %179 = tt.expand_dims %176 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %180 = tt.broadcast %179 : tensor<128x1xf32> -> tensor<128x64xf32>
        %181 = arith.mulf %147, %180 : tensor<128x64xf32>
        %182 = tt.expand_dims %13 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %183 = arith.addi %154, %182 : tensor<64x1xi32>
        %184 = tt.expand_dims %151 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %185 = arith.muli %184, %cst : tensor<1x64xi32>
        %186 = tt.broadcast %183 : tensor<64x1xi32> -> tensor<64x64xi32>
        %187 = tt.broadcast %185 : tensor<1x64xi32> -> tensor<64x64xi32>
        %188 = arith.addi %186, %187 : tensor<64x64xi32>
        %189 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %190 = tt.addptr %189, %188 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %191 = tt.load %190 : tensor<64x64x!tt.ptr<f8E5M2>>
        %192 = tt.fp_to_fp %173, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %193 = tt.trans %191 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %194 = tt.dot %192, %193, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %195 = arith.addf %181, %194 : tensor<128x64xf32>
        %c3_i32 = arith.constant 3 : i32
        %196 = arith.muli %c64_i32, %c3_i32 : i32
        %197 = arith.addi %arg7, %196 : i32
        %198 = tt.splat %197 : i32 -> tensor<64xi32>
        %199 = arith.addi %198, %13 : tensor<64xi32>
        %200 = tt.expand_dims %199 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %201 = arith.muli %200, %cst_1 : tensor<64x1xi32>
        %202 = tt.splat %15 : i32 -> tensor<64x1xi32>
        %203 = arith.addi %202, %201 : tensor<64x1xi32>
        %204 = tt.broadcast %203 : tensor<64x1xi32> -> tensor<64x64xi32>
        %205 = tt.broadcast %20 : tensor<1x64xi32> -> tensor<64x64xi32>
        %206 = arith.addi %204, %205 : tensor<64x64xi32>
        %207 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %208 = tt.addptr %207, %206 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %209 = tt.load %208 : tensor<64x64x!tt.ptr<f8E5M2>>
        %210 = tt.trans %209 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %211 = tt.dot %26, %210, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %212 = arith.mulf %211, %cst_0 : tensor<128x64xf32>
        %213 = "tt.reduce"(%212) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %244 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %244 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %214 = arith.cmpf ogt, %169, %213 : tensor<128xf32>
        %215 = arith.cmpf une, %169, %169 : tensor<128xf32>
        %216 = arith.ori %214, %215 : tensor<128xi1>
        %217 = arith.select %216, %169, %213 : tensor<128xi1>, tensor<128xf32>
        %218 = tt.expand_dims %217 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %219 = tt.broadcast %218 : tensor<128x1xf32> -> tensor<128x64xf32>
        %220 = arith.subf %212, %219 : tensor<128x64xf32>
        %221 = tt.extern_elementwise %220 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %222 = "tt.reduce"(%221) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %244 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %244 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %223 = arith.subf %169, %217 : tensor<128xf32>
        %224 = tt.extern_elementwise %223 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %225 = arith.mulf %178, %224 : tensor<128xf32>
        %226 = arith.addf %225, %222 : tensor<128xf32>
        %227 = tt.expand_dims %224 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %228 = tt.broadcast %227 : tensor<128x1xf32> -> tensor<128x64xf32>
        %229 = arith.mulf %195, %228 : tensor<128x64xf32>
        %230 = tt.expand_dims %13 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %231 = arith.addi %202, %230 : tensor<64x1xi32>
        %232 = tt.expand_dims %199 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %233 = arith.muli %232, %cst : tensor<1x64xi32>
        %234 = tt.broadcast %231 : tensor<64x1xi32> -> tensor<64x64xi32>
        %235 = tt.broadcast %233 : tensor<1x64xi32> -> tensor<64x64xi32>
        %236 = arith.addi %234, %235 : tensor<64x64xi32>
        %237 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %238 = tt.addptr %237, %236 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %239 = tt.load %238 : tensor<64x64x!tt.ptr<f8E5M2>>
        %240 = tt.fp_to_fp %221, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %241 = tt.trans %239 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %242 = tt.dot %240, %241, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %243 = arith.addf %229, %242 : tensor<128x64xf32>
        scf.yield %226, %243, %217 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.num_stages = 2 : i32}
      %28:3 = scf.for %arg7 = %c0_i32_8 to %c128_i32 step %c64_i32 iter_args(%arg8 = %27#0, %arg9 = %27#1, %arg10 = %27#2) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %54 = tt.splat %arg7 : i32 -> tensor<64xi32>
        %55 = arith.addi %54, %13 : tensor<64xi32>
        %56 = tt.expand_dims %55 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %57 = arith.muli %56, %cst_1 : tensor<64x1xi32>
        %58 = tt.splat %15 : i32 -> tensor<64x1xi32>
        %59 = arith.addi %58, %57 : tensor<64x1xi32>
        %60 = tt.broadcast %59 : tensor<64x1xi32> -> tensor<64x64xi32>
        %61 = tt.broadcast %20 : tensor<1x64xi32> -> tensor<64x64xi32>
        %62 = arith.addi %60, %61 : tensor<64x64xi32>
        %63 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %64 = tt.addptr %63, %62 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %65 = tt.load %64 : tensor<64x64x!tt.ptr<f8E5M2>>
        %66 = tt.trans %65 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %67 = tt.dot %26, %66, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %68 = arith.mulf %67, %cst_0 : tensor<128x64xf32>
        %69 = "tt.reduce"(%68) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %100 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %100 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %70 = arith.cmpf ogt, %arg10, %69 : tensor<128xf32>
        %71 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %72 = arith.ori %70, %71 : tensor<128xi1>
        %73 = arith.select %72, %arg10, %69 : tensor<128xi1>, tensor<128xf32>
        %74 = tt.expand_dims %73 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %75 = tt.broadcast %74 : tensor<128x1xf32> -> tensor<128x64xf32>
        %76 = arith.subf %68, %75 : tensor<128x64xf32>
        %77 = tt.extern_elementwise %76 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %78 = "tt.reduce"(%77) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %100 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %100 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %79 = arith.subf %arg10, %73 : tensor<128xf32>
        %80 = tt.extern_elementwise %79 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %81 = arith.mulf %arg8, %80 : tensor<128xf32>
        %82 = arith.addf %81, %78 : tensor<128xf32>
        %83 = tt.expand_dims %80 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %84 = tt.broadcast %83 : tensor<128x1xf32> -> tensor<128x64xf32>
        %85 = arith.mulf %arg9, %84 : tensor<128x64xf32>
        %86 = tt.expand_dims %13 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %87 = arith.addi %58, %86 : tensor<64x1xi32>
        %88 = tt.expand_dims %55 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %89 = arith.muli %88, %cst : tensor<1x64xi32>
        %90 = tt.broadcast %87 : tensor<64x1xi32> -> tensor<64x64xi32>
        %91 = tt.broadcast %89 : tensor<1x64xi32> -> tensor<64x64xi32>
        %92 = arith.addi %90, %91 : tensor<64x64xi32>
        %93 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %94 = tt.addptr %93, %92 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %95 = tt.load %94 : tensor<64x64x!tt.ptr<f8E5M2>>
        %96 = tt.fp_to_fp %77, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %97 = tt.trans %95 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %98 = tt.dot %96, %97, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %99 = arith.addf %85, %98 : tensor<128x64xf32>
        scf.yield %82, %99, %73 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.num_stages = 1 : i32}
      %29 = tt.expand_dims %28#0 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
      %30 = tt.broadcast %29 : tensor<128x1xf32> -> tensor<128x64xf32>
      %31 = arith.divf %28#1, %30 : tensor<128x64xf32>
      %32 = tt.fp_to_fp %31, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
      %33 = arith.divsi %arg6, %arg5 : i32
      %34 = arith.remsi %arg6, %arg5 : i32
      %35 = arith.cmpi ne, %34, %c0_i32 : i32
      %36 = arith.subi %33, %c1_i32 : i32
      %37 = arith.select %35, %36, %33 : i32
      %38 = arith.cmpi slt, %arg6, %c0_i32 : i32
      %39 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %40 = arith.cmpi ne, %38, %39 : i1
      %41 = arith.select %40, %37, %33 : i32
      %42 = arith.andi %35, %40 : i1
      %43 = arith.addi %34, %arg5 : i32
      %44 = arith.select %42, %43, %34 : i32
      %45 = arith.muli %41, %arg4 : i32
      %46 = arith.muli %44, %c8192_i32 : i32
      %47 = arith.addi %45, %46 : i32
      %48 = tt.splat %47 : i32 -> tensor<128x1xi32>
      %49 = arith.addi %48, %17 : tensor<128x1xi32>
      %50 = tt.broadcast %49 : tensor<128x1xi32> -> tensor<128x64xi32>
      %51 = arith.addi %50, %22 : tensor<128x64xi32>
      %52 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %53 = tt.addptr %52, %51 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      tt.store %53, %32 : tensor<128x64x!tt.ptr<f8E5M2>>
    } {tt.flatten, tt.num_stages = 1 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(convert-triton-to-tritongpu{enable-source-remat=false num-ctas=1 num-warps=8 target=cuda:90 threads-per-warp=32}, tritongpu-coalesce, tritongpu-F32DotTC, triton-nvidia-gpu-plan-cta, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritongpu-accelerate-matmul, tritongpu-remove-layout-conversions, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, triton-nvidia-optimize-descriptor-encoding, triton-loop-aware-cse, tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-combine-tensor-select-and-if, nvgpu-warp-specialization{dump-intermediate-steps=false num-stages=3}, tritongpu-assign-latencies{num-stages=3}, tritongpu-schedule-loops, tritongpu-pipeline{dump-intermediate-steps=false num-stages=3}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-loop-aware-cse, tritongpu-prefetch, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-coalesce-async-copy, triton-nvidia-optimize-tmem-layouts, tritongpu-remove-layout-conversions, triton-nvidia-interleave-tmem, tritongpu-reduce-data-duplication, tritongpu-reorder-instructions, triton-loop-aware-cse, symbol-dce, triton-nvidia-tma-lowering, triton-nvidia-gpu-fence-insertion{compute-capability=90}, sccp, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true})",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_willfeng/xt/cxtdqw3vwg4sfbh5csi43if5i7vcethnfl23j6jfximeeknqi7sj.py:13:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_willfeng/xt/cxtdqw3vwg4sfbh5csi43if5i7vcethnfl23j6jfximeeknqi7sj.py:13:0: note: Pipeline failed while executing [`TritonGPUAssignLatencies` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
python: /root/.triton/llvm/llvm-8957e64a-almalinux-x64/include/llvm/ADT/SmallVector.h:296: const_reference llvm::SmallVectorTemplateCommon<long>::operator[](size_type) const [T = long]: Assertion `idx < size()' failed.
module {
  tt.func public @_fp8_attention_kernel_kernel(%arg0: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %c1_i32 = arith.constant 1 : i32
    %c32_i32 = arith.constant 32 : i32
    %c128_i32 = arith.constant 128 : i32
    %c0_i32 = arith.constant 0 : i32
    %c132_i32 = arith.constant 132 : i32
    %c192_i32 = arith.constant 192 : i32
    %cst = arith.constant dense<64> : tensor<1x32xi32>
    %cst_0 = arith.constant dense<0.180336878> : tensor<128x32xf32>
    %cst_1 = arith.constant dense<0.000000e+00> : tensor<128x32xf32>
    %cst_2 = arith.constant dense<64> : tensor<32x1xi32>
    %cst_3 = arith.constant dense<64> : tensor<128x1xi32>
    %c8192_i32 = arith.constant 8192 : i32
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<128x64xf32>
    %cst_5 = arith.constant dense<0.000000e+00> : tensor<128xf32>
    %cst_6 = arith.constant dense<0xFF800000> : tensor<128xf32>
    %0 = tt.get_program_id x : i32
    scf.for %arg6 = %0 to %c192_i32 step %c132_i32  : i32 {
      %1 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
      %2 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>
      %3 = arith.muli %arg6, %c8192_i32 : i32
      %4 = tt.expand_dims %2 {axis = 1 : i32} : tensor<128xi32> -> tensor<128x1xi32>
      %5 = arith.muli %4, %cst_3 : tensor<128x1xi32>
      %6 = tt.splat %3 : i32 -> tensor<128x1xi32>
      %7 = arith.addi %6, %5 : tensor<128x1xi32>
      %8 = tt.expand_dims %1 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %9 = tt.broadcast %7 : tensor<128x1xi32> -> tensor<128x64xi32>
      %10 = tt.broadcast %8 : tensor<1x64xi32> -> tensor<128x64xi32>
      %11 = arith.addi %9, %10 : tensor<128x64xi32>
      %12 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %13 = tt.addptr %12, %11 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      %14 = tt.load %13 : tensor<128x64x!tt.ptr<f8E5M2>>
      %15:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c32_i32 iter_args(%arg8 = %cst_5, %arg9 = %cst_4, %arg10 = %cst_6) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %41 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32>
        %42 = tt.splat %arg7 : i32 -> tensor<32xi32>
        %43 = arith.addi %42, %41 : tensor<32xi32>
        %44 = tt.expand_dims %43 {axis = 1 : i32} : tensor<32xi32> -> tensor<32x1xi32>
        %45 = arith.muli %44, %cst_2 : tensor<32x1xi32>
        %46 = tt.splat %3 : i32 -> tensor<32x1xi32>
        %47 = arith.addi %46, %45 : tensor<32x1xi32>
        %48 = tt.broadcast %47 : tensor<32x1xi32> -> tensor<32x64xi32>
        %49 = tt.broadcast %8 : tensor<1x64xi32> -> tensor<32x64xi32>
        %50 = arith.addi %48, %49 : tensor<32x64xi32>
        %51 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<32x64x!tt.ptr<f8E5M2>>
        %52 = tt.addptr %51, %50 : tensor<32x64x!tt.ptr<f8E5M2>>, tensor<32x64xi32>
        %53 = tt.load %52 : tensor<32x64x!tt.ptr<f8E5M2>>
        %54 = tt.trans %53 {order = array<i32: 1, 0>} : tensor<32x64xf8E5M2> -> tensor<64x32xf8E5M2>
        %55 = tt.dot %14, %54, %cst_1, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x32xf8E5M2> -> tensor<128x32xf32>
        %56 = arith.mulf %55, %cst_0 : tensor<128x32xf32>
        %57 = "tt.reduce"(%56) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %89 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %89 : f32
        }) : (tensor<128x32xf32>) -> tensor<128xf32>
        %58 = arith.cmpf ogt, %arg10, %57 : tensor<128xf32>
        %59 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %60 = arith.ori %58, %59 : tensor<128xi1>
        %61 = arith.select %60, %arg10, %57 : tensor<128xi1>, tensor<128xf32>
        %62 = tt.expand_dims %61 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %63 = tt.broadcast %62 : tensor<128x1xf32> -> tensor<128x32xf32>
        %64 = arith.subf %56, %63 : tensor<128x32xf32>
        %65 = tt.extern_elementwise %64 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x32xf32>) -> tensor<128x32xf32>
        %66 = "tt.reduce"(%65) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %89 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %89 : f32
        }) : (tensor<128x32xf32>) -> tensor<128xf32>
        %67 = arith.subf %arg10, %61 : tensor<128xf32>
        %68 = tt.extern_elementwise %67 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %69 = arith.mulf %arg8, %68 : tensor<128xf32>
        %70 = arith.addf %69, %66 : tensor<128xf32>
        %71 = tt.expand_dims %68 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %72 = tt.broadcast %71 : tensor<128x1xf32> -> tensor<128x64xf32>
        %73 = arith.mulf %arg9, %72 : tensor<128x64xf32>
        %74 = tt.expand_dims %1 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %75 = tt.splat %3 : i32 -> tensor<64x1xi32>
        %76 = arith.addi %75, %74 : tensor<64x1xi32>
        %77 = tt.expand_dims %43 {axis = 0 : i32} : tensor<32xi32> -> tensor<1x32xi32>
        %78 = arith.muli %77, %cst : tensor<1x32xi32>
        %79 = tt.broadcast %76 : tensor<64x1xi32> -> tensor<64x32xi32>
        %80 = tt.broadcast %78 : tensor<1x32xi32> -> tensor<64x32xi32>
        %81 = arith.addi %79, %80 : tensor<64x32xi32>
        %82 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x32x!tt.ptr<f8E5M2>>
        %83 = tt.addptr %82, %81 : tensor<64x32x!tt.ptr<f8E5M2>>, tensor<64x32xi32>
        %84 = tt.load %83 : tensor<64x32x!tt.ptr<f8E5M2>>
        %85 = tt.fp_to_fp %65, rounding = rtne : tensor<128x32xf32> -> tensor<128x32xf8E5M2>
        %86 = tt.trans %84 {order = array<i32: 1, 0>} : tensor<64x32xf8E5M2> -> tensor<32x64xf8E5M2>
        %87 = tt.dot %85, %86, %cst_4, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x32xf8E5M2> * tensor<32x64xf8E5M2> -> tensor<128x64xf32>
        %88 = arith.addf %73, %87 : tensor<128x64xf32>
        scf.yield %70, %88, %61 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.loop_unroll_factor = 1 : i32}
      %16 = tt.expand_dims %15#0 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
      %17 = tt.broadcast %16 : tensor<128x1xf32> -> tensor<128x64xf32>
      %18 = arith.divf %15#1, %17 : tensor<128x64xf32>
      %19 = tt.fp_to_fp %18, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
      %20 = arith.divsi %arg6, %arg5 : i32
      %21 = arith.remsi %arg6, %arg5 : i32
      %22 = arith.cmpi ne, %21, %c0_i32 : i32
      %23 = arith.subi %20, %c1_i32 : i32
      %24 = arith.select %22, %23, %20 : i32
      %25 = arith.cmpi slt, %arg6, %c0_i32 : i32
      %26 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %27 = arith.cmpi ne, %25, %26 : i1
      %28 = arith.select %27, %24, %20 : i32
      %29 = arith.andi %22, %27 : i1
      %30 = arith.addi %21, %arg5 : i32
      %31 = arith.select %29, %30, %21 : i32
      %32 = arith.muli %28, %arg4 : i32
      %33 = arith.muli %31, %c8192_i32 : i32
      %34 = arith.addi %32, %33 : i32
      %35 = tt.splat %34 : i32 -> tensor<128x1xi32>
      %36 = arith.addi %35, %5 : tensor<128x1xi32>
      %37 = tt.broadcast %36 : tensor<128x1xi32> -> tensor<128x64xi32>
      %38 = arith.addi %37, %10 : tensor<128x64xi32>
      %39 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %40 = tt.addptr %39, %38 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      tt.store %40, %19 : tensor<128x64x!tt.ptr<f8E5M2>>
    } {tt.disallow_acc_multi_buffer, tt.flatten}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(convert-triton-to-tritongpu{enable-source-remat=false num-ctas=1 num-warps=8 target=cuda:90 threads-per-warp=32}, tritongpu-coalesce, tritongpu-F32DotTC, triton-nvidia-gpu-plan-cta, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritongpu-accelerate-matmul, tritongpu-remove-layout-conversions, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, triton-nvidia-optimize-descriptor-encoding, triton-loop-aware-cse, tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-combine-tensor-select-and-if, nvgpu-warp-specialization{dump-intermediate-steps=false num-stages=6}, tritongpu-assign-latencies{num-stages=6}, tritongpu-schedule-loops, tritongpu-pipeline{dump-intermediate-steps=false num-stages=6}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-loop-aware-cse, tritongpu-prefetch, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-coalesce-async-copy, triton-nvidia-optimize-tmem-layouts, tritongpu-remove-layout-conversions, triton-nvidia-interleave-tmem, tritongpu-reduce-data-duplication, tritongpu-reorder-instructions, triton-loop-aware-cse, symbol-dce, triton-nvidia-tma-lowering, triton-nvidia-gpu-fence-insertion{compute-capability=90}, sccp, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true})",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_willfeng/ss/css4bvxumzi74sfc5jgiadmx5a5yhjzpssprp4dleu22m2kt2chv.py:13:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_willfeng/ss/css4bvxumzi74sfc5jgiadmx5a5yhjzpssprp4dleu22m2kt2chv.py:13:0: note: Pipeline failed while executing [`TritonGPUAssignLatencies` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
python: /root/.triton/llvm/llvm-8957e64a-almalinux-x64/include/llvm/ADT/SmallVector.h:296: const_reference llvm::SmallVectorTemplateCommon<long>::operator[](size_type) const [T = long]: Assertion `idx < size()' failed.
module {
  tt.func public @_fp8_attention_kernel_kernel(%arg0: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %c1_i32 = arith.constant 1 : i32
    %c32_i32 = arith.constant 32 : i32
    %c128_i32 = arith.constant 128 : i32
    %c0_i32 = arith.constant 0 : i32
    %c132_i32 = arith.constant 132 : i32
    %c192_i32 = arith.constant 192 : i32
    %cst = arith.constant dense<64> : tensor<1x32xi32>
    %cst_0 = arith.constant dense<0.180336878> : tensor<128x32xf32>
    %cst_1 = arith.constant dense<0.000000e+00> : tensor<128x32xf32>
    %cst_2 = arith.constant dense<64> : tensor<32x1xi32>
    %cst_3 = arith.constant dense<64> : tensor<128x1xi32>
    %c8192_i32 = arith.constant 8192 : i32
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<128x64xf32>
    %cst_5 = arith.constant dense<0.000000e+00> : tensor<128xf32>
    %cst_6 = arith.constant dense<0xFF800000> : tensor<128xf32>
    %0 = tt.get_program_id x : i32
    scf.for %arg6 = %0 to %c192_i32 step %c132_i32  : i32 {
      %1 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
      %2 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>
      %3 = arith.muli %arg6, %c8192_i32 : i32
      %4 = tt.expand_dims %2 {axis = 1 : i32} : tensor<128xi32> -> tensor<128x1xi32>
      %5 = arith.muli %4, %cst_3 : tensor<128x1xi32>
      %6 = tt.splat %3 : i32 -> tensor<128x1xi32>
      %7 = arith.addi %6, %5 : tensor<128x1xi32>
      %8 = tt.expand_dims %1 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %9 = tt.broadcast %7 : tensor<128x1xi32> -> tensor<128x64xi32>
      %10 = tt.broadcast %8 : tensor<1x64xi32> -> tensor<128x64xi32>
      %11 = arith.addi %9, %10 : tensor<128x64xi32>
      %12 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %13 = tt.addptr %12, %11 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      %14 = tt.load %13 : tensor<128x64x!tt.ptr<f8E5M2>>
      %15:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c32_i32 iter_args(%arg8 = %cst_5, %arg9 = %cst_4, %arg10 = %cst_6) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %41 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32>
        %42 = tt.splat %arg7 : i32 -> tensor<32xi32>
        %43 = arith.addi %42, %41 : tensor<32xi32>
        %44 = tt.expand_dims %43 {axis = 1 : i32} : tensor<32xi32> -> tensor<32x1xi32>
        %45 = arith.muli %44, %cst_2 : tensor<32x1xi32>
        %46 = tt.splat %3 : i32 -> tensor<32x1xi32>
        %47 = arith.addi %46, %45 : tensor<32x1xi32>
        %48 = tt.broadcast %47 : tensor<32x1xi32> -> tensor<32x64xi32>
        %49 = tt.broadcast %8 : tensor<1x64xi32> -> tensor<32x64xi32>
        %50 = arith.addi %48, %49 : tensor<32x64xi32>
        %51 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<32x64x!tt.ptr<f8E5M2>>
        %52 = tt.addptr %51, %50 : tensor<32x64x!tt.ptr<f8E5M2>>, tensor<32x64xi32>
        %53 = tt.load %52 : tensor<32x64x!tt.ptr<f8E5M2>>
        %54 = tt.trans %53 {order = array<i32: 1, 0>} : tensor<32x64xf8E5M2> -> tensor<64x32xf8E5M2>
        %55 = tt.dot %14, %54, %cst_1, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x32xf8E5M2> -> tensor<128x32xf32>
        %56 = arith.mulf %55, %cst_0 : tensor<128x32xf32>
        %57 = "tt.reduce"(%56) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %89 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %89 : f32
        }) : (tensor<128x32xf32>) -> tensor<128xf32>
        %58 = arith.cmpf ogt, %arg10, %57 : tensor<128xf32>
        %59 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %60 = arith.ori %58, %59 : tensor<128xi1>
        %61 = arith.select %60, %arg10, %57 : tensor<128xi1>, tensor<128xf32>
        %62 = tt.expand_dims %61 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %63 = tt.broadcast %62 : tensor<128x1xf32> -> tensor<128x32xf32>
        %64 = arith.subf %56, %63 : tensor<128x32xf32>
        %65 = tt.extern_elementwise %64 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x32xf32>) -> tensor<128x32xf32>
        %66 = "tt.reduce"(%65) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %89 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %89 : f32
        }) : (tensor<128x32xf32>) -> tensor<128xf32>
        %67 = arith.subf %arg10, %61 : tensor<128xf32>
        %68 = tt.extern_elementwise %67 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %69 = arith.mulf %arg8, %68 : tensor<128xf32>
        %70 = arith.addf %69, %66 : tensor<128xf32>
        %71 = tt.expand_dims %68 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %72 = tt.broadcast %71 : tensor<128x1xf32> -> tensor<128x64xf32>
        %73 = arith.mulf %arg9, %72 : tensor<128x64xf32>
        %74 = tt.expand_dims %1 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %75 = tt.splat %3 : i32 -> tensor<64x1xi32>
        %76 = arith.addi %75, %74 : tensor<64x1xi32>
        %77 = tt.expand_dims %43 {axis = 0 : i32} : tensor<32xi32> -> tensor<1x32xi32>
        %78 = arith.muli %77, %cst : tensor<1x32xi32>
        %79 = tt.broadcast %76 : tensor<64x1xi32> -> tensor<64x32xi32>
        %80 = tt.broadcast %78 : tensor<1x32xi32> -> tensor<64x32xi32>
        %81 = arith.addi %79, %80 : tensor<64x32xi32>
        %82 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x32x!tt.ptr<f8E5M2>>
        %83 = tt.addptr %82, %81 : tensor<64x32x!tt.ptr<f8E5M2>>, tensor<64x32xi32>
        %84 = tt.load %83 : tensor<64x32x!tt.ptr<f8E5M2>>
        %85 = tt.fp_to_fp %65, rounding = rtne : tensor<128x32xf32> -> tensor<128x32xf8E5M2>
        %86 = tt.trans %84 {order = array<i32: 1, 0>} : tensor<64x32xf8E5M2> -> tensor<32x64xf8E5M2>
        %87 = tt.dot %85, %86, %cst_4, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x32xf8E5M2> * tensor<32x64xf8E5M2> -> tensor<128x64xf32>
        %88 = arith.addf %73, %87 : tensor<128x64xf32>
        scf.yield %70, %88, %61 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.disallow_acc_multi_buffer, tt.num_stages = 4 : i32}
      %16 = tt.expand_dims %15#0 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
      %17 = tt.broadcast %16 : tensor<128x1xf32> -> tensor<128x64xf32>
      %18 = arith.divf %15#1, %17 : tensor<128x64xf32>
      %19 = tt.fp_to_fp %18, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
      %20 = arith.divsi %arg6, %arg5 : i32
      %21 = arith.remsi %arg6, %arg5 : i32
      %22 = arith.cmpi ne, %21, %c0_i32 : i32
      %23 = arith.subi %20, %c1_i32 : i32
      %24 = arith.select %22, %23, %20 : i32
      %25 = arith.cmpi slt, %arg6, %c0_i32 : i32
      %26 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %27 = arith.cmpi ne, %25, %26 : i1
      %28 = arith.select %27, %24, %20 : i32
      %29 = arith.andi %22, %27 : i1
      %30 = arith.addi %21, %arg5 : i32
      %31 = arith.select %29, %30, %21 : i32
      %32 = arith.muli %28, %arg4 : i32
      %33 = arith.muli %31, %c8192_i32 : i32
      %34 = arith.addi %32, %33 : i32
      %35 = tt.splat %34 : i32 -> tensor<128x1xi32>
      %36 = arith.addi %35, %5 : tensor<128x1xi32>
      %37 = tt.broadcast %36 : tensor<128x1xi32> -> tensor<128x64xi32>
      %38 = arith.addi %37, %10 : tensor<128x64xi32>
      %39 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %40 = tt.addptr %39, %38 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      tt.store %40, %19 : tensor<128x64x!tt.ptr<f8E5M2>>
    } {tt.disallow_acc_multi_buffer, tt.flatten}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(convert-triton-to-tritongpu{enable-source-remat=false num-ctas=1 num-warps=8 target=cuda:90 threads-per-warp=32}, tritongpu-coalesce, tritongpu-F32DotTC, triton-nvidia-gpu-plan-cta, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritongpu-accelerate-matmul, tritongpu-remove-layout-conversions, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, triton-nvidia-optimize-descriptor-encoding, triton-loop-aware-cse, tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-combine-tensor-select-and-if, nvgpu-warp-specialization{dump-intermediate-steps=false num-stages=7}, tritongpu-assign-latencies{num-stages=7}, tritongpu-schedule-loops, tritongpu-pipeline{dump-intermediate-steps=false num-stages=7}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-loop-aware-cse, tritongpu-prefetch, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-coalesce-async-copy, triton-nvidia-optimize-tmem-layouts, tritongpu-remove-layout-conversions, triton-nvidia-interleave-tmem, tritongpu-reduce-data-duplication, tritongpu-reorder-instructions, triton-loop-aware-cse, symbol-dce, triton-nvidia-tma-lowering, triton-nvidia-gpu-fence-insertion{compute-capability=90}, sccp, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true})",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_willfeng/k3/ck3jgq6hnji5z73qoekd4mloyc3z5xsulnxseqr4wd5mrr2jcdpj.py:13:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_willfeng/k3/ck3jgq6hnji5z73qoekd4mloyc3z5xsulnxseqr4wd5mrr2jcdpj.py:13:0: note: Pipeline failed while executing [`TritonGPUAssignLatencies` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
python: /root/.triton/llvm/llvm-8957e64a-almalinux-x64/include/llvm/ADT/SmallVector.h:296: const_reference llvm::SmallVectorTemplateCommon<long>::operator[](size_type) const [T = long]: Assertion `idx < size()' failed.
module {
  tt.func public @_fp8_attention_kernel_kernel(%arg0: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %c1_i32 = arith.constant 1 : i32
    %c32_i32 = arith.constant 32 : i32
    %c64_i32 = arith.constant 64 : i32
    %c128_i32 = arith.constant 128 : i32
    %c0_i32 = arith.constant 0 : i32
    %cst = arith.constant dense<64> : tensor<1x32xi32>
    %cst_0 = arith.constant dense<0.180336878> : tensor<64x32xf32>
    %cst_1 = arith.constant dense<0.000000e+00> : tensor<64x32xf32>
    %cst_2 = arith.constant dense<64> : tensor<32x1xi32>
    %cst_3 = arith.constant dense<64> : tensor<64x1xi32>
    %c8192_i32 = arith.constant 8192 : i32
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<64x64xf32>
    %cst_5 = arith.constant dense<0.000000e+00> : tensor<64xf32>
    %cst_6 = arith.constant dense<0xFF800000> : tensor<64xf32>
    %0 = tt.get_program_id x : i32
    %1 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
    scf.for %arg6 = %c0_i32 to %c128_i32 step %c64_i32  : i32 {
      %2 = tt.splat %arg6 : i32 -> tensor<64xi32>
      %3 = arith.addi %2, %1 : tensor<64xi32>
      %4 = arith.muli %0, %c8192_i32 : i32
      %5 = tt.expand_dims %3 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
      %6 = arith.muli %5, %cst_3 : tensor<64x1xi32>
      %7 = tt.splat %4 : i32 -> tensor<64x1xi32>
      %8 = arith.addi %7, %6 : tensor<64x1xi32>
      %9 = tt.expand_dims %1 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %10 = tt.broadcast %8 : tensor<64x1xi32> -> tensor<64x64xi32>
      %11 = tt.broadcast %9 : tensor<1x64xi32> -> tensor<64x64xi32>
      %12 = arith.addi %10, %11 : tensor<64x64xi32>
      %13 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %14 = tt.addptr %13, %12 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      %15 = tt.load %14 : tensor<64x64x!tt.ptr<f8E5M2>>
      %16:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c32_i32 iter_args(%arg8 = %cst_5, %arg9 = %cst_4, %arg10 = %cst_6) -> (tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>)  : i32 {
        %42 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32>
        %43 = tt.splat %arg7 : i32 -> tensor<32xi32>
        %44 = arith.addi %43, %42 : tensor<32xi32>
        %45 = tt.expand_dims %44 {axis = 1 : i32} : tensor<32xi32> -> tensor<32x1xi32>
        %46 = arith.muli %45, %cst_2 : tensor<32x1xi32>
        %47 = tt.splat %4 : i32 -> tensor<32x1xi32>
        %48 = arith.addi %47, %46 : tensor<32x1xi32>
        %49 = tt.broadcast %48 : tensor<32x1xi32> -> tensor<32x64xi32>
        %50 = tt.broadcast %9 : tensor<1x64xi32> -> tensor<32x64xi32>
        %51 = arith.addi %49, %50 : tensor<32x64xi32>
        %52 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<32x64x!tt.ptr<f8E5M2>>
        %53 = tt.addptr %52, %51 : tensor<32x64x!tt.ptr<f8E5M2>>, tensor<32x64xi32>
        %54 = tt.load %53 : tensor<32x64x!tt.ptr<f8E5M2>>
        %55 = tt.trans %54 {order = array<i32: 1, 0>} : tensor<32x64xf8E5M2> -> tensor<64x32xf8E5M2>
        %56 = tt.dot %15, %55, %cst_1, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x64xf8E5M2> * tensor<64x32xf8E5M2> -> tensor<64x32xf32>
        %57 = arith.mulf %56, %cst_0 : tensor<64x32xf32>
        %58 = "tt.reduce"(%57) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %89 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %89 : f32
        }) : (tensor<64x32xf32>) -> tensor<64xf32>
        %59 = arith.cmpf ogt, %arg10, %58 : tensor<64xf32>
        %60 = arith.cmpf une, %arg10, %arg10 : tensor<64xf32>
        %61 = arith.ori %59, %60 : tensor<64xi1>
        %62 = arith.select %61, %arg10, %58 : tensor<64xi1>, tensor<64xf32>
        %63 = tt.expand_dims %62 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %64 = tt.broadcast %63 : tensor<64x1xf32> -> tensor<64x32xf32>
        %65 = arith.subf %57, %64 : tensor<64x32xf32>
        %66 = tt.extern_elementwise %65 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64x32xf32>) -> tensor<64x32xf32>
        %67 = "tt.reduce"(%66) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %89 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %89 : f32
        }) : (tensor<64x32xf32>) -> tensor<64xf32>
        %68 = arith.subf %arg10, %62 : tensor<64xf32>
        %69 = tt.extern_elementwise %68 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64xf32>) -> tensor<64xf32>
        %70 = arith.mulf %arg8, %69 : tensor<64xf32>
        %71 = arith.addf %70, %67 : tensor<64xf32>
        %72 = tt.expand_dims %69 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %73 = tt.broadcast %72 : tensor<64x1xf32> -> tensor<64x64xf32>
        %74 = arith.mulf %arg9, %73 : tensor<64x64xf32>
        %75 = tt.expand_dims %1 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %76 = arith.addi %7, %75 : tensor<64x1xi32>
        %77 = tt.expand_dims %44 {axis = 0 : i32} : tensor<32xi32> -> tensor<1x32xi32>
        %78 = arith.muli %77, %cst : tensor<1x32xi32>
        %79 = tt.broadcast %76 : tensor<64x1xi32> -> tensor<64x32xi32>
        %80 = tt.broadcast %78 : tensor<1x32xi32> -> tensor<64x32xi32>
        %81 = arith.addi %79, %80 : tensor<64x32xi32>
        %82 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x32x!tt.ptr<f8E5M2>>
        %83 = tt.addptr %82, %81 : tensor<64x32x!tt.ptr<f8E5M2>>, tensor<64x32xi32>
        %84 = tt.load %83 : tensor<64x32x!tt.ptr<f8E5M2>>
        %85 = tt.fp_to_fp %66, rounding = rtne : tensor<64x32xf32> -> tensor<64x32xf8E5M2>
        %86 = tt.trans %84 {order = array<i32: 1, 0>} : tensor<64x32xf8E5M2> -> tensor<32x64xf8E5M2>
        %87 = tt.dot %85, %86, %cst_4, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x32xf8E5M2> * tensor<32x64xf8E5M2> -> tensor<64x64xf32>
        %88 = arith.addf %74, %87 : tensor<64x64xf32>
        scf.yield %71, %88, %62 : tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>
      } {tt.disallow_acc_multi_buffer, tt.loop_unroll_factor = 1 : i32}
      %17 = tt.expand_dims %16#0 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
      %18 = tt.broadcast %17 : tensor<64x1xf32> -> tensor<64x64xf32>
      %19 = arith.divf %16#1, %18 : tensor<64x64xf32>
      %20 = tt.fp_to_fp %19, rounding = rtne : tensor<64x64xf32> -> tensor<64x64xf8E5M2>
      %21 = arith.divsi %0, %arg5 : i32
      %22 = arith.remsi %0, %arg5 : i32
      %23 = arith.cmpi ne, %22, %c0_i32 : i32
      %24 = arith.subi %21, %c1_i32 : i32
      %25 = arith.select %23, %24, %21 : i32
      %26 = arith.cmpi slt, %0, %c0_i32 : i32
      %27 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %28 = arith.cmpi ne, %26, %27 : i1
      %29 = arith.select %28, %25, %21 : i32
      %30 = arith.andi %23, %28 : i1
      %31 = arith.addi %22, %arg5 : i32
      %32 = arith.select %30, %31, %22 : i32
      %33 = arith.muli %29, %arg4 : i32
      %34 = arith.muli %32, %c8192_i32 : i32
      %35 = arith.addi %33, %34 : i32
      %36 = tt.splat %35 : i32 -> tensor<64x1xi32>
      %37 = arith.addi %36, %6 : tensor<64x1xi32>
      %38 = tt.broadcast %37 : tensor<64x1xi32> -> tensor<64x64xi32>
      %39 = arith.addi %38, %11 : tensor<64x64xi32>
      %40 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %41 = tt.addptr %40, %39 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      tt.store %41, %20 : tensor<64x64x!tt.ptr<f8E5M2>>
    } {tt.flatten}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(convert-triton-to-tritongpu{enable-source-remat=false num-ctas=1 num-warps=16 target=cuda:90 threads-per-warp=32}, tritongpu-coalesce, tritongpu-F32DotTC, triton-nvidia-gpu-plan-cta, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritongpu-accelerate-matmul, tritongpu-remove-layout-conversions, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, triton-nvidia-optimize-descriptor-encoding, triton-loop-aware-cse, tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-combine-tensor-select-and-if, nvgpu-warp-specialization{dump-intermediate-steps=false num-stages=6}, tritongpu-assign-latencies{num-stages=6}, tritongpu-schedule-loops, tritongpu-pipeline{dump-intermediate-steps=false num-stages=6}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-loop-aware-cse, tritongpu-prefetch, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-coalesce-async-copy, triton-nvidia-optimize-tmem-layouts, tritongpu-remove-layout-conversions, triton-nvidia-interleave-tmem, tritongpu-reduce-data-duplication, tritongpu-reorder-instructions, triton-loop-aware-cse, symbol-dce, triton-nvidia-tma-lowering, triton-nvidia-gpu-fence-insertion{compute-capability=90}, sccp, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true})",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_willfeng/5r/c5rep4nhtbefp5ifomqdqr6xp3hgaawbss2fb2f5zvvdxkxfccsw.py:12:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_willfeng/5r/c5rep4nhtbefp5ifomqdqr6xp3hgaawbss2fb2f5zvvdxkxfccsw.py:12:0: note: Pipeline failed while executing [`TritonGPUAssignLatencies` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
python: /root/.triton/llvm/llvm-8957e64a-almalinux-x64/include/llvm/ADT/SmallVector.h:296: const_reference llvm::SmallVectorTemplateCommon<long>::operator[](size_type) const [T = long]: Assertion `idx < size()' failed.
module {
  tt.func public @_fp8_attention_kernel_kernel(%arg0: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %c1_i32 = arith.constant 1 : i32
    %c32_i32 = arith.constant 32 : i32
    %c64_i32 = arith.constant 64 : i32
    %c128_i32 = arith.constant 128 : i32
    %c0_i32 = arith.constant 0 : i32
    %cst = arith.constant dense<64> : tensor<1x32xi32>
    %cst_0 = arith.constant dense<0.180336878> : tensor<64x32xf32>
    %cst_1 = arith.constant dense<0.000000e+00> : tensor<64x32xf32>
    %cst_2 = arith.constant dense<64> : tensor<32x1xi32>
    %cst_3 = arith.constant dense<64> : tensor<64x1xi32>
    %c8192_i32 = arith.constant 8192 : i32
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<64x64xf32>
    %cst_5 = arith.constant dense<0.000000e+00> : tensor<64xf32>
    %cst_6 = arith.constant dense<0xFF800000> : tensor<64xf32>
    %0 = tt.get_program_id x : i32
    %1 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
    scf.for %arg6 = %c0_i32 to %c128_i32 step %c64_i32  : i32 {
      %2 = tt.splat %arg6 : i32 -> tensor<64xi32>
      %3 = arith.addi %2, %1 : tensor<64xi32>
      %4 = arith.muli %0, %c8192_i32 : i32
      %5 = tt.expand_dims %3 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
      %6 = arith.muli %5, %cst_3 : tensor<64x1xi32>
      %7 = tt.splat %4 : i32 -> tensor<64x1xi32>
      %8 = arith.addi %7, %6 : tensor<64x1xi32>
      %9 = tt.expand_dims %1 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %10 = tt.broadcast %8 : tensor<64x1xi32> -> tensor<64x64xi32>
      %11 = tt.broadcast %9 : tensor<1x64xi32> -> tensor<64x64xi32>
      %12 = arith.addi %10, %11 : tensor<64x64xi32>
      %13 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %14 = tt.addptr %13, %12 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      %15 = tt.load %14 : tensor<64x64x!tt.ptr<f8E5M2>>
      %16:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c32_i32 iter_args(%arg8 = %cst_5, %arg9 = %cst_4, %arg10 = %cst_6) -> (tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>)  : i32 {
        %42 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32>
        %43 = tt.splat %arg7 : i32 -> tensor<32xi32>
        %44 = arith.addi %43, %42 : tensor<32xi32>
        %45 = tt.expand_dims %44 {axis = 1 : i32} : tensor<32xi32> -> tensor<32x1xi32>
        %46 = arith.muli %45, %cst_2 : tensor<32x1xi32>
        %47 = tt.splat %4 : i32 -> tensor<32x1xi32>
        %48 = arith.addi %47, %46 : tensor<32x1xi32>
        %49 = tt.broadcast %48 : tensor<32x1xi32> -> tensor<32x64xi32>
        %50 = tt.broadcast %9 : tensor<1x64xi32> -> tensor<32x64xi32>
        %51 = arith.addi %49, %50 : tensor<32x64xi32>
        %52 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<32x64x!tt.ptr<f8E5M2>>
        %53 = tt.addptr %52, %51 : tensor<32x64x!tt.ptr<f8E5M2>>, tensor<32x64xi32>
        %54 = tt.load %53 : tensor<32x64x!tt.ptr<f8E5M2>>
        %55 = tt.trans %54 {order = array<i32: 1, 0>} : tensor<32x64xf8E5M2> -> tensor<64x32xf8E5M2>
        %56 = tt.dot %15, %55, %cst_1, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x64xf8E5M2> * tensor<64x32xf8E5M2> -> tensor<64x32xf32>
        %57 = arith.mulf %56, %cst_0 : tensor<64x32xf32>
        %58 = "tt.reduce"(%57) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %89 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %89 : f32
        }) : (tensor<64x32xf32>) -> tensor<64xf32>
        %59 = arith.cmpf ogt, %arg10, %58 : tensor<64xf32>
        %60 = arith.cmpf une, %arg10, %arg10 : tensor<64xf32>
        %61 = arith.ori %59, %60 : tensor<64xi1>
        %62 = arith.select %61, %arg10, %58 : tensor<64xi1>, tensor<64xf32>
        %63 = tt.expand_dims %62 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %64 = tt.broadcast %63 : tensor<64x1xf32> -> tensor<64x32xf32>
        %65 = arith.subf %57, %64 : tensor<64x32xf32>
        %66 = tt.extern_elementwise %65 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64x32xf32>) -> tensor<64x32xf32>
        %67 = "tt.reduce"(%66) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %89 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %89 : f32
        }) : (tensor<64x32xf32>) -> tensor<64xf32>
        %68 = arith.subf %arg10, %62 : tensor<64xf32>
        %69 = tt.extern_elementwise %68 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64xf32>) -> tensor<64xf32>
        %70 = arith.mulf %arg8, %69 : tensor<64xf32>
        %71 = arith.addf %70, %67 : tensor<64xf32>
        %72 = tt.expand_dims %69 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %73 = tt.broadcast %72 : tensor<64x1xf32> -> tensor<64x64xf32>
        %74 = arith.mulf %arg9, %73 : tensor<64x64xf32>
        %75 = tt.expand_dims %1 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %76 = arith.addi %7, %75 : tensor<64x1xi32>
        %77 = tt.expand_dims %44 {axis = 0 : i32} : tensor<32xi32> -> tensor<1x32xi32>
        %78 = arith.muli %77, %cst : tensor<1x32xi32>
        %79 = tt.broadcast %76 : tensor<64x1xi32> -> tensor<64x32xi32>
        %80 = tt.broadcast %78 : tensor<1x32xi32> -> tensor<64x32xi32>
        %81 = arith.addi %79, %80 : tensor<64x32xi32>
        %82 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x32x!tt.ptr<f8E5M2>>
        %83 = tt.addptr %82, %81 : tensor<64x32x!tt.ptr<f8E5M2>>, tensor<64x32xi32>
        %84 = tt.load %83 : tensor<64x32x!tt.ptr<f8E5M2>>
        %85 = tt.fp_to_fp %66, rounding = rtne : tensor<64x32xf32> -> tensor<64x32xf8E5M2>
        %86 = tt.trans %84 {order = array<i32: 1, 0>} : tensor<64x32xf8E5M2> -> tensor<32x64xf8E5M2>
        %87 = tt.dot %85, %86, %cst_4, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x32xf8E5M2> * tensor<32x64xf8E5M2> -> tensor<64x64xf32>
        %88 = arith.addf %74, %87 : tensor<64x64xf32>
        scf.yield %71, %88, %62 : tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>
      } {tt.loop_unroll_factor = 1 : i32, tt.num_stages = 4 : i32}
      %17 = tt.expand_dims %16#0 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
      %18 = tt.broadcast %17 : tensor<64x1xf32> -> tensor<64x64xf32>
      %19 = arith.divf %16#1, %18 : tensor<64x64xf32>
      %20 = tt.fp_to_fp %19, rounding = rtne : tensor<64x64xf32> -> tensor<64x64xf8E5M2>
      %21 = arith.divsi %0, %arg5 : i32
      %22 = arith.remsi %0, %arg5 : i32
      %23 = arith.cmpi ne, %22, %c0_i32 : i32
      %24 = arith.subi %21, %c1_i32 : i32
      %25 = arith.select %23, %24, %21 : i32
      %26 = arith.cmpi slt, %0, %c0_i32 : i32
      %27 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %28 = arith.cmpi ne, %26, %27 : i1
      %29 = arith.select %28, %25, %21 : i32
      %30 = arith.andi %23, %28 : i1
      %31 = arith.addi %22, %arg5 : i32
      %32 = arith.select %30, %31, %22 : i32
      %33 = arith.muli %29, %arg4 : i32
      %34 = arith.muli %32, %c8192_i32 : i32
      %35 = arith.addi %33, %34 : i32
      %36 = tt.splat %35 : i32 -> tensor<64x1xi32>
      %37 = arith.addi %36, %6 : tensor<64x1xi32>
      %38 = tt.broadcast %37 : tensor<64x1xi32> -> tensor<64x64xi32>
      %39 = arith.addi %38, %11 : tensor<64x64xi32>
      %40 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %41 = tt.addptr %40, %39 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      tt.store %41, %20 : tensor<64x64x!tt.ptr<f8E5M2>>
    } {tt.disallow_acc_multi_buffer, tt.flatten, tt.num_stages = 1 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(convert-triton-to-tritongpu{enable-source-remat=false num-ctas=1 num-warps=4 target=cuda:90 threads-per-warp=32}, tritongpu-coalesce, tritongpu-F32DotTC, triton-nvidia-gpu-plan-cta, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritongpu-accelerate-matmul, tritongpu-remove-layout-conversions, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, triton-nvidia-optimize-descriptor-encoding, triton-loop-aware-cse, tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-combine-tensor-select-and-if, nvgpu-warp-specialization{dump-intermediate-steps=false num-stages=5}, tritongpu-assign-latencies{num-stages=5}, tritongpu-schedule-loops, tritongpu-pipeline{dump-intermediate-steps=false num-stages=5}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-loop-aware-cse, tritongpu-prefetch, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-coalesce-async-copy, triton-nvidia-optimize-tmem-layouts, tritongpu-remove-layout-conversions, triton-nvidia-interleave-tmem, tritongpu-reduce-data-duplication, tritongpu-reorder-instructions, triton-loop-aware-cse, symbol-dce, triton-nvidia-tma-lowering, triton-nvidia-gpu-fence-insertion{compute-capability=90}, sccp, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true})",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_willfeng/a2/ca2hlmfmixr3wzu7oovg3m6nlh2l6e7lqbx4y5sfmwcf4kzcylhp.py:12:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_willfeng/a2/ca2hlmfmixr3wzu7oovg3m6nlh2l6e7lqbx4y5sfmwcf4kzcylhp.py:12:0: note: Pipeline failed while executing [`TritonGPUAssignLatencies` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
Process ForkProcess-637:
Traceback (most recent call last):
  File "/home/willfeng/local/miniconda3/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/willfeng/local/miniconda3/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/data/users/willfeng/helion/helion/runtime/precompile_shim.py", line 54, in finish_it
    kernel_cache[key] = fn.compile(
                        ^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/compiler/compiler.py", line 359, in compile
    next_module = compile_ir(module, metadata)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/backends/nvidia/compiler.py", line 456, in <lambda>
    stages["ttgir"] = lambda src, metadata: self.make_ttgir(src, metadata, options, capability)
                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/backends/nvidia/compiler.py", line 298, in make_ttgir
    pm.run(mod)
RuntimeError: PassManager::run failed
python: /root/.triton/llvm/llvm-8957e64a-almalinux-x64/include/llvm/ADT/SmallVector.h:296: const_reference llvm::SmallVectorTemplateCommon<long>::operator[](size_type) const [T = long]: Assertion `idx < size()' failed.
module {
  tt.func public @_fp8_attention_kernel_kernel(%arg0: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %c1_i32 = arith.constant 1 : i32
    %c32_i32 = arith.constant 32 : i32
    %c64_i32 = arith.constant 64 : i32
    %c128_i32 = arith.constant 128 : i32
    %c0_i32 = arith.constant 0 : i32
    %cst = arith.constant dense<64> : tensor<1x32xi32>
    %cst_0 = arith.constant dense<0.180336878> : tensor<64x32xf32>
    %cst_1 = arith.constant dense<0.000000e+00> : tensor<64x32xf32>
    %cst_2 = arith.constant dense<64> : tensor<32x1xi32>
    %cst_3 = arith.constant dense<64> : tensor<64x1xi32>
    %c8192_i32 = arith.constant 8192 : i32
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<64x64xf32>
    %cst_5 = arith.constant dense<0.000000e+00> : tensor<64xf32>
    %cst_6 = arith.constant dense<0xFF800000> : tensor<64xf32>
    %0 = tt.get_program_id x : i32
    %1 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
    scf.for %arg6 = %c0_i32 to %c128_i32 step %c64_i32  : i32 {
      %2 = tt.splat %arg6 : i32 -> tensor<64xi32>
      %3 = arith.addi %2, %1 : tensor<64xi32>
      %4 = arith.muli %0, %c8192_i32 : i32
      %5 = tt.expand_dims %3 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
      %6 = arith.muli %5, %cst_3 : tensor<64x1xi32>
      %7 = tt.splat %4 : i32 -> tensor<64x1xi32>
      %8 = arith.addi %7, %6 : tensor<64x1xi32>
      %9 = tt.expand_dims %1 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %10 = tt.broadcast %8 : tensor<64x1xi32> -> tensor<64x64xi32>
      %11 = tt.broadcast %9 : tensor<1x64xi32> -> tensor<64x64xi32>
      %12 = arith.addi %10, %11 : tensor<64x64xi32>
      %13 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %14 = tt.addptr %13, %12 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      %15 = tt.load %14 : tensor<64x64x!tt.ptr<f8E5M2>>
      %16:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c32_i32 iter_args(%arg8 = %cst_5, %arg9 = %cst_4, %arg10 = %cst_6) -> (tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>)  : i32 {
        %42 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32>
        %43 = tt.splat %arg7 : i32 -> tensor<32xi32>
        %44 = arith.addi %43, %42 : tensor<32xi32>
        %45 = tt.expand_dims %44 {axis = 1 : i32} : tensor<32xi32> -> tensor<32x1xi32>
        %46 = arith.muli %45, %cst_2 : tensor<32x1xi32>
        %47 = tt.splat %4 : i32 -> tensor<32x1xi32>
        %48 = arith.addi %47, %46 : tensor<32x1xi32>
        %49 = tt.broadcast %48 : tensor<32x1xi32> -> tensor<32x64xi32>
        %50 = tt.broadcast %9 : tensor<1x64xi32> -> tensor<32x64xi32>
        %51 = arith.addi %49, %50 : tensor<32x64xi32>
        %52 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<32x64x!tt.ptr<f8E5M2>>
        %53 = tt.addptr %52, %51 : tensor<32x64x!tt.ptr<f8E5M2>>, tensor<32x64xi32>
        %54 = tt.load %53 : tensor<32x64x!tt.ptr<f8E5M2>>
        %55 = tt.trans %54 {order = array<i32: 1, 0>} : tensor<32x64xf8E5M2> -> tensor<64x32xf8E5M2>
        %56 = tt.dot %15, %55, %cst_1, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x64xf8E5M2> * tensor<64x32xf8E5M2> -> tensor<64x32xf32>
        %57 = arith.mulf %56, %cst_0 : tensor<64x32xf32>
        %58 = "tt.reduce"(%57) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %89 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %89 : f32
        }) : (tensor<64x32xf32>) -> tensor<64xf32>
        %59 = arith.cmpf ogt, %arg10, %58 : tensor<64xf32>
        %60 = arith.cmpf une, %arg10, %arg10 : tensor<64xf32>
        %61 = arith.ori %59, %60 : tensor<64xi1>
        %62 = arith.select %61, %arg10, %58 : tensor<64xi1>, tensor<64xf32>
        %63 = tt.expand_dims %62 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %64 = tt.broadcast %63 : tensor<64x1xf32> -> tensor<64x32xf32>
        %65 = arith.subf %57, %64 : tensor<64x32xf32>
        %66 = tt.extern_elementwise %65 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64x32xf32>) -> tensor<64x32xf32>
        %67 = "tt.reduce"(%66) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %89 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %89 : f32
        }) : (tensor<64x32xf32>) -> tensor<64xf32>
        %68 = arith.subf %arg10, %62 : tensor<64xf32>
        %69 = tt.extern_elementwise %68 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64xf32>) -> tensor<64xf32>
        %70 = arith.mulf %arg8, %69 : tensor<64xf32>
        %71 = arith.addf %70, %67 : tensor<64xf32>
        %72 = tt.expand_dims %69 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %73 = tt.broadcast %72 : tensor<64x1xf32> -> tensor<64x64xf32>
        %74 = arith.mulf %arg9, %73 : tensor<64x64xf32>
        %75 = tt.expand_dims %1 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %76 = arith.addi %7, %75 : tensor<64x1xi32>
        %77 = tt.expand_dims %44 {axis = 0 : i32} : tensor<32xi32> -> tensor<1x32xi32>
        %78 = arith.muli %77, %cst : tensor<1x32xi32>
        %79 = tt.broadcast %76 : tensor<64x1xi32> -> tensor<64x32xi32>
        %80 = tt.broadcast %78 : tensor<1x32xi32> -> tensor<64x32xi32>
        %81 = arith.addi %79, %80 : tensor<64x32xi32>
        %82 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x32x!tt.ptr<f8E5M2>>
        %83 = tt.addptr %82, %81 : tensor<64x32x!tt.ptr<f8E5M2>>, tensor<64x32xi32>
        %84 = tt.load %83 : tensor<64x32x!tt.ptr<f8E5M2>>
        %85 = tt.fp_to_fp %66, rounding = rtne : tensor<64x32xf32> -> tensor<64x32xf8E5M2>
        %86 = tt.trans %84 {order = array<i32: 1, 0>} : tensor<64x32xf8E5M2> -> tensor<32x64xf8E5M2>
        %87 = tt.dot %85, %86, %cst_4, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x32xf8E5M2> * tensor<32x64xf8E5M2> -> tensor<64x64xf32>
        %88 = arith.addf %74, %87 : tensor<64x64xf32>
        scf.yield %71, %88, %62 : tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>
      } {tt.loop_unroll_factor = 1 : i32, tt.num_stages = 4 : i32}
      %17 = tt.expand_dims %16#0 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
      %18 = tt.broadcast %17 : tensor<64x1xf32> -> tensor<64x64xf32>
      %19 = arith.divf %16#1, %18 : tensor<64x64xf32>
      %20 = tt.fp_to_fp %19, rounding = rtne : tensor<64x64xf32> -> tensor<64x64xf8E5M2>
      %21 = arith.divsi %0, %arg5 : i32
      %22 = arith.remsi %0, %arg5 : i32
      %23 = arith.cmpi ne, %22, %c0_i32 : i32
      %24 = arith.subi %21, %c1_i32 : i32
      %25 = arith.select %23, %24, %21 : i32
      %26 = arith.cmpi slt, %0, %c0_i32 : i32
      %27 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %28 = arith.cmpi ne, %26, %27 : i1
      %29 = arith.select %28, %25, %21 : i32
      %30 = arith.andi %23, %28 : i1
      %31 = arith.addi %22, %arg5 : i32
      %32 = arith.select %30, %31, %22 : i32
      %33 = arith.muli %29, %arg4 : i32
      %34 = arith.muli %32, %c8192_i32 : i32
      %35 = arith.addi %33, %34 : i32
      %36 = tt.splat %35 : i32 -> tensor<64x1xi32>
      %37 = arith.addi %36, %6 : tensor<64x1xi32>
      %38 = tt.broadcast %37 : tensor<64x1xi32> -> tensor<64x64xi32>
      %39 = arith.addi %38, %11 : tensor<64x64xi32>
      %40 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %41 = tt.addptr %40, %39 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      tt.store %41, %20 : tensor<64x64x!tt.ptr<f8E5M2>>
    } {tt.disallow_acc_multi_buffer, tt.flatten, tt.num_stages = 1 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(convert-triton-to-tritongpu{enable-source-remat=false num-ctas=1 num-warps=4 target=cuda:90 threads-per-warp=32}, tritongpu-coalesce, tritongpu-F32DotTC, triton-nvidia-gpu-plan-cta, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritongpu-accelerate-matmul, tritongpu-remove-layout-conversions, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, triton-nvidia-optimize-descriptor-encoding, triton-loop-aware-cse, tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-combine-tensor-select-and-if, nvgpu-warp-specialization{dump-intermediate-steps=false num-stages=5}, tritongpu-assign-latencies{num-stages=5}, tritongpu-schedule-loops, tritongpu-pipeline{dump-intermediate-steps=false num-stages=5}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-loop-aware-cse, tritongpu-prefetch, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-coalesce-async-copy, triton-nvidia-optimize-tmem-layouts, tritongpu-remove-layout-conversions, triton-nvidia-interleave-tmem, tritongpu-reduce-data-duplication, tritongpu-reorder-instructions, triton-loop-aware-cse, symbol-dce, triton-nvidia-tma-lowering, triton-nvidia-gpu-fence-insertion{compute-capability=90}, sccp, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true})",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_willfeng/a2/ca2hlmfmixr3wzu7oovg3m6nlh2l6e7lqbx4y5sfmwcf4kzcylhp.py:12:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_willfeng/a2/ca2hlmfmixr3wzu7oovg3m6nlh2l6e7lqbx4y5sfmwcf4kzcylhp.py:12:0: note: Pipeline failed while executing [`TritonGPUAssignLatencies` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
[902s] Generation 8: replaced=11 min=0.0113 mid=0.0138 max=0.0236 best=Config(block_sizes=[128, 128], range_unroll_factors=[0, 2, 2], range_num_stages=[0, 4, 2], range_multi_buffers=[None, False, True], range_flattens=[None, False, False], num_warps=8, num_stages=7, indexing='tensor_descriptor', pid_type='flat', range_warp_specializes=[])
python: /root/.triton/llvm/llvm-8957e64a-almalinux-x64/include/llvm/ADT/SmallVector.h:296: const_reference llvm::SmallVectorTemplateCommon<long>::operator[](size_type) const [T = long]: Assertion `idx < size()' failed.
module {
  tt.func public @_fp8_attention_kernel_kernel(%arg0: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %c1_i32 = arith.constant 1 : i32
    %c32_i32 = arith.constant 32 : i32
    %c128_i32 = arith.constant 128 : i32
    %c0_i32 = arith.constant 0 : i32
    %c132_i32 = arith.constant 132 : i32
    %c192_i32 = arith.constant 192 : i32
    %cst = arith.constant dense<64> : tensor<1x32xi32>
    %cst_0 = arith.constant dense<0.180336878> : tensor<128x32xf32>
    %cst_1 = arith.constant dense<0.000000e+00> : tensor<128x32xf32>
    %cst_2 = arith.constant dense<64> : tensor<32x1xi32>
    %cst_3 = arith.constant dense<64> : tensor<128x1xi32>
    %c8192_i32 = arith.constant 8192 : i32
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<128x64xf32>
    %cst_5 = arith.constant dense<0.000000e+00> : tensor<128xf32>
    %cst_6 = arith.constant dense<0xFF800000> : tensor<128xf32>
    %0 = tt.get_program_id x : i32
    scf.for %arg6 = %0 to %c192_i32 step %c132_i32  : i32 {
      %1 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
      %2 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>
      %3 = arith.muli %arg6, %c8192_i32 : i32
      %4 = tt.expand_dims %2 {axis = 1 : i32} : tensor<128xi32> -> tensor<128x1xi32>
      %5 = arith.muli %4, %cst_3 : tensor<128x1xi32>
      %6 = tt.splat %3 : i32 -> tensor<128x1xi32>
      %7 = arith.addi %6, %5 : tensor<128x1xi32>
      %8 = tt.expand_dims %1 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %9 = tt.broadcast %7 : tensor<128x1xi32> -> tensor<128x64xi32>
      %10 = tt.broadcast %8 : tensor<1x64xi32> -> tensor<128x64xi32>
      %11 = arith.addi %9, %10 : tensor<128x64xi32>
      %12 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %13 = tt.addptr %12, %11 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      %14 = tt.load %13 : tensor<128x64x!tt.ptr<f8E5M2>>
      %15:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c32_i32 iter_args(%arg8 = %cst_5, %arg9 = %cst_4, %arg10 = %cst_6) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %41 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32>
        %42 = tt.splat %arg7 : i32 -> tensor<32xi32>
        %43 = arith.addi %42, %41 : tensor<32xi32>
        %44 = tt.expand_dims %43 {axis = 1 : i32} : tensor<32xi32> -> tensor<32x1xi32>
        %45 = arith.muli %44, %cst_2 : tensor<32x1xi32>
        %46 = tt.splat %3 : i32 -> tensor<32x1xi32>
        %47 = arith.addi %46, %45 : tensor<32x1xi32>
        %48 = tt.broadcast %47 : tensor<32x1xi32> -> tensor<32x64xi32>
        %49 = tt.broadcast %8 : tensor<1x64xi32> -> tensor<32x64xi32>
        %50 = arith.addi %48, %49 : tensor<32x64xi32>
        %51 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<32x64x!tt.ptr<f8E5M2>>
        %52 = tt.addptr %51, %50 : tensor<32x64x!tt.ptr<f8E5M2>>, tensor<32x64xi32>
        %53 = tt.load %52 : tensor<32x64x!tt.ptr<f8E5M2>>
        %54 = tt.trans %53 {order = array<i32: 1, 0>} : tensor<32x64xf8E5M2> -> tensor<64x32xf8E5M2>
        %55 = tt.dot %14, %54, %cst_1, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x32xf8E5M2> -> tensor<128x32xf32>
        %56 = arith.mulf %55, %cst_0 : tensor<128x32xf32>
        %57 = "tt.reduce"(%56) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %89 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %89 : f32
        }) : (tensor<128x32xf32>) -> tensor<128xf32>
        %58 = arith.cmpf ogt, %arg10, %57 : tensor<128xf32>
        %59 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %60 = arith.ori %58, %59 : tensor<128xi1>
        %61 = arith.select %60, %arg10, %57 : tensor<128xi1>, tensor<128xf32>
        %62 = tt.expand_dims %61 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %63 = tt.broadcast %62 : tensor<128x1xf32> -> tensor<128x32xf32>
        %64 = arith.subf %56, %63 : tensor<128x32xf32>
        %65 = tt.extern_elementwise %64 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x32xf32>) -> tensor<128x32xf32>
        %66 = "tt.reduce"(%65) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %89 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %89 : f32
        }) : (tensor<128x32xf32>) -> tensor<128xf32>
        %67 = arith.subf %arg10, %61 : tensor<128xf32>
        %68 = tt.extern_elementwise %67 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %69 = arith.mulf %arg8, %68 : tensor<128xf32>
        %70 = arith.addf %69, %66 : tensor<128xf32>
        %71 = tt.expand_dims %68 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %72 = tt.broadcast %71 : tensor<128x1xf32> -> tensor<128x64xf32>
        %73 = arith.mulf %arg9, %72 : tensor<128x64xf32>
        %74 = tt.expand_dims %1 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %75 = tt.splat %3 : i32 -> tensor<64x1xi32>
        %76 = arith.addi %75, %74 : tensor<64x1xi32>
        %77 = tt.expand_dims %43 {axis = 0 : i32} : tensor<32xi32> -> tensor<1x32xi32>
        %78 = arith.muli %77, %cst : tensor<1x32xi32>
        %79 = tt.broadcast %76 : tensor<64x1xi32> -> tensor<64x32xi32>
        %80 = tt.broadcast %78 : tensor<1x32xi32> -> tensor<64x32xi32>
        %81 = arith.addi %79, %80 : tensor<64x32xi32>
        %82 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x32x!tt.ptr<f8E5M2>>
        %83 = tt.addptr %82, %81 : tensor<64x32x!tt.ptr<f8E5M2>>, tensor<64x32xi32>
        %84 = tt.load %83 : tensor<64x32x!tt.ptr<f8E5M2>>
        %85 = tt.fp_to_fp %65, rounding = rtne : tensor<128x32xf32> -> tensor<128x32xf8E5M2>
        %86 = tt.trans %84 {order = array<i32: 1, 0>} : tensor<64x32xf8E5M2> -> tensor<32x64xf8E5M2>
        %87 = tt.dot %85, %86, %cst_4, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x32xf8E5M2> * tensor<32x64xf8E5M2> -> tensor<128x64xf32>
        %88 = arith.addf %73, %87 : tensor<128x64xf32>
        scf.yield %70, %88, %61 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.flatten, tt.loop_unroll_factor = 1 : i32}
      %16 = tt.expand_dims %15#0 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
      %17 = tt.broadcast %16 : tensor<128x1xf32> -> tensor<128x64xf32>
      %18 = arith.divf %15#1, %17 : tensor<128x64xf32>
      %19 = tt.fp_to_fp %18, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
      %20 = arith.divsi %arg6, %arg5 : i32
      %21 = arith.remsi %arg6, %arg5 : i32
      %22 = arith.cmpi ne, %21, %c0_i32 : i32
      %23 = arith.subi %20, %c1_i32 : i32
      %24 = arith.select %22, %23, %20 : i32
      %25 = arith.cmpi slt, %arg6, %c0_i32 : i32
      %26 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %27 = arith.cmpi ne, %25, %26 : i1
      %28 = arith.select %27, %24, %20 : i32
      %29 = arith.andi %22, %27 : i1
      %30 = arith.addi %21, %arg5 : i32
      %31 = arith.select %29, %30, %21 : i32
      %32 = arith.muli %28, %arg4 : i32
      %33 = arith.muli %31, %c8192_i32 : i32
      %34 = arith.addi %32, %33 : i32
      %35 = tt.splat %34 : i32 -> tensor<128x1xi32>
      %36 = arith.addi %35, %5 : tensor<128x1xi32>
      %37 = tt.broadcast %36 : tensor<128x1xi32> -> tensor<128x64xi32>
      %38 = arith.addi %37, %10 : tensor<128x64xi32>
      %39 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %40 = tt.addptr %39, %38 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      tt.store %40, %19 : tensor<128x64x!tt.ptr<f8E5M2>>
    } {tt.disallow_acc_multi_buffer, tt.flatten, tt.loop_unroll_factor = 1 : i32, tt.num_stages = 4 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(convert-triton-to-tritongpu{enable-source-remat=false num-ctas=1 num-warps=4 target=cuda:90 threads-per-warp=32}, tritongpu-coalesce, tritongpu-F32DotTC, triton-nvidia-gpu-plan-cta, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritongpu-accelerate-matmul, tritongpu-remove-layout-conversions, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, triton-nvidia-optimize-descriptor-encoding, triton-loop-aware-cse, tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-combine-tensor-select-and-if, nvgpu-warp-specialization{dump-intermediate-steps=false num-stages=1}, tritongpu-assign-latencies{num-stages=1}, tritongpu-schedule-loops, tritongpu-pipeline{dump-intermediate-steps=false num-stages=1}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-loop-aware-cse, tritongpu-prefetch, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-coalesce-async-copy, triton-nvidia-optimize-tmem-layouts, tritongpu-remove-layout-conversions, triton-nvidia-interleave-tmem, tritongpu-reduce-data-duplication, tritongpu-reorder-instructions, triton-loop-aware-cse, symbol-dce, triton-nvidia-tma-lowering, triton-nvidia-gpu-fence-insertion{compute-capability=90}, sccp, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true})",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_willfeng/xd/cxdk6bvdeqfj2cgv6ksk2mz7kqwhbimdkilstg7z66kqeeowvaen.py:13:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_willfeng/xd/cxdk6bvdeqfj2cgv6ksk2mz7kqwhbimdkilstg7z66kqeeowvaen.py:13:0: note: Pipeline failed while executing [`TritonGPUAssignLatencies` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
Process ForkProcess-670:
Traceback (most recent call last):
  File "/home/willfeng/local/miniconda3/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/willfeng/local/miniconda3/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/data/users/willfeng/helion/helion/runtime/precompile_shim.py", line 54, in finish_it
    kernel_cache[key] = fn.compile(
                        ^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/compiler/compiler.py", line 359, in compile
    next_module = compile_ir(module, metadata)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/backends/nvidia/compiler.py", line 456, in <lambda>
    stages["ttgir"] = lambda src, metadata: self.make_ttgir(src, metadata, options, capability)
                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/backends/nvidia/compiler.py", line 298, in make_ttgir
    pm.run(mod)
RuntimeError: PassManager::run failed
python: /root/.triton/llvm/llvm-8957e64a-almalinux-x64/include/llvm/ADT/SmallVector.h:296: const_reference llvm::SmallVectorTemplateCommon<long>::operator[](size_type) const [T = long]: Assertion `idx < size()' failed.
module {
  tt.func public @_fp8_attention_kernel_kernel(%arg0: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %c1_i32 = arith.constant 1 : i32
    %c32_i32 = arith.constant 32 : i32
    %c128_i32 = arith.constant 128 : i32
    %c0_i32 = arith.constant 0 : i32
    %c132_i32 = arith.constant 132 : i32
    %c192_i32 = arith.constant 192 : i32
    %cst = arith.constant dense<64> : tensor<1x32xi32>
    %cst_0 = arith.constant dense<0.180336878> : tensor<128x32xf32>
    %cst_1 = arith.constant dense<0.000000e+00> : tensor<128x32xf32>
    %cst_2 = arith.constant dense<64> : tensor<32x1xi32>
    %cst_3 = arith.constant dense<64> : tensor<128x1xi32>
    %c8192_i32 = arith.constant 8192 : i32
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<128x64xf32>
    %cst_5 = arith.constant dense<0.000000e+00> : tensor<128xf32>
    %cst_6 = arith.constant dense<0xFF800000> : tensor<128xf32>
    %0 = tt.get_program_id x : i32
    scf.for %arg6 = %0 to %c192_i32 step %c132_i32  : i32 {
      %1 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
      %2 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>
      %3 = arith.muli %arg6, %c8192_i32 : i32
      %4 = tt.expand_dims %2 {axis = 1 : i32} : tensor<128xi32> -> tensor<128x1xi32>
      %5 = arith.muli %4, %cst_3 : tensor<128x1xi32>
      %6 = tt.splat %3 : i32 -> tensor<128x1xi32>
      %7 = arith.addi %6, %5 : tensor<128x1xi32>
      %8 = tt.expand_dims %1 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %9 = tt.broadcast %7 : tensor<128x1xi32> -> tensor<128x64xi32>
      %10 = tt.broadcast %8 : tensor<1x64xi32> -> tensor<128x64xi32>
      %11 = arith.addi %9, %10 : tensor<128x64xi32>
      %12 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %13 = tt.addptr %12, %11 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      %14 = tt.load %13 : tensor<128x64x!tt.ptr<f8E5M2>>
      %15:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c32_i32 iter_args(%arg8 = %cst_5, %arg9 = %cst_4, %arg10 = %cst_6) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %41 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32>
        %42 = tt.splat %arg7 : i32 -> tensor<32xi32>
        %43 = arith.addi %42, %41 : tensor<32xi32>
        %44 = tt.expand_dims %43 {axis = 1 : i32} : tensor<32xi32> -> tensor<32x1xi32>
        %45 = arith.muli %44, %cst_2 : tensor<32x1xi32>
        %46 = tt.splat %3 : i32 -> tensor<32x1xi32>
        %47 = arith.addi %46, %45 : tensor<32x1xi32>
        %48 = tt.broadcast %47 : tensor<32x1xi32> -> tensor<32x64xi32>
        %49 = tt.broadcast %8 : tensor<1x64xi32> -> tensor<32x64xi32>
        %50 = arith.addi %48, %49 : tensor<32x64xi32>
        %51 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<32x64x!tt.ptr<f8E5M2>>
        %52 = tt.addptr %51, %50 : tensor<32x64x!tt.ptr<f8E5M2>>, tensor<32x64xi32>
        %53 = tt.load %52 : tensor<32x64x!tt.ptr<f8E5M2>>
        %54 = tt.trans %53 {order = array<i32: 1, 0>} : tensor<32x64xf8E5M2> -> tensor<64x32xf8E5M2>
        %55 = tt.dot %14, %54, %cst_1, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x32xf8E5M2> -> tensor<128x32xf32>
        %56 = arith.mulf %55, %cst_0 : tensor<128x32xf32>
        %57 = "tt.reduce"(%56) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %89 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %89 : f32
        }) : (tensor<128x32xf32>) -> tensor<128xf32>
        %58 = arith.cmpf ogt, %arg10, %57 : tensor<128xf32>
        %59 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %60 = arith.ori %58, %59 : tensor<128xi1>
        %61 = arith.select %60, %arg10, %57 : tensor<128xi1>, tensor<128xf32>
        %62 = tt.expand_dims %61 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %63 = tt.broadcast %62 : tensor<128x1xf32> -> tensor<128x32xf32>
        %64 = arith.subf %56, %63 : tensor<128x32xf32>
        %65 = tt.extern_elementwise %64 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x32xf32>) -> tensor<128x32xf32>
        %66 = "tt.reduce"(%65) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %89 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %89 : f32
        }) : (tensor<128x32xf32>) -> tensor<128xf32>
        %67 = arith.subf %arg10, %61 : tensor<128xf32>
        %68 = tt.extern_elementwise %67 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %69 = arith.mulf %arg8, %68 : tensor<128xf32>
        %70 = arith.addf %69, %66 : tensor<128xf32>
        %71 = tt.expand_dims %68 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %72 = tt.broadcast %71 : tensor<128x1xf32> -> tensor<128x64xf32>
        %73 = arith.mulf %arg9, %72 : tensor<128x64xf32>
        %74 = tt.expand_dims %1 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %75 = tt.splat %3 : i32 -> tensor<64x1xi32>
        %76 = arith.addi %75, %74 : tensor<64x1xi32>
        %77 = tt.expand_dims %43 {axis = 0 : i32} : tensor<32xi32> -> tensor<1x32xi32>
        %78 = arith.muli %77, %cst : tensor<1x32xi32>
        %79 = tt.broadcast %76 : tensor<64x1xi32> -> tensor<64x32xi32>
        %80 = tt.broadcast %78 : tensor<1x32xi32> -> tensor<64x32xi32>
        %81 = arith.addi %79, %80 : tensor<64x32xi32>
        %82 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x32x!tt.ptr<f8E5M2>>
        %83 = tt.addptr %82, %81 : tensor<64x32x!tt.ptr<f8E5M2>>, tensor<64x32xi32>
        %84 = tt.load %83 : tensor<64x32x!tt.ptr<f8E5M2>>
        %85 = tt.fp_to_fp %65, rounding = rtne : tensor<128x32xf32> -> tensor<128x32xf8E5M2>
        %86 = tt.trans %84 {order = array<i32: 1, 0>} : tensor<64x32xf8E5M2> -> tensor<32x64xf8E5M2>
        %87 = tt.dot %85, %86, %cst_4, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x32xf8E5M2> * tensor<32x64xf8E5M2> -> tensor<128x64xf32>
        %88 = arith.addf %73, %87 : tensor<128x64xf32>
        scf.yield %70, %88, %61 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.flatten, tt.loop_unroll_factor = 1 : i32}
      %16 = tt.expand_dims %15#0 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
      %17 = tt.broadcast %16 : tensor<128x1xf32> -> tensor<128x64xf32>
      %18 = arith.divf %15#1, %17 : tensor<128x64xf32>
      %19 = tt.fp_to_fp %18, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
      %20 = arith.divsi %arg6, %arg5 : i32
      %21 = arith.remsi %arg6, %arg5 : i32
      %22 = arith.cmpi ne, %21, %c0_i32 : i32
      %23 = arith.subi %20, %c1_i32 : i32
      %24 = arith.select %22, %23, %20 : i32
      %25 = arith.cmpi slt, %arg6, %c0_i32 : i32
      %26 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %27 = arith.cmpi ne, %25, %26 : i1
      %28 = arith.select %27, %24, %20 : i32
      %29 = arith.andi %22, %27 : i1
      %30 = arith.addi %21, %arg5 : i32
      %31 = arith.select %29, %30, %21 : i32
      %32 = arith.muli %28, %arg4 : i32
      %33 = arith.muli %31, %c8192_i32 : i32
      %34 = arith.addi %32, %33 : i32
      %35 = tt.splat %34 : i32 -> tensor<128x1xi32>
      %36 = arith.addi %35, %5 : tensor<128x1xi32>
      %37 = tt.broadcast %36 : tensor<128x1xi32> -> tensor<128x64xi32>
      %38 = arith.addi %37, %10 : tensor<128x64xi32>
      %39 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %40 = tt.addptr %39, %38 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      tt.store %40, %19 : tensor<128x64x!tt.ptr<f8E5M2>>
    } {tt.disallow_acc_multi_buffer, tt.flatten, tt.loop_unroll_factor = 1 : i32, tt.num_stages = 4 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(convert-triton-to-tritongpu{enable-source-remat=false num-ctas=1 num-warps=4 target=cuda:90 threads-per-warp=32}, tritongpu-coalesce, tritongpu-F32DotTC, triton-nvidia-gpu-plan-cta, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritongpu-accelerate-matmul, tritongpu-remove-layout-conversions, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, triton-nvidia-optimize-descriptor-encoding, triton-loop-aware-cse, tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-combine-tensor-select-and-if, nvgpu-warp-specialization{dump-intermediate-steps=false num-stages=1}, tritongpu-assign-latencies{num-stages=1}, tritongpu-schedule-loops, tritongpu-pipeline{dump-intermediate-steps=false num-stages=1}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-loop-aware-cse, tritongpu-prefetch, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-coalesce-async-copy, triton-nvidia-optimize-tmem-layouts, tritongpu-remove-layout-conversions, triton-nvidia-interleave-tmem, tritongpu-reduce-data-duplication, tritongpu-reorder-instructions, triton-loop-aware-cse, symbol-dce, triton-nvidia-tma-lowering, triton-nvidia-gpu-fence-insertion{compute-capability=90}, sccp, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true})",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_willfeng/xd/cxdk6bvdeqfj2cgv6ksk2mz7kqwhbimdkilstg7z66kqeeowvaen.py:13:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_willfeng/xd/cxdk6bvdeqfj2cgv6ksk2mz7kqwhbimdkilstg7z66kqeeowvaen.py:13:0: note: Pipeline failed while executing [`TritonGPUAssignLatencies` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
python: /root/.triton/llvm/llvm-8957e64a-almalinux-x64/include/llvm/ADT/SmallVector.h:296: const_reference llvm::SmallVectorTemplateCommon<long>::operator[](size_type) const [T = long]: Assertion `idx < size()' failed.
module {
  tt.func public @_fp8_attention_kernel_kernel(%arg0: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %c1_i32 = arith.constant 1 : i32
    %c64_i32 = arith.constant 64 : i32
    %c128_i32 = arith.constant 128 : i32
    %c0_i32 = arith.constant 0 : i32
    %c132_i32 = arith.constant 132 : i32
    %c192_i32 = arith.constant 192 : i32
    %cst = arith.constant dense<64> : tensor<1x64xi32>
    %cst_0 = arith.constant dense<0.180336878> : tensor<128x64xf32>
    %cst_1 = arith.constant dense<64> : tensor<64x1xi32>
    %cst_2 = arith.constant dense<64> : tensor<128x1xi32>
    %c8192_i32 = arith.constant 8192 : i32
    %cst_3 = arith.constant dense<0.000000e+00> : tensor<128x64xf32>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<128xf32>
    %cst_5 = arith.constant dense<0xFF800000> : tensor<128xf32>
    %0 = tt.get_program_id x : i32
    scf.for %arg6 = %0 to %c192_i32 step %c132_i32  : i32 {
      %1 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
      %2 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>
      %3 = arith.muli %arg6, %c8192_i32 : i32
      %4 = tt.expand_dims %2 {axis = 1 : i32} : tensor<128xi32> -> tensor<128x1xi32>
      %5 = arith.muli %4, %cst_2 : tensor<128x1xi32>
      %6 = tt.splat %3 : i32 -> tensor<128x1xi32>
      %7 = arith.addi %6, %5 : tensor<128x1xi32>
      %8 = tt.expand_dims %1 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %9 = tt.broadcast %7 : tensor<128x1xi32> -> tensor<128x64xi32>
      %10 = tt.broadcast %8 : tensor<1x64xi32> -> tensor<128x64xi32>
      %11 = arith.addi %9, %10 : tensor<128x64xi32>
      %12 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %13 = tt.addptr %12, %11 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      %14 = tt.load %13 : tensor<128x64x!tt.ptr<f8E5M2>>
      %c0_i32_6 = arith.constant 0 : i32
      %c256_i32 = arith.constant 256 : i32
      %15:3 = scf.for %arg7 = %c0_i32 to %c0_i32_6 step %c256_i32 iter_args(%arg8 = %cst_4, %arg9 = %cst_3, %arg10 = %cst_5) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %42 = tt.splat %arg7 : i32 -> tensor<64xi32>
        %43 = arith.addi %42, %1 : tensor<64xi32>
        %44 = tt.expand_dims %43 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %45 = arith.muli %44, %cst_1 : tensor<64x1xi32>
        %46 = tt.splat %3 : i32 -> tensor<64x1xi32>
        %47 = arith.addi %46, %45 : tensor<64x1xi32>
        %48 = tt.broadcast %47 : tensor<64x1xi32> -> tensor<64x64xi32>
        %49 = tt.broadcast %8 : tensor<1x64xi32> -> tensor<64x64xi32>
        %50 = arith.addi %48, %49 : tensor<64x64xi32>
        %51 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %52 = tt.addptr %51, %50 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %53 = tt.load %52 : tensor<64x64x!tt.ptr<f8E5M2>>
        %54 = tt.trans %53 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %55 = tt.dot %14, %54, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %56 = arith.mulf %55, %cst_0 : tensor<128x64xf32>
        %57 = "tt.reduce"(%56) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %232 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %232 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %58 = arith.cmpf ogt, %arg10, %57 : tensor<128xf32>
        %59 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %60 = arith.ori %58, %59 : tensor<128xi1>
        %61 = arith.select %60, %arg10, %57 : tensor<128xi1>, tensor<128xf32>
        %62 = tt.expand_dims %61 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %63 = tt.broadcast %62 : tensor<128x1xf32> -> tensor<128x64xf32>
        %64 = arith.subf %56, %63 : tensor<128x64xf32>
        %65 = tt.extern_elementwise %64 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %66 = "tt.reduce"(%65) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %232 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %232 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %67 = arith.subf %arg10, %61 : tensor<128xf32>
        %68 = tt.extern_elementwise %67 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %69 = arith.mulf %arg8, %68 : tensor<128xf32>
        %70 = arith.addf %69, %66 : tensor<128xf32>
        %71 = tt.expand_dims %68 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %72 = tt.broadcast %71 : tensor<128x1xf32> -> tensor<128x64xf32>
        %73 = arith.mulf %arg9, %72 : tensor<128x64xf32>
        %74 = tt.expand_dims %1 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %75 = arith.addi %46, %74 : tensor<64x1xi32>
        %76 = tt.expand_dims %43 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %77 = arith.muli %76, %cst : tensor<1x64xi32>
        %78 = tt.broadcast %75 : tensor<64x1xi32> -> tensor<64x64xi32>
        %79 = tt.broadcast %77 : tensor<1x64xi32> -> tensor<64x64xi32>
        %80 = arith.addi %78, %79 : tensor<64x64xi32>
        %81 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %82 = tt.addptr %81, %80 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %83 = tt.load %82 : tensor<64x64x!tt.ptr<f8E5M2>>
        %84 = tt.fp_to_fp %65, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %85 = tt.trans %83 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %86 = tt.dot %84, %85, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %87 = arith.addf %73, %86 : tensor<128x64xf32>
        %c1_i32_7 = arith.constant 1 : i32
        %88 = arith.muli %c64_i32, %c1_i32_7 : i32
        %89 = arith.addi %arg7, %88 : i32
        %90 = tt.splat %89 : i32 -> tensor<64xi32>
        %91 = arith.addi %90, %1 : tensor<64xi32>
        %92 = tt.expand_dims %91 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %93 = arith.muli %92, %cst_1 : tensor<64x1xi32>
        %94 = tt.splat %3 : i32 -> tensor<64x1xi32>
        %95 = arith.addi %94, %93 : tensor<64x1xi32>
        %96 = tt.broadcast %95 : tensor<64x1xi32> -> tensor<64x64xi32>
        %97 = tt.broadcast %8 : tensor<1x64xi32> -> tensor<64x64xi32>
        %98 = arith.addi %96, %97 : tensor<64x64xi32>
        %99 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %100 = tt.addptr %99, %98 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %101 = tt.load %100 : tensor<64x64x!tt.ptr<f8E5M2>>
        %102 = tt.trans %101 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %103 = tt.dot %14, %102, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %104 = arith.mulf %103, %cst_0 : tensor<128x64xf32>
        %105 = "tt.reduce"(%104) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %232 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %232 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %106 = arith.cmpf ogt, %61, %105 : tensor<128xf32>
        %107 = arith.cmpf une, %61, %61 : tensor<128xf32>
        %108 = arith.ori %106, %107 : tensor<128xi1>
        %109 = arith.select %108, %61, %105 : tensor<128xi1>, tensor<128xf32>
        %110 = tt.expand_dims %109 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %111 = tt.broadcast %110 : tensor<128x1xf32> -> tensor<128x64xf32>
        %112 = arith.subf %104, %111 : tensor<128x64xf32>
        %113 = tt.extern_elementwise %112 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %114 = "tt.reduce"(%113) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %232 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %232 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %115 = arith.subf %61, %109 : tensor<128xf32>
        %116 = tt.extern_elementwise %115 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %117 = arith.mulf %70, %116 : tensor<128xf32>
        %118 = arith.addf %117, %114 : tensor<128xf32>
        %119 = tt.expand_dims %116 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %120 = tt.broadcast %119 : tensor<128x1xf32> -> tensor<128x64xf32>
        %121 = arith.mulf %87, %120 : tensor<128x64xf32>
        %122 = tt.expand_dims %1 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %123 = arith.addi %94, %122 : tensor<64x1xi32>
        %124 = tt.expand_dims %91 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %125 = arith.muli %124, %cst : tensor<1x64xi32>
        %126 = tt.broadcast %123 : tensor<64x1xi32> -> tensor<64x64xi32>
        %127 = tt.broadcast %125 : tensor<1x64xi32> -> tensor<64x64xi32>
        %128 = arith.addi %126, %127 : tensor<64x64xi32>
        %129 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %130 = tt.addptr %129, %128 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %131 = tt.load %130 : tensor<64x64x!tt.ptr<f8E5M2>>
        %132 = tt.fp_to_fp %113, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %133 = tt.trans %131 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %134 = tt.dot %132, %133, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %135 = arith.addf %121, %134 : tensor<128x64xf32>
        %c2_i32 = arith.constant 2 : i32
        %136 = arith.muli %c64_i32, %c2_i32 : i32
        %137 = arith.addi %arg7, %136 : i32
        %138 = tt.splat %137 : i32 -> tensor<64xi32>
        %139 = arith.addi %138, %1 : tensor<64xi32>
        %140 = tt.expand_dims %139 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %141 = arith.muli %140, %cst_1 : tensor<64x1xi32>
        %142 = tt.splat %3 : i32 -> tensor<64x1xi32>
        %143 = arith.addi %142, %141 : tensor<64x1xi32>
        %144 = tt.broadcast %143 : tensor<64x1xi32> -> tensor<64x64xi32>
        %145 = tt.broadcast %8 : tensor<1x64xi32> -> tensor<64x64xi32>
        %146 = arith.addi %144, %145 : tensor<64x64xi32>
        %147 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %148 = tt.addptr %147, %146 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %149 = tt.load %148 : tensor<64x64x!tt.ptr<f8E5M2>>
        %150 = tt.trans %149 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %151 = tt.dot %14, %150, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %152 = arith.mulf %151, %cst_0 : tensor<128x64xf32>
        %153 = "tt.reduce"(%152) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %232 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %232 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %154 = arith.cmpf ogt, %109, %153 : tensor<128xf32>
        %155 = arith.cmpf une, %109, %109 : tensor<128xf32>
        %156 = arith.ori %154, %155 : tensor<128xi1>
        %157 = arith.select %156, %109, %153 : tensor<128xi1>, tensor<128xf32>
        %158 = tt.expand_dims %157 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %159 = tt.broadcast %158 : tensor<128x1xf32> -> tensor<128x64xf32>
        %160 = arith.subf %152, %159 : tensor<128x64xf32>
        %161 = tt.extern_elementwise %160 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %162 = "tt.reduce"(%161) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %232 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %232 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %163 = arith.subf %109, %157 : tensor<128xf32>
        %164 = tt.extern_elementwise %163 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %165 = arith.mulf %118, %164 : tensor<128xf32>
        %166 = arith.addf %165, %162 : tensor<128xf32>
        %167 = tt.expand_dims %164 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %168 = tt.broadcast %167 : tensor<128x1xf32> -> tensor<128x64xf32>
        %169 = arith.mulf %135, %168 : tensor<128x64xf32>
        %170 = tt.expand_dims %1 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %171 = arith.addi %142, %170 : tensor<64x1xi32>
        %172 = tt.expand_dims %139 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %173 = arith.muli %172, %cst : tensor<1x64xi32>
        %174 = tt.broadcast %171 : tensor<64x1xi32> -> tensor<64x64xi32>
        %175 = tt.broadcast %173 : tensor<1x64xi32> -> tensor<64x64xi32>
        %176 = arith.addi %174, %175 : tensor<64x64xi32>
        %177 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %178 = tt.addptr %177, %176 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %179 = tt.load %178 : tensor<64x64x!tt.ptr<f8E5M2>>
        %180 = tt.fp_to_fp %161, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %181 = tt.trans %179 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %182 = tt.dot %180, %181, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %183 = arith.addf %169, %182 : tensor<128x64xf32>
        %c3_i32 = arith.constant 3 : i32
        %184 = arith.muli %c64_i32, %c3_i32 : i32
        %185 = arith.addi %arg7, %184 : i32
        %186 = tt.splat %185 : i32 -> tensor<64xi32>
        %187 = arith.addi %186, %1 : tensor<64xi32>
        %188 = tt.expand_dims %187 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %189 = arith.muli %188, %cst_1 : tensor<64x1xi32>
        %190 = tt.splat %3 : i32 -> tensor<64x1xi32>
        %191 = arith.addi %190, %189 : tensor<64x1xi32>
        %192 = tt.broadcast %191 : tensor<64x1xi32> -> tensor<64x64xi32>
        %193 = tt.broadcast %8 : tensor<1x64xi32> -> tensor<64x64xi32>
        %194 = arith.addi %192, %193 : tensor<64x64xi32>
        %195 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %196 = tt.addptr %195, %194 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %197 = tt.load %196 : tensor<64x64x!tt.ptr<f8E5M2>>
        %198 = tt.trans %197 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %199 = tt.dot %14, %198, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %200 = arith.mulf %199, %cst_0 : tensor<128x64xf32>
        %201 = "tt.reduce"(%200) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %232 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %232 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %202 = arith.cmpf ogt, %157, %201 : tensor<128xf32>
        %203 = arith.cmpf une, %157, %157 : tensor<128xf32>
        %204 = arith.ori %202, %203 : tensor<128xi1>
        %205 = arith.select %204, %157, %201 : tensor<128xi1>, tensor<128xf32>
        %206 = tt.expand_dims %205 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %207 = tt.broadcast %206 : tensor<128x1xf32> -> tensor<128x64xf32>
        %208 = arith.subf %200, %207 : tensor<128x64xf32>
        %209 = tt.extern_elementwise %208 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %210 = "tt.reduce"(%209) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %232 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %232 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %211 = arith.subf %157, %205 : tensor<128xf32>
        %212 = tt.extern_elementwise %211 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %213 = arith.mulf %166, %212 : tensor<128xf32>
        %214 = arith.addf %213, %210 : tensor<128xf32>
        %215 = tt.expand_dims %212 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %216 = tt.broadcast %215 : tensor<128x1xf32> -> tensor<128x64xf32>
        %217 = arith.mulf %183, %216 : tensor<128x64xf32>
        %218 = tt.expand_dims %1 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %219 = arith.addi %190, %218 : tensor<64x1xi32>
        %220 = tt.expand_dims %187 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %221 = arith.muli %220, %cst : tensor<1x64xi32>
        %222 = tt.broadcast %219 : tensor<64x1xi32> -> tensor<64x64xi32>
        %223 = tt.broadcast %221 : tensor<1x64xi32> -> tensor<64x64xi32>
        %224 = arith.addi %222, %223 : tensor<64x64xi32>
        %225 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %226 = tt.addptr %225, %224 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %227 = tt.load %226 : tensor<64x64x!tt.ptr<f8E5M2>>
        %228 = tt.fp_to_fp %209, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %229 = tt.trans %227 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %230 = tt.dot %228, %229, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %231 = arith.addf %217, %230 : tensor<128x64xf32>
        scf.yield %214, %231, %205 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      }
      %16:3 = scf.for %arg7 = %c0_i32_6 to %c128_i32 step %c64_i32 iter_args(%arg8 = %15#0, %arg9 = %15#1, %arg10 = %15#2) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %42 = tt.splat %arg7 : i32 -> tensor<64xi32>
        %43 = arith.addi %42, %1 : tensor<64xi32>
        %44 = tt.expand_dims %43 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %45 = arith.muli %44, %cst_1 : tensor<64x1xi32>
        %46 = tt.splat %3 : i32 -> tensor<64x1xi32>
        %47 = arith.addi %46, %45 : tensor<64x1xi32>
        %48 = tt.broadcast %47 : tensor<64x1xi32> -> tensor<64x64xi32>
        %49 = tt.broadcast %8 : tensor<1x64xi32> -> tensor<64x64xi32>
        %50 = arith.addi %48, %49 : tensor<64x64xi32>
        %51 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %52 = tt.addptr %51, %50 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %53 = tt.load %52 : tensor<64x64x!tt.ptr<f8E5M2>>
        %54 = tt.trans %53 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %55 = tt.dot %14, %54, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %56 = arith.mulf %55, %cst_0 : tensor<128x64xf32>
        %57 = "tt.reduce"(%56) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %88 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %88 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %58 = arith.cmpf ogt, %arg10, %57 : tensor<128xf32>
        %59 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %60 = arith.ori %58, %59 : tensor<128xi1>
        %61 = arith.select %60, %arg10, %57 : tensor<128xi1>, tensor<128xf32>
        %62 = tt.expand_dims %61 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %63 = tt.broadcast %62 : tensor<128x1xf32> -> tensor<128x64xf32>
        %64 = arith.subf %56, %63 : tensor<128x64xf32>
        %65 = tt.extern_elementwise %64 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %66 = "tt.reduce"(%65) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %88 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %88 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %67 = arith.subf %arg10, %61 : tensor<128xf32>
        %68 = tt.extern_elementwise %67 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %69 = arith.mulf %arg8, %68 : tensor<128xf32>
        %70 = arith.addf %69, %66 : tensor<128xf32>
        %71 = tt.expand_dims %68 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %72 = tt.broadcast %71 : tensor<128x1xf32> -> tensor<128x64xf32>
        %73 = arith.mulf %arg9, %72 : tensor<128x64xf32>
        %74 = tt.expand_dims %1 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %75 = arith.addi %46, %74 : tensor<64x1xi32>
        %76 = tt.expand_dims %43 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %77 = arith.muli %76, %cst : tensor<1x64xi32>
        %78 = tt.broadcast %75 : tensor<64x1xi32> -> tensor<64x64xi32>
        %79 = tt.broadcast %77 : tensor<1x64xi32> -> tensor<64x64xi32>
        %80 = arith.addi %78, %79 : tensor<64x64xi32>
        %81 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %82 = tt.addptr %81, %80 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %83 = tt.load %82 : tensor<64x64x!tt.ptr<f8E5M2>>
        %84 = tt.fp_to_fp %65, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %85 = tt.trans %83 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %86 = tt.dot %84, %85, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %87 = arith.addf %73, %86 : tensor<128x64xf32>
        scf.yield %70, %87, %61 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.num_stages = 1 : i32}
      %17 = tt.expand_dims %16#0 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
      %18 = tt.broadcast %17 : tensor<128x1xf32> -> tensor<128x64xf32>
      %19 = arith.divf %16#1, %18 : tensor<128x64xf32>
      %20 = tt.fp_to_fp %19, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
      %21 = arith.divsi %arg6, %arg5 : i32
      %22 = arith.remsi %arg6, %arg5 : i32
      %23 = arith.cmpi ne, %22, %c0_i32 : i32
      %24 = arith.subi %21, %c1_i32 : i32
      %25 = arith.select %23, %24, %21 : i32
      %26 = arith.cmpi slt, %arg6, %c0_i32 : i32
      %27 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %28 = arith.cmpi ne, %26, %27 : i1
      %29 = arith.select %28, %25, %21 : i32
      %30 = arith.andi %23, %28 : i1
      %31 = arith.addi %22, %arg5 : i32
      %32 = arith.select %30, %31, %22 : i32
      %33 = arith.muli %29, %arg4 : i32
      %34 = arith.muli %32, %c8192_i32 : i32
      %35 = arith.addi %33, %34 : i32
      %36 = tt.splat %35 : i32 -> tensor<128x1xi32>
      %37 = arith.addi %36, %5 : tensor<128x1xi32>
      %38 = tt.broadcast %37 : tensor<128x1xi32> -> tensor<128x64xi32>
      %39 = arith.addi %38, %10 : tensor<128x64xi32>
      %40 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %41 = tt.addptr %40, %39 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      tt.store %41, %20 : tensor<128x64x!tt.ptr<f8E5M2>>
    } {tt.disallow_acc_multi_buffer, tt.flatten, tt.loop_unroll_factor = 1 : i32, tt.num_stages = 3 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(convert-triton-to-tritongpu{enable-source-remat=false num-ctas=1 num-warps=4 target=cuda:90 threads-per-warp=32}, tritongpu-coalesce, tritongpu-F32DotTC, triton-nvidia-gpu-plan-cta, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritongpu-accelerate-matmul, tritongpu-remove-layout-conversions, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, triton-nvidia-optimize-descriptor-encoding, triton-loop-aware-cse, tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-combine-tensor-select-and-if, nvgpu-warp-specialization{dump-intermediate-steps=false num-stages=1}, tritongpu-assign-latencies{num-stages=1}, tritongpu-schedule-loops, tritongpu-pipeline{dump-intermediate-steps=false num-stages=1}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-loop-aware-cse, tritongpu-prefetch, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-coalesce-async-copy, triton-nvidia-optimize-tmem-layouts, tritongpu-remove-layout-conversions, triton-nvidia-interleave-tmem, tritongpu-reduce-data-duplication, tritongpu-reorder-instructions, triton-loop-aware-cse, symbol-dce, triton-nvidia-tma-lowering, triton-nvidia-gpu-fence-insertion{compute-capability=90}, sccp, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true})",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_willfeng/mu/cmugzhntleqdfvx3ixkxseg7y6zjp5rvkfpghiro2uscnbrgz6sv.py:13:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_willfeng/mu/cmugzhntleqdfvx3ixkxseg7y6zjp5rvkfpghiro2uscnbrgz6sv.py:13:0: note: Pipeline failed while executing [`TritonGPUAssignLatencies` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
Process ForkProcess-709:
Traceback (most recent call last):
  File "/home/willfeng/local/miniconda3/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/willfeng/local/miniconda3/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/data/users/willfeng/helion/helion/runtime/precompile_shim.py", line 54, in finish_it
    kernel_cache[key] = fn.compile(
                        ^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/compiler/compiler.py", line 359, in compile
    next_module = compile_ir(module, metadata)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/backends/nvidia/compiler.py", line 456, in <lambda>
    stages["ttgir"] = lambda src, metadata: self.make_ttgir(src, metadata, options, capability)
                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/backends/nvidia/compiler.py", line 298, in make_ttgir
    pm.run(mod)
RuntimeError: PassManager::run failed
python: /root/.triton/llvm/llvm-8957e64a-almalinux-x64/include/llvm/ADT/SmallVector.h:296: const_reference llvm::SmallVectorTemplateCommon<long>::operator[](size_type) const [T = long]: Assertion `idx < size()' failed.
module {
  tt.func public @_fp8_attention_kernel_kernel(%arg0: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %c1_i32 = arith.constant 1 : i32
    %c64_i32 = arith.constant 64 : i32
    %c128_i32 = arith.constant 128 : i32
    %c0_i32 = arith.constant 0 : i32
    %c132_i32 = arith.constant 132 : i32
    %c192_i32 = arith.constant 192 : i32
    %cst = arith.constant dense<64> : tensor<1x64xi32>
    %cst_0 = arith.constant dense<0.180336878> : tensor<128x64xf32>
    %cst_1 = arith.constant dense<64> : tensor<64x1xi32>
    %cst_2 = arith.constant dense<64> : tensor<128x1xi32>
    %c8192_i32 = arith.constant 8192 : i32
    %cst_3 = arith.constant dense<0.000000e+00> : tensor<128x64xf32>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<128xf32>
    %cst_5 = arith.constant dense<0xFF800000> : tensor<128xf32>
    %0 = tt.get_program_id x : i32
    scf.for %arg6 = %0 to %c192_i32 step %c132_i32  : i32 {
      %1 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
      %2 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>
      %3 = arith.muli %arg6, %c8192_i32 : i32
      %4 = tt.expand_dims %2 {axis = 1 : i32} : tensor<128xi32> -> tensor<128x1xi32>
      %5 = arith.muli %4, %cst_2 : tensor<128x1xi32>
      %6 = tt.splat %3 : i32 -> tensor<128x1xi32>
      %7 = arith.addi %6, %5 : tensor<128x1xi32>
      %8 = tt.expand_dims %1 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %9 = tt.broadcast %7 : tensor<128x1xi32> -> tensor<128x64xi32>
      %10 = tt.broadcast %8 : tensor<1x64xi32> -> tensor<128x64xi32>
      %11 = arith.addi %9, %10 : tensor<128x64xi32>
      %12 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %13 = tt.addptr %12, %11 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      %14 = tt.load %13 : tensor<128x64x!tt.ptr<f8E5M2>>
      %c0_i32_6 = arith.constant 0 : i32
      %c256_i32 = arith.constant 256 : i32
      %15:3 = scf.for %arg7 = %c0_i32 to %c0_i32_6 step %c256_i32 iter_args(%arg8 = %cst_4, %arg9 = %cst_3, %arg10 = %cst_5) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %42 = tt.splat %arg7 : i32 -> tensor<64xi32>
        %43 = arith.addi %42, %1 : tensor<64xi32>
        %44 = tt.expand_dims %43 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %45 = arith.muli %44, %cst_1 : tensor<64x1xi32>
        %46 = tt.splat %3 : i32 -> tensor<64x1xi32>
        %47 = arith.addi %46, %45 : tensor<64x1xi32>
        %48 = tt.broadcast %47 : tensor<64x1xi32> -> tensor<64x64xi32>
        %49 = tt.broadcast %8 : tensor<1x64xi32> -> tensor<64x64xi32>
        %50 = arith.addi %48, %49 : tensor<64x64xi32>
        %51 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %52 = tt.addptr %51, %50 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %53 = tt.load %52 : tensor<64x64x!tt.ptr<f8E5M2>>
        %54 = tt.trans %53 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %55 = tt.dot %14, %54, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %56 = arith.mulf %55, %cst_0 : tensor<128x64xf32>
        %57 = "tt.reduce"(%56) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %232 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %232 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %58 = arith.cmpf ogt, %arg10, %57 : tensor<128xf32>
        %59 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %60 = arith.ori %58, %59 : tensor<128xi1>
        %61 = arith.select %60, %arg10, %57 : tensor<128xi1>, tensor<128xf32>
        %62 = tt.expand_dims %61 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %63 = tt.broadcast %62 : tensor<128x1xf32> -> tensor<128x64xf32>
        %64 = arith.subf %56, %63 : tensor<128x64xf32>
        %65 = tt.extern_elementwise %64 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %66 = "tt.reduce"(%65) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %232 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %232 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %67 = arith.subf %arg10, %61 : tensor<128xf32>
        %68 = tt.extern_elementwise %67 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %69 = arith.mulf %arg8, %68 : tensor<128xf32>
        %70 = arith.addf %69, %66 : tensor<128xf32>
        %71 = tt.expand_dims %68 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %72 = tt.broadcast %71 : tensor<128x1xf32> -> tensor<128x64xf32>
        %73 = arith.mulf %arg9, %72 : tensor<128x64xf32>
        %74 = tt.expand_dims %1 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %75 = arith.addi %46, %74 : tensor<64x1xi32>
        %76 = tt.expand_dims %43 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %77 = arith.muli %76, %cst : tensor<1x64xi32>
        %78 = tt.broadcast %75 : tensor<64x1xi32> -> tensor<64x64xi32>
        %79 = tt.broadcast %77 : tensor<1x64xi32> -> tensor<64x64xi32>
        %80 = arith.addi %78, %79 : tensor<64x64xi32>
        %81 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %82 = tt.addptr %81, %80 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %83 = tt.load %82 : tensor<64x64x!tt.ptr<f8E5M2>>
        %84 = tt.fp_to_fp %65, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %85 = tt.trans %83 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %86 = tt.dot %84, %85, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %87 = arith.addf %73, %86 : tensor<128x64xf32>
        %c1_i32_7 = arith.constant 1 : i32
        %88 = arith.muli %c64_i32, %c1_i32_7 : i32
        %89 = arith.addi %arg7, %88 : i32
        %90 = tt.splat %89 : i32 -> tensor<64xi32>
        %91 = arith.addi %90, %1 : tensor<64xi32>
        %92 = tt.expand_dims %91 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %93 = arith.muli %92, %cst_1 : tensor<64x1xi32>
        %94 = tt.splat %3 : i32 -> tensor<64x1xi32>
        %95 = arith.addi %94, %93 : tensor<64x1xi32>
        %96 = tt.broadcast %95 : tensor<64x1xi32> -> tensor<64x64xi32>
        %97 = tt.broadcast %8 : tensor<1x64xi32> -> tensor<64x64xi32>
        %98 = arith.addi %96, %97 : tensor<64x64xi32>
        %99 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %100 = tt.addptr %99, %98 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %101 = tt.load %100 : tensor<64x64x!tt.ptr<f8E5M2>>
        %102 = tt.trans %101 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %103 = tt.dot %14, %102, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %104 = arith.mulf %103, %cst_0 : tensor<128x64xf32>
        %105 = "tt.reduce"(%104) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %232 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %232 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %106 = arith.cmpf ogt, %61, %105 : tensor<128xf32>
        %107 = arith.cmpf une, %61, %61 : tensor<128xf32>
        %108 = arith.ori %106, %107 : tensor<128xi1>
        %109 = arith.select %108, %61, %105 : tensor<128xi1>, tensor<128xf32>
        %110 = tt.expand_dims %109 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %111 = tt.broadcast %110 : tensor<128x1xf32> -> tensor<128x64xf32>
        %112 = arith.subf %104, %111 : tensor<128x64xf32>
        %113 = tt.extern_elementwise %112 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %114 = "tt.reduce"(%113) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %232 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %232 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %115 = arith.subf %61, %109 : tensor<128xf32>
        %116 = tt.extern_elementwise %115 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %117 = arith.mulf %70, %116 : tensor<128xf32>
        %118 = arith.addf %117, %114 : tensor<128xf32>
        %119 = tt.expand_dims %116 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %120 = tt.broadcast %119 : tensor<128x1xf32> -> tensor<128x64xf32>
        %121 = arith.mulf %87, %120 : tensor<128x64xf32>
        %122 = tt.expand_dims %1 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %123 = arith.addi %94, %122 : tensor<64x1xi32>
        %124 = tt.expand_dims %91 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %125 = arith.muli %124, %cst : tensor<1x64xi32>
        %126 = tt.broadcast %123 : tensor<64x1xi32> -> tensor<64x64xi32>
        %127 = tt.broadcast %125 : tensor<1x64xi32> -> tensor<64x64xi32>
        %128 = arith.addi %126, %127 : tensor<64x64xi32>
        %129 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %130 = tt.addptr %129, %128 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %131 = tt.load %130 : tensor<64x64x!tt.ptr<f8E5M2>>
        %132 = tt.fp_to_fp %113, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %133 = tt.trans %131 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %134 = tt.dot %132, %133, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %135 = arith.addf %121, %134 : tensor<128x64xf32>
        %c2_i32 = arith.constant 2 : i32
        %136 = arith.muli %c64_i32, %c2_i32 : i32
        %137 = arith.addi %arg7, %136 : i32
        %138 = tt.splat %137 : i32 -> tensor<64xi32>
        %139 = arith.addi %138, %1 : tensor<64xi32>
        %140 = tt.expand_dims %139 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %141 = arith.muli %140, %cst_1 : tensor<64x1xi32>
        %142 = tt.splat %3 : i32 -> tensor<64x1xi32>
        %143 = arith.addi %142, %141 : tensor<64x1xi32>
        %144 = tt.broadcast %143 : tensor<64x1xi32> -> tensor<64x64xi32>
        %145 = tt.broadcast %8 : tensor<1x64xi32> -> tensor<64x64xi32>
        %146 = arith.addi %144, %145 : tensor<64x64xi32>
        %147 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %148 = tt.addptr %147, %146 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %149 = tt.load %148 : tensor<64x64x!tt.ptr<f8E5M2>>
        %150 = tt.trans %149 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %151 = tt.dot %14, %150, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %152 = arith.mulf %151, %cst_0 : tensor<128x64xf32>
        %153 = "tt.reduce"(%152) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %232 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %232 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %154 = arith.cmpf ogt, %109, %153 : tensor<128xf32>
        %155 = arith.cmpf une, %109, %109 : tensor<128xf32>
        %156 = arith.ori %154, %155 : tensor<128xi1>
        %157 = arith.select %156, %109, %153 : tensor<128xi1>, tensor<128xf32>
        %158 = tt.expand_dims %157 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %159 = tt.broadcast %158 : tensor<128x1xf32> -> tensor<128x64xf32>
        %160 = arith.subf %152, %159 : tensor<128x64xf32>
        %161 = tt.extern_elementwise %160 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %162 = "tt.reduce"(%161) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %232 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %232 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %163 = arith.subf %109, %157 : tensor<128xf32>
        %164 = tt.extern_elementwise %163 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %165 = arith.mulf %118, %164 : tensor<128xf32>
        %166 = arith.addf %165, %162 : tensor<128xf32>
        %167 = tt.expand_dims %164 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %168 = tt.broadcast %167 : tensor<128x1xf32> -> tensor<128x64xf32>
        %169 = arith.mulf %135, %168 : tensor<128x64xf32>
        %170 = tt.expand_dims %1 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %171 = arith.addi %142, %170 : tensor<64x1xi32>
        %172 = tt.expand_dims %139 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %173 = arith.muli %172, %cst : tensor<1x64xi32>
        %174 = tt.broadcast %171 : tensor<64x1xi32> -> tensor<64x64xi32>
        %175 = tt.broadcast %173 : tensor<1x64xi32> -> tensor<64x64xi32>
        %176 = arith.addi %174, %175 : tensor<64x64xi32>
        %177 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %178 = tt.addptr %177, %176 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %179 = tt.load %178 : tensor<64x64x!tt.ptr<f8E5M2>>
        %180 = tt.fp_to_fp %161, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %181 = tt.trans %179 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %182 = tt.dot %180, %181, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %183 = arith.addf %169, %182 : tensor<128x64xf32>
        %c3_i32 = arith.constant 3 : i32
        %184 = arith.muli %c64_i32, %c3_i32 : i32
        %185 = arith.addi %arg7, %184 : i32
        %186 = tt.splat %185 : i32 -> tensor<64xi32>
        %187 = arith.addi %186, %1 : tensor<64xi32>
        %188 = tt.expand_dims %187 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %189 = arith.muli %188, %cst_1 : tensor<64x1xi32>
        %190 = tt.splat %3 : i32 -> tensor<64x1xi32>
        %191 = arith.addi %190, %189 : tensor<64x1xi32>
        %192 = tt.broadcast %191 : tensor<64x1xi32> -> tensor<64x64xi32>
        %193 = tt.broadcast %8 : tensor<1x64xi32> -> tensor<64x64xi32>
        %194 = arith.addi %192, %193 : tensor<64x64xi32>
        %195 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %196 = tt.addptr %195, %194 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %197 = tt.load %196 : tensor<64x64x!tt.ptr<f8E5M2>>
        %198 = tt.trans %197 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %199 = tt.dot %14, %198, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %200 = arith.mulf %199, %cst_0 : tensor<128x64xf32>
        %201 = "tt.reduce"(%200) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %232 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %232 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %202 = arith.cmpf ogt, %157, %201 : tensor<128xf32>
        %203 = arith.cmpf une, %157, %157 : tensor<128xf32>
        %204 = arith.ori %202, %203 : tensor<128xi1>
        %205 = arith.select %204, %157, %201 : tensor<128xi1>, tensor<128xf32>
        %206 = tt.expand_dims %205 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %207 = tt.broadcast %206 : tensor<128x1xf32> -> tensor<128x64xf32>
        %208 = arith.subf %200, %207 : tensor<128x64xf32>
        %209 = tt.extern_elementwise %208 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %210 = "tt.reduce"(%209) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %232 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %232 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %211 = arith.subf %157, %205 : tensor<128xf32>
        %212 = tt.extern_elementwise %211 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %213 = arith.mulf %166, %212 : tensor<128xf32>
        %214 = arith.addf %213, %210 : tensor<128xf32>
        %215 = tt.expand_dims %212 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %216 = tt.broadcast %215 : tensor<128x1xf32> -> tensor<128x64xf32>
        %217 = arith.mulf %183, %216 : tensor<128x64xf32>
        %218 = tt.expand_dims %1 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %219 = arith.addi %190, %218 : tensor<64x1xi32>
        %220 = tt.expand_dims %187 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %221 = arith.muli %220, %cst : tensor<1x64xi32>
        %222 = tt.broadcast %219 : tensor<64x1xi32> -> tensor<64x64xi32>
        %223 = tt.broadcast %221 : tensor<1x64xi32> -> tensor<64x64xi32>
        %224 = arith.addi %222, %223 : tensor<64x64xi32>
        %225 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %226 = tt.addptr %225, %224 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %227 = tt.load %226 : tensor<64x64x!tt.ptr<f8E5M2>>
        %228 = tt.fp_to_fp %209, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %229 = tt.trans %227 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %230 = tt.dot %228, %229, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %231 = arith.addf %217, %230 : tensor<128x64xf32>
        scf.yield %214, %231, %205 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      }
      %16:3 = scf.for %arg7 = %c0_i32_6 to %c128_i32 step %c64_i32 iter_args(%arg8 = %15#0, %arg9 = %15#1, %arg10 = %15#2) -> (tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>)  : i32 {
        %42 = tt.splat %arg7 : i32 -> tensor<64xi32>
        %43 = arith.addi %42, %1 : tensor<64xi32>
        %44 = tt.expand_dims %43 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %45 = arith.muli %44, %cst_1 : tensor<64x1xi32>
        %46 = tt.splat %3 : i32 -> tensor<64x1xi32>
        %47 = arith.addi %46, %45 : tensor<64x1xi32>
        %48 = tt.broadcast %47 : tensor<64x1xi32> -> tensor<64x64xi32>
        %49 = tt.broadcast %8 : tensor<1x64xi32> -> tensor<64x64xi32>
        %50 = arith.addi %48, %49 : tensor<64x64xi32>
        %51 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %52 = tt.addptr %51, %50 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %53 = tt.load %52 : tensor<64x64x!tt.ptr<f8E5M2>>
        %54 = tt.trans %53 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %55 = tt.dot %14, %54, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %56 = arith.mulf %55, %cst_0 : tensor<128x64xf32>
        %57 = "tt.reduce"(%56) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %88 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %88 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %58 = arith.cmpf ogt, %arg10, %57 : tensor<128xf32>
        %59 = arith.cmpf une, %arg10, %arg10 : tensor<128xf32>
        %60 = arith.ori %58, %59 : tensor<128xi1>
        %61 = arith.select %60, %arg10, %57 : tensor<128xi1>, tensor<128xf32>
        %62 = tt.expand_dims %61 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %63 = tt.broadcast %62 : tensor<128x1xf32> -> tensor<128x64xf32>
        %64 = arith.subf %56, %63 : tensor<128x64xf32>
        %65 = tt.extern_elementwise %64 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128x64xf32>) -> tensor<128x64xf32>
        %66 = "tt.reduce"(%65) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %88 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %88 : f32
        }) : (tensor<128x64xf32>) -> tensor<128xf32>
        %67 = arith.subf %arg10, %61 : tensor<128xf32>
        %68 = tt.extern_elementwise %67 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<128xf32>) -> tensor<128xf32>
        %69 = arith.mulf %arg8, %68 : tensor<128xf32>
        %70 = arith.addf %69, %66 : tensor<128xf32>
        %71 = tt.expand_dims %68 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
        %72 = tt.broadcast %71 : tensor<128x1xf32> -> tensor<128x64xf32>
        %73 = arith.mulf %arg9, %72 : tensor<128x64xf32>
        %74 = tt.expand_dims %1 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %75 = arith.addi %46, %74 : tensor<64x1xi32>
        %76 = tt.expand_dims %43 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %77 = arith.muli %76, %cst : tensor<1x64xi32>
        %78 = tt.broadcast %75 : tensor<64x1xi32> -> tensor<64x64xi32>
        %79 = tt.broadcast %77 : tensor<1x64xi32> -> tensor<64x64xi32>
        %80 = arith.addi %78, %79 : tensor<64x64xi32>
        %81 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %82 = tt.addptr %81, %80 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %83 = tt.load %82 : tensor<64x64x!tt.ptr<f8E5M2>>
        %84 = tt.fp_to_fp %65, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
        %85 = tt.trans %83 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %86 = tt.dot %84, %85, %cst_3, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<128x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<128x64xf32>
        %87 = arith.addf %73, %86 : tensor<128x64xf32>
        scf.yield %70, %87, %61 : tensor<128xf32>, tensor<128x64xf32>, tensor<128xf32>
      } {tt.num_stages = 1 : i32}
      %17 = tt.expand_dims %16#0 {axis = 1 : i32} : tensor<128xf32> -> tensor<128x1xf32>
      %18 = tt.broadcast %17 : tensor<128x1xf32> -> tensor<128x64xf32>
      %19 = arith.divf %16#1, %18 : tensor<128x64xf32>
      %20 = tt.fp_to_fp %19, rounding = rtne : tensor<128x64xf32> -> tensor<128x64xf8E5M2>
      %21 = arith.divsi %arg6, %arg5 : i32
      %22 = arith.remsi %arg6, %arg5 : i32
      %23 = arith.cmpi ne, %22, %c0_i32 : i32
      %24 = arith.subi %21, %c1_i32 : i32
      %25 = arith.select %23, %24, %21 : i32
      %26 = arith.cmpi slt, %arg6, %c0_i32 : i32
      %27 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %28 = arith.cmpi ne, %26, %27 : i1
      %29 = arith.select %28, %25, %21 : i32
      %30 = arith.andi %23, %28 : i1
      %31 = arith.addi %22, %arg5 : i32
      %32 = arith.select %30, %31, %22 : i32
      %33 = arith.muli %29, %arg4 : i32
      %34 = arith.muli %32, %c8192_i32 : i32
      %35 = arith.addi %33, %34 : i32
      %36 = tt.splat %35 : i32 -> tensor<128x1xi32>
      %37 = arith.addi %36, %5 : tensor<128x1xi32>
      %38 = tt.broadcast %37 : tensor<128x1xi32> -> tensor<128x64xi32>
      %39 = arith.addi %38, %10 : tensor<128x64xi32>
      %40 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<128x64x!tt.ptr<f8E5M2>>
      %41 = tt.addptr %40, %39 : tensor<128x64x!tt.ptr<f8E5M2>>, tensor<128x64xi32>
      tt.store %41, %20 : tensor<128x64x!tt.ptr<f8E5M2>>
    } {tt.disallow_acc_multi_buffer, tt.flatten, tt.loop_unroll_factor = 1 : i32, tt.num_stages = 3 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(convert-triton-to-tritongpu{enable-source-remat=false num-ctas=1 num-warps=4 target=cuda:90 threads-per-warp=32}, tritongpu-coalesce, tritongpu-F32DotTC, triton-nvidia-gpu-plan-cta, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritongpu-accelerate-matmul, tritongpu-remove-layout-conversions, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, triton-nvidia-optimize-descriptor-encoding, triton-loop-aware-cse, tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-combine-tensor-select-and-if, nvgpu-warp-specialization{dump-intermediate-steps=false num-stages=1}, tritongpu-assign-latencies{num-stages=1}, tritongpu-schedule-loops, tritongpu-pipeline{dump-intermediate-steps=false num-stages=1}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-loop-aware-cse, tritongpu-prefetch, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-coalesce-async-copy, triton-nvidia-optimize-tmem-layouts, tritongpu-remove-layout-conversions, triton-nvidia-interleave-tmem, tritongpu-reduce-data-duplication, tritongpu-reorder-instructions, triton-loop-aware-cse, symbol-dce, triton-nvidia-tma-lowering, triton-nvidia-gpu-fence-insertion{compute-capability=90}, sccp, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true})",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_willfeng/mu/cmugzhntleqdfvx3ixkxseg7y6zjp5rvkfpghiro2uscnbrgz6sv.py:13:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_willfeng/mu/cmugzhntleqdfvx3ixkxseg7y6zjp5rvkfpghiro2uscnbrgz6sv.py:13:0: note: Pipeline failed while executing [`TritonGPUAssignLatencies` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
[973s] Generation 9: replaced=10 min=0.0113 mid=0.0130 max=0.0236 best=Config(block_sizes=[128, 128], range_unroll_factors=[0, 2, 2], range_num_stages=[0, 4, 2], range_multi_buffers=[None, False, True], range_flattens=[None, False, False], num_warps=8, num_stages=7, indexing='tensor_descriptor', pid_type='flat', range_warp_specializes=[])
python: /root/.triton/llvm/llvm-8957e64a-almalinux-x64/include/llvm/ADT/SmallVector.h:296: const_reference llvm::SmallVectorTemplateCommon<long>::operator[](size_type) const [T = long]: Assertion `idx < size()' failed.
module {
  tt.func public @_fp8_attention_kernel_kernel(%arg0: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %c1_i32 = arith.constant 1 : i32
    %c64_i32 = arith.constant 64 : i32
    %c128_i32 = arith.constant 128 : i32
    %c0_i32 = arith.constant 0 : i32
    %cst = arith.constant dense<64> : tensor<1x64xi32>
    %cst_0 = arith.constant dense<0.180336878> : tensor<64x64xf32>
    %cst_1 = arith.constant dense<64> : tensor<64x1xi32>
    %c8192_i32 = arith.constant 8192 : i32
    %cst_2 = arith.constant dense<0.000000e+00> : tensor<64x64xf32>
    %cst_3 = arith.constant dense<0.000000e+00> : tensor<64xf32>
    %cst_4 = arith.constant dense<0xFF800000> : tensor<64xf32>
    %0 = tt.get_program_id x : i32
    %1 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
    scf.for %arg6 = %c0_i32 to %c128_i32 step %c64_i32  : i32 {
      %2 = tt.splat %arg6 : i32 -> tensor<64xi32>
      %3 = arith.addi %2, %1 : tensor<64xi32>
      %4 = arith.muli %0, %c8192_i32 : i32
      %5 = tt.expand_dims %3 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
      %6 = arith.muli %5, %cst_1 : tensor<64x1xi32>
      %7 = tt.splat %4 : i32 -> tensor<64x1xi32>
      %8 = arith.addi %7, %6 : tensor<64x1xi32>
      %9 = tt.expand_dims %1 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %10 = tt.broadcast %8 : tensor<64x1xi32> -> tensor<64x64xi32>
      %11 = tt.broadcast %9 : tensor<1x64xi32> -> tensor<64x64xi32>
      %12 = arith.addi %10, %11 : tensor<64x64xi32>
      %13 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %14 = tt.addptr %13, %12 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      %15 = tt.load %14 : tensor<64x64x!tt.ptr<f8E5M2>>
      %16:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c64_i32 iter_args(%arg8 = %cst_3, %arg9 = %cst_2, %arg10 = %cst_4) -> (tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>)  : i32 {
        %42 = tt.splat %arg7 : i32 -> tensor<64xi32>
        %43 = arith.addi %42, %1 : tensor<64xi32>
        %44 = tt.expand_dims %43 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %45 = arith.muli %44, %cst_1 : tensor<64x1xi32>
        %46 = arith.addi %7, %45 : tensor<64x1xi32>
        %47 = tt.broadcast %46 : tensor<64x1xi32> -> tensor<64x64xi32>
        %48 = arith.addi %47, %11 : tensor<64x64xi32>
        %49 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %50 = tt.addptr %49, %48 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %51 = tt.load %50 : tensor<64x64x!tt.ptr<f8E5M2>>
        %52 = tt.trans %51 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %53 = tt.dot %15, %52, %cst_2, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<64x64xf32>
        %54 = arith.mulf %53, %cst_0 : tensor<64x64xf32>
        %55 = "tt.reduce"(%54) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %86 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %86 : f32
        }) : (tensor<64x64xf32>) -> tensor<64xf32>
        %56 = arith.cmpf ogt, %arg10, %55 : tensor<64xf32>
        %57 = arith.cmpf une, %arg10, %arg10 : tensor<64xf32>
        %58 = arith.ori %56, %57 : tensor<64xi1>
        %59 = arith.select %58, %arg10, %55 : tensor<64xi1>, tensor<64xf32>
        %60 = tt.expand_dims %59 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %61 = tt.broadcast %60 : tensor<64x1xf32> -> tensor<64x64xf32>
        %62 = arith.subf %54, %61 : tensor<64x64xf32>
        %63 = tt.extern_elementwise %62 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64x64xf32>) -> tensor<64x64xf32>
        %64 = "tt.reduce"(%63) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %86 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %86 : f32
        }) : (tensor<64x64xf32>) -> tensor<64xf32>
        %65 = arith.subf %arg10, %59 : tensor<64xf32>
        %66 = tt.extern_elementwise %65 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64xf32>) -> tensor<64xf32>
        %67 = arith.mulf %arg8, %66 : tensor<64xf32>
        %68 = arith.addf %67, %64 : tensor<64xf32>
        %69 = tt.expand_dims %66 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %70 = tt.broadcast %69 : tensor<64x1xf32> -> tensor<64x64xf32>
        %71 = arith.mulf %arg9, %70 : tensor<64x64xf32>
        %72 = tt.expand_dims %1 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %73 = arith.addi %7, %72 : tensor<64x1xi32>
        %74 = tt.expand_dims %43 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %75 = arith.muli %74, %cst : tensor<1x64xi32>
        %76 = tt.broadcast %73 : tensor<64x1xi32> -> tensor<64x64xi32>
        %77 = tt.broadcast %75 : tensor<1x64xi32> -> tensor<64x64xi32>
        %78 = arith.addi %76, %77 : tensor<64x64xi32>
        %79 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %80 = tt.addptr %79, %78 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %81 = tt.load %80 : tensor<64x64x!tt.ptr<f8E5M2>>
        %82 = tt.fp_to_fp %63, rounding = rtne : tensor<64x64xf32> -> tensor<64x64xf8E5M2>
        %83 = tt.trans %81 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %84 = tt.dot %82, %83, %cst_2, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<64x64xf32>
        %85 = arith.addf %71, %84 : tensor<64x64xf32>
        scf.yield %68, %85, %59 : tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>
      } {tt.num_stages = 1 : i32}
      %17 = tt.expand_dims %16#0 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
      %18 = tt.broadcast %17 : tensor<64x1xf32> -> tensor<64x64xf32>
      %19 = arith.divf %16#1, %18 : tensor<64x64xf32>
      %20 = tt.fp_to_fp %19, rounding = rtne : tensor<64x64xf32> -> tensor<64x64xf8E5M2>
      %21 = arith.divsi %0, %arg5 : i32
      %22 = arith.remsi %0, %arg5 : i32
      %23 = arith.cmpi ne, %22, %c0_i32 : i32
      %24 = arith.subi %21, %c1_i32 : i32
      %25 = arith.select %23, %24, %21 : i32
      %26 = arith.cmpi slt, %0, %c0_i32 : i32
      %27 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %28 = arith.cmpi ne, %26, %27 : i1
      %29 = arith.select %28, %25, %21 : i32
      %30 = arith.andi %23, %28 : i1
      %31 = arith.addi %22, %arg5 : i32
      %32 = arith.select %30, %31, %22 : i32
      %33 = arith.muli %29, %arg4 : i32
      %34 = arith.muli %32, %c8192_i32 : i32
      %35 = arith.addi %33, %34 : i32
      %36 = tt.splat %35 : i32 -> tensor<64x1xi32>
      %37 = arith.addi %36, %6 : tensor<64x1xi32>
      %38 = tt.broadcast %37 : tensor<64x1xi32> -> tensor<64x64xi32>
      %39 = arith.addi %38, %11 : tensor<64x64xi32>
      %40 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %41 = tt.addptr %40, %39 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      tt.store %41, %20 : tensor<64x64x!tt.ptr<f8E5M2>>
    } {tt.disallow_acc_multi_buffer, tt.flatten, tt.num_stages = 4 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(convert-triton-to-tritongpu{enable-source-remat=false num-ctas=1 num-warps=8 target=cuda:90 threads-per-warp=32}, tritongpu-coalesce, tritongpu-F32DotTC, triton-nvidia-gpu-plan-cta, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritongpu-accelerate-matmul, tritongpu-remove-layout-conversions, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, triton-nvidia-optimize-descriptor-encoding, triton-loop-aware-cse, tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-combine-tensor-select-and-if, nvgpu-warp-specialization{dump-intermediate-steps=false num-stages=4}, tritongpu-assign-latencies{num-stages=4}, tritongpu-schedule-loops, tritongpu-pipeline{dump-intermediate-steps=false num-stages=4}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-loop-aware-cse, tritongpu-prefetch, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-coalesce-async-copy, triton-nvidia-optimize-tmem-layouts, tritongpu-remove-layout-conversions, triton-nvidia-interleave-tmem, tritongpu-reduce-data-duplication, tritongpu-reorder-instructions, triton-loop-aware-cse, symbol-dce, triton-nvidia-tma-lowering, triton-nvidia-gpu-fence-insertion{compute-capability=90}, sccp, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true})",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_willfeng/6g/c6guwnk5n7zxlthnqrvz4abq4vjpjgaxax5f6cqclrkyurkgw4on.py:12:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_willfeng/6g/c6guwnk5n7zxlthnqrvz4abq4vjpjgaxax5f6cqclrkyurkgw4on.py:12:0: note: Pipeline failed while executing [`TritonGPUAssignLatencies` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
Process ForkProcess-751:
Traceback (most recent call last):
  File "/home/willfeng/local/miniconda3/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/willfeng/local/miniconda3/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/data/users/willfeng/helion/helion/runtime/precompile_shim.py", line 54, in finish_it
    kernel_cache[key] = fn.compile(
                        ^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/compiler/compiler.py", line 359, in compile
    next_module = compile_ir(module, metadata)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/backends/nvidia/compiler.py", line 456, in <lambda>
    stages["ttgir"] = lambda src, metadata: self.make_ttgir(src, metadata, options, capability)
                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/backends/nvidia/compiler.py", line 298, in make_ttgir
    pm.run(mod)
RuntimeError: PassManager::run failed
python: /root/.triton/llvm/llvm-8957e64a-almalinux-x64/include/llvm/ADT/SmallVector.h:296: const_reference llvm::SmallVectorTemplateCommon<long>::operator[](size_type) const [T = long]: Assertion `idx < size()' failed.
module {
  tt.func public @_fp8_attention_kernel_kernel(%arg0: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<f8E5M2> {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %c1_i32 = arith.constant 1 : i32
    %c64_i32 = arith.constant 64 : i32
    %c128_i32 = arith.constant 128 : i32
    %c0_i32 = arith.constant 0 : i32
    %cst = arith.constant dense<64> : tensor<1x64xi32>
    %cst_0 = arith.constant dense<0.180336878> : tensor<64x64xf32>
    %cst_1 = arith.constant dense<64> : tensor<64x1xi32>
    %c8192_i32 = arith.constant 8192 : i32
    %cst_2 = arith.constant dense<0.000000e+00> : tensor<64x64xf32>
    %cst_3 = arith.constant dense<0.000000e+00> : tensor<64xf32>
    %cst_4 = arith.constant dense<0xFF800000> : tensor<64xf32>
    %0 = tt.get_program_id x : i32
    %1 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>
    scf.for %arg6 = %c0_i32 to %c128_i32 step %c64_i32  : i32 {
      %2 = tt.splat %arg6 : i32 -> tensor<64xi32>
      %3 = arith.addi %2, %1 : tensor<64xi32>
      %4 = arith.muli %0, %c8192_i32 : i32
      %5 = tt.expand_dims %3 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
      %6 = arith.muli %5, %cst_1 : tensor<64x1xi32>
      %7 = tt.splat %4 : i32 -> tensor<64x1xi32>
      %8 = arith.addi %7, %6 : tensor<64x1xi32>
      %9 = tt.expand_dims %1 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
      %10 = tt.broadcast %8 : tensor<64x1xi32> -> tensor<64x64xi32>
      %11 = tt.broadcast %9 : tensor<1x64xi32> -> tensor<64x64xi32>
      %12 = arith.addi %10, %11 : tensor<64x64xi32>
      %13 = tt.splat %arg0 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %14 = tt.addptr %13, %12 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      %15 = tt.load %14 : tensor<64x64x!tt.ptr<f8E5M2>>
      %16:3 = scf.for %arg7 = %c0_i32 to %c128_i32 step %c64_i32 iter_args(%arg8 = %cst_3, %arg9 = %cst_2, %arg10 = %cst_4) -> (tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>)  : i32 {
        %42 = tt.splat %arg7 : i32 -> tensor<64xi32>
        %43 = arith.addi %42, %1 : tensor<64xi32>
        %44 = tt.expand_dims %43 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %45 = arith.muli %44, %cst_1 : tensor<64x1xi32>
        %46 = arith.addi %7, %45 : tensor<64x1xi32>
        %47 = tt.broadcast %46 : tensor<64x1xi32> -> tensor<64x64xi32>
        %48 = arith.addi %47, %11 : tensor<64x64xi32>
        %49 = tt.splat %arg1 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %50 = tt.addptr %49, %48 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %51 = tt.load %50 : tensor<64x64x!tt.ptr<f8E5M2>>
        %52 = tt.trans %51 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %53 = tt.dot %15, %52, %cst_2, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<64x64xf32>
        %54 = arith.mulf %53, %cst_0 : tensor<64x64xf32>
        %55 = "tt.reduce"(%54) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %86 = arith.maxnumf %arg11, %arg12 : f32
          tt.reduce.return %86 : f32
        }) : (tensor<64x64xf32>) -> tensor<64xf32>
        %56 = arith.cmpf ogt, %arg10, %55 : tensor<64xf32>
        %57 = arith.cmpf une, %arg10, %arg10 : tensor<64xf32>
        %58 = arith.ori %56, %57 : tensor<64xi1>
        %59 = arith.select %58, %arg10, %55 : tensor<64xi1>, tensor<64xf32>
        %60 = tt.expand_dims %59 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %61 = tt.broadcast %60 : tensor<64x1xf32> -> tensor<64x64xf32>
        %62 = arith.subf %54, %61 : tensor<64x64xf32>
        %63 = tt.extern_elementwise %62 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64x64xf32>) -> tensor<64x64xf32>
        %64 = "tt.reduce"(%63) <{axis = 1 : i32}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %86 = arith.addf %arg11, %arg12 : f32
          tt.reduce.return %86 : f32
        }) : (tensor<64x64xf32>) -> tensor<64xf32>
        %65 = arith.subf %arg10, %59 : tensor<64xf32>
        %66 = tt.extern_elementwise %65 {libname = "", libpath = "", pure = true, symbol = "__nv_exp2f"} : (tensor<64xf32>) -> tensor<64xf32>
        %67 = arith.mulf %arg8, %66 : tensor<64xf32>
        %68 = arith.addf %67, %64 : tensor<64xf32>
        %69 = tt.expand_dims %66 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
        %70 = tt.broadcast %69 : tensor<64x1xf32> -> tensor<64x64xf32>
        %71 = arith.mulf %arg9, %70 : tensor<64x64xf32>
        %72 = tt.expand_dims %1 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32>
        %73 = arith.addi %7, %72 : tensor<64x1xi32>
        %74 = tt.expand_dims %43 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32>
        %75 = arith.muli %74, %cst : tensor<1x64xi32>
        %76 = tt.broadcast %73 : tensor<64x1xi32> -> tensor<64x64xi32>
        %77 = tt.broadcast %75 : tensor<1x64xi32> -> tensor<64x64xi32>
        %78 = arith.addi %76, %77 : tensor<64x64xi32>
        %79 = tt.splat %arg2 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
        %80 = tt.addptr %79, %78 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
        %81 = tt.load %80 : tensor<64x64x!tt.ptr<f8E5M2>>
        %82 = tt.fp_to_fp %63, rounding = rtne : tensor<64x64xf32> -> tensor<64x64xf8E5M2>
        %83 = tt.trans %81 {order = array<i32: 1, 0>} : tensor<64x64xf8E5M2> -> tensor<64x64xf8E5M2>
        %84 = tt.dot %82, %83, %cst_2, inputPrecision = tf32 {maxNumImpreciseAcc = 1073741824 : i32} : tensor<64x64xf8E5M2> * tensor<64x64xf8E5M2> -> tensor<64x64xf32>
        %85 = arith.addf %71, %84 : tensor<64x64xf32>
        scf.yield %68, %85, %59 : tensor<64xf32>, tensor<64x64xf32>, tensor<64xf32>
      } {tt.num_stages = 1 : i32}
      %17 = tt.expand_dims %16#0 {axis = 1 : i32} : tensor<64xf32> -> tensor<64x1xf32>
      %18 = tt.broadcast %17 : tensor<64x1xf32> -> tensor<64x64xf32>
      %19 = arith.divf %16#1, %18 : tensor<64x64xf32>
      %20 = tt.fp_to_fp %19, rounding = rtne : tensor<64x64xf32> -> tensor<64x64xf8E5M2>
      %21 = arith.divsi %0, %arg5 : i32
      %22 = arith.remsi %0, %arg5 : i32
      %23 = arith.cmpi ne, %22, %c0_i32 : i32
      %24 = arith.subi %21, %c1_i32 : i32
      %25 = arith.select %23, %24, %21 : i32
      %26 = arith.cmpi slt, %0, %c0_i32 : i32
      %27 = arith.cmpi slt, %arg5, %c0_i32 : i32
      %28 = arith.cmpi ne, %26, %27 : i1
      %29 = arith.select %28, %25, %21 : i32
      %30 = arith.andi %23, %28 : i1
      %31 = arith.addi %22, %arg5 : i32
      %32 = arith.select %30, %31, %22 : i32
      %33 = arith.muli %29, %arg4 : i32
      %34 = arith.muli %32, %c8192_i32 : i32
      %35 = arith.addi %33, %34 : i32
      %36 = tt.splat %35 : i32 -> tensor<64x1xi32>
      %37 = arith.addi %36, %6 : tensor<64x1xi32>
      %38 = tt.broadcast %37 : tensor<64x1xi32> -> tensor<64x64xi32>
      %39 = arith.addi %38, %11 : tensor<64x64xi32>
      %40 = tt.splat %arg3 : !tt.ptr<f8E5M2> -> tensor<64x64x!tt.ptr<f8E5M2>>
      %41 = tt.addptr %40, %39 : tensor<64x64x!tt.ptr<f8E5M2>>, tensor<64x64xi32>
      tt.store %41, %20 : tensor<64x64x!tt.ptr<f8E5M2>>
    } {tt.disallow_acc_multi_buffer, tt.flatten, tt.num_stages = 4 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(convert-triton-to-tritongpu{enable-source-remat=false num-ctas=1 num-warps=8 target=cuda:90 threads-per-warp=32}, tritongpu-coalesce, tritongpu-F32DotTC, triton-nvidia-gpu-plan-cta, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritongpu-accelerate-matmul, tritongpu-remove-layout-conversions, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, triton-nvidia-optimize-descriptor-encoding, triton-loop-aware-cse, tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-combine-tensor-select-and-if, nvgpu-warp-specialization{dump-intermediate-steps=false num-stages=4}, tritongpu-assign-latencies{num-stages=4}, tritongpu-schedule-loops, tritongpu-pipeline{dump-intermediate-steps=false num-stages=4}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-loop-aware-cse, tritongpu-prefetch, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-coalesce-async-copy, triton-nvidia-optimize-tmem-layouts, tritongpu-remove-layout-conversions, triton-nvidia-interleave-tmem, tritongpu-reduce-data-duplication, tritongpu-reorder-instructions, triton-loop-aware-cse, symbol-dce, triton-nvidia-tma-lowering, triton-nvidia-gpu-fence-insertion{compute-capability=90}, sccp, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true})",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_willfeng/6g/c6guwnk5n7zxlthnqrvz4abq4vjpjgaxax5f6cqclrkyurkgw4on.py:12:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_willfeng/6g/c6guwnk5n7zxlthnqrvz4abq4vjpjgaxax5f6cqclrkyurkgw4on.py:12:0: note: Pipeline failed while executing [`TritonGPUAssignLatencies` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
Process ForkProcess-796:
  0%|          | 0/8 [17:37<?, ?it/s]
Traceback (most recent call last):
  File "/home/willfeng/local/miniconda3/lib/python3.12/subprocess.py", line 1264, in wait
    return self._wait(timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/miniconda3/lib/python3.12/subprocess.py", line 2051, in _wait
    (pid, sts) = self._try_wait(0)
                 ^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/miniconda3/lib/python3.12/subprocess.py", line 2009, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/willfeng/local/miniconda3/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/willfeng/local/miniconda3/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/data/users/willfeng/helion/helion/runtime/precompile_shim.py", line 54, in finish_it
    kernel_cache[key] = fn.compile(
                        ^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/compiler/compiler.py", line 359, in compile
    next_module = compile_ir(module, metadata)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/backends/nvidia/compiler.py", line 461, in <lambda>
    stages["cubin"] = lambda src, metadata: self.make_cubin(src, metadata, options, self.target.arch)
                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/backends/nvidia/compiler.py", line 424, in make_cubin
    subprocess.run(ptxas_cmd, check=True, close_fds=False, stderr=flog)
  File "/home/willfeng/local/miniconda3/lib/python3.12/subprocess.py", line 550, in run
    stdout, stderr = process.communicate(input, timeout=timeout)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/miniconda3/lib/python3.12/subprocess.py", line 1201, in communicate
    self.wait()
  File "/home/willfeng/local/miniconda3/lib/python3.12/subprocess.py", line 1277, in wait
    self._wait(timeout=sigint_timeout)
  File "/home/willfeng/local/miniconda3/lib/python3.12/subprocess.py", line 2019, in _wait
    def _wait(self, timeout):
    
KeyboardInterrupt
WARNING:tritonbench.utils.triton_op:Caught exception, terminating early with partial results
Traceback (most recent call last):
  File "/home/willfeng/local/miniconda3/lib/python3.12/multiprocessing/connection.py", line 1135, in wait
    ready = selector.select(timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/miniconda3/lib/python3.12/selectors.py", line 415, in select
    fd_event_list = self._selector.poll(timeout)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/data/users/willfeng/helion/helion/autotuner/base_search.py", line 465, in __call__
    process.join(self.seconds_left())
  File "/home/willfeng/local/miniconda3/lib/python3.12/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/miniconda3/lib/python3.12/multiprocessing/popen_fork.py", line 40, in wait
    if not wait([self.sentinel], timeout):
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/miniconda3/lib/python3.12/multiprocessing/connection.py", line 1127, in wait
    with _WaitSelector() as selector:
  File "/home/willfeng/local/miniconda3/lib/python3.12/selectors.py", line 202, in __exit__
    def __exit__(self, *args):
    
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/data/users/willfeng/helion/benchmarks/tritonbench/tritonbench/utils/triton_op.py", line 890, in run
    y_vals: Dict[str, BenchmarkOperatorMetrics] = functools.reduce(
                                                  ^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/benchmarks/tritonbench/tritonbench/utils/triton_op.py", line 878, in _reduce_benchmarks
    acc[bm_name] = self._do_bench(
                   ^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/benchmarks/tritonbench/tritonbench/utils/triton_op.py", line 1179, in _do_bench
    metrics.latency = do_bench_wrapper(
                      ^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/benchmarks/tritonbench/tritonbench/components/do_bench/run.py", line 202, in do_bench_wrapper
    times=triton.testing.do_bench(
          ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/testing.py", line 149, in do_bench
    fn()
  File "/data/users/willfeng/helion/benchmarks/run.py", line 302, in _inner
    return result()
           ^^^^^^^^
  File "/data/users/willfeng/helion/examples/fp8_attention.py", line 129, in <lambda>
    return lambda: fp8_attention_kernel(q_fp8, k_fp8, v_fp8, batch, heads)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/helion/runtime/kernel.py", line 252, in __call__
    return self.bind(args)(*args)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/helion/runtime/kernel.py", line 535, in __call__
    self.autotune(args)
  File "/data/users/willfeng/helion/helion/runtime/kernel.py", line 447, in autotune
    ).autotune()
      ^^^^^^^^^^
  File "/data/users/willfeng/helion/helion/autotuner/base_search.py", line 242, in autotune
    best = self._autotune()
           ^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/helion/autotuner/differential_evolution.py", line 99, in _autotune
    replaced = self.evolve_population()
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/helion/autotuner/differential_evolution.py", line 84, in evolve_population
    candidate = self.benchmark_flat(self.mutate(i))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/helion/autotuner/base_search.py", line 341, in benchmark_flat
    return PopulationMember(self.benchmark(config), flat_values, config)
                            ^^^^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/helion/autotuner/base_search.py", line 99, in benchmark
    if self.start_precompile_and_check_for_hangs(config, fn)():
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/helion/autotuner/base_search.py", line 467, in __call__
    self._mark_complete()
  File "/data/users/willfeng/helion/helion/autotuner/base_search.py", line 517, in _mark_complete
    def _mark_complete(self) -> bool:
    
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/willfeng/local/miniconda3/lib/python3.12/multiprocessing/connection.py", line 1135, in wait
    ready = selector.select(timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/miniconda3/lib/python3.12/selectors.py", line 415, in select
    fd_event_list = self._selector.poll(timeout)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/data/users/willfeng/helion/helion/autotuner/base_search.py", line 465, in __call__
    process.join(self.seconds_left())
  File "/home/willfeng/local/miniconda3/lib/python3.12/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/miniconda3/lib/python3.12/multiprocessing/popen_fork.py", line 40, in wait
    if not wait([self.sentinel], timeout):
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/miniconda3/lib/python3.12/multiprocessing/connection.py", line 1127, in wait
    with _WaitSelector() as selector:
  File "/home/willfeng/local/miniconda3/lib/python3.12/selectors.py", line 202, in __exit__
    def __exit__(self, *args):

KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/data/users/willfeng/helion/benchmarks/run.py", line 486, in <module>
    main()
  File "/data/users/willfeng/helion/benchmarks/run.py", line 482, in main
    run_kernel(kernel_name, tritonbench_args.copy())
  File "/data/users/willfeng/helion/benchmarks/run.py", line 339, in run_kernel
    op.run(warmup=warmup, rep=rep)
  File "/data/users/willfeng/helion/benchmarks/tritonbench/tritonbench/utils/triton_op.py", line 890, in run
    y_vals: Dict[str, BenchmarkOperatorMetrics] = functools.reduce(
                                                  ^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/benchmarks/tritonbench/tritonbench/utils/triton_op.py", line 878, in _reduce_benchmarks
    acc[bm_name] = self._do_bench(
                   ^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/benchmarks/tritonbench/tritonbench/utils/triton_op.py", line 1179, in _do_bench
    metrics.latency = do_bench_wrapper(
                      ^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/benchmarks/tritonbench/tritonbench/components/do_bench/run.py", line 202, in do_bench_wrapper
    times=triton.testing.do_bench(
          ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/testing.py", line 149, in do_bench
    fn()
  File "/data/users/willfeng/helion/benchmarks/run.py", line 302, in _inner
    return result()
           ^^^^^^^^
  File "/data/users/willfeng/helion/examples/fp8_attention.py", line 129, in <lambda>
    return lambda: fp8_attention_kernel(q_fp8, k_fp8, v_fp8, batch, heads)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/helion/runtime/kernel.py", line 252, in __call__
    return self.bind(args)(*args)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/helion/runtime/kernel.py", line 535, in __call__
    self.autotune(args)
  File "/data/users/willfeng/helion/helion/runtime/kernel.py", line 447, in autotune
    ).autotune()
      ^^^^^^^^^^
  File "/data/users/willfeng/helion/helion/autotuner/base_search.py", line 242, in autotune
    best = self._autotune()
           ^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/helion/autotuner/differential_evolution.py", line 99, in _autotune
    replaced = self.evolve_population()
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/helion/autotuner/differential_evolution.py", line 84, in evolve_population
    candidate = self.benchmark_flat(self.mutate(i))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/helion/autotuner/base_search.py", line 341, in benchmark_flat
    return PopulationMember(self.benchmark(config), flat_values, config)
                            ^^^^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/helion/autotuner/base_search.py", line 99, in benchmark
    if self.start_precompile_and_check_for_hangs(config, fn)():
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/helion/autotuner/base_search.py", line 467, in __call__
    self._mark_complete()
  File "/data/users/willfeng/helion/helion/autotuner/base_search.py", line 517, in _mark_complete
    def _mark_complete(self) -> bool:

KeyboardInterrupt
