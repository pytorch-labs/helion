Running part 2/4: kernels 3 to 4 of 7 total
Running 2 kernels...


============================================================
Kernel: jagged_mean
============================================================

Running jagged_mean benchmark with Helion implementation...

  0%|          | 0/3616 [00:00<?, ?it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.03ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.05ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 42.18ms to get benchmark function for torch_compile_nested_tensor_integration
/home/willfeng/local/pytorch-nightly/torch/backends/cuda/__init__.py:131: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:78.)
  return torch._C._get_cublas_allow_tf32()
INFO:tritonbench.utils.triton_op:Took 0.04ms to get benchmark function for helion_jagged_mean
[0s] Starting DifferentialEvolutionSearch with population=40, generations=20, crossover_rate=0.8
Removed 6 outliers from 210 samples
Removed 19 outliers from 304 samples
Removed 14 outliers from 281 samples
Removed 21 outliers from 481 samples
Removed 12 outliers from 278 samples
[62s] Timeout after 60s compiling Config(block_sizes=[4, 2, 2048], range_unroll_factors=[0, 3, 3], range_num_stages=[0, 4, 0], range_multi_buffers=[None, False, False], range_flattens=[None, True, False], num_warps=1, num_stages=5, indexing='block_ptr', pid_type='flat', range_warp_specializes=[])
[73s] Initial population: failed=3 min=0.0079 mid=0.0115 max=0.2039 best=Config(block_sizes=[1, 4, 512], range_unroll_factors=[0, 1, 2], range_num_stages=[0, 0, 0], range_multi_buffers=[None, True, True], range_flattens=[None, None, True], num_warps=2, num_stages=8, indexing='block_ptr', pid_type='flat', range_warp_specializes=[])
[135s] Timeout after 60s compiling Config(block_sizes=[4, 1, 1024], range_unroll_factors=[2, 4, 3], range_num_stages=[1, 1, 0], range_multi_buffers=[False, None, True], range_flattens=[None, True, True], num_warps=1, num_stages=8, indexing='block_ptr', pid_type='persistent_interleaved')
[262s] Generation 2: replaced=21 min=0.0079 mid=0.0089 max=0.0115 best=Config(block_sizes=[1, 4, 512], range_unroll_factors=[0, 1, 2], range_num_stages=[0, 0, 0], range_multi_buffers=[None, True, True], range_flattens=[None, None, True], num_warps=2, num_stages=8, indexing='block_ptr', pid_type='flat', range_warp_specializes=[])
[376s] Generation 3: replaced=19 min=0.0078 mid=0.0086 max=0.0113 best=Config(block_sizes=[1, 2, 128], range_unroll_factors=[0, 0, 2], range_num_stages=[0, 0, 1], range_multi_buffers=[None, False, True], range_flattens=[None, False, None], num_warps=1, num_stages=6, indexing='pointer', pid_type='flat', range_warp_specializes=[])
Exception ignored in: <generator object tqdm.__iter__ at 0x7f04ac63dd80>
Traceback (most recent call last):
  File "/home/willfeng/local/miniconda3/lib/python3.12/site-packages/tqdm/std.py", line 1196, in __iter__
Process ForkProcess-248:
  File "/home/willfeng/local/miniconda3/lib/python3.12/site-packages/tqdm/std.py", line 1265, in close
    def close(self):

KeyboardInterrupt: 
Traceback (most recent call last):
  File "/home/willfeng/local/miniconda3/lib/python3.12/subprocess.py", line 1264, in wait
    return self._wait(timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/miniconda3/lib/python3.12/subprocess.py", line 2051, in _wait
    (pid, sts) = self._try_wait(0)
                 ^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/miniconda3/lib/python3.12/subprocess.py", line 2009, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/willfeng/local/miniconda3/lib/python3.12/subprocess.py", line 550, in run
    stdout, stderr = process.communicate(input, timeout=timeout)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/miniconda3/lib/python3.12/subprocess.py", line 1201, in communicate
    self.wait()
  File "/home/willfeng/local/miniconda3/lib/python3.12/subprocess.py", line 1277, in wait
    self._wait(timeout=sigint_timeout)
  File "/home/willfeng/local/miniconda3/lib/python3.12/subprocess.py", line 2025, in _wait
    endtime = _time() + timeout
              ^^^^^^^
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/willfeng/local/miniconda3/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/willfeng/local/miniconda3/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/data/users/willfeng/helion/helion/runtime/precompile_shim.py", line 54, in finish_it
    kernel_cache[key] = fn.compile(
                        ^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/compiler/compiler.py", line 359, in compile
    next_module = compile_ir(module, metadata)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/backends/nvidia/compiler.py", line 461, in <lambda>
    stages["cubin"] = lambda src, metadata: self.make_cubin(src, metadata, options, self.target.arch)
                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/backends/nvidia/compiler.py", line 424, in make_cubin
    subprocess.run(ptxas_cmd, check=True, close_fds=False, stderr=flog)
  File "/home/willfeng/local/miniconda3/lib/python3.12/subprocess.py", line 566, in run
    process.kill()
  File "/home/willfeng/local/miniconda3/lib/python3.12/subprocess.py", line 2214, in kill
    self.send_signal(signal.SIGKILL)
  File "/home/willfeng/local/miniconda3/lib/python3.12/subprocess.py", line 2192, in send_signal
    self.poll()
  File "/home/willfeng/local/miniconda3/lib/python3.12/subprocess.py", line 1236, in poll
    return self._internal_poll()
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/miniconda3/lib/python3.12/subprocess.py", line 1988, in _internal_poll
    pid, sts = _waitpid(self.pid, _WNOHANG)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
WARNING:tritonbench.utils.triton_op:Caught exception, terminating early with partial results
Traceback (most recent call last):
  File "/data/users/willfeng/helion/benchmarks/tritonbench/tritonbench/utils/triton_op.py", line 890, in run
    y_vals: Dict[str, BenchmarkOperatorMetrics] = functools.reduce(
                                                  ^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/benchmarks/tritonbench/tritonbench/utils/triton_op.py", line 878, in _reduce_benchmarks
    acc[bm_name] = self._do_bench(
                   ^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/benchmarks/tritonbench/tritonbench/utils/triton_op.py", line 1179, in _do_bench
    metrics.latency = do_bench_wrapper(
                      ^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/benchmarks/tritonbench/tritonbench/components/do_bench/run.py", line 202, in do_bench_wrapper
    times=triton.testing.do_bench(
          ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/testing.py", line 149, in do_bench
    fn()
  File "/data/users/willfeng/helion/benchmarks/run.py", line 300, in _inner
    result = kernel_func(*args)
             ^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/examples/jagged_mean.py", line 144, in jagged_mean_tritonbench
    return jagged_mean_kernel(x_values, x_offsets, feature_counts, max_M_tensor)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/helion/runtime/kernel.py", line 252, in __call__
    return self.bind(args)(*args)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/helion/runtime/kernel.py", line 535, in __call__
    self.autotune(args)
  File "/data/users/willfeng/helion/helion/runtime/kernel.py", line 447, in autotune
    ).autotune()
      ^^^^^^^^^^
  File "/data/users/willfeng/helion/helion/autotuner/base_search.py", line 242, in autotune
    best = self._autotune()
           ^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/helion/autotuner/differential_evolution.py", line 99, in _autotune
    replaced = self.evolve_population()
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/helion/autotuner/differential_evolution.py", line 83, in evolve_population
    for i, candidate in self.iter_candidates():
  File "/data/users/willfeng/helion/helion/autotuner/differential_evolution.py", line 76, in iter_candidates
    self.parallel_benchmark_flat(
  File "/data/users/willfeng/helion/helion/autotuner/base_search.py", line 358, in parallel_benchmark_flat
    to_check, configs, self.parallel_benchmark(configs), strict=True
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/helion/autotuner/base_search.py", line 212, in parallel_benchmark
    is_workings = PrecompileFuture.wait_for_all(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/helion/autotuner/base_search.py", line 487, in wait_for_all
    remaining = PrecompileFuture._wait_for_all_step(remaining)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/helion/autotuner/base_search.py", line 505, in _wait_for_all_step
    connection.wait(
  File "/home/willfeng/local/miniconda3/lib/python3.12/multiprocessing/connection.py", line 1135, in wait
    ready = selector.select(timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/miniconda3/lib/python3.12/selectors.py", line 415, in select
    fd_event_list = self._selector.poll(timeout)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
Traceback (most recent call last):
  File "/data/users/willfeng/helion/benchmarks/run.py", line 486, in <module>
    main()
  File "/data/users/willfeng/helion/benchmarks/run.py", line 482, in main
    run_kernel(kernel_name, tritonbench_args.copy())
  File "/data/users/willfeng/helion/benchmarks/run.py", line 339, in run_kernel
    op.run(warmup=warmup, rep=rep)
  File "/data/users/willfeng/helion/benchmarks/tritonbench/tritonbench/utils/triton_op.py", line 890, in run
    y_vals: Dict[str, BenchmarkOperatorMetrics] = functools.reduce(
                                                  ^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/benchmarks/tritonbench/tritonbench/utils/triton_op.py", line 878, in _reduce_benchmarks
    acc[bm_name] = self._do_bench(
                   ^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/benchmarks/tritonbench/tritonbench/utils/triton_op.py", line 1179, in _do_bench
    metrics.latency = do_bench_wrapper(
                      ^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/benchmarks/tritonbench/tritonbench/components/do_bench/run.py", line 202, in do_bench_wrapper
    times=triton.testing.do_bench(
          ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/testing.py", line 149, in do_bench
    fn()
  File "/data/users/willfeng/helion/benchmarks/run.py", line 300, in _inner
    result = kernel_func(*args)
             ^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/examples/jagged_mean.py", line 144, in jagged_mean_tritonbench
    return jagged_mean_kernel(x_values, x_offsets, feature_counts, max_M_tensor)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/helion/runtime/kernel.py", line 252, in __call__
    return self.bind(args)(*args)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/helion/runtime/kernel.py", line 535, in __call__
    self.autotune(args)
  File "/data/users/willfeng/helion/helion/runtime/kernel.py", line 447, in autotune
    ).autotune()
      ^^^^^^^^^^
  File "/data/users/willfeng/helion/helion/autotuner/base_search.py", line 242, in autotune
    best = self._autotune()
           ^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/helion/autotuner/differential_evolution.py", line 99, in _autotune
    replaced = self.evolve_population()
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/helion/autotuner/differential_evolution.py", line 83, in evolve_population
    for i, candidate in self.iter_candidates():
  File "/data/users/willfeng/helion/helion/autotuner/differential_evolution.py", line 76, in iter_candidates
    self.parallel_benchmark_flat(
  File "/data/users/willfeng/helion/helion/autotuner/base_search.py", line 358, in parallel_benchmark_flat
    to_check, configs, self.parallel_benchmark(configs), strict=True
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/helion/autotuner/base_search.py", line 212, in parallel_benchmark
    is_workings = PrecompileFuture.wait_for_all(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/helion/autotuner/base_search.py", line 487, in wait_for_all
    remaining = PrecompileFuture._wait_for_all_step(remaining)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/users/willfeng/helion/helion/autotuner/base_search.py", line 505, in _wait_for_all_step
    connection.wait(
  File "/home/willfeng/local/miniconda3/lib/python3.12/multiprocessing/connection.py", line 1135, in wait
    ready = selector.select(timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/miniconda3/lib/python3.12/selectors.py", line 415, in select
    fd_event_list = self._selector.poll(timeout)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
Process ForkProcess-245:
Traceback (most recent call last):
  File "/home/willfeng/local/miniconda3/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/willfeng/local/miniconda3/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/data/users/willfeng/helion/helion/runtime/precompile_shim.py", line 54, in finish_it
    kernel_cache[key] = fn.compile(
                        ^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/compiler/compiler.py", line 359, in compile
    next_module = compile_ir(module, metadata)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/backends/nvidia/compiler.py", line 459, in <lambda>
    stages["llir"] = lambda src, metadata: self.make_llir(src, metadata, options, capability)
                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/backends/nvidia/compiler.py", line 344, in make_llir
    pm.run(mod)
KeyboardInterrupt
Process ForkProcess-272:
Traceback (most recent call last):
  File "/home/willfeng/local/miniconda3/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/willfeng/local/miniconda3/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/data/users/willfeng/helion/helion/runtime/precompile_shim.py", line 54, in finish_it
    kernel_cache[key] = fn.compile(
                        ^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/compiler/compiler.py", line 359, in compile
    next_module = compile_ir(module, metadata)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/backends/nvidia/compiler.py", line 456, in <lambda>
    stages["ttgir"] = lambda src, metadata: self.make_ttgir(src, metadata, options, capability)
                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/willfeng/local/pytorch-nightly/triton/backends/nvidia/compiler.py", line 298, in make_ttgir
    pm.run(mod)
KeyboardInterrupt
  0%|          | 0/3616 [15:23<?, ?it/s]
