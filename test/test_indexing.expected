This file is automatically generated by assertExpectedJournal calls in test_indexing.py.
Update expected outputs by running tests with the EXPECTTEST_ACCEPT=1 environment variable set.

--- assertExpectedJournal(TestIndexing.test_arange)
from __future__ import annotations

import torch
import helion
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

helion.runtime.set_triton_allocator()

@triton.jit
def _arange_kernel(out, out_stride_0, length, _BLOCK_SIZE_0: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < length
    tl.store(out + indices_0 * out_stride_0, indices_0, mask_0)

def arange(length: int, device: torch.device, *, _launcher=_default_launcher):
    out = torch.empty([length], dtype=torch.int32, device=device)
    _BLOCK_SIZE_0 = 32
    _launcher(_arange_kernel, (triton.cdiv(length, _BLOCK_SIZE_0),), out, out.stride(0), length, _BLOCK_SIZE_0, num_warps=4, num_stages=3)
    return out

--- assertExpectedJournal(TestIndexing.test_arange_three_args_step)
from __future__ import annotations

import torch
import helion
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

helion.runtime.set_triton_allocator()

@triton.jit
def _arange_three_args_step_kernel(out, out_size_0, out_stride_0, _BLOCK_SIZE_0: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < out_size_0
    mul = 2 * offset_0
    iota = (mul + 2 * tl.arange(0, _BLOCK_SIZE_0)).to(tl.int64)
    v_0 = iota.to(tl.int32)
    tl.store(out + indices_0 * out_stride_0, v_0, mask_0)

def arange_three_args_step(x: torch.Tensor, *, _launcher=_default_launcher):
    out = torch.zeros([x.size(0) // 2], dtype=torch.int32, device=x.device)
    _BLOCK_SIZE_0 = 8
    _launcher(_arange_three_args_step_kernel, (triton.cdiv(out.size(0), _BLOCK_SIZE_0),), out, out.size(0), out.stride(0), _BLOCK_SIZE_0, num_warps=4, num_stages=3)
    return out

--- assertExpectedJournal(TestIndexing.test_broadcasting_block_ptr_indexing)
from __future__ import annotations

import torch
import helion
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

helion.runtime.set_triton_allocator()

@triton.jit
def _broadcast_add_3d_kernel(x, bias1, bias2, out, bias1_size_1, bias1_size_2, bias2_size_0, bias2_size_2, out_size_0, out_size_1, out_size_2, x_size_0, x_size_1, x_size_2, bias1_stride_0, bias1_stride_1, bias1_stride_2, bias2_stride_0, bias2_stride_1, bias2_stride_2, out_stride_0, out_stride_1, out_stride_2, x_stride_0, x_stride_1, x_stride_2, d0, d1, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    num_blocks_0 = tl.cdiv(d0, _BLOCK_SIZE_0)
    num_blocks_1 = tl.cdiv(d1, _BLOCK_SIZE_1)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0 % num_blocks_1
    pid_2 = tl.program_id(0) // (num_blocks_0 * num_blocks_1)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    offset_1 = pid_1 * _BLOCK_SIZE_1
    offset_2 = pid_2 * _BLOCK_SIZE_2
    load = tl.load(tl.make_block_ptr(x, [x_size_0, x_size_1, x_size_2], [x_stride_0, x_stride_1, x_stride_2], [offset_0, offset_1, offset_2], [_BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2], [2, 1, 0]), boundary_check=[0, 1, 2], padding_option='zero')
    load_1 = tl.load(tl.make_block_ptr(bias1, [1, bias1_size_1, bias1_size_2], [bias1_stride_0, bias1_stride_1, bias1_stride_2], [0, offset_1, offset_2], [1, _BLOCK_SIZE_1, _BLOCK_SIZE_2], [2, 1, 0]), boundary_check=[1, 2], padding_option='zero')
    v_0 = load + load_1
    load_2 = tl.load(tl.make_block_ptr(bias2, [bias2_size_0, 1, bias2_size_2], [bias2_stride_0, bias2_stride_1, bias2_stride_2], [offset_0, 0, offset_2], [_BLOCK_SIZE_0, 1, _BLOCK_SIZE_2], [2, 1, 0]), boundary_check=[0, 2], padding_option='zero')
    v_1 = v_0 + load_2
    tl.store(tl.make_block_ptr(out, [out_size_0, out_size_1, out_size_2], [out_stride_0, out_stride_1, out_stride_2], [offset_0, offset_1, offset_2], [_BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2], [2, 1, 0]), v_1, boundary_check=[0, 1, 2])

def broadcast_add_3d(x: torch.Tensor, bias1: torch.Tensor, bias2: torch.Tensor, *, _launcher=_default_launcher):
    d0, d1, d2 = x.size()
    out = torch.empty_like(x)
    _BLOCK_SIZE_0 = 8
    _BLOCK_SIZE_1 = 8
    _BLOCK_SIZE_2 = 8
    _launcher(_broadcast_add_3d_kernel, (triton.cdiv(d0, _BLOCK_SIZE_0) * triton.cdiv(d1, _BLOCK_SIZE_1) * triton.cdiv(d2, _BLOCK_SIZE_2),), x, bias1, bias2, out, bias1.size(1), bias1.size(2), bias2.size(0), bias2.size(2), out.size(0), out.size(1), out.size(2), x.size(0), x.size(1), x.size(2), bias1.stride(0), bias1.stride(1), bias1.stride(2), bias2.stride(0), bias2.stride(1), bias2.stride(2), out.stride(0), out.stride(1), out.stride(2), x.stride(0), x.stride(1), x.stride(2), d0, d1, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

--- assertExpectedJournal(TestIndexing.test_broadcasting_pointer_indexing)
from __future__ import annotations

import torch
import helion
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

helion.runtime.set_triton_allocator()

@triton.jit
def _broadcast_add_3d_kernel(x, bias1, bias2, out, bias1_stride_1, bias1_stride_2, bias2_stride_0, bias2_stride_2, out_stride_0, out_stride_1, out_stride_2, x_stride_0, x_stride_1, x_stride_2, d0, d1, d2, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    num_blocks_0 = tl.cdiv(d0, _BLOCK_SIZE_0)
    num_blocks_1 = tl.cdiv(d1, _BLOCK_SIZE_1)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0 % num_blocks_1
    pid_2 = tl.program_id(0) // (num_blocks_0 * num_blocks_1)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < d0
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    mask_1 = indices_1 < d1
    offset_2 = pid_2 * _BLOCK_SIZE_2
    indices_2 = (offset_2 + tl.arange(0, _BLOCK_SIZE_2)).to(tl.int32)
    mask_2 = indices_2 < d2
    load = tl.load(x + (indices_0[:, None, None] * x_stride_0 + indices_1[None, :, None] * x_stride_1 + indices_2[None, None, :] * x_stride_2), mask_0[:, None, None] & mask_1[None, :, None] & mask_2[None, None, :], other=0)
    load_1 = tl.load(bias1 + (indices_1[None, :, None] * bias1_stride_1 + indices_2[None, None, :] * bias1_stride_2), mask_1[None, :, None] & mask_2[None, None, :], other=0)
    v_0 = load + load_1
    load_2 = tl.load(bias2 + (indices_0[:, None, None] * bias2_stride_0 + indices_2[None, None, :] * bias2_stride_2), mask_0[:, None, None] & mask_2[None, None, :], other=0)
    v_1 = v_0 + load_2
    tl.store(out + (indices_0[:, None, None] * out_stride_0 + indices_1[None, :, None] * out_stride_1 + indices_2[None, None, :] * out_stride_2), v_1, mask_0[:, None, None] & mask_1[None, :, None] & mask_2[None, None, :])

def broadcast_add_3d(x: torch.Tensor, bias1: torch.Tensor, bias2: torch.Tensor, *, _launcher=_default_launcher):
    d0, d1, d2 = x.size()
    out = torch.empty_like(x)
    _BLOCK_SIZE_0 = 8
    _BLOCK_SIZE_1 = 8
    _BLOCK_SIZE_2 = 8
    _launcher(_broadcast_add_3d_kernel, (triton.cdiv(d0, _BLOCK_SIZE_0) * triton.cdiv(d1, _BLOCK_SIZE_1) * triton.cdiv(d2, _BLOCK_SIZE_2),), x, bias1, bias2, out, bias1.stride(1), bias1.stride(2), bias2.stride(0), bias2.stride(2), out.stride(0), out.stride(1), out.stride(2), x.stride(0), x.stride(1), x.stride(2), d0, d1, d2, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

--- assertExpectedJournal(TestIndexing.test_broadcasting_tensor_descriptor_indexing)
from __future__ import annotations

import torch
import helion
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

helion.runtime.set_triton_allocator()

@triton.jit
def _broadcast_add_3d_kernel(x, bias1, bias2, out, bias1_size_1, bias1_size_2, bias2_size_0, bias2_size_2, out_size_0, out_size_1, out_size_2, x_size_0, x_size_1, x_size_2, bias1_stride_0, bias1_stride_1, bias1_stride_2, bias2_stride_0, bias2_stride_1, bias2_stride_2, out_stride_0, out_stride_1, out_stride_2, x_stride_0, x_stride_1, x_stride_2, d0, d1, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    x_desc = tl.make_tensor_descriptor(x, [x_size_0, x_size_1, x_size_2], [x_stride_0, x_stride_1, x_stride_2], [_BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2])
    bias1_desc = tl.make_tensor_descriptor(bias1, [1, bias1_size_1, bias1_size_2], [bias1_stride_0, bias1_stride_1, bias1_stride_2], [1, _BLOCK_SIZE_1, _BLOCK_SIZE_2])
    bias2_desc = tl.make_tensor_descriptor(bias2, [bias2_size_0, 1, bias2_size_2], [bias2_stride_0, bias2_stride_1, bias2_stride_2], [_BLOCK_SIZE_0, 1, _BLOCK_SIZE_2])
    out_desc = tl.make_tensor_descriptor(out, [out_size_0, out_size_1, out_size_2], [out_stride_0, out_stride_1, out_stride_2], [_BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2])
    num_blocks_0 = tl.cdiv(d0, _BLOCK_SIZE_0)
    num_blocks_1 = tl.cdiv(d1, _BLOCK_SIZE_1)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0 % num_blocks_1
    pid_2 = tl.program_id(0) // (num_blocks_0 * num_blocks_1)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    offset_1 = pid_1 * _BLOCK_SIZE_1
    offset_2 = pid_2 * _BLOCK_SIZE_2
    load = x_desc.load([offset_0, offset_1, offset_2])
    load_1 = bias1_desc.load([0, offset_1, offset_2])
    v_0 = load + load_1
    load_2 = bias2_desc.load([offset_0, 0, offset_2])
    v_1 = v_0 + load_2
    out_desc.store([offset_0, offset_1, offset_2], v_1)

def broadcast_add_3d(x: torch.Tensor, bias1: torch.Tensor, bias2: torch.Tensor, *, _launcher=_default_launcher):
    d0, d1, d2 = x.size()
    out = torch.empty_like(x)
    _BLOCK_SIZE_0 = 8
    _BLOCK_SIZE_1 = 8
    _BLOCK_SIZE_2 = 8
    _launcher(_broadcast_add_3d_kernel, (triton.cdiv(d0, _BLOCK_SIZE_0) * triton.cdiv(d1, _BLOCK_SIZE_1) * triton.cdiv(d2, _BLOCK_SIZE_2),), x, bias1, bias2, out, bias1.size(1), bias1.size(2), bias2.size(0), bias2.size(2), out.size(0), out.size(1), out.size(2), x.size(0), x.size(1), x.size(2), bias1.stride(0), bias1.stride(1), bias1.stride(2), bias2.stride(0), bias2.stride(1), bias2.stride(2), out.stride(0), out.stride(1), out.stride(2), x.stride(0), x.stride(1), x.stride(2), d0, d1, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

--- assertExpectedJournal(TestIndexing.test_ellipsis_indexing_block_ptr)
from __future__ import annotations

import torch
import helion
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

helion.runtime.set_triton_allocator()

@triton.jit
def _kernel_kernel(src, dst, dst_size_0, dst_size_1, dst_size_2, src_size_0, src_size_1, src_size_2, dst_stride_0, dst_stride_1, dst_stride_2, src_stride_0, src_stride_1, src_stride_2, _RDIM_SIZE_1: tl.constexpr, _RDIM_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    tl.store(tl.make_block_ptr(dst, [dst_size_0, dst_size_1, dst_size_2], [dst_stride_0, dst_stride_1, dst_stride_2], [0, 0, offset_0], [_RDIM_SIZE_1, _RDIM_SIZE_2, 1], [2, 1, 0]), 1.0, boundary_check=[0, 1])
    load = tl.reshape(tl.load(tl.make_block_ptr(dst, [dst_size_0, dst_size_1, dst_size_2], [dst_stride_0, dst_stride_1, dst_stride_2], [0, 0, offset_0], [_RDIM_SIZE_1, _RDIM_SIZE_2, 1], [2, 1, 0]), boundary_check=[0, 1], padding_option='zero'), [_RDIM_SIZE_1, _RDIM_SIZE_2])
    tl.store(tl.make_block_ptr(src, [src_size_0, src_size_1, src_size_2], [src_stride_0, src_stride_1, src_stride_2], [0, 0, offset_0], [_RDIM_SIZE_1, _RDIM_SIZE_2, 1], [2, 1, 0]), tl.reshape(load, [_RDIM_SIZE_1, _RDIM_SIZE_2, 1]), boundary_check=[0, 1])

def kernel(src: torch.Tensor, dst: torch.Tensor, *, _launcher=_default_launcher):
    N = src.shape[-1]
    _RDIM_SIZE_1 = triton.next_power_of_2(src.size(0))
    _RDIM_SIZE_2 = triton.next_power_of_2(dst.size(1))
    _launcher(_kernel_kernel, (N,), src, dst, dst.size(0), dst.size(1), dst.size(2), src.size(0), src.size(1), src.size(2), dst.stride(0), dst.stride(1), dst.stride(2), src.stride(0), src.stride(1), src.stride(2), _RDIM_SIZE_1, _RDIM_SIZE_2, num_warps=4, num_stages=3)
    return (src, dst)

--- assertExpectedJournal(TestIndexing.test_ellipsis_indexing_tensor_descriptor)
from __future__ import annotations

import torch
import helion
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

helion.runtime.set_triton_allocator()

@triton.jit
def _kernel_kernel(src, dst, dst_size_1, src_size_0, dst_stride_0, dst_stride_1, dst_stride_2, src_stride_0, src_stride_1, src_stride_2, _RDIM_SIZE_1: tl.constexpr, _RDIM_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < src_size_0
    indices_2 = tl.arange(0, _RDIM_SIZE_2).to(tl.int32)
    mask_2 = indices_2 < dst_size_1
    tl.store(dst + (indices_1[:, None] * dst_stride_0 + indices_2[None, :] * dst_stride_1 + offset_0 * dst_stride_2), 1.0, mask_1[:, None] & mask_2[None, :])
    load = tl.load(dst + (indices_1[:, None] * dst_stride_0 + indices_2[None, :] * dst_stride_1 + offset_0 * dst_stride_2), mask_1[:, None] & mask_2[None, :], other=0)
    tl.store(src + (indices_1[:, None] * src_stride_0 + indices_2[None, :] * src_stride_1 + offset_0 * src_stride_2), load, mask_1[:, None] & mask_2[None, :])

def kernel(src: torch.Tensor, dst: torch.Tensor, *, _launcher=_default_launcher):
    N = src.shape[-1]
    _RDIM_SIZE_1 = triton.next_power_of_2(src.size(0))
    _RDIM_SIZE_2 = triton.next_power_of_2(dst.size(1))
    _launcher(_kernel_kernel, (N,), src, dst, dst.size(1), src.size(0), dst.stride(0), dst.stride(1), dst.stride(2), src.stride(0), src.stride(1), src.stride(2), _RDIM_SIZE_1, _RDIM_SIZE_2, num_warps=4, num_stages=3)
    return (src, dst)

--- assertExpectedJournal(TestIndexing.test_mask_load)
from __future__ import annotations

import torch
import helion
import triton
import triton.language as tl
from torch._inductor.runtime.triton_compat import libdevice
from helion.runtime import default_launcher as _default_launcher

helion.runtime.set_triton_allocator()

@triton.jit
def _masked_load_kernel(x, out, x_size_0, out_stride_0, x_stride_0, _BLOCK_SIZE_0: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < x_size_0
    v_0 = tl.full([], 2, tl.int32)
    v_1 = indices_0 % v_0
    v_2 = tl.full([], 0, tl.int32)
    v_3 = v_1 != v_2
    v_4 = libdevice.signbit(v_1) != 0 if v_1.dtype is tl.float32 else v_1 < 0
    v_5 = libdevice.signbit(v_0) != 0 if v_0.dtype is tl.float32 else v_0 < 0
    v_6 = v_4 != v_5
    v_7 = v_3 & v_6
    v_8 = v_1 + v_0
    v_9 = tl.where(v_7, v_8, v_1)
    v_10 = tl.full([], 0, tl.int32)
    v_11 = v_9 == v_10
    load = tl.load(x + indices_0 * x_stride_0, mask_0 & v_11, other=0)
    tl.store(out + indices_0 * out_stride_0, load, mask_0)

def masked_load(x: torch.Tensor, *, _launcher=_default_launcher):
    out = torch.zeros_like(x)
    _BLOCK_SIZE_0 = 16
    _launcher(_masked_load_kernel, (triton.cdiv(x.size(0), _BLOCK_SIZE_0),), x, out, x.size(0), out.stride(0), x.stride(0), _BLOCK_SIZE_0, num_warps=4, num_stages=3)
    return out

--- assertExpectedJournal(TestIndexing.test_mask_store)
from __future__ import annotations

import torch
import helion
import triton
import triton.language as tl
from torch._inductor.runtime.triton_compat import libdevice
from helion.runtime import default_launcher as _default_launcher

helion.runtime.set_triton_allocator()

@triton.jit
def _masked_store_kernel(x, out, x_size_0, out_stride_0, x_stride_0, _BLOCK_SIZE_0: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < x_size_0
    load = tl.load(x + indices_0 * x_stride_0, mask_0, other=0)
    v_0 = tl.full([], 2, tl.int32)
    v_1 = indices_0 % v_0
    v_2 = tl.full([], 0, tl.int32)
    v_3 = v_1 != v_2
    v_4 = libdevice.signbit(v_1) != 0 if v_1.dtype is tl.float32 else v_1 < 0
    v_5 = libdevice.signbit(v_0) != 0 if v_0.dtype is tl.float32 else v_0 < 0
    v_6 = v_4 != v_5
    v_7 = v_3 & v_6
    v_8 = v_1 + v_0
    v_9 = tl.where(v_7, v_8, v_1)
    v_10 = tl.full([], 0, tl.int32)
    v_11 = v_9 == v_10
    tl.store(out + indices_0 * out_stride_0, load, mask_0 & v_11)

def masked_store(x: torch.Tensor, *, _launcher=_default_launcher):
    out = torch.zeros_like(x)
    _BLOCK_SIZE_0 = 16
    _launcher(_masked_store_kernel, (triton.cdiv(x.size(0), _BLOCK_SIZE_0),), x, out, x.size(0), out.stride(0), x.stride(0), _BLOCK_SIZE_0, num_warps=4, num_stages=3)
    return out

--- assertExpectedJournal(TestIndexing.test_multi_dim_slice_block_ptr)
from __future__ import annotations

import torch
import helion
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

helion.runtime.set_triton_allocator()

@triton.jit
def _kernel_kernel(src, dst, dst_size_0, dst_size_1, dst_size_2, src_size_0, src_size_1, src_size_2, dst_stride_0, dst_stride_1, dst_stride_2, src_stride_0, src_stride_1, src_stride_2, _RDIM_SIZE_1: tl.constexpr, _RDIM_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    tl.store(tl.make_block_ptr(dst, [dst_size_0, dst_size_1, dst_size_2], [dst_stride_0, dst_stride_1, dst_stride_2], [0, 0, offset_0], [_RDIM_SIZE_1, _RDIM_SIZE_2, 1], [2, 1, 0]), 1.0, boundary_check=[0, 1])
    load = tl.reshape(tl.load(tl.make_block_ptr(dst, [dst_size_0, dst_size_1, dst_size_2], [dst_stride_0, dst_stride_1, dst_stride_2], [0, 0, offset_0], [_RDIM_SIZE_1, _RDIM_SIZE_2, 1], [2, 1, 0]), boundary_check=[0, 1], padding_option='zero'), [_RDIM_SIZE_1, _RDIM_SIZE_2])
    tl.store(tl.make_block_ptr(src, [src_size_0, src_size_1, src_size_2], [src_stride_0, src_stride_1, src_stride_2], [0, 0, offset_0], [_RDIM_SIZE_1, _RDIM_SIZE_2, 1], [2, 1, 0]), tl.reshape(load, [_RDIM_SIZE_1, _RDIM_SIZE_2, 1]), boundary_check=[0, 1])

def kernel(src: torch.Tensor, dst: torch.Tensor, *, _launcher=_default_launcher):
    N = src.shape[-1]
    _RDIM_SIZE_1 = triton.next_power_of_2(src.size(0))
    _RDIM_SIZE_2 = triton.next_power_of_2(dst.size(1))
    _launcher(_kernel_kernel, (N,), src, dst, dst.size(0), dst.size(1), dst.size(2), src.size(0), src.size(1), src.size(2), dst.stride(0), dst.stride(1), dst.stride(2), src.stride(0), src.stride(1), src.stride(2), _RDIM_SIZE_1, _RDIM_SIZE_2, num_warps=4, num_stages=3)
    return (src, dst)

--- assertExpectedJournal(TestIndexing.test_multi_dim_slice_tensor_descriptor)
from __future__ import annotations

import torch
import helion
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

helion.runtime.set_triton_allocator()

@triton.jit
def _kernel_kernel(src, dst, dst_size_1, src_size_0, dst_stride_0, dst_stride_1, dst_stride_2, src_stride_0, src_stride_1, src_stride_2, _RDIM_SIZE_1: tl.constexpr, _RDIM_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < src_size_0
    indices_2 = tl.arange(0, _RDIM_SIZE_2).to(tl.int32)
    mask_2 = indices_2 < dst_size_1
    tl.store(dst + (indices_1[:, None] * dst_stride_0 + indices_2[None, :] * dst_stride_1 + offset_0 * dst_stride_2), 1.0, mask_1[:, None] & mask_2[None, :])
    load = tl.load(dst + (indices_1[:, None] * dst_stride_0 + indices_2[None, :] * dst_stride_1 + offset_0 * dst_stride_2), mask_1[:, None] & mask_2[None, :], other=0)
    tl.store(src + (indices_1[:, None] * src_stride_0 + indices_2[None, :] * src_stride_1 + offset_0 * src_stride_2), load, mask_1[:, None] & mask_2[None, :])

def kernel(src: torch.Tensor, dst: torch.Tensor, *, _launcher=_default_launcher):
    N = src.shape[-1]
    _RDIM_SIZE_1 = triton.next_power_of_2(src.size(0))
    _RDIM_SIZE_2 = triton.next_power_of_2(dst.size(1))
    _launcher(_kernel_kernel, (N,), src, dst, dst.size(1), src.size(0), dst.stride(0), dst.stride(1), dst.stride(2), src.stride(0), src.stride(1), src.stride(2), _RDIM_SIZE_1, _RDIM_SIZE_2, num_warps=4, num_stages=3)
    return (src, dst)

--- assertExpectedJournal(TestIndexing.test_negative_indexing_block_ptr)
from __future__ import annotations

import torch
import helion
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

helion.runtime.set_triton_allocator()

@triton.jit
def _kernel_kernel(dst, src, dst_size_0, src_size_0, dst_stride_0, src_stride_0):
    tl.store(dst + (-1 + dst_size_0) * dst_stride_0, 1.0, None)
    load = tl.load(dst + (-1 + dst_size_0) * dst_stride_0, None)
    tl.store(src + (-1 + src_size_0) * src_stride_0, load, None)

def kernel(src: torch.Tensor, dst: torch.Tensor, *, _launcher=_default_launcher):
    _launcher(_kernel_kernel, (1,), dst, src, dst.size(0), src.size(0), dst.stride(0), src.stride(0), num_warps=4, num_stages=3)
    return (src, dst)

--- assertExpectedJournal(TestIndexing.test_negative_indexing_tensor_descriptor)
from __future__ import annotations

import torch
import helion
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

helion.runtime.set_triton_allocator()

@triton.jit
def _kernel_kernel(dst, src, dst_size_0, src_size_0, dst_stride_0, src_stride_0):
    tl.store(dst + (-1 + dst_size_0) * dst_stride_0, 1.0, None)
    load = tl.load(dst + (-1 + dst_size_0) * dst_stride_0, None)
    tl.store(src + (-1 + src_size_0) * src_stride_0, load, None)

def kernel(src: torch.Tensor, dst: torch.Tensor, *, _launcher=_default_launcher):
    _launcher(_kernel_kernel, (1,), dst, src, dst.size(0), src.size(0), dst.stride(0), src.stride(0), num_warps=4, num_stages=3)
    return (src, dst)

--- assertExpectedJournal(TestIndexing.test_pairwise_add)
from __future__ import annotations

import torch
import helion
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

helion.runtime.set_triton_allocator()

@triton.jit
def _pairwise_add_kernel(out, x, out_size_0, out_stride_0, x_stride_0, _BLOCK_SIZE_0: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < out_size_0
    load = tl.load(x + indices_0 * x_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    load_1 = tl.load(x + v_1 * x_stride_0, mask_0, other=0)
    v_2 = load + load_1
    tl.store(out + indices_0 * out_stride_0, v_2, mask_0)

def pairwise_add(x: torch.Tensor, *, _launcher=_default_launcher):
    out = x.new_empty([x.size(0) - 1])
    _BLOCK_SIZE_0 = 32
    _launcher(_pairwise_add_kernel, (triton.cdiv(out.size(0), _BLOCK_SIZE_0),), out, x, out.size(0), out.stride(0), x.stride(0), _BLOCK_SIZE_0, num_warps=4, num_stages=3)
    return out

--- assertExpectedJournal(TestIndexing.test_range_slice_block_ptr)
from __future__ import annotations

import torch
import helion
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

helion.runtime.set_triton_allocator()

@triton.jit
def _kernel_kernel(dst, src, dst_stride_0, src_stride_0, _RDIM_SIZE_1: tl.constexpr):
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < 10
    tl.store(dst + (10 + indices_1) * dst_stride_0, 1.0, mask_1)
    load = tl.load(dst + (10 + indices_1) * dst_stride_0, mask_1, other=0)
    tl.store(src + (10 + indices_1) * src_stride_0, load, mask_1)

def kernel(src: torch.Tensor, dst: torch.Tensor, *, _launcher=_default_launcher):
    _RDIM_SIZE_1 = 16
    _launcher(_kernel_kernel, (1,), dst, src, dst.stride(0), src.stride(0), _RDIM_SIZE_1, num_warps=4, num_stages=3)
    return (src, dst)

--- assertExpectedJournal(TestIndexing.test_range_slice_tensor_descriptor)
from __future__ import annotations

import torch
import helion
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

helion.runtime.set_triton_allocator()

@triton.jit
def _kernel_kernel(dst, src, dst_stride_0, src_stride_0, _RDIM_SIZE_1: tl.constexpr):
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < 10
    tl.store(dst + (10 + indices_1) * dst_stride_0, 1.0, mask_1)
    load = tl.load(dst + (10 + indices_1) * dst_stride_0, mask_1, other=0)
    tl.store(src + (10 + indices_1) * src_stride_0, load, mask_1)

def kernel(src: torch.Tensor, dst: torch.Tensor, *, _launcher=_default_launcher):
    _RDIM_SIZE_1 = 16
    _launcher(_kernel_kernel, (1,), dst, src, dst.stride(0), src.stride(0), _RDIM_SIZE_1, num_warps=4, num_stages=3)
    return (src, dst)

--- assertExpectedJournal(TestIndexing.test_tensor_value_block_ptr)
from __future__ import annotations

import torch
import helion
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

helion.runtime.set_triton_allocator()

@triton.jit
def _kernel_kernel(dst, val, src, dst_size_0, dst_size_1, src_size_0, src_size_1, val_size_0, dst_stride_0, dst_stride_1, src_stride_0, src_stride_1, val_stride_0, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    tl.store(tl.make_block_ptr(dst, [dst_size_0, dst_size_1], [dst_stride_0, dst_stride_1], [offset_0, 0], [1, _RDIM_SIZE_1], [1, 0]), tl.reshape(tl.load(tl.make_block_ptr(val, [val_size_0], [val_stride_0], [0], [_RDIM_SIZE_1], [0]), boundary_check=[0], padding_option='zero'), [1, _RDIM_SIZE_1]), boundary_check=[1])
    load = tl.reshape(tl.load(tl.make_block_ptr(dst, [dst_size_0, dst_size_1], [dst_stride_0, dst_stride_1], [offset_0, 0], [1, _RDIM_SIZE_1], [1, 0]), boundary_check=[1], padding_option='zero'), [_RDIM_SIZE_1])
    tl.store(tl.make_block_ptr(src, [src_size_0, src_size_1], [src_stride_0, src_stride_1], [offset_0, 0], [1, _RDIM_SIZE_1], [1, 0]), tl.reshape(load, [1, _RDIM_SIZE_1]), boundary_check=[1])

def kernel(src: torch.Tensor, dst: torch.Tensor, val: torch.Tensor, *, _launcher=_default_launcher):
    N = src.shape[0]
    _RDIM_SIZE_1 = triton.next_power_of_2(dst.size(1))
    _launcher(_kernel_kernel, (N,), dst, val, src, dst.size(0), dst.size(1), src.size(0), src.size(1), val.size(0), dst.stride(0), dst.stride(1), src.stride(0), src.stride(1), val.stride(0), _RDIM_SIZE_1, num_warps=4, num_stages=3)
    return (src, dst)

--- assertExpectedJournal(TestIndexing.test_tensor_value_tensor_descriptor)
from __future__ import annotations

import torch
import helion
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

helion.runtime.set_triton_allocator()

@triton.jit
def _kernel_kernel(dst, val, src, dst_size_1, dst_stride_0, dst_stride_1, src_stride_0, src_stride_1, val_stride_0, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < dst_size_1
    tl.store(dst + (offset_0 * dst_stride_0 + indices_1 * dst_stride_1), tl.load(val + indices_1 * val_stride_0, mask_1), mask_1)
    load = tl.load(dst + (offset_0 * dst_stride_0 + indices_1 * dst_stride_1), mask_1, other=0)
    tl.store(src + (offset_0 * src_stride_0 + indices_1 * src_stride_1), load, mask_1)

def kernel(src: torch.Tensor, dst: torch.Tensor, val: torch.Tensor, *, _launcher=_default_launcher):
    N = src.shape[0]
    _RDIM_SIZE_1 = triton.next_power_of_2(dst.size(1))
    _launcher(_kernel_kernel, (N,), dst, val, src, dst.size(1), dst.stride(0), dst.stride(1), src.stride(0), src.stride(1), val.stride(0), _RDIM_SIZE_1, num_warps=4, num_stages=3)
    return (src, dst)
