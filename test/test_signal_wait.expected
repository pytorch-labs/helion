This file is automatically generated by assertExpectedJournal calls in test_signal_wait.py.
Update expected outputs by running tests with the EXPECTTEST_ACCEPT=1 environment variable set.

--- assertExpectedJournal(TestWait.test_global_sync)
from __future__ import annotations

import torch
import helion
import triton
import triton.language as tl

@triton.jit
def _gmem_multi_bar_sync_kernel_kernel(signal_pad, signal_pad_stride_0, signal_pad_stride_1, N, _BLOCK_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    for offset_1 in tl.range(0, N.to(tl.int32), step=_BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        helion.runtime.triton_send_signal(addr=signal_pad + (indices_1 * signal_pad_stride_0 + offset_0 * signal_pad_stride_1), update=1, sem='release', scope='gpu', op='atomic_xchg', skip_sync=True)
        helion.runtime.triton_wait_multiple_signal(addr=signal_pad + (offset_0 * signal_pad_stride_0 + indices_1 * signal_pad_stride_1), expect=1, update=0, sem='acquire', scope='gpu', op='ld', skip_sync=False)

def gmem_multi_bar_sync_kernel(signal_pad: torch.Tensor):
    M, N = signal_pad.shape
    assert M == N
    _BLOCK_SIZE_1 = N
    _gmem_multi_bar_sync_kernel_kernel[N,](signal_pad, signal_pad.stride(0), signal_pad.stride(1), N, _BLOCK_SIZE_1, num_warps=4, num_stages=3)
    return signal_pad

def _gmem_multi_bar_sync_kernel_make_precompiler(signal_pad: torch.Tensor):
    M, N = signal_pad.shape
    assert M == N
    _BLOCK_SIZE_1 = N
    from helion.runtime.precompile_shim import make_precompiler
    return make_precompiler(_gmem_multi_bar_sync_kernel_kernel)(signal_pad, signal_pad.stride(0), signal_pad.stride(1), N, _BLOCK_SIZE_1, num_warps=4, num_stages=3)

--- assertExpectedJournal(TestWait.test_signal_basic)
from __future__ import annotations

import torch
import helion
import triton
import triton.language as tl

@triton.jit
def _gmem_signal_scalar_bar_kernel_kernel(signal_pad, signal_pad_stride_0):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    helion.runtime.triton_send_signal(addr=signal_pad + offset_0 * signal_pad_stride_0, update=1, sem='release', scope='gpu', op='atomic_xchg', skip_sync=False)

def gmem_signal_scalar_bar_kernel(signal_pad: torch.Tensor):
    n, = signal_pad.shape
    _gmem_signal_scalar_bar_kernel_kernel[n,](signal_pad, signal_pad.stride(0), num_warps=4, num_stages=3)
    return signal_pad

def _gmem_signal_scalar_bar_kernel_make_precompiler(signal_pad: torch.Tensor):
    n, = signal_pad.shape
    from helion.runtime.precompile_shim import make_precompiler
    return make_precompiler(_gmem_signal_scalar_bar_kernel_kernel)(signal_pad, signal_pad.stride(0), num_warps=4, num_stages=3)

--- assertExpectedJournal(TestWait.test_signal_cas)
from __future__ import annotations

import torch
import helion
import triton
import triton.language as tl

@triton.jit
def _gmem_signal_cas_kernel_kernel(signal_pad, signal_pad_stride_0):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    helion.runtime.triton_wait_signal(addr=signal_pad + offset_0 * signal_pad_stride_0, expect=0, update=1, sem='release', scope='gpu', op='atomic_cas', skip_sync=True, sync_before=not False)

def gmem_signal_cas_kernel(signal_pad: torch.Tensor):
    n, = signal_pad.shape
    _gmem_signal_cas_kernel_kernel[n,](signal_pad, signal_pad.stride(0), num_warps=4, num_stages=3)
    return signal_pad

def _gmem_signal_cas_kernel_make_precompiler(signal_pad: torch.Tensor):
    n, = signal_pad.shape
    from helion.runtime.precompile_shim import make_precompiler
    return make_precompiler(_gmem_signal_cas_kernel_kernel)(signal_pad, signal_pad.stride(0), num_warps=4, num_stages=3)

--- assertExpectedJournal(TestWait.test_signal_multiple)
from __future__ import annotations

import torch
import helion
import triton
import triton.language as tl

@triton.jit
def _gmem_signal_tensor_bar_kernel_kernel(signal_pad, signal_pad_stride_0, _BLOCK_SIZE_0: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    helion.runtime.triton_send_signal(addr=signal_pad + indices_0 * signal_pad_stride_0, update=1, sem='release', scope='gpu', op='atomic_xchg', skip_sync=False)

def gmem_signal_tensor_bar_kernel(signal_pad: torch.Tensor):
    n, = signal_pad.shape
    _BLOCK_SIZE_0 = 4
    _gmem_signal_tensor_bar_kernel_kernel[triton.cdiv(n, _BLOCK_SIZE_0),](signal_pad, signal_pad.stride(0), _BLOCK_SIZE_0, num_warps=4, num_stages=3)
    return signal_pad

def _gmem_signal_tensor_bar_kernel_make_precompiler(signal_pad: torch.Tensor):
    n, = signal_pad.shape
    _BLOCK_SIZE_0 = 4
    from helion.runtime.precompile_shim import make_precompiler
    return make_precompiler(_gmem_signal_tensor_bar_kernel_kernel)(signal_pad, signal_pad.stride(0), _BLOCK_SIZE_0, num_warps=4, num_stages=3)

--- assertExpectedJournal(TestWait.test_signal_multiple_cas)
from __future__ import annotations

import torch
import helion
import triton
import triton.language as tl

@triton.jit
def _gmem_signal_tensor_bar_kernel_kernel(signal_pad, signal_pad_stride_0, _BLOCK_SIZE_0: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    helion.runtime.triton_wait_multiple_signal(addr=signal_pad + indices_0 * signal_pad_stride_0, expect=0, update=1, sem='release', scope='gpu', op='atomic_cas', skip_sync=True, sync_before=not False)

def gmem_signal_tensor_bar_kernel(signal_pad: torch.Tensor):
    n, = signal_pad.shape
    _BLOCK_SIZE_0 = 4
    _gmem_signal_tensor_bar_kernel_kernel[triton.cdiv(n, _BLOCK_SIZE_0),](signal_pad, signal_pad.stride(0), _BLOCK_SIZE_0, num_warps=4, num_stages=3)
    return signal_pad

def _gmem_signal_tensor_bar_kernel_make_precompiler(signal_pad: torch.Tensor):
    n, = signal_pad.shape
    _BLOCK_SIZE_0 = 4
    from helion.runtime.precompile_shim import make_precompiler
    return make_precompiler(_gmem_signal_tensor_bar_kernel_kernel)(signal_pad, signal_pad.stride(0), _BLOCK_SIZE_0, num_warps=4, num_stages=3)

--- assertExpectedJournal(TestWait.test_wait_2d_tile)
from __future__ import annotations

import torch
import helion
import triton
import triton.language as tl

@triton.jit
def _wait_for_2d_tile_kernel_kernel(signal_pad, x, out, out_stride_0, out_stride_1, signal_pad_stride_0, signal_pad_stride_1, x_stride_0, x_stride_1, n, m, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr):
    num_blocks_0 = tl.cdiv(n, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < n
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    mask_1 = indices_1 < m
    tile_id = offset_0 // _BLOCK_SIZE_0
    tile_id_1 = offset_1 // _BLOCK_SIZE_1
    helion.runtime.triton_wait_signal(addr=signal_pad + (tile_id * signal_pad_stride_0 + tile_id_1 * signal_pad_stride_1), expect=1, update=0, sem='acquire', scope='gpu', op='ld', skip_sync=False)
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), load, mask_0[:, None] & mask_1[None, :])

def wait_for_2d_tile_kernel(signal_pad: torch.Tensor, x: torch.Tensor):
    out = torch.empty_like(x)
    n, m = x.shape
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 16
    _wait_for_2d_tile_kernel_kernel[triton.cdiv(n, _BLOCK_SIZE_0) * triton.cdiv(m, _BLOCK_SIZE_1),](signal_pad, x, out, out.stride(0), out.stride(1), signal_pad.stride(0), signal_pad.stride(1), x.stride(0), x.stride(1), n, m, _BLOCK_SIZE_0, _BLOCK_SIZE_1, num_warps=4, num_stages=3)
    return out

def _wait_for_2d_tile_kernel_make_precompiler(signal_pad: torch.Tensor, x: torch.Tensor):
    out = torch.empty_like(x)
    n, m = x.shape
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 16
    from helion.runtime.precompile_shim import make_precompiler
    return make_precompiler(_wait_for_2d_tile_kernel_kernel)(signal_pad, x, out, out.stride(0), out.stride(1), signal_pad.stride(0), signal_pad.stride(1), x.stride(0), x.stride(1), n, m, _BLOCK_SIZE_0, _BLOCK_SIZE_1, num_warps=4, num_stages=3)

--- assertExpectedJournal(TestWait.test_wait_basic)
from __future__ import annotations

import torch
import helion
import triton
import triton.language as tl

@triton.jit
def _gmem_wait_kernel_kernel(signal_pad, out, out_stride_0, signal_pad_stride_0):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    helion.runtime.triton_wait_signal(addr=signal_pad + offset_0 * signal_pad_stride_0, expect=1, update=0, sem='acquire', scope='gpu', op='ld', skip_sync=False)
    tl.store(out + offset_0 * out_stride_0, offset_0, None)

def gmem_wait_kernel(signal_pad: torch.Tensor):
    out = torch.empty_like(signal_pad)
    n, = signal_pad.shape
    _gmem_wait_kernel_kernel[n,](signal_pad, out, out.stride(0), signal_pad.stride(0), num_warps=4, num_stages=3)
    return out

def _gmem_wait_kernel_make_precompiler(signal_pad: torch.Tensor):
    out = torch.empty_like(signal_pad)
    n, = signal_pad.shape
    from helion.runtime.precompile_shim import make_precompiler
    return make_precompiler(_gmem_wait_kernel_kernel)(signal_pad, out, out.stride(0), signal_pad.stride(0), num_warps=4, num_stages=3)

--- assertExpectedJournal(TestWait.test_wait_multi_bar)
from __future__ import annotations

import torch
import helion
import triton
import triton.language as tl

import test.test_signal_wait as _source_module

@triton.jit
def _gmem_wait_multi_bar_kernel_kernel(signal_pad, out, out_stride_0, signal_pad_stride_0, _BLOCK_SIZE_0: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    helion.runtime.triton_wait_multiple_signal(addr=signal_pad + indices_0 * signal_pad_stride_0, expect=1, update=0, sem='acquire', scope='gpu', op='ld', skip_sync=False)
    tile_id = offset_0 // _BLOCK_SIZE_0
    tl.store(out + tile_id * out_stride_0, tile_id, None)

def gmem_wait_multi_bar_kernel(signal_pad: torch.Tensor):
    N, = signal_pad.shape
    n = 4
    out = torch.empty(n, dtype=torch.int32, device=_source_module.DEVICE)
    _BLOCK_SIZE_0 = 4
    _gmem_wait_multi_bar_kernel_kernel[triton.cdiv(N, _BLOCK_SIZE_0),](signal_pad, out, out.stride(0), signal_pad.stride(0), _BLOCK_SIZE_0, num_warps=4, num_stages=3)
    return out

def _gmem_wait_multi_bar_kernel_make_precompiler(signal_pad: torch.Tensor):
    N, = signal_pad.shape
    n = 4
    out = torch.empty(n, dtype=torch.int32, device=_source_module.DEVICE)
    _BLOCK_SIZE_0 = 4
    from helion.runtime.precompile_shim import make_precompiler
    return make_precompiler(_gmem_wait_multi_bar_kernel_kernel)(signal_pad, out, out.stride(0), signal_pad.stride(0), _BLOCK_SIZE_0, num_warps=4, num_stages=3)

--- assertExpectedJournal(TestWait.test_wait_multi_bar_cas)
from __future__ import annotations

import torch
import helion
import triton
import triton.language as tl

@triton.jit
def _gmem_wait_multi_bar_kernel_cas_kernel(signal_pad, signal_pad_stride_0, _BLOCK_SIZE_0: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    helion.runtime.triton_wait_multiple_signal(addr=signal_pad + indices_0 * signal_pad_stride_0, expect=1, update=2, sem='acquire', scope='gpu', op='atomic_cas', skip_sync=False)

def gmem_wait_multi_bar_kernel_cas(signal_pad: torch.Tensor):
    N, = signal_pad.shape
    n = 4
    _BLOCK_SIZE_0 = 4
    _gmem_wait_multi_bar_kernel_cas_kernel[triton.cdiv(N, _BLOCK_SIZE_0),](signal_pad, signal_pad.stride(0), _BLOCK_SIZE_0, num_warps=4, num_stages=3)
    return signal_pad

def _gmem_wait_multi_bar_kernel_cas_make_precompiler(signal_pad: torch.Tensor):
    N, = signal_pad.shape
    n = 4
    _BLOCK_SIZE_0 = 4
    from helion.runtime.precompile_shim import make_precompiler
    return make_precompiler(_gmem_wait_multi_bar_kernel_cas_kernel)(signal_pad, signal_pad.stride(0), _BLOCK_SIZE_0, num_warps=4, num_stages=3)
