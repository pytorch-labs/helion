Running 6 kernels...


============================================================
Kernel: jagged_mean
============================================================

Running jagged_mean benchmark with Helion implementation...


  0%|          | 0/100 [00:00<?, ?it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 34.09ms to get benchmark function for torch_compile_nested_tensor_integration
/home/willfeng/local/pytorch-nightly/torch/backends/cuda/__init__.py:131: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:78.)
  return torch._C._get_cublas_allow_tf32()
INFO:tritonbench.utils.triton_op:Took 0.04ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

  1%|          | 1/100 [00:06<11:28,  6.95s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.34ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.54ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

  2%|▏         | 2/100 [00:08<05:52,  3.60s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.30ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.56ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

  3%|▎         | 3/100 [00:09<04:05,  2.53s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.30ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.57ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

  4%|▍         | 4/100 [00:10<03:14,  2.03s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.32ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.58ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

  5%|▌         | 5/100 [00:11<02:46,  1.75s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.30ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.60ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

  6%|▌         | 6/100 [00:13<02:28,  1.58s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.32ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.60ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

  7%|▋         | 7/100 [00:14<02:17,  1.47s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.30ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.60ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

  8%|▊         | 8/100 [00:15<02:09,  1.41s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.30ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.61ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

  9%|▉         | 9/100 [00:17<02:05,  1.37s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.30ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.61ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 10%|█         | 10/100 [00:18<02:00,  1.34s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.32ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.62ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 11%|█         | 11/100 [00:19<01:56,  1.31s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.30ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.59ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 12%|█▏        | 12/100 [00:20<01:53,  1.29s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.33ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.58ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 13%|█▎        | 13/100 [00:22<01:51,  1.28s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.32ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.60ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 14%|█▍        | 14/100 [00:23<01:49,  1.27s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.29ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.60ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 15%|█▌        | 15/100 [00:24<01:47,  1.27s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.31ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.61ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 16%|█▌        | 16/100 [00:25<01:46,  1.26s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.30ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.61ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 17%|█▋        | 17/100 [00:27<01:44,  1.26s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.30ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.60ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 18%|█▊        | 18/100 [00:28<01:43,  1.26s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.02ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.28ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.60ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 19%|█▉        | 19/100 [00:29<01:42,  1.26s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.28ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.62ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 20%|██        | 20/100 [00:30<01:40,  1.26s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.30ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.63ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 21%|██        | 21/100 [00:32<01:41,  1.28s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.30ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.62ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 22%|██▏       | 22/100 [00:33<01:39,  1.28s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.02ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.41ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.63ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 23%|██▎       | 23/100 [00:34<01:38,  1.27s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.30ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.61ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 24%|██▍       | 24/100 [00:35<01:36,  1.27s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.31ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.63ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 25%|██▌       | 25/100 [00:37<01:34,  1.26s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.32ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.62ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 26%|██▌       | 26/100 [00:38<01:33,  1.26s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.32ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.62ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 27%|██▋       | 27/100 [00:39<01:31,  1.26s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.30ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.60ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 28%|██▊       | 28/100 [00:40<01:30,  1.26s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.32ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.61ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 29%|██▉       | 29/100 [00:42<01:37,  1.38s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.32ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.61ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 30%|███       | 30/100 [00:43<01:33,  1.34s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.81ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.66ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 31%|███       | 31/100 [00:50<03:18,  2.87s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.31ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.65ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 32%|███▏      | 32/100 [00:51<02:42,  2.39s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.32ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.65ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 33%|███▎      | 33/100 [00:52<02:17,  2.05s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
Removed 1 outliers from 388 samples
Removed 18 outliers from 490 samples
Removed 39 outliers from 727 samples
Removed 1 outliers from 439 samples
Removed 1 outliers from 442 samples
Removed 2 outliers from 337 samples
Removed 20 outliers from 773 samples
Removed 4 outliers from 451 samples
Removed 16 outliers from 669 samples
Removed 6 outliers from 738 samples
Removed 1 outliers from 610 samples
Removed 1 outliers from 665 samples
Removed 2 outliers from 395 samples
Removed 25 outliers from 785 samples
Removed 2 outliers from 451 samples
Removed 9 outliers from 638 samples
Removed 44 outliers from 755 samples
Removed 5 outliers from 631 samples
Removed 1 outliers from 663 samples
Removed 3 outliers from 402 samples
Removed 8 outliers from 789 samples
Removed 1 outliers from 441 samples
Removed 9 outliers from 664 samples
Removed 22 outliers from 746 samples
Removed 3 outliers from 605 samples
Removed 6 outliers from 664 samples
Removed 4 outliers from 391 samples
Removed 32 outliers from 796 samples
Removed 2 outliers from 442 samples
Removed 9 outliers from 670 samples
Removed 17 outliers from 749 samples
Removed 2 outliers from 637 samples
Removed 2 outliers from 677 samples
Removed 4 outliers from 394 samples
Removed 3 outliers from 789 samples
Removed 1 outliers from 439 samples
Removed 33 outliers from 675 samples
Removed 16 outliers from 766 samples
Removed 2 outliers from 621 samples
Removed 2 outliers from 655 samples
Removed 4 outliers from 393 samples
Removed 4 outliers from 707 samples
Removed 5 outliers from 441 samples
Removed 25 outliers from 673 samples
Removed 10 outliers from 768 samples
Removed 1 outliers from 626 samples
Removed 1 outliers from 706 samples
Removed 4 outliers from 395 samples
Removed 11 outliers from 705 samples
Removed 2 outliers from 450 samples
Removed 11 outliers from 680 samples
Removed 17 outliers from 770 samples
Removed 1 outliers from 701 samples
Removed 2 outliers from 401 samples
Removed 12 outliers from 725 samples
Removed 1 outliers from 441 samples
Removed 6 outliers from 677 samples
Removed 22 outliers from 765 samples
Removed 2 outliers from 628 samples
Removed 3 outliers from 700 samples
Removed 4 outliers from 367 samples
Removed 15 outliers from 774 samples
Removed 28 outliers from 417 samples
Removed 19 outliers from 683 samples
Removed 19 outliers from 767 samples
Removed 2 outliers from 617 samples
Removed 4 outliers from 683 samples
Removed 4 outliers from 379 samples
Removed 3 outliers from 762 samples
Removed 12 outliers from 440 samples
Removed 21 outliers from 668 samples
Removed 11 outliers from 751 samples
Removed 4 outliers from 606 samples
Removed 5 outliers from 691 samples
Removed 3 outliers from 395 samples
Removed 4 outliers from 649 samples
Removed 12 outliers from 445 samples
Removed 9 outliers from 668 samples
Removed 17 outliers from 747 samples
Removed 1 outliers from 599 samples
Removed 3 outliers from 392 samples
Removed 3 outliers from 635 samples
Removed 20 outliers from 670 samples
Removed 8 outliers from 738 samples
Removed 2 outliers from 618 samples
Removed 2 outliers from 675 samples
Removed 6 outliers from 391 samples
Removed 2 outliers from 670 samples
Removed 40 outliers from 421 samples
Removed 11 outliers from 669 samples
Removed 19 outliers from 738 samples
Removed 3 outliers from 614 samples
Removed 3 outliers from 668 samples
Removed 7 outliers from 389 samples
Removed 8 outliers from 761 samples
Removed 10 outliers from 438 samples
Removed 8 outliers from 666 samples
Removed 21 outliers from 758 samples
Removed 3 outliers from 617 samples
Removed 6 outliers from 667 samples
Removed 5 outliers from 388 samples
Removed 10 outliers from 750 samples
Removed 11 outliers from 443 samples
Removed 8 outliers from 629 samples
Removed 15 outliers from 736 samples
Removed 3 outliers from 707 samples
Removed 3 outliers from 385 samples
Removed 5 outliers from 440 samples
Removed 3 outliers from 436 samples
Removed 9 outliers from 619 samples
Removed 7 outliers from 721 samples
Removed 1 outliers from 619 samples
Removed 4 outliers from 400 samples
Removed 29 outliers from 462 samples
Removed 5 outliers from 436 samples
Removed 3 outliers from 630 samples
Removed 19 outliers from 735 samples
Removed 3 outliers from 614 samples
Removed 3 outliers from 670 samples
Removed 4 outliers from 390 samples
Removed 4 outliers from 497 samples
Removed 5 outliers from 630 samples
Removed 9 outliers from 733 samples
Removed 4 outliers from 622 samples
Removed 3 outliers from 703 samples
Removed 3 outliers from 397 samples
Removed 1 outliers from 581 samples
Removed 11 outliers from 434 samples
Removed 9 outliers from 626 samples
Removed 17 outliers from 728 samples
Removed 6 outliers from 634 samples
Removed 1 outliers from 686 samples
Removed 3 outliers from 397 samples
Removed 12 outliers from 604 samples
Removed 6 outliers from 436 samples
Removed 6 outliers from 638 samples
Removed 16 outliers from 731 samples
Removed 3 outliers from 418 samples
Removed 19 outliers from 621 samples
Removed 9 outliers from 388 samples
Removed 2 outliers from 208 samples
Removed 6 outliers from 435 samples
Removed 6 outliers from 637 samples
Removed 17 outliers from 733 samples
Removed 5 outliers from 578 samples
Removed 15 outliers from 647 samples
Removed 9 outliers from 389 samples
Removed 6 outliers from 236 samples
Removed 7 outliers from 445 samples
Removed 34 outliers from 642 samples
Removed 13 outliers from 741 samples
Removed 1 outliers from 579 samples
Removed 20 outliers from 656 samples
Removed 8 outliers from 374 samples
Removed 7 outliers from 252 samples
Removed 2 outliers from 449 samples
Removed 12 outliers from 637 samples
Removed 15 outliers from 727 samples
Removed 7 outliers from 615 samples
Removed 5 outliers from 659 samples
Removed 8 outliers from 391 samples
Removed 9 outliers from 449 samples
Removed 10 outliers from 638 samples
Removed 18 outliers from 729 samples
Removed 100 outliers from 609 samples
Removed 9 outliers from 712 samples
Removed 6 outliers from 345 samples
Removed 6 outliers from 402 samples
Removed 39 outliers from 641 samples
Removed 31 outliers from 738 samples
Removed 9 outliers from 487 samples
Removed 3 outliers from 537 samples
Removed 8 outliers from 380 samples
Removed 1 outliers from 132 samples
Removed 31 outliers from 407 samples
Removed 6 outliers from 633 samples
Removed 12 outliers from 734 samples
Removed 3 outliers from 481 samples
Removed 4 outliers from 537 samples
Removed 12 outliers from 385 samples
Removed 2 outliers from 129 samples
Removed 8 outliers from 435 samples
Removed 4 outliers from 636 samples
Removed 21 outliers from 736 samples
Removed 1 outliers from 500 samples
Removed 2 outliers from 566 samples
Removed 13 outliers from 391 samples
Removed 1 outliers from 159 samples
Removed 11 outliers from 445 samples
Removed 21 outliers from 636 samples
Removed 66 outliers from 616 samples
Removed 1 outliers from 536 samples
Removed 10 outliers from 620 samples
Removed 12 outliers from 393 samples
Removed 17 outliers from 210 samples
Removed 11 outliers from 437 samples
Removed 9 outliers from 627 samples
Removed 14 outliers from 726 samples
Removed 1 outliers from 573 samples
Removed 79 outliers from 657 samples
Removed 9 outliers from 386 samples
Removed 32 outliers from 269 samples
Removed 7 outliers from 437 samples
Removed 19 outliers from 682 samples
Removed 12 outliers from 766 samples
Removed 14 outliers from 441 samples
Removed 10 outliers from 464 samples
Removed 5 outliers from 298 samples
Removed 6 outliers from 713 samples
Removed 4 outliers from 437 samples
Removed 13 outliers from 683 samples
Removed 24 outliers from 763 samples
Removed 2 outliers from 608 samples
Removed 10 outliers from 659 samples
Removed 15 outliers from 359 samples
Removed 10 outliers from 707 samples
Removed 9 outliers from 453 samples
Removed 14 outliers from 680 samples
Removed 29 outliers from 765 samples
Removed 20 outliers from 582 samples
Removed 15 outliers from 585 samples
Removed 9 outliers from 356 samples
Removed 45 outliers from 744 samples
Removed 12 outliers from 441 samples
Removed 9 outliers from 680 samples
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.30ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.64ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 34%|███▍      | 34/100 [00:54<02:00,  1.82s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.32ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.64ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 35%|███▌      | 35/100 [00:55<01:47,  1.65s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.43ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.64ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 36%|███▌      | 36/100 [00:56<01:38,  1.54s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.29ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.63ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 37%|███▋      | 37/100 [00:57<01:31,  1.46s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.29ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.62ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 38%|███▊      | 38/100 [00:59<01:26,  1.39s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.28ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.63ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 39%|███▉      | 39/100 [01:00<01:22,  1.35s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.31ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.58ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 40%|████      | 40/100 [01:02<01:26,  1.43s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.30ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.64ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 41%|████      | 41/100 [01:03<01:21,  1.38s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.31ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.67ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 42%|████▏     | 42/100 [01:04<01:17,  1.34s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.29ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.67ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 43%|████▎     | 43/100 [01:05<01:15,  1.32s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.29ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.65ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 44%|████▍     | 44/100 [01:07<01:12,  1.30s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.30ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.64ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 45%|████▌     | 45/100 [01:08<01:10,  1.28s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.30ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.63ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 46%|████▌     | 46/100 [01:09<01:08,  1.28s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.29ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.65ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 47%|████▋     | 47/100 [01:10<01:07,  1.27s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.30ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.68ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 48%|████▊     | 48/100 [01:12<01:05,  1.27s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.30ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.64ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 49%|████▉     | 49/100 [01:13<01:04,  1.26s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.29ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.64ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 50%|█████     | 50/100 [01:14<01:02,  1.26s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.35ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.61ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 51%|█████     | 51/100 [01:15<01:02,  1.28s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.32ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.63ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 52%|█████▏    | 52/100 [01:17<01:01,  1.28s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.31ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.63ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 53%|█████▎    | 53/100 [01:18<01:06,  1.41s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.32ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.65ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 54%|█████▍    | 54/100 [01:20<01:02,  1.36s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.34ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.66ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 55%|█████▌    | 55/100 [01:21<00:59,  1.33s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.35ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.67ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 56%|█████▌    | 56/100 [01:22<00:57,  1.31s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.37ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.64ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 57%|█████▋    | 57/100 [01:23<00:55,  1.29s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.33ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.66ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 58%|█████▊    | 58/100 [01:25<00:53,  1.28s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.35ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.68ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 59%|█████▉    | 59/100 [01:26<00:52,  1.28s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.31ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.68ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 60%|██████    | 60/100 [01:27<00:50,  1.27s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.29ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.65ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 61%|██████    | 61/100 [01:34<01:48,  2.79s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.31ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.67ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 62%|██████▏   | 62/100 [01:35<01:28,  2.33s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.30ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.62ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 63%|██████▎   | 63/100 [01:36<01:14,  2.01s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.31ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.64ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 64%|██████▍   | 64/100 [01:39<01:18,  2.18s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.28ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.82ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 65%|██████▌   | 65/100 [01:40<01:07,  1.92s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.46ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.64ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 66%|██████▌   | 66/100 [01:41<00:59,  1.74s/it]INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.28ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.66ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 67%|██████▋   | 67/100 [01:43<00:53,  1.61s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
Removed 24 outliers from 762 samples
Removed 5 outliers from 620 samples
Removed 8 outliers from 693 samples
Removed 3 outliers from 351 samples
Removed 2 outliers from 754 samples
Removed 7 outliers from 438 samples
Removed 33 outliers from 672 samples
Removed 10 outliers from 767 samples
Removed 13 outliers from 620 samples
Removed 21 outliers from 696 samples
Removed 12 outliers from 361 samples
Removed 59 outliers from 761 samples
Removed 25 outliers from 448 samples
Removed 5 outliers from 619 samples
Removed 10 outliers from 725 samples
Removed 14 outliers from 628 samples
Removed 29 outliers from 696 samples
Removed 8 outliers from 360 samples
Removed 2 outliers from 498 samples
Removed 12 outliers from 442 samples
Removed 8 outliers from 619 samples
Removed 5 outliers from 726 samples
Removed 15 outliers from 613 samples
Removed 20 outliers from 712 samples
Removed 7 outliers from 356 samples
Removed 24 outliers from 516 samples
Removed 15 outliers from 432 samples
Removed 12 outliers from 618 samples
Removed 6 outliers from 723 samples
Removed 10 outliers from 612 samples
Removed 11 outliers from 703 samples
Removed 4 outliers from 361 samples
Removed 38 outliers from 549 samples
Removed 11 outliers from 440 samples
Removed 5 outliers from 618 samples
Removed 9 outliers from 715 samples
Removed 12 outliers from 631 samples
Removed 16 outliers from 675 samples
Removed 12 outliers from 363 samples
Removed 7 outliers from 638 samples
Removed 17 outliers from 438 samples
Removed 9 outliers from 608 samples
Removed 11 outliers from 712 samples
Removed 13 outliers from 622 samples
Removed 22 outliers from 664 samples
Removed 8 outliers from 359 samples
Removed 4 outliers from 672 samples
Removed 16 outliers from 414 samples
Removed 4 outliers from 559 samples
Removed 1 outliers from 654 samples
Removed 11 outliers from 613 samples
Removed 23 outliers from 673 samples
Removed 14 outliers from 356 samples
Removed 29 outliers from 377 samples
Removed 18 outliers from 437 samples
Removed 4 outliers from 562 samples
Removed 3 outliers from 684 samples
Removed 17 outliers from 609 samples
Removed 17 outliers from 693 samples
Removed 14 outliers from 346 samples
Removed 7 outliers from 437 samples
Removed 9 outliers from 429 samples
Removed 17 outliers from 563 samples
Removed 1 outliers from 680 samples
Removed 20 outliers from 604 samples
Removed 11 outliers from 705 samples
Removed 13 outliers from 356 samples
Removed 1 outliers from 437 samples
Removed 20 outliers from 442 samples
Removed 9 outliers from 558 samples
Removed 1 outliers from 679 samples
Removed 20 outliers from 637 samples
Removed 18 outliers from 679 samples
Removed 9 outliers from 360 samples
Removed 9 outliers from 490 samples
Removed 23 outliers from 427 samples
Removed 2 outliers from 561 samples
Removed 1 outliers from 608 samples
Removed 3 outliers from 674 samples
Removed 6 outliers from 359 samples
Removed 33 outliers from 559 samples
Removed 2 outliers from 417 samples
Removed 17 outliers from 600 samples
Removed 16 outliers from 714 samples
Removed 2 outliers from 608 samples
Removed 78 outliers from 663 samples
Removed 3 outliers from 354 samples
Removed 3 outliers from 177 samples
Removed 35 outliers from 357 samples
Removed 10 outliers from 604 samples
Removed 17 outliers from 711 samples
Removed 5 outliers from 606 samples
Removed 22 outliers from 653 samples
Removed 4 outliers from 354 samples
Removed 35 outliers from 367 samples
Removed 20 outliers from 609 samples
Removed 5 outliers from 714 samples
Removed 1 outliers from 615 samples
Removed 2 outliers from 679 samples
Removed 5 outliers from 356 samples
Removed 6 outliers from 248 samples
Removed 10 outliers from 377 samples
Removed 12 outliers from 608 samples
Removed 23 outliers from 718 samples
Removed 1 outliers from 622 samples
Removed 1 outliers from 668 samples
Removed 4 outliers from 355 samples
Removed 3 outliers from 304 samples
Removed 10 outliers from 386 samples
Removed 8 outliers from 610 samples
Removed 35 outliers from 710 samples
Removed 2 outliers from 598 samples
Removed 2 outliers from 692 samples
Removed 5 outliers from 360 samples
Removed 13 outliers from 344 samples
Removed 2 outliers from 406 samples
Removed 5 outliers from 511 samples
Removed 16 outliers from 652 samples
Removed 4 outliers from 411 samples
Removed 2 outliers from 496 samples
Removed 3 outliers from 352 samples
Removed 1 outliers from 415 samples
Removed 7 outliers from 511 samples
Removed 26 outliers from 639 samples
Removed 3 outliers from 569 samples
Removed 2 outliers from 521 samples
Removed 5 outliers from 347 samples
Removed 4 outliers from 416 samples
Removed 5 outliers from 517 samples
Removed 20 outliers from 660 samples
Removed 6 outliers from 605 samples
Removed 2 outliers from 565 samples
Removed 5 outliers from 355 samples
Removed 1 outliers from 86 samples
Removed 4 outliers from 514 samples
Removed 34 outliers from 667 samples
Removed 3 outliers from 605 samples
Removed 4 outliers from 612 samples
Removed 3 outliers from 358 samples
Removed 2 outliers from 113 samples
Removed 27 outliers from 658 samples
Removed 45 outliers from 604 samples
Removed 4 outliers from 640 samples
Removed 4 outliers from 344 samples
Removed 2 outliers from 147 samples
Removed 1 outliers from 409 samples
Removed 4 outliers from 426 samples
Removed 5 outliers from 595 samples
Removed 3 outliers from 468 samples
Removed 1 outliers from 405 samples
Removed 4 outliers from 335 samples
Removed 3 outliers from 388 samples
Removed 5 outliers from 427 samples
Removed 2 outliers from 597 samples
Removed 2 outliers from 479 samples
Removed 3 outliers from 340 samples
Removed 1 outliers from 35 samples
Removed 1 outliers from 403 samples
Removed 2 outliers from 607 samples
Removed 1 outliers from 503 samples
Removed 1 outliers from 434 samples
Removed 3 outliers from 346 samples
Removed 2 outliers from 47 samples
Removed 3 outliers from 411 samples
Removed 2 outliers from 431 samples
Removed 7 outliers from 607 samples
Removed 5 outliers from 552 samples
Removed 3 outliers from 508 samples
Removed 5 outliers from 353 samples
Removed 5 outliers from 417 samples
Removed 4 outliers from 436 samples
Removed 23 outliers from 617 samples
Removed 4 outliers from 581 samples
Removed 3 outliers from 586 samples
Removed 3 outliers from 356 samples
Removed 3 outliers from 103 samples
Removed 15 outliers from 441 samples
Removed 11 outliers from 639 samples
Removed 5 outliers from 714 samples
Removed 4 outliers from 444 samples
Removed 4 outliers from 450 samples
Removed 3 outliers from 354 samples
Removed 3 outliers from 247 samples
Removed 14 outliers from 440 samples
Removed 4 outliers from 636 samples
Removed 5 outliers from 712 samples
Removed 4 outliers from 605 samples
Removed 4 outliers from 670 samples
Removed 4 outliers from 357 samples
Removed 5 outliers from 248 samples
Removed 18 outliers from 436 samples
Removed 4 outliers from 637 samples
Removed 5 outliers from 708 samples
Removed 3 outliers from 606 samples
Removed 2 outliers from 685 samples
Removed 4 outliers from 362 samples
Removed 1 outliers from 306 samples
Removed 8 outliers from 448 samples
Removed 28 outliers from 641 samples
Removed 8 outliers from 714 samples
Removed 3 outliers from 609 samples
Removed 2 outliers from 692 samples
Removed 1 outliers from 268 samples
Removed 26 outliers from 406 samples
Removed 31 outliers from 422 samples
Removed 20 outliers from 636 samples
Removed 11 outliers from 700 samples
Removed 31 outliers from 567 samples
Removed 39 outliers from 652 samples
Removed 22 outliers from 314 samples
Removed 9 outliers from 406 samples
Removed 15 outliers from 382 samples
Removed 9 outliers from 553 samples
Removed 3 outliers from 670 samples
Removed 45 outliers from 553 samples
Removed 69 outliers from 694 samples
Removed 31 outliers from 357 samples
Removed 6 outliers from 556 samples
Removed 6 outliers from 671 samples
Removed 52 outliers from 613 samples
Removed 70 outliers from 582 samples
Removed 34 outliers from 359 samples
Removed 2 outliers from 96 samples
Removed 25 outliers from 387 samples
Removed 2 outliers from 545 samples
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.27ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.64ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 68%|██████▊   | 68/100 [01:44<00:48,  1.51s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.28ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.64ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 69%|██████▉   | 69/100 [01:45<00:44,  1.44s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.28ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.63ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 70%|███████   | 70/100 [01:46<00:41,  1.39s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.27ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.64ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 71%|███████   | 71/100 [01:48<00:39,  1.37s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.35ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.67ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 72%|███████▏  | 72/100 [01:49<00:37,  1.36s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.28ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.63ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 73%|███████▎  | 73/100 [01:50<00:36,  1.35s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.39ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.67ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 74%|███████▍  | 74/100 [01:52<00:34,  1.33s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.29ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.59ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 75%|███████▌  | 75/100 [01:53<00:33,  1.33s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.33ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.60ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 76%|███████▌  | 76/100 [01:54<00:31,  1.32s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.30ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.60ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 77%|███████▋  | 77/100 [01:56<00:34,  1.49s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.31ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.55ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 78%|███████▊  | 78/100 [01:58<00:31,  1.43s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.36ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.62ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 79%|███████▉  | 79/100 [01:59<00:29,  1.38s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.29ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.67ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 80%|████████  | 80/100 [02:00<00:27,  1.36s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.34ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.66ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 81%|████████  | 81/100 [02:02<00:26,  1.38s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.34ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.69ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 82%|████████▏ | 82/100 [02:03<00:24,  1.38s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.30ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.66ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 83%|████████▎ | 83/100 [02:04<00:23,  1.39s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.31ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.66ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 84%|████████▍ | 84/100 [02:06<00:22,  1.39s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.31ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.67ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 85%|████████▌ | 85/100 [02:07<00:20,  1.37s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.37ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.87ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 86%|████████▌ | 86/100 [02:09<00:20,  1.43s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.38ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.67ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 87%|████████▋ | 87/100 [02:10<00:19,  1.48s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.33ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.64ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 88%|████████▊ | 88/100 [02:12<00:17,  1.49s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.30ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.60ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 89%|████████▉ | 89/100 [02:13<00:16,  1.46s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.31ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.60ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 90%|█████████ | 90/100 [02:15<00:14,  1.44s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.28ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.59ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 91%|█████████ | 91/100 [02:21<00:26,  2.92s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.30ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.61ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 92%|█████████▏| 92/100 [02:22<00:19,  2.43s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.27ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.62ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 93%|█████████▎| 93/100 [02:23<00:14,  2.09s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.30ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.62ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 94%|█████████▍| 94/100 [02:25<00:12,  2.03s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.31ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.54ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 95%|█████████▌| 95/100 [02:27<00:09,  1.81s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.31ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.65ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 96%|█████████▌| 96/100 [02:28<00:06,  1.66s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.31ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.67ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 97%|█████████▋| 97/100 [02:29<00:04,  1.55s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.31ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.65ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 98%|█████████▊| 98/100 [02:31<00:02,  1.47s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.33ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.69ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 99%|█████████▉| 99/100 [02:32<00:01,  1.42s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_jagged_mean_unbind_torch_mean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_nanmean
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_jagged_mean_torch_sum
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_simple_fused
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_jagged_mean_variable_length_loop
INFO:tritonbench.utils.triton_op:Took 1.33ms to get benchmark function for torch_compile_nested_tensor_integration
INFO:tritonbench.utils.triton_op:Took 0.69ms to get benchmark function for helion_jagged_mean
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _jagged_mean_kernel_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, -9223372036854775808)
    max_nnz = tl.max(_mask_to, 0)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = subscript_2.to(tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = subscript_4.to(tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = subscript_5.to(tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.sum(x_slice, 1)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = v_2_copy_0.to(tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M_tensor: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args
    ----
    x_data          : 2-D tensor of shape (total_elements, max_M) holding all elements.
    x_offsets       : (num_rows + 1) tensor. Row i is the slice
                      x_data[x_offsets[i] : x_offsets[i+1], :].
    x_feature_counts: (num_rows) tensor. Number of valid features for each row.
    max_M_tensor    : Dummy tensor whose numel() gives max number of features.

    Returns
    -------
    result : 2-D tensor of shape (num_rows, max_M) containing the mean of each row.
             Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    max_M = max_M_tensor.numel()
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_jagged_mean_kernel_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

100%|██████████| 100/100 [02:33<00:00,  1.38s/it]
100%|██████████| 100/100 [02:33<00:00,  1.54s/it]

Benchmark Results:
  x_val    torch_jagged_mean_torch_nanmean-speedup    torch_jagged_mean_torch_sum-speedup    triton_jagged_mean_simple_fused-speedup    triton_jagged_mean_variable_length_loop-speedup    torch_compile_nested_tensor_integration-speedup    helion_jagged_mean-speedup
-------  -----------------------------------------  -------------------------------------  -----------------------------------------  -------------------------------------------------  -------------------------------------------------  ----------------------------
      4                                   2.90025                                5.30294                                     4.95094                                            5.17496                                           0.958648                    8.53483
      4                                   2.85163                                4.94477                                     4.51793                                            4.71191                                           0.925966                    8.15827
      4                                   2.97293                                5.43963                                     4.87379                                            5.04161                                           0.984865                    8.59169
      4                                   2.83957                                4.85227                                     4.23821                                            4.51255                                           0.941566                   10.1065
      4                                   2.87156                                4.91857                                     4.90456                                            4.94684                                           0.956655                   10.2776
      4                                   2.99914                                6.03986                                     4.66533                                            4.86053                                           0.968863                    4.23451
      4                                   2.94701                                5.91424                                     4.80893                                            5.23217                                           0.979545                    3.97693
      4                                   3.06875                                5.96701                                     4.74724                                            5.06186                                           0.950761                    4.78691
      4                                   3.065                                  5.94473                                     5.0842                                             4.98841                                           1.00408                     7.17083
      4                                   3.26234                                6.55304                                     5.22607                                            5.57396                                           1.04232                     7.09604
      4                                   2.92494                                5.66559                                     4.44487                                            4.78445                                           0.983455                    2.81913
      4                                   2.93912                                5.76759                                     4.67995                                            4.90808                                           0.979433                    2.57226
      4                                   2.8675                                 5.56796                                     4.74621                                            4.77254                                           0.973409                    3.11121
      4                                   3.07974                                6.18464                                     5.14266                                            5.16371                                           1.04356                     6.94495
      4                                   2.87276                                5.71753                                     4.37516                                            4.77883                                           0.950351                    5.93929
      4                                   2.36388                                4.55759                                     4.17506                                            4.33624                                           0.972626                    0.983616
      4                                   2.30365                                4.52611                                     4.42784                                            4.57388                                           0.970605                    1.08717
      4                                   2.34239                                4.53684                                     4.27261                                            4.74931                                           0.948034                    1.27468
      4                                   2.3302                                 4.53856                                     4.37831                                            4.58047                                           0.948634                    1.89934
      4                                   2.34067                                4.63804                                     4.09685                                            4.51866                                           0.940782                    2.20263
      4                                   2.41725                                4.67204                                     2.23537                                            2.88704                                           0.901219                    0.303873
      4                                   2.40083                                4.66577                                     2.4346                                             3.17615                                           0.85334                     0.359539
      4                                   2.35817                                4.65587                                     2.5405                                             3.43968                                           0.880102                    0.392983
      4                                   2.42737                                4.72925                                     3.16288                                            4.04186                                           0.958368                    0.601488
      4                                   2.39085                                4.70668                                     3.46734                                            4.79833                                           0.958333                    0.769231
      4                                   2.83917                                5.49349                                     1.62645                                            2.06712                                           1.10301                     0.210855
      4                                   2.71883                                5.19819                                     1.52008                                            1.92562                                           1.07587                     0.194523
      4                                   2.41455                                4.5867                                      1.5289                                             2.13083                                           0.92995                     0.219491
      4                                   2.33922                                4.5336                                      1.80535                                            2.8044                                            0.924255                    0.304298
      4                                   2.36531                                4.61753                                     2.15963                                            3.69894                                           0.912359                    0.432893
      4                                   3.11091                                5.75698                                     4.05787                                            3.79437                                           0.797181                    4.53558
      4                                   2.95576                                5.90167                                     3.84891                                            4.07949                                           0.775685                    4.0748
      4                                   3.16728                                5.76539                                     4.46521                                            4.23594                                           0.791819                    5.53514
      4                                   3.02783                                5.71757                                     4.60582                                            5.0981                                            0.784058                    6.64504
      4                                   3.07016                                5.71405                                     4.66532                                            5.12908                                           0.778428                    6.36648
      4                                   2.24498                                4.25031                                     4.39594                                            4.19879                                           0.784243                    1.29495
      4                                   2.20443                                4.24756                                     4.63782                                            4.47112                                           0.807559                    1.39599
      4                                   2.17519                                4.28624                                     4.53117                                            4.90717                                           0.810265                    1.63343
      4                                   2.23982                                4.28837                                     4.63855                                            5                                                 0.802641                    2.58582
      4                                   2.09914                                4.29287                                     4.74412                                            5.39308                                           0.794349                    3.15547
      4                                   1.85429                                3.58564                                     5.03101                                            5.31241                                           0.884397                    0.824825
      4                                   1.64981                                3.21751                                     4.53377                                            4.96586                                           0.792149                    0.977324
      4                                   1.66363                                3.19834                                     4.18215                                            4.37201                                           0.798296                    0.976895
      4                                   1.66051                                3.24422                                     4.61447                                            5.03879                                           0.808251                    1.24318
      4                                   1.70257                                3.29174                                     4.74189                                            4.94225                                           0.800228                    1.75538
      4                                   2.21891                                4.3521                                      4.39576                                            4.31873                                           0.89188                     0.281422
      4                                   2.92584                                5.56663                                     6.15985                                            5.81404                                           1.1425                      0.400855
      4                                   2.72707                                5.40791                                     5.90136                                            5.90864                                           1.07551                     0.53054
      4                                   2.50562                                4.86452                                     5.46005                                            5.75679                                           0.957082                    0.63286
      4                                   2.45958                                4.86857                                     5.53966                                            5.62748                                           0.975945                    0.77342
      4                                   1.45273                                2.97043                                     2.25043                                            1.58367                                           0.837716                    0.081821
      4                                   1.47303                                2.96282                                     2.3681                                             1.68174                                           0.86643                     0.0897392
      4                                   1.46912                                3.07943                                     2.93935                                            2.19909                                           0.902678                    0.118577
      4                                   1.46785                                3.16874                                     3.51661                                            2.73458                                           0.876927                    0.159531
      4                                   1.50911                                3.17667                                     4.10776                                            3.3975                                            0.856629                    0.217505
      4                                   0.957087                               2.16833                                     1.4344                                             1.00205                                           0.833618                    0.0445065
      4                                   0.983064                               2.17993                                     1.44842                                            1.0186                                            0.826392                    0.0458414
      4                                   1.01132                                2.36036                                     1.65613                                            1.15998                                           0.842624                    0.0628709
      4                                   1.03093                                2.40964                                     2.24972                                            1.6632                                            0.87241                     0.0906968
      4                                   0.989242                               2.39213                                     2.92475                                            2.35625                                           0.840955                    0.141687
      4                                   2.36068                                3.82217                                     4.07728                                            4.22573                                           0.787426                    0.383185
      4                                   2.435                                  3.8245                                      4.45373                                            4.80583                                           0.766763                    0.385428
      4                                   2.45218                                3.86157                                     4.28905                                            4.75068                                           0.782716                    0.526419
      4                                   2.35513                                3.84494                                     4.76602                                            5.0472                                            0.626396                    0.82637
      4                                   2.40342                                3.93386                                     3.6138                                             3.91629                                           0.771548                    0.855645
      4                                   1.88303                                3.4123                                      4.51595                                            4.69143                                           0.913034                    0.105568
      4                                   1.78045                                3.2916                                      4.16259                                            4.68541                                           0.876286                    0.13579
      4                                   1.60118                                2.97382                                     4.639                                              4.47964                                           0.782792                    0.120574
      4                                   1.58646                                2.99388                                     4.60565                                            5.03976                                           0.804987                    0.348792
      4                                   1.62708                                2.97051                                     4.42506                                            4.47128                                           0.795032                    0.440458
      4                                   1.41043                                3.44444                                     6.22594                                            6.21294                                           1.31973                     0.0699939
      4                                   1.35666                                3.33726                                     5.67134                                            5.78732                                           1.23392                     0.0711466
      4                                   0.967638                               2.47598                                     4.40607                                            4.58129                                           0.840558                    0.0826912
      4                                   0.898094                               2.33333                                     4.36461                                            4.25347                                           0.799782                    0.0951407
      4                                   0.867574                               2.24392                                     4.21781                                            4.28484                                           0.774928                    0.198843
      4                                   0.507838                               1.34393                                     2.62549                                            2.35601                                           0.85815                     0.0136457
      4                                   0.50032                                1.36551                                     2.79857                                            2.55665                                           0.848696                    0.0149513
      4                                   0.556806                               1.59886                                     4.15925                                            3.78829                                           0.922149                    0.0218107
      4                                   0.553049                               1.70942                                     4.87337                                            4.71167                                           0.944712                    0.0352463
      4                                   0.525372                               1.6818                                      4.74749                                            4.82631                                           0.860428                    0.0593483
      4                                   0.2515                                 0.736503                                    1.93908                                            1.68939                                           1.24626                     0.00661773
      4                                   0.22038                                0.685129                                    2.13765                                            1.94044                                           1.07807                     0.00854408
      4                                   0.205478                               0.649943                                    2.02065                                            1.71285                                           0.976226                    0.00727448
      4                                   0.200736                               0.642385                                    2.20138                                            1.99331                                           0.929908                    0.00905425
      4                                   0.180423                               0.614997                                    3.79029                                            3.75024                                           0.840474                    0.0202869
      4                                   0.19847                                0.601097                                    1.65034                                            1.34981                                           1.64372                     0.00466428
      4                                   0.218511                               0.646024                                    1.75379                                            1.46214                                           1.7602                      0.00485142
      4                                   0.174607                               0.545653                                    1.79441                                            1.53901                                           1.44537                     0.0055191
      4                                   0.142823                               0.487341                                    2.07794                                            1.76032                                           1.15162                     0.00669251
      4                                   0.119707                               0.412969                                    1.9831                                             1.8034                                            0.953175                    0.00752723
      4                                   1.38499                                2.01049                                     4.66847                                            5.0885                                            0.808531                    0.110262
      4                                   1.39636                                2.03414                                     4.98701                                            4.77348                                           0.792116                    0.113587
      4                                   1.43561                                2.05513                                     4.70421                                            4.94579                                           0.812705                    0.13281
      4                                   1.42651                                2.09394                                     4.83894                                            4.73288                                           0.810082                    0.228309
      4                                   1.42463                                2.10915                                     5.02032                                            5.24091                                           0.820641                    0.332436
      4                                   0.734317                               1.58123                                     3.9807                                             4.03635                                           0.935421                    0.0212835
      4                                   0.742155                               1.59977                                     3.96                                               3.99343                                           0.931102                    0.0212647
      4                                   0.60395                                1.38224                                     4.3037                                             4.33582                                           0.806198                    0.0449134
      4                                   0.61271                                1.44987                                     4.47871                                            4.83426                                           0.789043                    0.0538248
      4                                   0.616726                               1.48947                                     4.29492                                            4.44359                                           0.791324                    0.0555654
average                                   1.8174                                 3.54414                                     3.89508                                            4.003                                             0.92162                     1.58216

============================================================
Kernel: cross_entropy
============================================================

Running cross_entropy benchmark with Helion implementation...

Removed 11 outliers from 652 samples
Removed 64 outliers from 582 samples
Removed 86 outliers from 673 samples
Removed 28 outliers from 330 samples
Removed 3 outliers from 95 samples
Removed 24 outliers from 432 samples
Removed 5 outliers from 559 samples
Removed 3 outliers from 678 samples
Removed 16 outliers from 613 samples
Removed 12 outliers from 694 samples
Removed 4 outliers from 360 samples
Removed 6 outliers from 233 samples
Removed 26 outliers from 442 samples
Removed 5 outliers from 559 samples
Removed 1 outliers from 665 samples
Removed 15 outliers from 612 samples
Removed 10 outliers from 682 samples
Removed 6 outliers from 361 samples
Removed 14 outliers from 275 samples
Removed 18 outliers from 317 samples
Removed 6 outliers from 412 samples
Removed 5 outliers from 606 samples
Removed 69 outliers from 628 samples
Removed 50 outliers from 630 samples
Removed 27 outliers from 365 samples
Removed 12 outliers from 330 samples
Removed 2 outliers from 414 samples
Removed 2 outliers from 607 samples
Removed 56 outliers from 603 samples
Removed 71 outliers from 668 samples
Removed 33 outliers from 356 samples
Removed 1 outliers from 37 samples
Removed 19 outliers from 410 samples
Removed 6 outliers from 420 samples
Removed 5 outliers from 618 samples
Removed 65 outliers from 619 samples
Removed 62 outliers from 639 samples
Removed 39 outliers from 344 samples
Removed 41 outliers from 406 samples
Removed 3 outliers from 416 samples
Removed 2 outliers from 616 samples
Removed 46 outliers from 520 samples
Removed 65 outliers from 675 samples
Removed 42 outliers from 352 samples
Removed 3 outliers from 73 samples
Removed 41 outliers from 407 samples
Removed 2 outliers from 418 samples
Removed 9 outliers from 621 samples
Removed 65 outliers from 616 samples
Removed 74 outliers from 697 samples
Removed 50 outliers from 365 samples
Removed 2 outliers from 147 samples
Removed 32 outliers from 392 samples
Removed 4 outliers from 277 samples
Removed 5 outliers from 493 samples
Removed 1 outliers from 577 samples
Removed 4 outliers from 587 samples
Removed 4 outliers from 357 samples
Removed 12 outliers from 416 samples
Removed 2 outliers from 280 samples
Removed 6 outliers from 610 samples
Removed 3 outliers from 614 samples
Removed 5 outliers from 359 samples
Removed 17 outliers from 393 samples
Removed 3 outliers from 287 samples
Removed 43 outliers from 607 samples
Removed 11 outliers from 641 samples
Removed 3 outliers from 357 samples
Removed 24 outliers from 397 samples
Removed 2 outliers from 292 samples
Removed 2 outliers from 543 samples
Removed 15 outliers from 626 samples
Removed 23 outliers from 664 samples
Removed 5 outliers from 349 samples
Removed 19 outliers from 414 samples
Removed 5 outliers from 295 samples
Removed 3 outliers from 558 samples
Removed 11 outliers from 619 samples
Removed 8 outliers from 687 samples
Removed 5 outliers from 355 samples
Removed 1 outliers from 46 samples
Removed 17 outliers from 339 samples
Removed 3 outliers from 117 samples
Removed 3 outliers from 278 samples
Removed 5 outliers from 465 samples
Removed 7 outliers from 444 samples
Removed 4 outliers from 335 samples
Removed 18 outliers from 371 samples
Removed 4 outliers from 121 samples
Removed 3 outliers from 294 samples
Removed 1 outliers from 514 samples
Removed 7 outliers from 504 samples
Removed 8 outliers from 346 samples
Removed 17 outliers from 382 samples
Removed 6 outliers from 121 samples
Removed 3 outliers from 299 samples
Removed 2 outliers from 524 samples
Removed 3 outliers from 488 samples
Removed 4 outliers from 349 samples
Removed 56 outliers from 388 samples
Removed 1 outliers from 121 samples
Removed 6 outliers from 302 samples
Removed 12 outliers from 495 samples
Removed 6 outliers from 539 samples
Removed 33 outliers from 334 samples
Removed 41 outliers from 406 samples
Removed 1 outliers from 125 samples
Removed 6 outliers from 320 samples
Removed 72 outliers from 600 samples
Removed 94 outliers from 563 samples
Removed 38 outliers from 321 samples
Removed 20 outliers from 264 samples
Removed 2 outliers from 191 samples
Removed 3 outliers from 357 samples
Removed 4 outliers from 330 samples
Removed 33 outliers from 289 samples
Removed 8 outliers from 248 samples
Removed 1 outliers from 185 samples
Removed 5 outliers from 359 samples
Removed 5 outliers from 314 samples
Removed 9 outliers from 291 samples
Removed 22 outliers from 302 samples
Removed 1 outliers from 74 samples
Removed 7 outliers from 410 samples
Removed 1 outliers from 386 samples
Removed 2 outliers from 334 samples
Removed 22 outliers from 350 samples
Removed 1 outliers from 76 samples
Removed 7 outliers from 219 samples
Removed 3 outliers from 499 samples
Removed 4 outliers from 471 samples
Removed 4 outliers from 348 samples
Removed 3 outliers from 391 samples
Removed 3 outliers from 220 samples
Removed 7 outliers from 519 samples
Removed 6 outliers from 510 samples
Removed 4 outliers from 352 samples
Removed 19 outliers from 443 samples
Removed 3 outliers from 528 samples
Removed 3 outliers from 601 samples
Removed 6 outliers from 444 samples
Removed 12 outliers from 459 samples
Removed 5 outliers from 371 samples
Removed 5 outliers from 89 samples
Removed 4 outliers from 445 samples
Removed 4 outliers from 529 samples
Removed 11 outliers from 601 samples
Removed 2 outliers from 614 samples
Removed 1 outliers from 668 samples
Removed 5 outliers from 359 samples
Removed 11 outliers from 91 samples
Removed 14 outliers from 434 samples
Removed 4 outliers from 527 samples
Removed 4 outliers from 604 samples
Removed 3 outliers from 639 samples
Removed 4 outliers from 363 samples
Removed 3 outliers from 434 samples
Removed 9 outliers from 533 samples
Removed 1 outliers from 610 samples
Removed 2 outliers from 626 samples
Removed 3 outliers from 684 samples
Removed 3 outliers from 365 samples
Removed 8 outliers from 167 samples
Removed 10 outliers from 441 samples
Removed 4 outliers from 533 samples
Removed 2 outliers from 630 samples
Removed 2 outliers from 700 samples
Removed 6 outliers from 360 samples
Removed 3 outliers from 224 samples
Removed 7 outliers from 395 samples
Removed 2 outliers from 341 samples
Removed 1 outliers from 509 samples
Removed 4 outliers from 618 samples
Removed 2 outliers from 690 samples
Removed 7 outliers from 356 samples
Removed 7 outliers from 388 samples
Removed 2 outliers from 622 samples
Removed 3 outliers from 670 samples
Removed 4 outliers from 358 samples
Removed 7 outliers from 438 samples
Removed 7 outliers from 345 samples
Removed 2 outliers from 670 samples
Removed 6 outliers from 362 samples
Removed 1 outliers from 38 samples
Removed 6 outliers from 433 samples
Removed 1 outliers from 346 samples
Removed 2 outliers from 537 samples
Removed 1 outliers from 621 samples
Removed 2 outliers from 673 samples
Removed 3 outliers from 351 samples
Removed 2 outliers from 45 samples
Removed 4 outliers from 437 samples
Removed 1 outliers from 349 samples
Removed 1 outliers from 541 samples
Removed 3 outliers from 610 samples
Removed 1 outliers from 671 samples
Removed 3 outliers from 355 samples
Removed 2 outliers from 47 samples

  0%|          | 0/6 [00:00<?, ?it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for cross_entropy_loss
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for liger_cross_entropy_loss
INFO:tritonbench.utils.triton_op:Took 1.51ms to get benchmark function for inductor_cross_entropy_loss
INFO:tritonbench.utils.triton_op:Took 0.03ms to get benchmark function for helion_cross_entropy
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _cross_entropy_kernel(labels, logits_flat, logits, losses, labels_stride_0, logits_stride_0, logits_stride_1, logits_flat_stride_0, losses_stride_0, v, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < v
    labels_tile = tl.load(labels + indices_0 * labels_stride_0, None)
    v_0 = v.to(tl.int32)
    v_1 = indices_0 * v_0
    v_2 = v_1.to(tl.int64)
    v_3 = v_2 + labels_tile
    logits_at_target = tl.load(logits_flat + v_3 * logits_flat_stride_0, None)
    logits_rows = tl.load(logits + (indices_0[:, None] * logits_stride_0 + indices_1[None, :] * logits_stride_1), mask_1[None, :], other=0)
    _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), logits_rows, float('-inf'))
    max_logits = tl.reshape(tl.max(_mask_to, 1), [1, 1])
    v_4 = logits_rows - max_logits
    v_5 = tl_math.exp(v_4)
    _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), v_5, 0)
    sum_exp = tl.reshape(tl.sum(_mask_to_1, 1), [1, 1])
    squeeze = tl.reshape(max_logits, [1])
    squeeze_1 = tl.reshape(sum_exp, [1])
    v_6 = tl_math.log(squeeze_1)
    v_7 = squeeze + v_6
    v_8 = v_7 - logits_at_target
    tl.store(losses + indices_0 * losses_stride_0, v_8, None)

def cross_entropy(logits: torch.Tensor, labels: torch.Tensor, *, _launcher=_default_launcher):
    n, v = logits.shape
    losses = torch.zeros([n], dtype=logits.dtype, device=logits.device)
    logits_flat = logits.view(-1)
    _RDIM_SIZE_1 = triton.next_power_of_2(v)
    _launcher(_cross_entropy_kernel, (n,), labels, logits_flat, logits, losses, labels.stride(0), logits.stride(0), logits.stride(1), logits_flat.stride(0), losses.stride(0), v, _RDIM_SIZE_1, num_warps=4, num_stages=3)
    return losses.mean()

 17%|█▋        | 1/6 [00:00<00:03,  1.38it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for cross_entropy_loss
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for liger_cross_entropy_loss
INFO:tritonbench.utils.triton_op:Took 1.50ms to get benchmark function for inductor_cross_entropy_loss
INFO:tritonbench.utils.triton_op:Took 0.36ms to get benchmark function for helion_cross_entropy
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _cross_entropy_kernel(labels, logits_flat, logits, losses, labels_stride_0, logits_stride_0, logits_stride_1, logits_flat_stride_0, losses_stride_0, v, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    labels_tile = tl.load(labels + indices_0 * labels_stride_0, None)
    v_0 = v.to(tl.int32)
    v_1 = indices_0 * v_0
    v_2 = v_1.to(tl.int64)
    v_3 = v_2 + labels_tile
    logits_at_target = tl.load(logits_flat + v_3 * logits_flat_stride_0, None)
    max_logits_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, v, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < v
        logits_rows = tl.load(logits + (indices_0[:, None] * logits_stride_0 + rindex_1[None, :] * logits_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), logits_rows, float('-inf'))
        v_4 = triton_helpers.maximum(max_logits_acc, _mask_to)
        max_logits_acc = v_4
    max_logits = tl.reshape(tl.max(max_logits_acc, 1), [1, 1])
    squeeze = tl.reshape(max_logits, [1])
    sum_exp_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, v, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < v
        max_logits_copy = max_logits
        logits_rows_1 = tl.load(logits + (indices_0[:, None] * logits_stride_0 + rindex_1[None, :] * logits_stride_1), mask_1[None, :], other=0)
        v_5 = logits_rows_1 - max_logits_copy
        v_6 = tl_math.exp(v_5)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_6, 0)
        v_7 = sum_exp_acc + _mask_to_1
        sum_exp_acc = v_7
    sum_exp = tl.reshape(tl.sum(sum_exp_acc, 1), [1, 1])
    squeeze_1 = tl.reshape(sum_exp, [1])
    v_8 = tl_math.log(squeeze_1)
    v_9 = squeeze + v_8
    v_10 = v_9 - logits_at_target
    tl.store(losses + indices_0 * losses_stride_0, v_10, None)

def cross_entropy(logits: torch.Tensor, labels: torch.Tensor, *, _launcher=_default_launcher):
    n, v = logits.shape
    losses = torch.zeros([n], dtype=logits.dtype, device=logits.device)
    logits_flat = logits.view(-1)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_cross_entropy_kernel, (n,), labels, logits_flat, logits, losses, labels.stride(0), logits.stride(0), logits.stride(1), logits_flat.stride(0), losses.stride(0), v, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return losses.mean()

 33%|███▎      | 2/6 [00:01<00:02,  1.41it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for cross_entropy_loss
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for liger_cross_entropy_loss
INFO:tritonbench.utils.triton_op:Took 1.52ms to get benchmark function for inductor_cross_entropy_loss
INFO:tritonbench.utils.triton_op:Took 0.34ms to get benchmark function for helion_cross_entropy
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _cross_entropy_kernel(labels, logits_flat, logits, losses, labels_stride_0, logits_stride_0, logits_stride_1, logits_flat_stride_0, losses_stride_0, v, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    labels_tile = tl.load(labels + indices_0 * labels_stride_0, None)
    v_0 = v.to(tl.int32)
    v_1 = indices_0 * v_0
    v_2 = v_1.to(tl.int64)
    v_3 = v_2 + labels_tile
    logits_at_target = tl.load(logits_flat + v_3 * logits_flat_stride_0, None)
    max_logits_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, v, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < v
        logits_rows = tl.load(logits + (indices_0[:, None] * logits_stride_0 + rindex_1[None, :] * logits_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), logits_rows, float('-inf'))
        v_4 = triton_helpers.maximum(max_logits_acc, _mask_to)
        max_logits_acc = v_4
    max_logits = tl.reshape(tl.max(max_logits_acc, 1), [1, 1])
    squeeze = tl.reshape(max_logits, [1])
    sum_exp_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, v, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < v
        max_logits_copy = max_logits
        logits_rows_1 = tl.load(logits + (indices_0[:, None] * logits_stride_0 + rindex_1[None, :] * logits_stride_1), mask_1[None, :], other=0)
        v_5 = logits_rows_1 - max_logits_copy
        v_6 = tl_math.exp(v_5)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_6, 0)
        v_7 = sum_exp_acc + _mask_to_1
        sum_exp_acc = v_7
    sum_exp = tl.reshape(tl.sum(sum_exp_acc, 1), [1, 1])
    squeeze_1 = tl.reshape(sum_exp, [1])
    v_8 = tl_math.log(squeeze_1)
    v_9 = squeeze + v_8
    v_10 = v_9 - logits_at_target
    tl.store(losses + indices_0 * losses_stride_0, v_10, None)

def cross_entropy(logits: torch.Tensor, labels: torch.Tensor, *, _launcher=_default_launcher):
    n, v = logits.shape
    losses = torch.zeros([n], dtype=logits.dtype, device=logits.device)
    logits_flat = logits.view(-1)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_cross_entropy_kernel, (n,), labels, logits_flat, logits, losses, labels.stride(0), logits.stride(0), logits.stride(1), logits_flat.stride(0), losses.stride(0), v, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return losses.mean()

 50%|█████     | 3/6 [00:02<00:02,  1.41it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for cross_entropy_loss
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for liger_cross_entropy_loss
INFO:tritonbench.utils.triton_op:Took 1.52ms to get benchmark function for inductor_cross_entropy_loss
INFO:tritonbench.utils.triton_op:Took 0.37ms to get benchmark function for helion_cross_entropy
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _cross_entropy_kernel(labels, logits_flat, logits, losses, labels_stride_0, logits_stride_0, logits_stride_1, logits_flat_stride_0, losses_stride_0, v, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    labels_tile = tl.load(labels + indices_0 * labels_stride_0, None)
    v_0 = v.to(tl.int32)
    v_1 = indices_0 * v_0
    v_2 = v_1.to(tl.int64)
    v_3 = v_2 + labels_tile
    logits_at_target = tl.load(logits_flat + v_3 * logits_flat_stride_0, None)
    max_logits_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, v, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < v
        logits_rows = tl.load(logits + (indices_0[:, None] * logits_stride_0 + rindex_1[None, :] * logits_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), logits_rows, float('-inf'))
        v_4 = triton_helpers.maximum(max_logits_acc, _mask_to)
        max_logits_acc = v_4
    max_logits = tl.reshape(tl.max(max_logits_acc, 1), [1, 1])
    squeeze = tl.reshape(max_logits, [1])
    sum_exp_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, v, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < v
        max_logits_copy = max_logits
        logits_rows_1 = tl.load(logits + (indices_0[:, None] * logits_stride_0 + rindex_1[None, :] * logits_stride_1), mask_1[None, :], other=0)
        v_5 = logits_rows_1 - max_logits_copy
        v_6 = tl_math.exp(v_5)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_6, 0)
        v_7 = sum_exp_acc + _mask_to_1
        sum_exp_acc = v_7
    sum_exp = tl.reshape(tl.sum(sum_exp_acc, 1), [1, 1])
    squeeze_1 = tl.reshape(sum_exp, [1])
    v_8 = tl_math.log(squeeze_1)
    v_9 = squeeze + v_8
    v_10 = v_9 - logits_at_target
    tl.store(losses + indices_0 * losses_stride_0, v_10, None)

def cross_entropy(logits: torch.Tensor, labels: torch.Tensor, *, _launcher=_default_launcher):
    n, v = logits.shape
    losses = torch.zeros([n], dtype=logits.dtype, device=logits.device)
    logits_flat = logits.view(-1)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_cross_entropy_kernel, (n,), labels, logits_flat, logits, losses, labels.stride(0), logits.stride(0), logits.stride(1), logits_flat.stride(0), losses.stride(0), v, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return losses.mean()

 67%|██████▋   | 4/6 [00:02<00:01,  1.39it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for cross_entropy_loss
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for liger_cross_entropy_loss
INFO:tritonbench.utils.triton_op:Took 1.52ms to get benchmark function for inductor_cross_entropy_loss
INFO:tritonbench.utils.triton_op:Took 0.34ms to get benchmark function for helion_cross_entropy
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _cross_entropy_kernel(labels, logits_flat, logits, losses, labels_stride_0, logits_stride_0, logits_stride_1, logits_flat_stride_0, losses_stride_0, v, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    labels_tile = tl.load(labels + indices_0 * labels_stride_0, None)
    v_0 = v.to(tl.int32)
    v_1 = indices_0 * v_0
    v_2 = v_1.to(tl.int64)
    v_3 = v_2 + labels_tile
    logits_at_target = tl.load(logits_flat + v_3 * logits_flat_stride_0, None)
    max_logits_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, v, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < v
        logits_rows = tl.load(logits + (indices_0[:, None] * logits_stride_0 + rindex_1[None, :] * logits_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), logits_rows, float('-inf'))
        v_4 = triton_helpers.maximum(max_logits_acc, _mask_to)
        max_logits_acc = v_4
    max_logits = tl.reshape(tl.max(max_logits_acc, 1), [1, 1])
    squeeze = tl.reshape(max_logits, [1])
    sum_exp_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, v, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < v
        max_logits_copy = max_logits
        logits_rows_1 = tl.load(logits + (indices_0[:, None] * logits_stride_0 + rindex_1[None, :] * logits_stride_1), mask_1[None, :], other=0)
        v_5 = logits_rows_1 - max_logits_copy
        v_6 = tl_math.exp(v_5)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_6, 0)
        v_7 = sum_exp_acc + _mask_to_1
        sum_exp_acc = v_7
    sum_exp = tl.reshape(tl.sum(sum_exp_acc, 1), [1, 1])
    squeeze_1 = tl.reshape(sum_exp, [1])
    v_8 = tl_math.log(squeeze_1)
    v_9 = squeeze + v_8
    v_10 = v_9 - logits_at_target
    tl.store(losses + indices_0 * losses_stride_0, v_10, None)

def cross_entropy(logits: torch.Tensor, labels: torch.Tensor, *, _launcher=_default_launcher):
    n, v = logits.shape
    losses = torch.zeros([n], dtype=logits.dtype, device=logits.device)
    logits_flat = logits.view(-1)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_cross_entropy_kernel, (n,), labels, logits_flat, logits, losses, labels.stride(0), logits.stride(0), logits.stride(1), logits_flat.stride(0), losses.stride(0), v, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return losses.mean()

 83%|████████▎ | 5/6 [00:03<00:00,  1.34it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for cross_entropy_loss
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for liger_cross_entropy_loss
INFO:tritonbench.utils.triton_op:Took 1.56ms to get benchmark function for inductor_cross_entropy_loss
INFO:tritonbench.utils.triton_op:Took 0.36ms to get benchmark function for helion_cross_entropy
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _cross_entropy_kernel(labels, logits_flat, logits, losses, labels_stride_0, logits_stride_0, logits_stride_1, logits_flat_stride_0, losses_stride_0, v, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    labels_tile = tl.load(labels + indices_0 * labels_stride_0, None)
    v_0 = v.to(tl.int32)
    v_1 = indices_0 * v_0
    v_2 = v_1.to(tl.int64)
    v_3 = v_2 + labels_tile
    logits_at_target = tl.load(logits_flat + v_3 * logits_flat_stride_0, None)
    max_logits_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, v, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < v
        logits_rows = tl.load(logits + (indices_0[:, None] * logits_stride_0 + rindex_1[None, :] * logits_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), logits_rows, float('-inf'))
        v_4 = triton_helpers.maximum(max_logits_acc, _mask_to)
        max_logits_acc = v_4
    max_logits = tl.reshape(tl.max(max_logits_acc, 1), [1, 1])
    squeeze = tl.reshape(max_logits, [1])
    sum_exp_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, v, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < v
        max_logits_copy = max_logits
        logits_rows_1 = tl.load(logits + (indices_0[:, None] * logits_stride_0 + rindex_1[None, :] * logits_stride_1), mask_1[None, :], other=0)
        v_5 = logits_rows_1 - max_logits_copy
        v_6 = tl_math.exp(v_5)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_6, 0)
        v_7 = sum_exp_acc + _mask_to_1
        sum_exp_acc = v_7
    sum_exp = tl.reshape(tl.sum(sum_exp_acc, 1), [1, 1])
    squeeze_1 = tl.reshape(sum_exp, [1])
    v_8 = tl_math.log(squeeze_1)
    v_9 = squeeze + v_8
    v_10 = v_9 - logits_at_target
    tl.store(losses + indices_0 * losses_stride_0, v_10, None)

def cross_entropy(logits: torch.Tensor, labels: torch.Tensor, *, _launcher=_default_launcher):
    n, v = logits.shape
    losses = torch.zeros([n], dtype=logits.dtype, device=logits.device)
    logits_flat = logits.view(-1)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_cross_entropy_kernel, (n,), labels, logits_flat, logits, losses, labels.stride(0), logits.stride(0), logits.stride(1), logits_flat.stride(0), losses.stride(0), v, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return losses.mean()

100%|██████████| 6/6 [00:04<00:00,  1.24it/s]
100%|██████████| 6/6 [00:04<00:00,  1.31it/s]

Benchmark Results:
        (B, T, V)    liger_cross_entropy_loss-speedup    inductor_cross_entropy_loss-speedup    helion_cross_entropy-speedup
-----------------  ----------------------------------  -------------------------------------  ------------------------------
  (8, 2048, 4096)                            0.668171                                1.97054                         1.90848
  (8, 2048, 8192)                            0.885161                                2.16692                         1.83204
 (8, 2048, 16384)                            0.865217                                2.02533                         1.09852
 (8, 2048, 32768)                            1.41793                                 3.16534                         1.66658
 (8, 2048, 65536)                            1.5225                                  3.99325                         2.06453
(8, 2048, 131072)                            1.30838                                 4.07752                         2.07165
          average                            1.11123                                 2.89982                         1.77363

============================================================
Kernel: softmax
============================================================

Running softmax benchmark with Helion implementation...

Removed 1 outliers from 383 samples
Removed 1 outliers from 140 samples
Removed 2 outliers from 125 samples
Removed 3 outliers from 253 samples
Removed 5 outliers from 230 samples
Removed 2 outliers from 75 samples
Removed 1 outliers from 159 samples
Removed 2 outliers from 30 samples
Removed 3 outliers from 42 samples
Removed 1 outliers from 90 samples
Removed 1 outliers from 50 samples
Removed 2 outliers from 49 samples

  0%|          | 0/98 [00:00<?, ?it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.05ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.04ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, n, _m, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < n
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    _mask_to = tl.where(mask_0[:, None] & mask_1[None, :], load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [_BLOCK_SIZE_0, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(mask_0[:, None] & mask_1[None, :], v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [_BLOCK_SIZE_0, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_0[:, None] & mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _BLOCK_SIZE_0 = 16
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (triton.cdiv(n, _BLOCK_SIZE_0),), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), n, _m, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=4, num_stages=3)
    return out

  1%|          | 1/98 [00:00<00:38,  2.51it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.05ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.19ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_1[None, :], other=0)
    _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [1, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [1, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _RDIM_SIZE_1, num_warps=4, num_stages=3)
    return out

  2%|▏         | 2/98 [00:00<00:37,  2.56it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.05ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.19ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_1[None, :], other=0)
    _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [1, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [1, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _RDIM_SIZE_1, num_warps=4, num_stages=3)
    return out

  3%|▎         | 3/98 [00:01<00:36,  2.60it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.04ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.19ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_1[None, :], other=0)
    _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [1, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [1, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _RDIM_SIZE_1, num_warps=4, num_stages=3)
    return out

  4%|▍         | 4/98 [00:01<00:36,  2.61it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.05ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.20ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_1[None, :], other=0)
    _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [1, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [1, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _RDIM_SIZE_1, num_warps=4, num_stages=3)
    return out

  5%|▌         | 5/98 [00:01<00:35,  2.62it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.05ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.20ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_1[None, :], other=0)
    _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [1, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [1, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _RDIM_SIZE_1, num_warps=4, num_stages=3)
    return out

  6%|▌         | 6/98 [00:02<00:35,  2.62it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.04ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.19ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_1[None, :], other=0)
    _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [1, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [1, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _RDIM_SIZE_1, num_warps=4, num_stages=3)
    return out

  7%|▋         | 7/98 [00:02<00:34,  2.62it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.05ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.21ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_1[None, :], other=0)
    _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [1, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [1, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _RDIM_SIZE_1, num_warps=4, num_stages=3)
    return out

  8%|▊         | 8/98 [00:03<00:34,  2.62it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.05ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.19ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_1[None, :], other=0)
    _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [1, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [1, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _RDIM_SIZE_1, num_warps=4, num_stages=3)
    return out

  9%|▉         | 9/98 [00:03<00:33,  2.62it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.06ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.20ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_1[None, :], other=0)
    _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [1, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [1, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _RDIM_SIZE_1, num_warps=4, num_stages=3)
    return out

 10%|█         | 10/98 [00:03<00:33,  2.62it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.05ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.19ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_1[None, :], other=0)
    _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [1, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [1, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _RDIM_SIZE_1, num_warps=4, num_stages=3)
    return out

 11%|█         | 11/98 [00:04<00:33,  2.62it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.05ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.20ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_1[None, :], other=0)
    _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [1, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [1, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _RDIM_SIZE_1, num_warps=4, num_stages=3)
    return out

 12%|█▏        | 12/98 [00:04<00:32,  2.62it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.05ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.19ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_1[None, :], other=0)
    _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [1, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [1, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _RDIM_SIZE_1, num_warps=4, num_stages=3)
    return out

 13%|█▎        | 13/98 [00:04<00:32,  2.61it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.05ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.20ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_1[None, :], other=0)
    _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [1, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [1, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _RDIM_SIZE_1, num_warps=4, num_stages=3)
    return out

 14%|█▍        | 14/98 [00:05<00:32,  2.61it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.05ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.21ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_1[None, :], other=0)
    _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [1, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [1, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _RDIM_SIZE_1, num_warps=4, num_stages=3)
    return out

 15%|█▌        | 15/98 [00:05<00:31,  2.60it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.05ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.20ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_1[None, :], other=0)
    _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [1, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [1, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _RDIM_SIZE_1, num_warps=4, num_stages=3)
    return out

 16%|█▋        | 16/98 [00:06<00:31,  2.59it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.05ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.21ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_1[None, :], other=0)
    _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [1, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [1, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _RDIM_SIZE_1, num_warps=4, num_stages=3)
    return out

 17%|█▋        | 17/98 [00:06<00:31,  2.58it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.05ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.20ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_1[None, :], other=0)
    _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [1, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [1, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _RDIM_SIZE_1, num_warps=4, num_stages=3)
    return out

 18%|█▊        | 18/98 [00:06<00:31,  2.57it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.06ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.19ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_1[None, :], other=0)
    _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [1, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [1, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _RDIM_SIZE_1, num_warps=4, num_stages=3)
    return out

 19%|█▉        | 19/98 [00:07<00:30,  2.57it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.05ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.20ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_1[None, :], other=0)
    _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [1, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [1, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _RDIM_SIZE_1, num_warps=4, num_stages=3)
    return out

 20%|██        | 20/98 [00:07<00:30,  2.57it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.06ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.20ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_1[None, :], other=0)
    _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [1, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [1, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _RDIM_SIZE_1, num_warps=4, num_stages=3)
    return out

 21%|██▏       | 21/98 [00:08<00:30,  2.56it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.06ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.19ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_1[None, :], other=0)
    _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [1, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [1, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _RDIM_SIZE_1, num_warps=4, num_stages=3)
    return out

 22%|██▏       | 22/98 [00:08<00:29,  2.56it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.06ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.19ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_1[None, :], other=0)
    _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [1, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [1, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _RDIM_SIZE_1, num_warps=4, num_stages=3)
    return out

 23%|██▎       | 23/98 [00:08<00:29,  2.55it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.05ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.19ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_1[None, :], other=0)
    _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [1, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [1, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _RDIM_SIZE_1, num_warps=4, num_stages=3)
    return out

 24%|██▍       | 24/98 [00:09<00:29,  2.55it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.06ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.20ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_1[None, :], other=0)
    _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [1, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [1, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _RDIM_SIZE_1, num_warps=4, num_stages=3)
    return out

 26%|██▌       | 25/98 [00:09<00:28,  2.54it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.05ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.20ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_1[None, :], other=0)
    _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [1, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [1, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _RDIM_SIZE_1, num_warps=4, num_stages=3)
    return out

 27%|██▋       | 26/98 [00:10<00:28,  2.53it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.06ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.20ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_1[None, :], other=0)
    _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [1, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [1, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _RDIM_SIZE_1, num_warps=4, num_stages=3)
    return out

 28%|██▊       | 27/98 [00:10<00:28,  2.53it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.06ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.20ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_1[None, :], other=0)
    _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [1, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [1, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _RDIM_SIZE_1, num_warps=4, num_stages=3)
    return out

 29%|██▊       | 28/98 [00:10<00:27,  2.53it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.06ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.19ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_1[None, :], other=0)
    _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [1, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [1, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _RDIM_SIZE_1, num_warps=4, num_stages=3)
    return out

 30%|██▉       | 29/98 [00:11<00:27,  2.52it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.07ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.20ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_1[None, :], other=0)
    _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [1, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [1, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _RDIM_SIZE_1, num_warps=4, num_stages=3)
    return out

 31%|███       | 30/98 [00:11<00:26,  2.52it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.06ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.20ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_1[None, :], other=0)
    _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), load, float('-inf'))
    amax = tl.reshape(tl.max(_mask_to, 1), [1, 1])
    v_0 = load - amax
    v_1 = tl_math.exp(v_0)
    _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), v_1, 0)
    sum_1 = tl.reshape(tl.sum(_mask_to_1, 1), [1, 1])
    v_2 = v_1 / sum_1
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_2, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _RDIM_SIZE_1, num_warps=4, num_stages=3)
    return out

 32%|███▏      | 31/98 [00:12<00:26,  2.52it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.06ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.20ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 33%|███▎      | 32/98 [00:12<00:32,  2.04it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.06ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.20ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 34%|███▎      | 33/98 [00:13<00:30,  2.15it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.06ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.20ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 35%|███▍      | 34/98 [00:13<00:28,  2.23it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.06ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.19ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 36%|███▌      | 35/98 [00:13<00:27,  2.29it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.06ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.19ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 37%|███▋      | 36/98 [00:14<00:26,  2.34it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.06ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.20ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 38%|███▊      | 37/98 [00:14<00:25,  2.37it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.06ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.19ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 39%|███▉      | 38/98 [00:15<00:25,  2.39it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.06ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.20ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 40%|███▉      | 39/98 [00:15<00:24,  2.41it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.06ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.18ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 41%|████      | 40/98 [00:16<00:24,  2.42it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.06ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.19ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 42%|████▏     | 41/98 [00:16<00:23,  2.42it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.06ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.19ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 43%|████▎     | 42/98 [00:16<00:23,  2.42it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.06ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.20ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 44%|████▍     | 43/98 [00:17<00:22,  2.42it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.07ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.18ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 45%|████▍     | 44/98 [00:17<00:22,  2.42it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.06ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.20ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 46%|████▌     | 45/98 [00:18<00:21,  2.42it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.07ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.20ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 47%|████▋     | 46/98 [00:18<00:21,  2.42it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.06ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.19ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 48%|████▊     | 47/98 [00:18<00:21,  2.42it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.06ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.19ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 49%|████▉     | 48/98 [00:19<00:20,  2.42it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.06ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.19ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 50%|█████     | 49/98 [00:19<00:20,  2.41it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.07ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.20ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 51%|█████     | 50/98 [00:20<00:19,  2.41it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.06ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.20ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 52%|█████▏    | 51/98 [00:20<00:19,  2.41it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.06ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.18ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 53%|█████▎    | 52/98 [00:20<00:19,  2.41it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.06ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.20ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 54%|█████▍    | 53/98 [00:21<00:18,  2.40it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.06ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.20ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 55%|█████▌    | 54/98 [00:21<00:18,  2.40it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.06ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.20ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 56%|█████▌    | 55/98 [00:22<00:17,  2.41it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.07ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.18ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 57%|█████▋    | 56/98 [00:22<00:17,  2.40it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.07ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.20ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 58%|█████▊    | 57/98 [00:23<00:17,  2.40it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.07ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.18ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 59%|█████▉    | 58/98 [00:23<00:16,  2.40it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.06ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.18ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 60%|██████    | 59/98 [00:23<00:16,  2.39it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.06ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.19ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 61%|██████    | 60/98 [00:24<00:15,  2.39it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.08ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.18ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 62%|██████▏   | 61/98 [00:24<00:15,  2.39it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.07ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.19ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 63%|██████▎   | 62/98 [00:25<00:15,  2.38it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.07ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.20ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 64%|██████▍   | 63/98 [00:25<00:14,  2.38it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.07ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.20ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 65%|██████▌   | 64/98 [00:26<00:15,  2.21it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.07ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.20ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 66%|██████▋   | 65/98 [00:26<00:14,  2.26it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.07ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.19ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 67%|██████▋   | 66/98 [00:26<00:13,  2.29it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.07ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.20ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 68%|██████▊   | 67/98 [00:27<00:13,  2.32it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.06ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.19ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 69%|██████▉   | 68/98 [00:27<00:12,  2.33it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.08ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.21ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 70%|███████   | 69/98 [00:28<00:12,  2.35it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.07ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.18ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 71%|███████▏  | 70/98 [00:28<00:11,  2.36it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.07ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.18ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 72%|███████▏  | 71/98 [00:29<00:11,  2.36it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.07ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.20ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 73%|███████▎  | 72/98 [00:29<00:11,  2.36it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.06ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.19ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 74%|███████▍  | 73/98 [00:29<00:10,  2.37it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.06ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.20ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 76%|███████▌  | 74/98 [00:30<00:10,  2.37it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.07ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.19ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 77%|███████▋  | 75/98 [00:30<00:09,  2.36it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.06ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.19ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 78%|███████▊  | 76/98 [00:31<00:09,  2.36it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.06ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.20ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 79%|███████▊  | 77/98 [00:31<00:08,  2.36it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.07ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.21ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 80%|███████▉  | 78/98 [00:32<00:08,  2.36it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.07ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.21ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 81%|████████  | 79/98 [00:32<00:08,  2.36it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.07ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.20ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 82%|████████▏ | 80/98 [00:32<00:07,  2.35it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.08ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.21ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 83%|████████▎ | 81/98 [00:33<00:07,  2.35it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.08ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.20ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 84%|████████▎ | 82/98 [00:33<00:06,  2.35it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.08ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.20ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 85%|████████▍ | 83/98 [00:34<00:06,  2.35it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.07ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.20ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 86%|████████▌ | 84/98 [00:34<00:05,  2.35it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.08ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.19ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 87%|████████▋ | 85/98 [00:35<00:05,  2.35it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.06ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.20ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 88%|████████▊ | 86/98 [00:35<00:05,  2.35it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.07ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.19ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 89%|████████▉ | 87/98 [00:35<00:04,  2.35it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.07ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.19ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 90%|████████▉ | 88/98 [00:36<00:04,  2.34it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.07ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.19ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 91%|█████████ | 89/98 [00:36<00:03,  2.34it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.07ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.20ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 92%|█████████▏| 90/98 [00:37<00:05,  1.52it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
Removed 15 outliers from 677 samples
Removed 7 outliers from 807 samples
Removed 10 outliers from 668 samples
Removed 4 outliers from 648 samples
Removed 1 outliers from 765 samples
Removed 1 outliers from 617 samples
Removed 1 outliers from 757 samples
Removed 4 outliers from 600 samples
Removed 2 outliers from 764 samples
Removed 1 outliers from 591 samples
Removed 5 outliers from 755 samples
Removed 4 outliers from 735 samples
Removed 3 outliers from 567 samples
Removed 6 outliers from 744 samples
Removed 5 outliers from 730 samples
Removed 3 outliers from 549 samples
Removed 3 outliers from 723 samples
Removed 13 outliers from 715 samples
Removed 4 outliers from 531 samples
Removed 12 outliers from 718 samples
Removed 6 outliers from 705 samples
Removed 1 outliers from 507 samples
Removed 4 outliers from 705 samples
Removed 16 outliers from 695 samples
Removed 5 outliers from 484 samples
Removed 1 outliers from 685 samples
Removed 3 outliers from 694 samples
Removed 4 outliers from 462 samples
Removed 5 outliers from 689 samples
Removed 10 outliers from 683 samples
Removed 2 outliers from 443 samples
Removed 8 outliers from 678 samples
Removed 9 outliers from 677 samples
Removed 5 outliers from 428 samples
Removed 13 outliers from 672 samples
Removed 11 outliers from 668 samples
Removed 3 outliers from 411 samples
Removed 10 outliers from 660 samples
Removed 26 outliers from 654 samples
Removed 4 outliers from 388 samples
Removed 6 outliers from 647 samples
Removed 7 outliers from 644 samples
Removed 6 outliers from 377 samples
Removed 5 outliers from 645 samples
Removed 13 outliers from 640 samples
Removed 13 outliers from 633 samples
Removed 19 outliers from 625 samples
Removed 1 outliers from 350 samples
Removed 20 outliers from 625 samples
Removed 25 outliers from 614 samples
Removed 2 outliers from 340 samples
Removed 17 outliers from 620 samples
Removed 14 outliers from 612 samples
Removed 2 outliers from 329 samples
Removed 18 outliers from 599 samples
Removed 17 outliers from 608 samples
Removed 4 outliers from 319 samples
Removed 14 outliers from 603 samples
Removed 20 outliers from 596 samples
Removed 8 outliers from 309 samples
Removed 35 outliers from 597 samples
Removed 14 outliers from 590 samples
Removed 12 outliers from 301 samples
Removed 30 outliers from 590 samples
Removed 25 outliers from 585 samples
Removed 10 outliers from 293 samples
Removed 48 outliers from 581 samples
Removed 33 outliers from 575 samples
Removed 8 outliers from 286 samples
Removed 40 outliers from 580 samples
Removed 34 outliers from 575 samples
Removed 25 outliers from 278 samples
Removed 37 outliers from 569 samples
Removed 53 outliers from 568 samples
Removed 19 outliers from 272 samples
Removed 53 outliers from 560 samples
Removed 41 outliers from 554 samples
Removed 15 outliers from 266 samples
Removed 61 outliers from 556 samples
Removed 46 outliers from 554 samples
Removed 9 outliers from 257 samples
Removed 48 outliers from 550 samples
Removed 44 outliers from 550 samples
Removed 6 outliers from 251 samples
Removed 51 outliers from 540 samples
Removed 44 outliers from 533 samples
Removed 6 outliers from 246 samples
Removed 61 outliers from 532 samples
Removed 18 outliers from 508 samples
Removed 5 outliers from 241 samples
Removed 73 outliers from 534 samples
Removed 21 outliers from 507 samples
Removed 1 outliers from 236 samples
Removed 79 outliers from 526 samples
Removed 29 outliers from 496 samples
Removed 64 outliers from 525 samples
Removed 18 outliers from 492 samples
Removed 1 outliers from 227 samples
Removed 65 outliers from 514 samples
Removed 7 outliers from 486 samples
Removed 75 outliers from 513 samples
Removed 8 outliers from 475 samples
Removed 19 outliers from 509 samples
Removed 7 outliers from 472 samples
Removed 25 outliers from 501 samples
Removed 3 outliers from 462 samples
Removed 1 outliers from 210 samples
Removed 1 outliers from 454 samples
Removed 1 outliers from 207 samples
Removed 4 outliers from 451 samples
Removed 2 outliers from 486 samples
Removed 7 outliers from 442 samples
Removed 6 outliers from 200 samples
Removed 4 outliers from 439 samples
Removed 7 outliers from 196 samples
Removed 3 outliers from 428 samples
Removed 7 outliers from 193 samples
Removed 4 outliers from 422 samples
Removed 1 outliers from 189 samples
Removed 6 outliers from 410 samples
Removed 3 outliers from 185 samples
Removed 13 outliers from 396 samples
Removed 3 outliers from 397 samples
Removed 4 outliers from 395 samples
Removed 2 outliers from 178 samples
Removed 1 outliers from 388 samples
Removed 1 outliers from 176 samples
Removed 2 outliers from 383 samples
Removed 1 outliers from 173 samples
Removed 1 outliers from 445 samples
Removed 4 outliers from 368 samples
Removed 4 outliers from 171 samples
Removed 1 outliers from 369 samples
Removed 2 outliers from 167 samples
Removed 31 outliers from 436 samples
Removed 11 outliers from 364 samples
Removed 6 outliers from 423 samples
Removed 3 outliers from 354 samples
Removed 37 outliers from 430 samples
Removed 6 outliers from 352 samples
Removed 2 outliers from 161 samples
Removed 31 outliers from 426 samples
Removed 8 outliers from 349 samples
Removed 1 outliers from 159 samples
Removed 16 outliers from 421 samples
Removed 4 outliers from 338 samples
Removed 2 outliers from 157 samples
Removed 13 outliers from 418 samples
Removed 2 outliers from 337 samples
Removed 1 outliers from 155 samples
Removed 13 outliers from 412 samples
Removed 4 outliers from 332 samples
Removed 13 outliers from 410 samples
Removed 6 outliers from 324 samples
Removed 1 outliers from 150 samples
Removed 14 outliers from 409 samples
Removed 5 outliers from 320 samples
Removed 13 outliers from 404 samples
Removed 1 outliers from 308 samples
Removed 9 outliers from 400 samples
Removed 6 outliers from 315 samples
Removed 1 outliers from 145 samples
Removed 19 outliers from 399 samples
Removed 4 outliers from 308 samples
Removed 4 outliers from 143 samples
Removed 7 outliers from 395 samples
Removed 7 outliers from 303 samples
Removed 3 outliers from 141 samples
Removed 11 outliers from 387 samples
Removed 1 outliers from 299 samples
Removed 1 outliers from 140 samples
Removed 12 outliers from 388 samples
Removed 4 outliers from 293 samples
Removed 1 outliers from 138 samples
Removed 12 outliers from 385 samples
Removed 1 outliers from 289 samples
Removed 5 outliers from 379 samples
Removed 4 outliers from 285 samples
Removed 1 outliers from 135 samples
Removed 14 outliers from 380 samples
Removed 6 outliers from 283 samples
Removed 2 outliers from 133 samples
Removed 5 outliers from 376 samples
Removed 1 outliers from 279 samples
Removed 6 outliers from 375 samples
Removed 2 outliers from 275 samples
Removed 1 outliers from 131 samples
Removed 8 outliers from 372 samples
Removed 1 outliers from 270 samples
Removed 12 outliers from 369 samples
Removed 3 outliers from 269 samples
Removed 6 outliers from 368 samples
Removed 2 outliers from 267 samples
Removed 1 outliers from 127 samples
Removed 4 outliers from 365 samples
Removed 3 outliers from 262 samples
Removed 5 outliers from 361 samples
Removed 8 outliers from 261 samples
Removed 3 outliers from 124 samples
Removed 4 outliers from 359 samples
Removed 3 outliers from 258 samples
Removed 6 outliers from 358 samples
Removed 4 outliers from 255 samples
Removed 4 outliers from 252 samples
Removed 3 outliers from 121 samples
Removed 4 outliers from 351 samples
Removed 7 outliers from 250 samples
Removed 1 outliers from 119 samples
Removed 4 outliers from 348 samples
Removed 5 outliers from 247 samples
Removed 1 outliers from 118 samples
Removed 4 outliers from 347 samples
Removed 6 outliers from 243 samples
Removed 1 outliers from 345 samples
Removed 8 outliers from 240 samples
Removed 2 outliers from 116 samples
Removed 1 outliers from 342 samples
Removed 3 outliers from 238 samples
Removed 4 outliers from 342 samples
Removed 1 outliers from 114 samples
Removed 2 outliers from 334 samples
Removed 4 outliers from 230 samples
Removed 5 outliers from 112 samples
Removed 2 outliers from 333 samples
Removed 1 outliers from 227 samples
INFO:tritonbench.utils.triton_op:Took 0.08ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.20ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 93%|█████████▎| 91/98 [00:38<00:04,  1.70it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.07ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.19ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 94%|█████████▍| 92/98 [00:38<00:03,  1.85it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.07ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.20ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 95%|█████████▍| 93/98 [00:39<00:02,  1.97it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.07ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.21ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 96%|█████████▌| 94/98 [00:39<00:01,  2.07it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.07ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.20ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 97%|█████████▋| 95/98 [00:40<00:01,  2.14it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.07ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.20ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 98%|█████████▊| 96/98 [00:40<00:00,  2.19it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.07ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.21ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

 99%|█████████▉| 97/98 [00:40<00:00,  2.23it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for naive_softmax
INFO:tritonbench.utils.triton_op:Took 0.07ms to get benchmark function for triton_softmax
INFO:tritonbench.utils.triton_op:Took 0.21ms to get benchmark function for helion_softmax
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _softmax_kernel(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.reshape(tl.max(amax_acc, 1), [1, 1])
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_1 = load_1 - amax_copy
        v_2 = tl_math.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, 0)
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.reshape(tl.sum(sum_1_acc, 1), [1, 1])
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_4 = load_2 - amax_copy_1
        v_5 = tl_math.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_6, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 4096
    _launcher(_softmax_kernel, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

100%|██████████| 98/98 [00:41<00:00,  2.25it/s]
100%|██████████| 98/98 [00:41<00:00,  2.37it/s]

Benchmark Results:
  x_val    triton_softmax-speedup    helion_softmax-speedup
-------  ------------------------  ------------------------
 2176.0                   3.64643                   3.30421
 2240.0                   3.27528                   3.30312
 2304.0                   3.17032                   3.17032
 2368.0                   3.15551                   3.16234
 2432.0                   3.08539                   3.15116
 2496.0                   3.15631                   3.15071
 2560.0                   3.17799                   3.14744
 2624.0                   2.93681                   3.15339
 2688.0                   3.06649                   3.16555
 2752.0                   3.15904                   3.37018
 2816.0                   3.25112                   3.41501
 2880.0                   3.36555                   3.60811
 2944.0                   3.54536                   3.72564
 3008.0                   3.58879                   3.7602
 3072.0                   3.70461                   3.86158
 3136.0                   3.72659                   3.77392
 3200.0                   3.849                     3.91823
 3264.0                   3.85509                   3.9844
 3328.0                   3.95186                   4.06147
 3392.0                   3.97003                   4.0548
 3456.0                   4.00467                   4.15443
 3520.0                   4.09954                   4.20347
 3584.0                   4.10621                   4.2162
 3648.0                   4.14018                   4.21921
 3712.0                   4.17259                   4.2979
 3776.0                   4.15949                   4.27675
 3840.0                   4.18394                   4.32959
 3904.0                   4.20789                   4.35458
 3968.0                   4.25785                   4.33141
 4032.0                   4.25607                   4.36636
 4096.0                   4.27943                   4.40374
 4160.0                   4.32425                   3.89915
 4224.0                   4.31319                   3.84333
 4288.0                   4.31847                   3.81181
 4352.0                   4.3122                    3.80517
 4416.0                   4.33686                   3.78216
 4480.0                   4.35853                   3.68436
 4544.0                   4.34094                   3.70492
 4608.0                   4.34167                   3.64849
 4672.0                   4.36078                   3.61575
 4736.0                   4.33892                   3.60012
 4800.0                   4.35101                   3.5533
 4864.0                   4.35361                   3.53399
 4928.0                   4.38724                   3.41779
 4992.0                   4.3508                    3.43243
 5056.0                   4.37943                   3.37448
 5120.0                   4.40719                   3.26184
 5184.0                   4.35047                   3.29273
 5248.0                   4.31383                   3.244
 5312.0                   4.33972                   3.20895
 5376.0                   4.29543                   3.15466
 5440.0                   4.32353                   3.13305
 5504.0                   4.3202                    3.09192
 5568.0                   4.33868                   3.09475
 5632.0                   4.30603                   3.01236
 5696.0                   4.31784                   3.00115
 5760.0                   4.32343                   3.00284
 5824.0                   4.31578                   2.931
 5888.0                   4.33272                   2.91622
 5952.0                   4.31331                   2.89553
 6016.0                   4.34282                   2.83133
 6080.0                   4.34014                   2.81304
 6144.0                   4.31771                   2.70831
 6208.0                   4.32084                   2.88243
 6272.0                   4.33486                   2.79729
 6336.0                   4.33207                   2.75879
 6400.0                   4.33404                   2.72619
 6464.0                   4.33711                   2.6997
 6528.0                   4.3169                    2.69508
 6592.0                   4.31719                   2.65861
 6656.0                   4.32365                   2.65364
 6720.0                   4.30962                   2.64051
 6784.0                   4.32497                   2.62233
 6848.0                   4.31281                   2.60523
 6912.0                   4.31583                   2.60831
 6976.0                   4.3005                    2.58308
 7040.0                   4.29793                   2.57918
 7104.0                   4.29101                   2.56122
 7168.0                   4.29488                   2.54578
 7232.0                   4.30631                   2.53516
 7296.0                   4.31622                   2.54623
 7360.0                   4.3148                    2.52385
 7424.0                   4.31264                   2.51194
 7488.0                   4.31524                   2.50126
 7552.0                   4.29806                   2.48908
 7616.0                   4.29559                   2.47714
 7680.0                   4.29558                   2.46866
 7744.0                   4.30491                   2.44811
 7808.0                   4.2933                    2.43621
 7872.0                   4.29291                   2.42729
 7936.0                   4.30018                   2.40989
 8000.0                   4.29687                   2.39547
 8064.0                   4.28696                   2.38301
 8128.0                   4.27558                   2.37425
 8192.0                   4.27571                   2.37265
 8256.0                   4.28498                   2.34368
 8320.0                   4.28555                   2.33267
 8384.0                   4.28856                   2.32508
average                   4.12221                   3.18995

============================================================
Kernel: flash_attention
============================================================

INFO:root:TMA benchmarks will be running without grid constant TMA descriptor.
TMA benchmarks will be running without grid constant TMA descriptor.
Running flash_attention benchmark with Helion implementation...

Removed 3 outliers from 111 samples
Removed 1 outliers from 331 samples
Removed 2 outliers from 225 samples
Removed 1 outliers from 110 samples
Removed 2 outliers from 325 samples
Removed 5 outliers from 221 samples
Removed 1 outliers from 326 samples
Removed 1 outliers from 220 samples
Removed 1 outliers from 108 samples
Removed 1 outliers from 322 samples
Removed 2 outliers from 322 samples
Removed 5 outliers from 216 samples
Removed 1 outliers from 106 samples
Removed 1 outliers from 211 samples
Removed 1 outliers from 105 samples
Removed 1 outliers from 320 samples
Removed 4 outliers from 104 samples
Removed 3 outliers from 317 samples
Removed 2 outliers from 208 samples

  0%|          | 0/8 [00:00<?, ?it/s]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for aten
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for sdpa
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_tutorial_flash_v2
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_tutorial_flash_v2_tma
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_tutorial_flash_v2_ws
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_tutorial_flash_v2_tma_ws
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_tutorial_flash_v2_tma_ws_persistent
INFO:tritonbench.utils.triton_op:Took 1.36ms to get benchmark function for flex_attention
INFO:tritonbench.utils.triton_op:Took 0.03ms to get benchmark function for helion_flash_attention
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_compat import libdevice
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _attention_kernel(q_view, k_view, v_view, out, _BLOCK_SIZE_1: tl.constexpr, _RDIM_SIZE_2: tl.constexpr, _BLOCK_SIZE_3: tl.constexpr):
    num_blocks_0 = 192
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    indices_4 = tl.arange(0, _RDIM_SIZE_2).to(tl.int32)
    m_i = tl.full([1, _BLOCK_SIZE_1], float('-inf'), tl.float32)
    l_i = tl.full([1, _BLOCK_SIZE_1], 1.0, tl.float32)
    acc = tl.full([1, _BLOCK_SIZE_1, 64], 0.0, tl.float32)
    q = tl.load(q_view + (indices_0[:, None, None] * 8192 + indices_1[None, :, None] * 64 + indices_4[None, None, :] * 1), None)
    for offset_2 in tl.range(0, 128, _BLOCK_SIZE_3):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_3).to(tl.int32)
        q_copy = q
        m_i_copy = m_i
        l_i_copy = l_i
        acc_copy = acc
        q_copy_0 = q_copy
        m_i_copy_0 = m_i_copy
        l_i_copy_0 = l_i_copy
        acc_copy_0 = acc_copy
        k = tl.load(k_view + (indices_0[:, None, None] * 8192 + indices_4[None, :, None] * 1 + indices_2[None, None, :] * 64), None)
        qk = tl.reshape(tl.dot(tl.reshape(q_copy_0, [_BLOCK_SIZE_1, 64]), tl.reshape(k, [64, _BLOCK_SIZE_3]), input_precision='tf32'), [1, _BLOCK_SIZE_1, _BLOCK_SIZE_3])
        amax = tl.max(qk, 2)
        v_0 = tl.full([], 0.18033688, tl.bfloat16)
        v_1 = amax * v_0
        v_2 = v_1.to(tl.float32)
        v_3 = triton_helpers.maximum(m_i_copy_0, v_2)
        v_4 = tl.full([], 0.18033688, tl.bfloat16)
        v_5 = qk * v_4
        subscript = v_3[:, :, None]
        v_6 = v_5.to(tl.float32)
        v_7 = v_6 - subscript
        v_8 = libdevice.exp2(v_7)
        l_ij = tl.sum(v_8, 2)
        v_9 = m_i_copy_0 - v_3
        v_10 = libdevice.exp2(v_9)
        v_11 = l_i_copy_0 * v_10
        l_i = v_11 + l_ij
        subscript_1 = v_10[:, :, None]
        v_13 = acc_copy_0 * subscript_1
        v = tl.load(v_view + (indices_0[:, None, None] * 8192 + indices_2[None, :, None] * 64 + indices_4[None, None, :] * 1), None)
        v_14 = v_8.to(tl.bfloat16)
        acc = tl.reshape(tl.dot(tl.reshape(v_14, [_BLOCK_SIZE_1, _BLOCK_SIZE_3]), tl.reshape(v, [_BLOCK_SIZE_3, 64]), acc=tl.reshape(v_13, [_BLOCK_SIZE_1, 64]), input_precision='tf32'), [1, _BLOCK_SIZE_1, 64])
        m_i = v_3
    subscript_2 = l_i[:, :, None]
    v_15 = acc / subscript_2
    v_16 = v_15.to(tl.bfloat16)
    tl.store(out + (indices_0[:, None, None] * 8192 + indices_1[None, :, None] * 64 + indices_4[None, None, :] * 1), v_16, None)

def attention(q_in: torch.Tensor, k_in: torch.Tensor, v_in: torch.Tensor, *, _launcher=_default_launcher):
    m_dim = q_in.size(-2)
    n_dim = k_in.size(-2)
    assert n_dim == v_in.size(-2)
    head_dim = 64
    assert head_dim == k_in.size(-1) == v_in.size(-1)
    q_view = q_in.reshape([-1, m_dim, head_dim])
    v_view = v_in.reshape([-1, n_dim, head_dim])
    k_view = k_in.reshape([-1, n_dim, head_dim]).transpose(1, 2)
    out = torch.empty_like(q_view)
    _BLOCK_SIZE_1 = 32
    _RDIM_SIZE_2 = 64
    _BLOCK_SIZE_3 = 32
    _launcher(_attention_kernel, (192 * triton.cdiv(128, _BLOCK_SIZE_1),), q_view, k_view, v_view, out, _BLOCK_SIZE_1, _RDIM_SIZE_2, _BLOCK_SIZE_3, num_warps=4, num_stages=3)
    return out.view(q_in.size())

 12%|█▎        | 1/8 [00:04<00:28,  4.05s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for aten
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for sdpa
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_tutorial_flash_v2
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_tutorial_flash_v2_tma
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_tutorial_flash_v2_ws
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_tutorial_flash_v2_tma_ws
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_tutorial_flash_v2_tma_ws_persistent
INFO:tritonbench.utils.triton_op:Took 1.35ms to get benchmark function for flex_attention
INFO:tritonbench.utils.triton_op:Took 0.73ms to get benchmark function for helion_flash_attention
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_compat import libdevice
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _attention_kernel(q_view, k_view, v_view, out, _BLOCK_SIZE_1: tl.constexpr, _RDIM_SIZE_2: tl.constexpr, _BLOCK_SIZE_3: tl.constexpr):
    num_blocks_0 = 192
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    indices_4 = tl.arange(0, _RDIM_SIZE_2).to(tl.int32)
    m_i = tl.full([1, _BLOCK_SIZE_1], float('-inf'), tl.float32)
    l_i = tl.full([1, _BLOCK_SIZE_1], 1.0, tl.float32)
    acc = tl.full([1, _BLOCK_SIZE_1, 64], 0.0, tl.float32)
    q = tl.load(q_view + (indices_0[:, None, None] * 16384 + indices_1[None, :, None] * 64 + indices_4[None, None, :] * 1), None)
    for offset_2 in tl.range(0, 256, _BLOCK_SIZE_3):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_3).to(tl.int32)
        q_copy = q
        m_i_copy = m_i
        l_i_copy = l_i
        acc_copy = acc
        q_copy_0 = q_copy
        m_i_copy_0 = m_i_copy
        l_i_copy_0 = l_i_copy
        acc_copy_0 = acc_copy
        k = tl.load(k_view + (indices_0[:, None, None] * 16384 + indices_4[None, :, None] * 1 + indices_2[None, None, :] * 64), None)
        qk = tl.reshape(tl.dot(tl.reshape(q_copy_0, [_BLOCK_SIZE_1, 64]), tl.reshape(k, [64, _BLOCK_SIZE_3]), input_precision='tf32'), [1, _BLOCK_SIZE_1, _BLOCK_SIZE_3])
        amax = tl.max(qk, 2)
        v_0 = tl.full([], 0.18033688, tl.bfloat16)
        v_1 = amax * v_0
        v_2 = v_1.to(tl.float32)
        v_3 = triton_helpers.maximum(m_i_copy_0, v_2)
        v_4 = tl.full([], 0.18033688, tl.bfloat16)
        v_5 = qk * v_4
        subscript = v_3[:, :, None]
        v_6 = v_5.to(tl.float32)
        v_7 = v_6 - subscript
        v_8 = libdevice.exp2(v_7)
        l_ij = tl.sum(v_8, 2)
        v_9 = m_i_copy_0 - v_3
        v_10 = libdevice.exp2(v_9)
        v_11 = l_i_copy_0 * v_10
        l_i = v_11 + l_ij
        subscript_1 = v_10[:, :, None]
        v_13 = acc_copy_0 * subscript_1
        v = tl.load(v_view + (indices_0[:, None, None] * 16384 + indices_2[None, :, None] * 64 + indices_4[None, None, :] * 1), None)
        v_14 = v_8.to(tl.bfloat16)
        acc = tl.reshape(tl.dot(tl.reshape(v_14, [_BLOCK_SIZE_1, _BLOCK_SIZE_3]), tl.reshape(v, [_BLOCK_SIZE_3, 64]), acc=tl.reshape(v_13, [_BLOCK_SIZE_1, 64]), input_precision='tf32'), [1, _BLOCK_SIZE_1, 64])
        m_i = v_3
    subscript_2 = l_i[:, :, None]
    v_15 = acc / subscript_2
    v_16 = v_15.to(tl.bfloat16)
    tl.store(out + (indices_0[:, None, None] * 16384 + indices_1[None, :, None] * 64 + indices_4[None, None, :] * 1), v_16, None)

def attention(q_in: torch.Tensor, k_in: torch.Tensor, v_in: torch.Tensor, *, _launcher=_default_launcher):
    m_dim = q_in.size(-2)
    n_dim = k_in.size(-2)
    assert n_dim == v_in.size(-2)
    head_dim = 64
    assert head_dim == k_in.size(-1) == v_in.size(-1)
    q_view = q_in.reshape([-1, m_dim, head_dim])
    v_view = v_in.reshape([-1, n_dim, head_dim])
    k_view = k_in.reshape([-1, n_dim, head_dim]).transpose(1, 2)
    out = torch.empty_like(q_view)
    _BLOCK_SIZE_1 = 32
    _RDIM_SIZE_2 = 64
    _BLOCK_SIZE_3 = 32
    _launcher(_attention_kernel, (192 * triton.cdiv(256, _BLOCK_SIZE_1),), q_view, k_view, v_view, out, _BLOCK_SIZE_1, _RDIM_SIZE_2, _BLOCK_SIZE_3, num_warps=4, num_stages=3)
    return out.view(q_in.size())

 25%|██▌       | 2/8 [00:07<00:23,  3.95s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for aten
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for sdpa
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_tutorial_flash_v2
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_tutorial_flash_v2_tma
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_tutorial_flash_v2_ws
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_tutorial_flash_v2_tma_ws
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_tutorial_flash_v2_tma_ws_persistent
INFO:tritonbench.utils.triton_op:Took 1.33ms to get benchmark function for flex_attention
INFO:tritonbench.utils.triton_op:Took 0.81ms to get benchmark function for helion_flash_attention
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_compat import libdevice
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _attention_kernel(q_view, k_view, v_view, out, _BLOCK_SIZE_1: tl.constexpr, _RDIM_SIZE_2: tl.constexpr, _BLOCK_SIZE_3: tl.constexpr):
    num_blocks_0 = 192
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    indices_4 = tl.arange(0, _RDIM_SIZE_2).to(tl.int32)
    m_i = tl.full([1, _BLOCK_SIZE_1], float('-inf'), tl.float32)
    l_i = tl.full([1, _BLOCK_SIZE_1], 1.0, tl.float32)
    acc = tl.full([1, _BLOCK_SIZE_1, 64], 0.0, tl.float32)
    q = tl.load(q_view + (indices_0[:, None, None] * 32768 + indices_1[None, :, None] * 64 + indices_4[None, None, :] * 1), None)
    for offset_2 in tl.range(0, 512, _BLOCK_SIZE_3):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_3).to(tl.int32)
        q_copy = q
        m_i_copy = m_i
        l_i_copy = l_i
        acc_copy = acc
        q_copy_0 = q_copy
        m_i_copy_0 = m_i_copy
        l_i_copy_0 = l_i_copy
        acc_copy_0 = acc_copy
        k = tl.load(k_view + (indices_0[:, None, None] * 32768 + indices_4[None, :, None] * 1 + indices_2[None, None, :] * 64), None)
        qk = tl.reshape(tl.dot(tl.reshape(q_copy_0, [_BLOCK_SIZE_1, 64]), tl.reshape(k, [64, _BLOCK_SIZE_3]), input_precision='tf32'), [1, _BLOCK_SIZE_1, _BLOCK_SIZE_3])
        amax = tl.max(qk, 2)
        v_0 = tl.full([], 0.18033688, tl.bfloat16)
        v_1 = amax * v_0
        v_2 = v_1.to(tl.float32)
        v_3 = triton_helpers.maximum(m_i_copy_0, v_2)
        v_4 = tl.full([], 0.18033688, tl.bfloat16)
        v_5 = qk * v_4
        subscript = v_3[:, :, None]
        v_6 = v_5.to(tl.float32)
        v_7 = v_6 - subscript
        v_8 = libdevice.exp2(v_7)
        l_ij = tl.sum(v_8, 2)
        v_9 = m_i_copy_0 - v_3
        v_10 = libdevice.exp2(v_9)
        v_11 = l_i_copy_0 * v_10
        l_i = v_11 + l_ij
        subscript_1 = v_10[:, :, None]
        v_13 = acc_copy_0 * subscript_1
        v = tl.load(v_view + (indices_0[:, None, None] * 32768 + indices_2[None, :, None] * 64 + indices_4[None, None, :] * 1), None)
        v_14 = v_8.to(tl.bfloat16)
        acc = tl.reshape(tl.dot(tl.reshape(v_14, [_BLOCK_SIZE_1, _BLOCK_SIZE_3]), tl.reshape(v, [_BLOCK_SIZE_3, 64]), acc=tl.reshape(v_13, [_BLOCK_SIZE_1, 64]), input_precision='tf32'), [1, _BLOCK_SIZE_1, 64])
        m_i = v_3
    subscript_2 = l_i[:, :, None]
    v_15 = acc / subscript_2
    v_16 = v_15.to(tl.bfloat16)
    tl.store(out + (indices_0[:, None, None] * 32768 + indices_1[None, :, None] * 64 + indices_4[None, None, :] * 1), v_16, None)

def attention(q_in: torch.Tensor, k_in: torch.Tensor, v_in: torch.Tensor, *, _launcher=_default_launcher):
    m_dim = q_in.size(-2)
    n_dim = k_in.size(-2)
    assert n_dim == v_in.size(-2)
    head_dim = 64
    assert head_dim == k_in.size(-1) == v_in.size(-1)
    q_view = q_in.reshape([-1, m_dim, head_dim])
    v_view = v_in.reshape([-1, n_dim, head_dim])
    k_view = k_in.reshape([-1, n_dim, head_dim]).transpose(1, 2)
    out = torch.empty_like(q_view)
    _BLOCK_SIZE_1 = 32
    _RDIM_SIZE_2 = 64
    _BLOCK_SIZE_3 = 32
    _launcher(_attention_kernel, (192 * triton.cdiv(512, _BLOCK_SIZE_1),), q_view, k_view, v_view, out, _BLOCK_SIZE_1, _RDIM_SIZE_2, _BLOCK_SIZE_3, num_warps=4, num_stages=3)
    return out.view(q_in.size())

 38%|███▊      | 3/8 [00:12<00:20,  4.05s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for aten
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for sdpa
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_tutorial_flash_v2
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_tutorial_flash_v2_tma
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_tutorial_flash_v2_ws
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_tutorial_flash_v2_tma_ws
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_tutorial_flash_v2_tma_ws_persistent
INFO:tritonbench.utils.triton_op:Took 1.40ms to get benchmark function for flex_attention
INFO:tritonbench.utils.triton_op:Took 0.82ms to get benchmark function for helion_flash_attention
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_compat import libdevice
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _attention_kernel(q_view, k_view, v_view, out, _BLOCK_SIZE_1: tl.constexpr, _RDIM_SIZE_2: tl.constexpr, _BLOCK_SIZE_3: tl.constexpr):
    num_blocks_0 = 192
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    indices_4 = tl.arange(0, _RDIM_SIZE_2).to(tl.int32)
    m_i = tl.full([1, _BLOCK_SIZE_1], float('-inf'), tl.float32)
    l_i = tl.full([1, _BLOCK_SIZE_1], 1.0, tl.float32)
    acc = tl.full([1, _BLOCK_SIZE_1, 64], 0.0, tl.float32)
    q = tl.load(q_view + (indices_0[:, None, None] * 65536 + indices_1[None, :, None] * 64 + indices_4[None, None, :] * 1), None)
    for offset_2 in tl.range(0, 1024, _BLOCK_SIZE_3):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_3).to(tl.int32)
        q_copy = q
        m_i_copy = m_i
        l_i_copy = l_i
        acc_copy = acc
        q_copy_0 = q_copy
        m_i_copy_0 = m_i_copy
        l_i_copy_0 = l_i_copy
        acc_copy_0 = acc_copy
        k = tl.load(k_view + (indices_0[:, None, None] * 65536 + indices_4[None, :, None] * 1 + indices_2[None, None, :] * 64), None)
        qk = tl.reshape(tl.dot(tl.reshape(q_copy_0, [_BLOCK_SIZE_1, 64]), tl.reshape(k, [64, _BLOCK_SIZE_3]), input_precision='tf32'), [1, _BLOCK_SIZE_1, _BLOCK_SIZE_3])
        amax = tl.max(qk, 2)
        v_0 = tl.full([], 0.18033688, tl.bfloat16)
        v_1 = amax * v_0
        v_2 = v_1.to(tl.float32)
        v_3 = triton_helpers.maximum(m_i_copy_0, v_2)
        v_4 = tl.full([], 0.18033688, tl.bfloat16)
        v_5 = qk * v_4
        subscript = v_3[:, :, None]
        v_6 = v_5.to(tl.float32)
        v_7 = v_6 - subscript
        v_8 = libdevice.exp2(v_7)
        l_ij = tl.sum(v_8, 2)
        v_9 = m_i_copy_0 - v_3
        v_10 = libdevice.exp2(v_9)
        v_11 = l_i_copy_0 * v_10
        l_i = v_11 + l_ij
        subscript_1 = v_10[:, :, None]
        v_13 = acc_copy_0 * subscript_1
        v = tl.load(v_view + (indices_0[:, None, None] * 65536 + indices_2[None, :, None] * 64 + indices_4[None, None, :] * 1), None)
        v_14 = v_8.to(tl.bfloat16)
        acc = tl.reshape(tl.dot(tl.reshape(v_14, [_BLOCK_SIZE_1, _BLOCK_SIZE_3]), tl.reshape(v, [_BLOCK_SIZE_3, 64]), acc=tl.reshape(v_13, [_BLOCK_SIZE_1, 64]), input_precision='tf32'), [1, _BLOCK_SIZE_1, 64])
        m_i = v_3
    subscript_2 = l_i[:, :, None]
    v_15 = acc / subscript_2
    v_16 = v_15.to(tl.bfloat16)
    tl.store(out + (indices_0[:, None, None] * 65536 + indices_1[None, :, None] * 64 + indices_4[None, None, :] * 1), v_16, None)

def attention(q_in: torch.Tensor, k_in: torch.Tensor, v_in: torch.Tensor, *, _launcher=_default_launcher):
    m_dim = q_in.size(-2)
    n_dim = k_in.size(-2)
    assert n_dim == v_in.size(-2)
    head_dim = 64
    assert head_dim == k_in.size(-1) == v_in.size(-1)
    q_view = q_in.reshape([-1, m_dim, head_dim])
    v_view = v_in.reshape([-1, n_dim, head_dim])
    k_view = k_in.reshape([-1, n_dim, head_dim]).transpose(1, 2)
    out = torch.empty_like(q_view)
    _BLOCK_SIZE_1 = 32
    _RDIM_SIZE_2 = 64
    _BLOCK_SIZE_3 = 32
    _launcher(_attention_kernel, (192 * triton.cdiv(1024, _BLOCK_SIZE_1),), q_view, k_view, v_view, out, _BLOCK_SIZE_1, _RDIM_SIZE_2, _BLOCK_SIZE_3, num_warps=4, num_stages=3)
    return out.view(q_in.size())

 50%|█████     | 4/8 [00:16<00:16,  4.16s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for aten
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for sdpa
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_tutorial_flash_v2
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_tutorial_flash_v2_tma
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_tutorial_flash_v2_ws
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_tutorial_flash_v2_tma_ws
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_tutorial_flash_v2_tma_ws_persistent
INFO:tritonbench.utils.triton_op:Took 1.45ms to get benchmark function for flex_attention
INFO:tritonbench.utils.triton_op:Took 0.87ms to get benchmark function for helion_flash_attention
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_compat import libdevice
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _attention_kernel(q_view, k_view, v_view, out, _BLOCK_SIZE_1: tl.constexpr, _RDIM_SIZE_2: tl.constexpr, _BLOCK_SIZE_3: tl.constexpr):
    num_blocks_0 = 192
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    indices_4 = tl.arange(0, _RDIM_SIZE_2).to(tl.int32)
    m_i = tl.full([1, _BLOCK_SIZE_1], float('-inf'), tl.float32)
    l_i = tl.full([1, _BLOCK_SIZE_1], 1.0, tl.float32)
    acc = tl.full([1, _BLOCK_SIZE_1, 64], 0.0, tl.float32)
    q = tl.load(q_view + (indices_0[:, None, None] * 131072 + indices_1[None, :, None] * 64 + indices_4[None, None, :] * 1), None)
    for offset_2 in tl.range(0, 2048, _BLOCK_SIZE_3):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_3).to(tl.int32)
        q_copy = q
        m_i_copy = m_i
        l_i_copy = l_i
        acc_copy = acc
        q_copy_0 = q_copy
        m_i_copy_0 = m_i_copy
        l_i_copy_0 = l_i_copy
        acc_copy_0 = acc_copy
        k = tl.load(k_view + (indices_0[:, None, None] * 131072 + indices_4[None, :, None] * 1 + indices_2[None, None, :] * 64), None)
        qk = tl.reshape(tl.dot(tl.reshape(q_copy_0, [_BLOCK_SIZE_1, 64]), tl.reshape(k, [64, _BLOCK_SIZE_3]), input_precision='tf32'), [1, _BLOCK_SIZE_1, _BLOCK_SIZE_3])
        amax = tl.max(qk, 2)
        v_0 = tl.full([], 0.18033688, tl.bfloat16)
        v_1 = amax * v_0
        v_2 = v_1.to(tl.float32)
        v_3 = triton_helpers.maximum(m_i_copy_0, v_2)
        v_4 = tl.full([], 0.18033688, tl.bfloat16)
        v_5 = qk * v_4
        subscript = v_3[:, :, None]
        v_6 = v_5.to(tl.float32)
        v_7 = v_6 - subscript
        v_8 = libdevice.exp2(v_7)
        l_ij = tl.sum(v_8, 2)
        v_9 = m_i_copy_0 - v_3
        v_10 = libdevice.exp2(v_9)
        v_11 = l_i_copy_0 * v_10
        l_i = v_11 + l_ij
        subscript_1 = v_10[:, :, None]
        v_13 = acc_copy_0 * subscript_1
        v = tl.load(v_view + (indices_0[:, None, None] * 131072 + indices_2[None, :, None] * 64 + indices_4[None, None, :] * 1), None)
        v_14 = v_8.to(tl.bfloat16)
        acc = tl.reshape(tl.dot(tl.reshape(v_14, [_BLOCK_SIZE_1, _BLOCK_SIZE_3]), tl.reshape(v, [_BLOCK_SIZE_3, 64]), acc=tl.reshape(v_13, [_BLOCK_SIZE_1, 64]), input_precision='tf32'), [1, _BLOCK_SIZE_1, 64])
        m_i = v_3
    subscript_2 = l_i[:, :, None]
    v_15 = acc / subscript_2
    v_16 = v_15.to(tl.bfloat16)
    tl.store(out + (indices_0[:, None, None] * 131072 + indices_1[None, :, None] * 64 + indices_4[None, None, :] * 1), v_16, None)

def attention(q_in: torch.Tensor, k_in: torch.Tensor, v_in: torch.Tensor, *, _launcher=_default_launcher):
    m_dim = q_in.size(-2)
    n_dim = k_in.size(-2)
    assert n_dim == v_in.size(-2)
    head_dim = 64
    assert head_dim == k_in.size(-1) == v_in.size(-1)
    q_view = q_in.reshape([-1, m_dim, head_dim])
    v_view = v_in.reshape([-1, n_dim, head_dim])
    k_view = k_in.reshape([-1, n_dim, head_dim]).transpose(1, 2)
    out = torch.empty_like(q_view)
    _BLOCK_SIZE_1 = 32
    _RDIM_SIZE_2 = 64
    _BLOCK_SIZE_3 = 32
    _launcher(_attention_kernel, (192 * triton.cdiv(2048, _BLOCK_SIZE_1),), q_view, k_view, v_view, out, _BLOCK_SIZE_1, _RDIM_SIZE_2, _BLOCK_SIZE_3, num_warps=4, num_stages=3)
    return out.view(q_in.size())

 62%|██████▎   | 5/8 [00:21<00:13,  4.35s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for aten
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for sdpa
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_tutorial_flash_v2
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_tutorial_flash_v2_tma
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_tutorial_flash_v2_ws
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_tutorial_flash_v2_tma_ws
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_tutorial_flash_v2_tma_ws_persistent
INFO:tritonbench.utils.triton_op:Took 1.41ms to get benchmark function for flex_attention
INFO:tritonbench.utils.triton_op:Took 0.85ms to get benchmark function for helion_flash_attention
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_compat import libdevice
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _attention_kernel(q_view, k_view, v_view, out, _BLOCK_SIZE_1: tl.constexpr, _RDIM_SIZE_2: tl.constexpr, _BLOCK_SIZE_3: tl.constexpr):
    num_blocks_0 = 192
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    indices_4 = tl.arange(0, _RDIM_SIZE_2).to(tl.int32)
    m_i = tl.full([1, _BLOCK_SIZE_1], float('-inf'), tl.float32)
    l_i = tl.full([1, _BLOCK_SIZE_1], 1.0, tl.float32)
    acc = tl.full([1, _BLOCK_SIZE_1, 64], 0.0, tl.float32)
    q = tl.load(q_view + (indices_0[:, None, None] * 262144 + indices_1[None, :, None] * 64 + indices_4[None, None, :] * 1), None)
    for offset_2 in tl.range(0, 4096, _BLOCK_SIZE_3):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_3).to(tl.int32)
        q_copy = q
        m_i_copy = m_i
        l_i_copy = l_i
        acc_copy = acc
        q_copy_0 = q_copy
        m_i_copy_0 = m_i_copy
        l_i_copy_0 = l_i_copy
        acc_copy_0 = acc_copy
        k = tl.load(k_view + (indices_0[:, None, None] * 262144 + indices_4[None, :, None] * 1 + indices_2[None, None, :] * 64), None)
        qk = tl.reshape(tl.dot(tl.reshape(q_copy_0, [_BLOCK_SIZE_1, 64]), tl.reshape(k, [64, _BLOCK_SIZE_3]), input_precision='tf32'), [1, _BLOCK_SIZE_1, _BLOCK_SIZE_3])
        amax = tl.max(qk, 2)
        v_0 = tl.full([], 0.18033688, tl.bfloat16)
        v_1 = amax * v_0
        v_2 = v_1.to(tl.float32)
        v_3 = triton_helpers.maximum(m_i_copy_0, v_2)
        v_4 = tl.full([], 0.18033688, tl.bfloat16)
        v_5 = qk * v_4
        subscript = v_3[:, :, None]
        v_6 = v_5.to(tl.float32)
        v_7 = v_6 - subscript
        v_8 = libdevice.exp2(v_7)
        l_ij = tl.sum(v_8, 2)
        v_9 = m_i_copy_0 - v_3
        v_10 = libdevice.exp2(v_9)
        v_11 = l_i_copy_0 * v_10
        l_i = v_11 + l_ij
        subscript_1 = v_10[:, :, None]
        v_13 = acc_copy_0 * subscript_1
        v = tl.load(v_view + (indices_0[:, None, None] * 262144 + indices_2[None, :, None] * 64 + indices_4[None, None, :] * 1), None)
        v_14 = v_8.to(tl.bfloat16)
        acc = tl.reshape(tl.dot(tl.reshape(v_14, [_BLOCK_SIZE_1, _BLOCK_SIZE_3]), tl.reshape(v, [_BLOCK_SIZE_3, 64]), acc=tl.reshape(v_13, [_BLOCK_SIZE_1, 64]), input_precision='tf32'), [1, _BLOCK_SIZE_1, 64])
        m_i = v_3
    subscript_2 = l_i[:, :, None]
    v_15 = acc / subscript_2
    v_16 = v_15.to(tl.bfloat16)
    tl.store(out + (indices_0[:, None, None] * 262144 + indices_1[None, :, None] * 64 + indices_4[None, None, :] * 1), v_16, None)

def attention(q_in: torch.Tensor, k_in: torch.Tensor, v_in: torch.Tensor, *, _launcher=_default_launcher):
    m_dim = q_in.size(-2)
    n_dim = k_in.size(-2)
    assert n_dim == v_in.size(-2)
    head_dim = 64
    assert head_dim == k_in.size(-1) == v_in.size(-1)
    q_view = q_in.reshape([-1, m_dim, head_dim])
    v_view = v_in.reshape([-1, n_dim, head_dim])
    k_view = k_in.reshape([-1, n_dim, head_dim]).transpose(1, 2)
    out = torch.empty_like(q_view)
    _BLOCK_SIZE_1 = 32
    _RDIM_SIZE_2 = 64
    _BLOCK_SIZE_3 = 32
    _launcher(_attention_kernel, (192 * triton.cdiv(4096, _BLOCK_SIZE_1),), q_view, k_view, v_view, out, _BLOCK_SIZE_1, _RDIM_SIZE_2, _BLOCK_SIZE_3, num_warps=4, num_stages=3)
    return out.view(q_in.size())

 75%|███████▌  | 6/8 [00:26<00:09,  4.78s/it]INFO:tritonbench.utils.triton_op:Took 0.02ms to get benchmark function for aten
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for sdpa
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_tutorial_flash_v2
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_tutorial_flash_v2_tma
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_tutorial_flash_v2_ws
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_tutorial_flash_v2_tma_ws
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_tutorial_flash_v2_tma_ws_persistent
INFO:tritonbench.utils.triton_op:Took 1.38ms to get benchmark function for flex_attention
INFO:tritonbench.utils.triton_op:Took 0.85ms to get benchmark function for helion_flash_attention
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_compat import libdevice
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _attention_kernel(q_view, k_view, v_view, out, _BLOCK_SIZE_1: tl.constexpr, _RDIM_SIZE_2: tl.constexpr, _BLOCK_SIZE_3: tl.constexpr):
    num_blocks_0 = 192
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    indices_4 = tl.arange(0, _RDIM_SIZE_2).to(tl.int32)
    m_i = tl.full([1, _BLOCK_SIZE_1], float('-inf'), tl.float32)
    l_i = tl.full([1, _BLOCK_SIZE_1], 1.0, tl.float32)
    acc = tl.full([1, _BLOCK_SIZE_1, 64], 0.0, tl.float32)
    q = tl.load(q_view + (indices_0[:, None, None] * 524288 + indices_1[None, :, None] * 64 + indices_4[None, None, :] * 1), None)
    for offset_2 in tl.range(0, 8192, _BLOCK_SIZE_3):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_3).to(tl.int32)
        q_copy = q
        m_i_copy = m_i
        l_i_copy = l_i
        acc_copy = acc
        q_copy_0 = q_copy
        m_i_copy_0 = m_i_copy
        l_i_copy_0 = l_i_copy
        acc_copy_0 = acc_copy
        k = tl.load(k_view + (indices_0[:, None, None] * 524288 + indices_4[None, :, None] * 1 + indices_2[None, None, :] * 64), None)
        qk = tl.reshape(tl.dot(tl.reshape(q_copy_0, [_BLOCK_SIZE_1, 64]), tl.reshape(k, [64, _BLOCK_SIZE_3]), input_precision='tf32'), [1, _BLOCK_SIZE_1, _BLOCK_SIZE_3])
        amax = tl.max(qk, 2)
        v_0 = tl.full([], 0.18033688, tl.bfloat16)
        v_1 = amax * v_0
        v_2 = v_1.to(tl.float32)
        v_3 = triton_helpers.maximum(m_i_copy_0, v_2)
        v_4 = tl.full([], 0.18033688, tl.bfloat16)
        v_5 = qk * v_4
        subscript = v_3[:, :, None]
        v_6 = v_5.to(tl.float32)
        v_7 = v_6 - subscript
        v_8 = libdevice.exp2(v_7)
        l_ij = tl.sum(v_8, 2)
        v_9 = m_i_copy_0 - v_3
        v_10 = libdevice.exp2(v_9)
        v_11 = l_i_copy_0 * v_10
        l_i = v_11 + l_ij
        subscript_1 = v_10[:, :, None]
        v_13 = acc_copy_0 * subscript_1
        v = tl.load(v_view + (indices_0[:, None, None] * 524288 + indices_2[None, :, None] * 64 + indices_4[None, None, :] * 1), None)
        v_14 = v_8.to(tl.bfloat16)
        acc = tl.reshape(tl.dot(tl.reshape(v_14, [_BLOCK_SIZE_1, _BLOCK_SIZE_3]), tl.reshape(v, [_BLOCK_SIZE_3, 64]), acc=tl.reshape(v_13, [_BLOCK_SIZE_1, 64]), input_precision='tf32'), [1, _BLOCK_SIZE_1, 64])
        m_i = v_3
    subscript_2 = l_i[:, :, None]
    v_15 = acc / subscript_2
    v_16 = v_15.to(tl.bfloat16)
    tl.store(out + (indices_0[:, None, None] * 524288 + indices_1[None, :, None] * 64 + indices_4[None, None, :] * 1), v_16, None)

def attention(q_in: torch.Tensor, k_in: torch.Tensor, v_in: torch.Tensor, *, _launcher=_default_launcher):
    m_dim = q_in.size(-2)
    n_dim = k_in.size(-2)
    assert n_dim == v_in.size(-2)
    head_dim = 64
    assert head_dim == k_in.size(-1) == v_in.size(-1)
    q_view = q_in.reshape([-1, m_dim, head_dim])
    v_view = v_in.reshape([-1, n_dim, head_dim])
    k_view = k_in.reshape([-1, n_dim, head_dim]).transpose(1, 2)
    out = torch.empty_like(q_view)
    _BLOCK_SIZE_1 = 32
    _RDIM_SIZE_2 = 64
    _BLOCK_SIZE_3 = 32
    _launcher(_attention_kernel, (192 * triton.cdiv(8192, _BLOCK_SIZE_1),), q_view, k_view, v_view, out, _BLOCK_SIZE_1, _RDIM_SIZE_2, _BLOCK_SIZE_3, num_warps=4, num_stages=3)
    return out.view(q_in.size())

 88%|████████▊ | 7/8 [00:34<00:05,  5.70s/it]INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for aten
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for sdpa
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_tutorial_flash_v2
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_tutorial_flash_v2_tma
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_tutorial_flash_v2_ws
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_tutorial_flash_v2_tma_ws
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_tutorial_flash_v2_tma_ws_persistent
INFO:tritonbench.utils.triton_op:Took 1.20ms to get benchmark function for flex_attention
INFO:tritonbench.utils.triton_op:Took 0.62ms to get benchmark function for helion_flash_attention
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_compat import libdevice
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _attention_kernel(q_view, k_view, v_view, out, _BLOCK_SIZE_1: tl.constexpr, _RDIM_SIZE_2: tl.constexpr, _BLOCK_SIZE_3: tl.constexpr):
    num_blocks_0 = 192
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    indices_4 = tl.arange(0, _RDIM_SIZE_2).to(tl.int32)
    m_i = tl.full([1, _BLOCK_SIZE_1], float('-inf'), tl.float32)
    l_i = tl.full([1, _BLOCK_SIZE_1], 1.0, tl.float32)
    acc = tl.full([1, _BLOCK_SIZE_1, 64], 0.0, tl.float32)
    q = tl.load(q_view + (indices_0[:, None, None] * 1048576 + indices_1[None, :, None] * 64 + indices_4[None, None, :] * 1), None)
    for offset_2 in tl.range(0, 16384, _BLOCK_SIZE_3):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_3).to(tl.int32)
        q_copy = q
        m_i_copy = m_i
        l_i_copy = l_i
        acc_copy = acc
        q_copy_0 = q_copy
        m_i_copy_0 = m_i_copy
        l_i_copy_0 = l_i_copy
        acc_copy_0 = acc_copy
        k = tl.load(k_view + (indices_0[:, None, None] * 1048576 + indices_4[None, :, None] * 1 + indices_2[None, None, :] * 64), None)
        qk = tl.reshape(tl.dot(tl.reshape(q_copy_0, [_BLOCK_SIZE_1, 64]), tl.reshape(k, [64, _BLOCK_SIZE_3]), input_precision='tf32'), [1, _BLOCK_SIZE_1, _BLOCK_SIZE_3])
        amax = tl.max(qk, 2)
        v_0 = tl.full([], 0.18033688, tl.bfloat16)
        v_1 = amax * v_0
        v_2 = v_1.to(tl.float32)
        v_3 = triton_helpers.maximum(m_i_copy_0, v_2)
        v_4 = tl.full([], 0.18033688, tl.bfloat16)
        v_5 = qk * v_4
        subscript = v_3[:, :, None]
        v_6 = v_5.to(tl.float32)
        v_7 = v_6 - subscript
        v_8 = libdevice.exp2(v_7)
        l_ij = tl.sum(v_8, 2)
        v_9 = m_i_copy_0 - v_3
        v_10 = libdevice.exp2(v_9)
        v_11 = l_i_copy_0 * v_10
        l_i = v_11 + l_ij
        subscript_1 = v_10[:, :, None]
        v_13 = acc_copy_0 * subscript_1
        v = tl.load(v_view + (indices_0[:, None, None] * 1048576 + indices_2[None, :, None] * 64 + indices_4[None, None, :] * 1), None)
        v_14 = v_8.to(tl.bfloat16)
        acc = tl.reshape(tl.dot(tl.reshape(v_14, [_BLOCK_SIZE_1, _BLOCK_SIZE_3]), tl.reshape(v, [_BLOCK_SIZE_3, 64]), acc=tl.reshape(v_13, [_BLOCK_SIZE_1, 64]), input_precision='tf32'), [1, _BLOCK_SIZE_1, 64])
        m_i = v_3
    subscript_2 = l_i[:, :, None]
    v_15 = acc / subscript_2
    v_16 = v_15.to(tl.bfloat16)
    tl.store(out + (indices_0[:, None, None] * 1048576 + indices_1[None, :, None] * 64 + indices_4[None, None, :] * 1), v_16, None)

def attention(q_in: torch.Tensor, k_in: torch.Tensor, v_in: torch.Tensor, *, _launcher=_default_launcher):
    m_dim = q_in.size(-2)
    n_dim = k_in.size(-2)
    assert n_dim == v_in.size(-2)
    head_dim = 64
    assert head_dim == k_in.size(-1) == v_in.size(-1)
    q_view = q_in.reshape([-1, m_dim, head_dim])
    v_view = v_in.reshape([-1, n_dim, head_dim])
    k_view = k_in.reshape([-1, n_dim, head_dim]).transpose(1, 2)
    out = torch.empty_like(q_view)
    _BLOCK_SIZE_1 = 32
    _RDIM_SIZE_2 = 64
    _BLOCK_SIZE_3 = 32
    _launcher(_attention_kernel, (192 * triton.cdiv(16384, _BLOCK_SIZE_1),), q_view, k_view, v_view, out, _BLOCK_SIZE_1, _RDIM_SIZE_2, _BLOCK_SIZE_3, num_warps=4, num_stages=3)
    return out.view(q_in.size())

100%|██████████| 8/8 [00:47<00:00,  8.14s/it]
100%|██████████| 8/8 [00:47<00:00,  5.96s/it]

Benchmark Results:
  (Batch, Heads, SeqLen, SeqLen_KV, Dhead)        sdpa-speedup    triton_tutorial_flash_v2-speedup    triton_tutorial_flash_v2_tma-speedup    triton_tutorial_flash_v2_ws-speedup    triton_tutorial_flash_v2_tma_ws-speedup    triton_tutorial_flash_v2_tma_ws_persistent-speedup    flex_attention-speedup    helion_flash_attention-speedup
------------------------------------------  ------------------  ----------------------------------  --------------------------------------  -------------------------------------  -----------------------------------------  ----------------------------------------------------  ------------------------  --------------------------------
                     (4, 48, 128, 128, 64)   3.691056700884567                   4.461916244756484                       3.452471507294051                      4.272941059813575                          3.362963046002702                                      3.27207216240001         2.236453134876754                 2.846394824737014
                     (4, 48, 256, 256, 64)   6.164718524516838                    8.11328725457329                       6.138624658268666                      6.873222752573999                          6.138624658268666                                     5.676125506654412         4.882996914601912                 4.059482117214176
                     (4, 48, 512, 512, 64)   12.06191925618177                  15.730820203902296                       12.26442794175447                     12.401060349022393                         12.105644099256509                                      8.17628546096764        11.287300768745874                 4.989540859963576
                   (4, 48, 1024, 1024, 64)  14.869351394276881                  20.240265764822077                       16.75176285420443                     16.255874030737434                         16.659650432652658                                     9.672280472030028         17.03101363785542                 5.112475442096454
                   (4, 48, 2048, 2048, 64)  15.714339052060128                  21.067182875778286                      20.374653302954226                      18.56778273619172                         19.783028712445006                                    11.184456896458626        19.777330122142615                  5.12689849366442
                   (4, 48, 4096, 4096, 64)   16.03756652196485                   20.13846316809631                       20.84323794002834                     19.898376732270734                         21.574572578787702                                     13.05260320346753        21.289763106586236                5.0594201537228605
                   (4, 48, 8192, 8192, 64)            CUDA OOM                            CUDA OOM                                CUDA OOM                               CUDA OOM                                   CUDA OOM                                              CUDA OOM                  CUDA OOM                          CUDA OOM
                 (4, 48, 16384, 16384, 64)            CUDA OOM                            CUDA OOM                                CUDA OOM                               CUDA OOM                                   CUDA OOM                                              CUDA OOM                  CUDA OOM                          CUDA OOM
                                   average    8.56736893123563                  11.218991938991092                       9.978147275563023                      9.783657207576232                          9.953060440926656                                     6.379227962747281         9.563107210601101                 3.399276486424813

============================================================
Kernel: fp8_attention
============================================================

Running fp8_attention benchmark with Helion implementation...

Removed 12 outliers from 529 samples
Removed 17 outliers from 631 samples
Removed 12 outliers from 651 samples
Removed 21 outliers from 658 samples
Removed 1 outliers from 731 samples
Removed 10 outliers from 646 samples
Removed 53 outliers from 559 samples
Removed 11 outliers from 723 samples
Removed 3 outliers from 336 samples
Removed 2 outliers from 692 samples
Removed 13 outliers from 622 samples
Removed 2 outliers from 611 samples
Removed 7 outliers from 613 samples
Removed 17 outliers from 701 samples
Removed 7 outliers from 599 samples
Removed 3 outliers from 546 samples
Removed 3 outliers from 116 samples
Removed 1 outliers from 574 samples
Removed 6 outliers from 517 samples
Removed 5 outliers from 516 samples
Removed 8 outliers from 563 samples
Removed 12 outliers from 445 samples
Removed 7 outliers from 508 samples
Removed 1 outliers from 34 samples
Removed 19 outliers from 362 samples
Removed 2 outliers from 350 samples
Removed 2 outliers from 232 samples
Removed 1 outliers from 342 samples
Removed 4 outliers from 148 samples
Removed 1 outliers from 89 samples
Removed 6 outliers from 48 samples
Removed 1 outliers from 11 samples
Removed 2 outliers from 12 samples

  0%|          | 0/8 [00:00<?, ?it/s]WARNING:tritonbench.utils.env_utils:[tritonbench] Precision fp8 is handled by operator.
INFO:tritonbench.utils.triton_op:Took 0.14ms to get benchmark function for triton_flash_v2
INFO:tritonbench.utils.triton_op:Took 0.11ms to get benchmark function for triton_flash_v2_tma
INFO:tritonbench.utils.triton_op:Took 0.14ms to get benchmark function for triton_flash_v2_ws
INFO:tritonbench.utils.triton_op:Took 0.03ms to get benchmark function for helion_fp8_attention
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import math
import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_compat import libdevice
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _fp8_attention_kernel_kernel(q, k, v, out, out_stride_0, heads, _RDIM_SIZE_2: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_3: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_5 = tl.arange(0, _RDIM_SIZE_2).to(tl.int32)
    for offset_4 in tl.range(0, 128, _BLOCK_SIZE_1):
        indices_4 = offset_4 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        m_i = tl.full([_BLOCK_SIZE_1], float('-inf'), tl.float32)
        l_i = tl.full([_BLOCK_SIZE_1], 0.0, tl.float32)
        acc = tl.full([_BLOCK_SIZE_1, 64], 0.0, tl.float32)
        q_tile = tl.load(q + (offset_0 * 8192 + indices_4[:, None] * 64 + indices_5[None, :] * 1), None)
        for offset_2 in tl.range(0, 128, _BLOCK_SIZE_3):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_3).to(tl.int32)
            q_tile_copy = q_tile
            m_i_copy = m_i
            l_i_copy = l_i
            acc_copy = acc
            q_tile_copy_0 = q_tile_copy
            m_i_copy_0 = m_i_copy
            l_i_copy_0 = l_i_copy
            acc_copy_0 = acc_copy
            k_tile = tl.load(k + (offset_0 * 8192 + indices_2[:, None] * 64 + indices_5[None, :] * 1), None)
            k_tile_t = tl.permute(k_tile, [1, 0])
            mm = tl.dot(q_tile_copy_0, k_tile_t)
            v_0 = mm.to(tl.float32)
            v_1 = 0.18033688
            v_2 = v_0 * v_1
            qk_max = tl.max(v_2, 1)
            v_3 = triton_helpers.maximum(m_i_copy_0, qk_max)
            subscript = v_3[:, None]
            v_4 = v_2 - subscript
            v_5 = libdevice.exp2(v_4)
            l_ij = tl.sum(v_5, 1)
            v_6 = m_i_copy_0 - v_3
            v_7 = libdevice.exp2(v_6)
            v_8 = l_i_copy_0 * v_7
            l_i = v_8 + l_ij
            subscript_1 = v_7[:, None]
            v_10 = acc_copy_0 * subscript_1
            v_tile = tl.load(v + (offset_0 * 8192 + indices_5[:, None] * 1 + indices_2[None, :] * 64), None)
            v_11 = v_5.to(tl.float8e5)
            v_t = tl.permute(v_tile, [1, 0])
            mm_1 = tl.dot(v_11, v_t)
            v_12 = mm_1.to(tl.float32)
            acc = v_10 + v_12
            m_i = v_3
        subscript_2 = l_i[:, None]
        v_14 = acc / subscript_2
        v_15 = v_14.to(tl.float8e5)
        symnode_0 = triton_helpers.div_floor_integer(offset_0, heads)
        symnode_1 = triton_helpers.remainder_integer(offset_0, heads)
        tl.store(out + (symnode_0 * out_stride_0 + symnode_1 * 8192 + indices_4[:, None] * 64 + indices_5[None, :] * 1), v_15, None)

def fp8_attention_kernel(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, batch: int, heads: int, *, _launcher=_default_launcher):
    batch_heads = q.size(0)
    seq_len = q.size(1)
    head_dim = q.size(2)
    out = torch.empty([batch, heads, seq_len, head_dim], dtype=torch.float8_e5m2, device=q.device)
    sm_scale = 1.0 / math.sqrt(float(head_dim))
    sm_scale = sm_scale * 1.44269504
    _RDIM_SIZE_2 = 64
    _BLOCK_SIZE_1 = 32
    _BLOCK_SIZE_3 = 32
    _launcher(_fp8_attention_kernel_kernel, (192,), q, k, v, out, out.stride(0), heads, _RDIM_SIZE_2, _BLOCK_SIZE_1, _BLOCK_SIZE_3, num_warps=4, num_stages=3)
    return out

 12%|█▎        | 1/8 [00:03<00:21,  3.06s/it]WARNING:tritonbench.utils.env_utils:[tritonbench] Precision fp8 is handled by operator.
INFO:tritonbench.utils.triton_op:Took 0.07ms to get benchmark function for triton_flash_v2
INFO:tritonbench.utils.triton_op:Took 0.11ms to get benchmark function for triton_flash_v2_tma
INFO:tritonbench.utils.triton_op:Took 0.11ms to get benchmark function for triton_flash_v2_ws
INFO:tritonbench.utils.triton_op:Took 0.55ms to get benchmark function for helion_fp8_attention
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import math
import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_compat import libdevice
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _fp8_attention_kernel_kernel(q, k, v, out, out_stride_0, heads, _RDIM_SIZE_2: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_3: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_5 = tl.arange(0, _RDIM_SIZE_2).to(tl.int32)
    for offset_4 in tl.range(0, 256, _BLOCK_SIZE_1):
        indices_4 = offset_4 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        m_i = tl.full([_BLOCK_SIZE_1], float('-inf'), tl.float32)
        l_i = tl.full([_BLOCK_SIZE_1], 0.0, tl.float32)
        acc = tl.full([_BLOCK_SIZE_1, 64], 0.0, tl.float32)
        q_tile = tl.load(q + (offset_0 * 16384 + indices_4[:, None] * 64 + indices_5[None, :] * 1), None)
        for offset_2 in tl.range(0, 256, _BLOCK_SIZE_3):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_3).to(tl.int32)
            q_tile_copy = q_tile
            m_i_copy = m_i
            l_i_copy = l_i
            acc_copy = acc
            q_tile_copy_0 = q_tile_copy
            m_i_copy_0 = m_i_copy
            l_i_copy_0 = l_i_copy
            acc_copy_0 = acc_copy
            k_tile = tl.load(k + (offset_0 * 16384 + indices_2[:, None] * 64 + indices_5[None, :] * 1), None)
            k_tile_t = tl.permute(k_tile, [1, 0])
            mm = tl.dot(q_tile_copy_0, k_tile_t)
            v_0 = mm.to(tl.float32)
            v_1 = 0.18033688
            v_2 = v_0 * v_1
            qk_max = tl.max(v_2, 1)
            v_3 = triton_helpers.maximum(m_i_copy_0, qk_max)
            subscript = v_3[:, None]
            v_4 = v_2 - subscript
            v_5 = libdevice.exp2(v_4)
            l_ij = tl.sum(v_5, 1)
            v_6 = m_i_copy_0 - v_3
            v_7 = libdevice.exp2(v_6)
            v_8 = l_i_copy_0 * v_7
            l_i = v_8 + l_ij
            subscript_1 = v_7[:, None]
            v_10 = acc_copy_0 * subscript_1
            v_tile = tl.load(v + (offset_0 * 16384 + indices_5[:, None] * 1 + indices_2[None, :] * 64), None)
            v_11 = v_5.to(tl.float8e5)
            v_t = tl.permute(v_tile, [1, 0])
            mm_1 = tl.dot(v_11, v_t)
            v_12 = mm_1.to(tl.float32)
            acc = v_10 + v_12
            m_i = v_3
        subscript_2 = l_i[:, None]
        v_14 = acc / subscript_2
        v_15 = v_14.to(tl.float8e5)
        symnode_0 = triton_helpers.div_floor_integer(offset_0, heads)
        symnode_1 = triton_helpers.remainder_integer(offset_0, heads)
        tl.store(out + (symnode_0 * out_stride_0 + symnode_1 * 16384 + indices_4[:, None] * 64 + indices_5[None, :] * 1), v_15, None)

def fp8_attention_kernel(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, batch: int, heads: int, *, _launcher=_default_launcher):
    batch_heads = q.size(0)
    seq_len = q.size(1)
    head_dim = q.size(2)
    out = torch.empty([batch, heads, seq_len, head_dim], dtype=torch.float8_e5m2, device=q.device)
    sm_scale = 1.0 / math.sqrt(float(head_dim))
    sm_scale = sm_scale * 1.44269504
    _RDIM_SIZE_2 = 64
    _BLOCK_SIZE_1 = 32
    _BLOCK_SIZE_3 = 32
    _launcher(_fp8_attention_kernel_kernel, (192,), q, k, v, out, out.stride(0), heads, _RDIM_SIZE_2, _BLOCK_SIZE_1, _BLOCK_SIZE_3, num_warps=4, num_stages=3)
    return out

 25%|██▌       | 2/8 [00:06<00:18,  3.17s/it]WARNING:tritonbench.utils.env_utils:[tritonbench] Precision fp8 is handled by operator.
INFO:tritonbench.utils.triton_op:Took 0.07ms to get benchmark function for triton_flash_v2
INFO:tritonbench.utils.triton_op:Took 0.13ms to get benchmark function for triton_flash_v2_tma
INFO:tritonbench.utils.triton_op:Took 0.13ms to get benchmark function for triton_flash_v2_ws
INFO:tritonbench.utils.triton_op:Took 0.59ms to get benchmark function for helion_fp8_attention
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import math
import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_compat import libdevice
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _fp8_attention_kernel_kernel(q, k, v, out, out_stride_0, heads, _RDIM_SIZE_2: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_3: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_5 = tl.arange(0, _RDIM_SIZE_2).to(tl.int32)
    for offset_4 in tl.range(0, 512, _BLOCK_SIZE_1):
        indices_4 = offset_4 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        m_i = tl.full([_BLOCK_SIZE_1], float('-inf'), tl.float32)
        l_i = tl.full([_BLOCK_SIZE_1], 0.0, tl.float32)
        acc = tl.full([_BLOCK_SIZE_1, 64], 0.0, tl.float32)
        q_tile = tl.load(q + (offset_0 * 32768 + indices_4[:, None] * 64 + indices_5[None, :] * 1), None)
        for offset_2 in tl.range(0, 512, _BLOCK_SIZE_3):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_3).to(tl.int32)
            q_tile_copy = q_tile
            m_i_copy = m_i
            l_i_copy = l_i
            acc_copy = acc
            q_tile_copy_0 = q_tile_copy
            m_i_copy_0 = m_i_copy
            l_i_copy_0 = l_i_copy
            acc_copy_0 = acc_copy
            k_tile = tl.load(k + (offset_0 * 32768 + indices_2[:, None] * 64 + indices_5[None, :] * 1), None)
            k_tile_t = tl.permute(k_tile, [1, 0])
            mm = tl.dot(q_tile_copy_0, k_tile_t)
            v_0 = mm.to(tl.float32)
            v_1 = 0.18033688
            v_2 = v_0 * v_1
            qk_max = tl.max(v_2, 1)
            v_3 = triton_helpers.maximum(m_i_copy_0, qk_max)
            subscript = v_3[:, None]
            v_4 = v_2 - subscript
            v_5 = libdevice.exp2(v_4)
            l_ij = tl.sum(v_5, 1)
            v_6 = m_i_copy_0 - v_3
            v_7 = libdevice.exp2(v_6)
            v_8 = l_i_copy_0 * v_7
            l_i = v_8 + l_ij
            subscript_1 = v_7[:, None]
            v_10 = acc_copy_0 * subscript_1
            v_tile = tl.load(v + (offset_0 * 32768 + indices_5[:, None] * 1 + indices_2[None, :] * 64), None)
            v_11 = v_5.to(tl.float8e5)
            v_t = tl.permute(v_tile, [1, 0])
            mm_1 = tl.dot(v_11, v_t)
            v_12 = mm_1.to(tl.float32)
            acc = v_10 + v_12
            m_i = v_3
        subscript_2 = l_i[:, None]
        v_14 = acc / subscript_2
        v_15 = v_14.to(tl.float8e5)
        symnode_0 = triton_helpers.div_floor_integer(offset_0, heads)
        symnode_1 = triton_helpers.remainder_integer(offset_0, heads)
        tl.store(out + (symnode_0 * out_stride_0 + symnode_1 * 32768 + indices_4[:, None] * 64 + indices_5[None, :] * 1), v_15, None)

def fp8_attention_kernel(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, batch: int, heads: int, *, _launcher=_default_launcher):
    batch_heads = q.size(0)
    seq_len = q.size(1)
    head_dim = q.size(2)
    out = torch.empty([batch, heads, seq_len, head_dim], dtype=torch.float8_e5m2, device=q.device)
    sm_scale = 1.0 / math.sqrt(float(head_dim))
    sm_scale = sm_scale * 1.44269504
    _RDIM_SIZE_2 = 64
    _BLOCK_SIZE_1 = 32
    _BLOCK_SIZE_3 = 32
    _launcher(_fp8_attention_kernel_kernel, (192,), q, k, v, out, out.stride(0), heads, _RDIM_SIZE_2, _BLOCK_SIZE_1, _BLOCK_SIZE_3, num_warps=4, num_stages=3)
    return out

 38%|███▊      | 3/8 [00:09<00:16,  3.21s/it]WARNING:tritonbench.utils.env_utils:[tritonbench] Precision fp8 is handled by operator.
INFO:tritonbench.utils.triton_op:Took 0.09ms to get benchmark function for triton_flash_v2
INFO:tritonbench.utils.triton_op:Took 0.15ms to get benchmark function for triton_flash_v2_tma
INFO:tritonbench.utils.triton_op:Took 0.15ms to get benchmark function for triton_flash_v2_ws
INFO:tritonbench.utils.triton_op:Took 0.61ms to get benchmark function for helion_fp8_attention
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import math
import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_compat import libdevice
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _fp8_attention_kernel_kernel(q, k, v, out, out_stride_0, heads, _RDIM_SIZE_2: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_3: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_5 = tl.arange(0, _RDIM_SIZE_2).to(tl.int32)
    for offset_4 in tl.range(0, 1024, _BLOCK_SIZE_1):
        indices_4 = offset_4 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        m_i = tl.full([_BLOCK_SIZE_1], float('-inf'), tl.float32)
        l_i = tl.full([_BLOCK_SIZE_1], 0.0, tl.float32)
        acc = tl.full([_BLOCK_SIZE_1, 64], 0.0, tl.float32)
        q_tile = tl.load(q + (offset_0 * 65536 + indices_4[:, None] * 64 + indices_5[None, :] * 1), None)
        for offset_2 in tl.range(0, 1024, _BLOCK_SIZE_3):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_3).to(tl.int32)
            q_tile_copy = q_tile
            m_i_copy = m_i
            l_i_copy = l_i
            acc_copy = acc
            q_tile_copy_0 = q_tile_copy
            m_i_copy_0 = m_i_copy
            l_i_copy_0 = l_i_copy
            acc_copy_0 = acc_copy
            k_tile = tl.load(k + (offset_0 * 65536 + indices_2[:, None] * 64 + indices_5[None, :] * 1), None)
            k_tile_t = tl.permute(k_tile, [1, 0])
            mm = tl.dot(q_tile_copy_0, k_tile_t)
            v_0 = mm.to(tl.float32)
            v_1 = 0.18033688
            v_2 = v_0 * v_1
            qk_max = tl.max(v_2, 1)
            v_3 = triton_helpers.maximum(m_i_copy_0, qk_max)
            subscript = v_3[:, None]
            v_4 = v_2 - subscript
            v_5 = libdevice.exp2(v_4)
            l_ij = tl.sum(v_5, 1)
            v_6 = m_i_copy_0 - v_3
            v_7 = libdevice.exp2(v_6)
            v_8 = l_i_copy_0 * v_7
            l_i = v_8 + l_ij
            subscript_1 = v_7[:, None]
            v_10 = acc_copy_0 * subscript_1
            v_tile = tl.load(v + (offset_0 * 65536 + indices_5[:, None] * 1 + indices_2[None, :] * 64), None)
            v_11 = v_5.to(tl.float8e5)
            v_t = tl.permute(v_tile, [1, 0])
            mm_1 = tl.dot(v_11, v_t)
            v_12 = mm_1.to(tl.float32)
            acc = v_10 + v_12
            m_i = v_3
        subscript_2 = l_i[:, None]
        v_14 = acc / subscript_2
        v_15 = v_14.to(tl.float8e5)
        symnode_0 = triton_helpers.div_floor_integer(offset_0, heads)
        symnode_1 = triton_helpers.remainder_integer(offset_0, heads)
        tl.store(out + (symnode_0 * out_stride_0 + symnode_1 * 65536 + indices_4[:, None] * 64 + indices_5[None, :] * 1), v_15, None)

def fp8_attention_kernel(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, batch: int, heads: int, *, _launcher=_default_launcher):
    batch_heads = q.size(0)
    seq_len = q.size(1)
    head_dim = q.size(2)
    out = torch.empty([batch, heads, seq_len, head_dim], dtype=torch.float8_e5m2, device=q.device)
    sm_scale = 1.0 / math.sqrt(float(head_dim))
    sm_scale = sm_scale * 1.44269504
    _RDIM_SIZE_2 = 64
    _BLOCK_SIZE_1 = 32
    _BLOCK_SIZE_3 = 32
    _launcher(_fp8_attention_kernel_kernel, (192,), q, k, v, out, out.stride(0), heads, _RDIM_SIZE_2, _BLOCK_SIZE_1, _BLOCK_SIZE_3, num_warps=4, num_stages=3)
    return out

 50%|█████     | 4/8 [00:12<00:13,  3.29s/it]WARNING:tritonbench.utils.env_utils:[tritonbench] Precision fp8 is handled by operator.
INFO:tritonbench.utils.triton_op:Took 0.11ms to get benchmark function for triton_flash_v2
INFO:tritonbench.utils.triton_op:Took 0.17ms to get benchmark function for triton_flash_v2_tma
INFO:tritonbench.utils.triton_op:Took 0.17ms to get benchmark function for triton_flash_v2_ws
INFO:tritonbench.utils.triton_op:Took 0.59ms to get benchmark function for helion_fp8_attention
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import math
import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_compat import libdevice
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _fp8_attention_kernel_kernel(q, k, v, out, out_stride_0, heads, _RDIM_SIZE_2: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_3: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_5 = tl.arange(0, _RDIM_SIZE_2).to(tl.int32)
    for offset_4 in tl.range(0, 2048, _BLOCK_SIZE_1):
        indices_4 = offset_4 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        m_i = tl.full([_BLOCK_SIZE_1], float('-inf'), tl.float32)
        l_i = tl.full([_BLOCK_SIZE_1], 0.0, tl.float32)
        acc = tl.full([_BLOCK_SIZE_1, 64], 0.0, tl.float32)
        q_tile = tl.load(q + (offset_0 * 131072 + indices_4[:, None] * 64 + indices_5[None, :] * 1), None)
        for offset_2 in tl.range(0, 2048, _BLOCK_SIZE_3):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_3).to(tl.int32)
            q_tile_copy = q_tile
            m_i_copy = m_i
            l_i_copy = l_i
            acc_copy = acc
            q_tile_copy_0 = q_tile_copy
            m_i_copy_0 = m_i_copy
            l_i_copy_0 = l_i_copy
            acc_copy_0 = acc_copy
            k_tile = tl.load(k + (offset_0 * 131072 + indices_2[:, None] * 64 + indices_5[None, :] * 1), None)
            k_tile_t = tl.permute(k_tile, [1, 0])
            mm = tl.dot(q_tile_copy_0, k_tile_t)
            v_0 = mm.to(tl.float32)
            v_1 = 0.18033688
            v_2 = v_0 * v_1
            qk_max = tl.max(v_2, 1)
            v_3 = triton_helpers.maximum(m_i_copy_0, qk_max)
            subscript = v_3[:, None]
            v_4 = v_2 - subscript
            v_5 = libdevice.exp2(v_4)
            l_ij = tl.sum(v_5, 1)
            v_6 = m_i_copy_0 - v_3
            v_7 = libdevice.exp2(v_6)
            v_8 = l_i_copy_0 * v_7
            l_i = v_8 + l_ij
            subscript_1 = v_7[:, None]
            v_10 = acc_copy_0 * subscript_1
            v_tile = tl.load(v + (offset_0 * 131072 + indices_5[:, None] * 1 + indices_2[None, :] * 64), None)
            v_11 = v_5.to(tl.float8e5)
            v_t = tl.permute(v_tile, [1, 0])
            mm_1 = tl.dot(v_11, v_t)
            v_12 = mm_1.to(tl.float32)
            acc = v_10 + v_12
            m_i = v_3
        subscript_2 = l_i[:, None]
        v_14 = acc / subscript_2
        v_15 = v_14.to(tl.float8e5)
        symnode_0 = triton_helpers.div_floor_integer(offset_0, heads)
        symnode_1 = triton_helpers.remainder_integer(offset_0, heads)
        tl.store(out + (symnode_0 * out_stride_0 + symnode_1 * 131072 + indices_4[:, None] * 64 + indices_5[None, :] * 1), v_15, None)

def fp8_attention_kernel(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, batch: int, heads: int, *, _launcher=_default_launcher):
    batch_heads = q.size(0)
    seq_len = q.size(1)
    head_dim = q.size(2)
    out = torch.empty([batch, heads, seq_len, head_dim], dtype=torch.float8_e5m2, device=q.device)
    sm_scale = 1.0 / math.sqrt(float(head_dim))
    sm_scale = sm_scale * 1.44269504
    _RDIM_SIZE_2 = 64
    _BLOCK_SIZE_1 = 32
    _BLOCK_SIZE_3 = 32
    _launcher(_fp8_attention_kernel_kernel, (192,), q, k, v, out, out.stride(0), heads, _RDIM_SIZE_2, _BLOCK_SIZE_1, _BLOCK_SIZE_3, num_warps=4, num_stages=3)
    return out

 62%|██████▎   | 5/8 [00:16<00:10,  3.41s/it]WARNING:tritonbench.utils.env_utils:[tritonbench] Precision fp8 is handled by operator.
INFO:tritonbench.utils.triton_op:Took 0.10ms to get benchmark function for triton_flash_v2
INFO:tritonbench.utils.triton_op:Took 0.18ms to get benchmark function for triton_flash_v2_tma
INFO:tritonbench.utils.triton_op:Took 0.17ms to get benchmark function for triton_flash_v2_ws
INFO:tritonbench.utils.triton_op:Took 0.64ms to get benchmark function for helion_fp8_attention
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import math
import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_compat import libdevice
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _fp8_attention_kernel_kernel(q, k, v, out, out_stride_0, heads, _RDIM_SIZE_2: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_3: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_5 = tl.arange(0, _RDIM_SIZE_2).to(tl.int32)
    for offset_4 in tl.range(0, 4096, _BLOCK_SIZE_1):
        indices_4 = offset_4 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        m_i = tl.full([_BLOCK_SIZE_1], float('-inf'), tl.float32)
        l_i = tl.full([_BLOCK_SIZE_1], 0.0, tl.float32)
        acc = tl.full([_BLOCK_SIZE_1, 64], 0.0, tl.float32)
        q_tile = tl.load(q + (offset_0 * 262144 + indices_4[:, None] * 64 + indices_5[None, :] * 1), None)
        for offset_2 in tl.range(0, 4096, _BLOCK_SIZE_3):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_3).to(tl.int32)
            q_tile_copy = q_tile
            m_i_copy = m_i
            l_i_copy = l_i
            acc_copy = acc
            q_tile_copy_0 = q_tile_copy
            m_i_copy_0 = m_i_copy
            l_i_copy_0 = l_i_copy
            acc_copy_0 = acc_copy
            k_tile = tl.load(k + (offset_0 * 262144 + indices_2[:, None] * 64 + indices_5[None, :] * 1), None)
            k_tile_t = tl.permute(k_tile, [1, 0])
            mm = tl.dot(q_tile_copy_0, k_tile_t)
            v_0 = mm.to(tl.float32)
            v_1 = 0.18033688
            v_2 = v_0 * v_1
            qk_max = tl.max(v_2, 1)
            v_3 = triton_helpers.maximum(m_i_copy_0, qk_max)
            subscript = v_3[:, None]
            v_4 = v_2 - subscript
            v_5 = libdevice.exp2(v_4)
            l_ij = tl.sum(v_5, 1)
            v_6 = m_i_copy_0 - v_3
            v_7 = libdevice.exp2(v_6)
            v_8 = l_i_copy_0 * v_7
            l_i = v_8 + l_ij
            subscript_1 = v_7[:, None]
            v_10 = acc_copy_0 * subscript_1
            v_tile = tl.load(v + (offset_0 * 262144 + indices_5[:, None] * 1 + indices_2[None, :] * 64), None)
            v_11 = v_5.to(tl.float8e5)
            v_t = tl.permute(v_tile, [1, 0])
            mm_1 = tl.dot(v_11, v_t)
            v_12 = mm_1.to(tl.float32)
            acc = v_10 + v_12
            m_i = v_3
        subscript_2 = l_i[:, None]
        v_14 = acc / subscript_2
        v_15 = v_14.to(tl.float8e5)
        symnode_0 = triton_helpers.div_floor_integer(offset_0, heads)
        symnode_1 = triton_helpers.remainder_integer(offset_0, heads)
        tl.store(out + (symnode_0 * out_stride_0 + symnode_1 * 262144 + indices_4[:, None] * 64 + indices_5[None, :] * 1), v_15, None)

def fp8_attention_kernel(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, batch: int, heads: int, *, _launcher=_default_launcher):
    batch_heads = q.size(0)
    seq_len = q.size(1)
    head_dim = q.size(2)
    out = torch.empty([batch, heads, seq_len, head_dim], dtype=torch.float8_e5m2, device=q.device)
    sm_scale = 1.0 / math.sqrt(float(head_dim))
    sm_scale = sm_scale * 1.44269504
    _RDIM_SIZE_2 = 64
    _BLOCK_SIZE_1 = 32
    _BLOCK_SIZE_3 = 32
    _launcher(_fp8_attention_kernel_kernel, (192,), q, k, v, out, out.stride(0), heads, _RDIM_SIZE_2, _BLOCK_SIZE_1, _BLOCK_SIZE_3, num_warps=4, num_stages=3)
    return out

 75%|███████▌  | 6/8 [00:20<00:07,  3.58s/it]WARNING:tritonbench.utils.env_utils:[tritonbench] Precision fp8 is handled by operator.
INFO:tritonbench.utils.triton_op:Took 0.10ms to get benchmark function for triton_flash_v2
INFO:tritonbench.utils.triton_op:Took 0.17ms to get benchmark function for triton_flash_v2_tma
INFO:tritonbench.utils.triton_op:Took 0.18ms to get benchmark function for triton_flash_v2_ws
INFO:tritonbench.utils.triton_op:Took 0.66ms to get benchmark function for helion_fp8_attention
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import math
import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_compat import libdevice
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _fp8_attention_kernel_kernel(q, k, v, out, out_stride_0, heads, _RDIM_SIZE_2: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_3: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_5 = tl.arange(0, _RDIM_SIZE_2).to(tl.int32)
    for offset_4 in tl.range(0, 8192, _BLOCK_SIZE_1):
        indices_4 = offset_4 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        m_i = tl.full([_BLOCK_SIZE_1], float('-inf'), tl.float32)
        l_i = tl.full([_BLOCK_SIZE_1], 0.0, tl.float32)
        acc = tl.full([_BLOCK_SIZE_1, 64], 0.0, tl.float32)
        q_tile = tl.load(q + (offset_0 * 524288 + indices_4[:, None] * 64 + indices_5[None, :] * 1), None)
        for offset_2 in tl.range(0, 8192, _BLOCK_SIZE_3):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_3).to(tl.int32)
            q_tile_copy = q_tile
            m_i_copy = m_i
            l_i_copy = l_i
            acc_copy = acc
            q_tile_copy_0 = q_tile_copy
            m_i_copy_0 = m_i_copy
            l_i_copy_0 = l_i_copy
            acc_copy_0 = acc_copy
            k_tile = tl.load(k + (offset_0 * 524288 + indices_2[:, None] * 64 + indices_5[None, :] * 1), None)
            k_tile_t = tl.permute(k_tile, [1, 0])
            mm = tl.dot(q_tile_copy_0, k_tile_t)
            v_0 = mm.to(tl.float32)
            v_1 = 0.18033688
            v_2 = v_0 * v_1
            qk_max = tl.max(v_2, 1)
            v_3 = triton_helpers.maximum(m_i_copy_0, qk_max)
            subscript = v_3[:, None]
            v_4 = v_2 - subscript
            v_5 = libdevice.exp2(v_4)
            l_ij = tl.sum(v_5, 1)
            v_6 = m_i_copy_0 - v_3
            v_7 = libdevice.exp2(v_6)
            v_8 = l_i_copy_0 * v_7
            l_i = v_8 + l_ij
            subscript_1 = v_7[:, None]
            v_10 = acc_copy_0 * subscript_1
            v_tile = tl.load(v + (offset_0 * 524288 + indices_5[:, None] * 1 + indices_2[None, :] * 64), None)
            v_11 = v_5.to(tl.float8e5)
            v_t = tl.permute(v_tile, [1, 0])
            mm_1 = tl.dot(v_11, v_t)
            v_12 = mm_1.to(tl.float32)
            acc = v_10 + v_12
            m_i = v_3
        subscript_2 = l_i[:, None]
        v_14 = acc / subscript_2
        v_15 = v_14.to(tl.float8e5)
        symnode_0 = triton_helpers.div_floor_integer(offset_0, heads)
        symnode_1 = triton_helpers.remainder_integer(offset_0, heads)
        tl.store(out + (symnode_0 * out_stride_0 + symnode_1 * 524288 + indices_4[:, None] * 64 + indices_5[None, :] * 1), v_15, None)

def fp8_attention_kernel(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, batch: int, heads: int, *, _launcher=_default_launcher):
    batch_heads = q.size(0)
    seq_len = q.size(1)
    head_dim = q.size(2)
    out = torch.empty([batch, heads, seq_len, head_dim], dtype=torch.float8_e5m2, device=q.device)
    sm_scale = 1.0 / math.sqrt(float(head_dim))
    sm_scale = sm_scale * 1.44269504
    _RDIM_SIZE_2 = 64
    _BLOCK_SIZE_1 = 32
    _BLOCK_SIZE_3 = 32
    _launcher(_fp8_attention_kernel_kernel, (192,), q, k, v, out, out.stride(0), heads, _RDIM_SIZE_2, _BLOCK_SIZE_1, _BLOCK_SIZE_3, num_warps=4, num_stages=3)
    return out

 88%|████████▊ | 7/8 [00:25<00:04,  4.01s/it]WARNING:tritonbench.utils.env_utils:[tritonbench] Precision fp8 is handled by operator.
INFO:tritonbench.utils.triton_op:Took 0.12ms to get benchmark function for triton_flash_v2
INFO:tritonbench.utils.triton_op:Took 0.18ms to get benchmark function for triton_flash_v2_tma
INFO:tritonbench.utils.triton_op:Took 0.18ms to get benchmark function for triton_flash_v2_ws
INFO:tritonbench.utils.triton_op:Took 0.65ms to get benchmark function for helion_fp8_attention
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import math
import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_compat import libdevice
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _fp8_attention_kernel_kernel(q, k, v, out, out_stride_0, heads, _RDIM_SIZE_2: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_3: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_5 = tl.arange(0, _RDIM_SIZE_2).to(tl.int32)
    for offset_4 in tl.range(0, 16384, _BLOCK_SIZE_1):
        indices_4 = offset_4 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        m_i = tl.full([_BLOCK_SIZE_1], float('-inf'), tl.float32)
        l_i = tl.full([_BLOCK_SIZE_1], 0.0, tl.float32)
        acc = tl.full([_BLOCK_SIZE_1, 64], 0.0, tl.float32)
        q_tile = tl.load(q + (offset_0 * 1048576 + indices_4[:, None] * 64 + indices_5[None, :] * 1), None)
        for offset_2 in tl.range(0, 16384, _BLOCK_SIZE_3):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_3).to(tl.int32)
            q_tile_copy = q_tile
            m_i_copy = m_i
            l_i_copy = l_i
            acc_copy = acc
            q_tile_copy_0 = q_tile_copy
            m_i_copy_0 = m_i_copy
            l_i_copy_0 = l_i_copy
            acc_copy_0 = acc_copy
            k_tile = tl.load(k + (offset_0 * 1048576 + indices_2[:, None] * 64 + indices_5[None, :] * 1), None)
            k_tile_t = tl.permute(k_tile, [1, 0])
            mm = tl.dot(q_tile_copy_0, k_tile_t)
            v_0 = mm.to(tl.float32)
            v_1 = 0.18033688
            v_2 = v_0 * v_1
            qk_max = tl.max(v_2, 1)
            v_3 = triton_helpers.maximum(m_i_copy_0, qk_max)
            subscript = v_3[:, None]
            v_4 = v_2 - subscript
            v_5 = libdevice.exp2(v_4)
            l_ij = tl.sum(v_5, 1)
            v_6 = m_i_copy_0 - v_3
            v_7 = libdevice.exp2(v_6)
            v_8 = l_i_copy_0 * v_7
            l_i = v_8 + l_ij
            subscript_1 = v_7[:, None]
            v_10 = acc_copy_0 * subscript_1
            v_tile = tl.load(v + (offset_0 * 1048576 + indices_5[:, None] * 1 + indices_2[None, :] * 64), None)
            v_11 = v_5.to(tl.float8e5)
            v_t = tl.permute(v_tile, [1, 0])
            mm_1 = tl.dot(v_11, v_t)
            v_12 = mm_1.to(tl.float32)
            acc = v_10 + v_12
            m_i = v_3
        subscript_2 = l_i[:, None]
        v_14 = acc / subscript_2
        v_15 = v_14.to(tl.float8e5)
        symnode_0 = triton_helpers.div_floor_integer(offset_0, heads)
        symnode_1 = triton_helpers.remainder_integer(offset_0, heads)
        tl.store(out + (symnode_0 * out_stride_0 + symnode_1 * 1048576 + indices_4[:, None] * 64 + indices_5[None, :] * 1), v_15, None)

def fp8_attention_kernel(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, batch: int, heads: int, *, _launcher=_default_launcher):
    batch_heads = q.size(0)
    seq_len = q.size(1)
    head_dim = q.size(2)
    out = torch.empty([batch, heads, seq_len, head_dim], dtype=torch.float8_e5m2, device=q.device)
    sm_scale = 1.0 / math.sqrt(float(head_dim))
    sm_scale = sm_scale * 1.44269504
    _RDIM_SIZE_2 = 64
    _BLOCK_SIZE_1 = 32
    _BLOCK_SIZE_3 = 32
    _launcher(_fp8_attention_kernel_kernel, (192,), q, k, v, out, out.stride(0), heads, _RDIM_SIZE_2, _BLOCK_SIZE_1, _BLOCK_SIZE_3, num_warps=4, num_stages=3)
    return out

100%|██████████| 8/8 [00:34<00:00,  5.69s/it]
100%|██████████| 8/8 [00:34<00:00,  4.34s/it]

Benchmark Results:
  x_val    triton_flash_v2_tma-speedup    triton_flash_v2_ws-speedup    helion_fp8_attention-speedup
-------  -----------------------------  ----------------------------  ------------------------------
      0                       0.685144                      0.855956                       0.23861
      1                       0.636691                      0.824534                       0.170685
      2                       0.721311                      0.821839                       0.117647
      3                       0.849511                      0.824346                       0.100792
      4                       0.905918                      0.812626                       0.0816883
      5                       0.986094                      0.87639                        0.0736901
      6                       1.00281                       0.907141                       0.0723761
      7                       0.986316                      0.915601                       0.0717547
average                       0.846725                      0.854804                       0.115905

============================================================
Kernel: fp8_gemm
============================================================

WARNING:tritonbench.operators.fp8_gemm.fp8_gemm:Failed to import TMA due to module not being found
Running fp8_gemm benchmark with Helion implementation...

Removed 11 outliers from 704 samples
Removed 11 outliers from 691 samples
Removed 1 outliers from 717 samples
Removed 1 outliers from 590 samples
Removed 3 outliers from 649 samples
Removed 2 outliers from 637 samples
Removed 6 outliers from 672 samples
Removed 1 outliers from 224 samples
Removed 4 outliers from 379 samples
Removed 15 outliers from 377 samples
Removed 2 outliers from 78 samples
Removed 2 outliers from 189 samples
Removed 19 outliers from 159 samples
Removed 1 outliers from 20 samples
Removed 13 outliers from 58 samples
Removed 2 outliers from 51 samples

  0%|          | 0/20 [00:00<?, ?it/s]WARNING:tritonbench.utils.env_utils:[tritonbench] Precision fp8 is handled by operator.
INFO:tritonbench.utils.triton_op:Took 40.82ms to get benchmark function for torch_fp8_gemm
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_fp8_gemm
INFO:tritonbench.utils.triton_op:Took 0.03ms to get benchmark function for helion_fp8_gemm
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _fp8_gemm_kernel(x, y, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    num_blocks_0 = tl.cdiv(1024, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
    for offset_2 in tl.range(0, 1024, _BLOCK_SIZE_2):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
        acc_copy = acc
        acc_copy_0 = acc_copy
        x_tile = tl.load(x + (indices_0[:, None] * 1024 + indices_2[None, :] * 1), None)
        y_tile = tl.load(y + (indices_2[:, None] * 1 + indices_1[None, :] * 1024), None)
        mm = tl.dot(x_tile, y_tile)
        v_0 = mm.to(tl.float32)
        acc = acc_copy_0 + v_0
    v_2 = acc.to(tl.float16)
    tl.store(out + (indices_0[:, None] * 1024 + indices_1[None, :] * 1), v_2, None)

def fp8_gemm(x: torch.Tensor, y: torch.Tensor, *, _launcher=_default_launcher):
    """FP8 General Matrix Multiplication (GEMM).

    This kernel demonstrates FP8 computation in Helion.
    When lowered to Triton, the tl.dot operation will handle
    FP8 inputs natively and accumulate to FP32.

    Args:
        x: Input tensor of shape [m, k] in FP8 format
        y: Input tensor of shape [k, n] in FP8 format

    Returns:
        Output tensor of shape [m, n] in FP16 format
    """
    m, k = x.size()
    k2, n = y.size()
    assert k == k2, f'size mismatch {k} != {k2}'
    out = torch.empty([m, n], dtype=torch.float16, device=x.device)
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 32
    _launcher(_fp8_gemm_kernel, (triton.cdiv(1024, _BLOCK_SIZE_0) * triton.cdiv(1024, _BLOCK_SIZE_1),), x, y, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

  5%|▌         | 1/20 [00:07<02:30,  7.94s/it]WARNING:tritonbench.utils.env_utils:[tritonbench] Precision fp8 is handled by operator.
INFO:tritonbench.utils.triton_op:Took 0.10ms to get benchmark function for torch_fp8_gemm
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_fp8_gemm
INFO:tritonbench.utils.triton_op:Took 0.26ms to get benchmark function for helion_fp8_gemm
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _fp8_gemm_kernel(x, y, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    num_blocks_0 = tl.cdiv(1280, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
    for offset_2 in tl.range(0, 1280, _BLOCK_SIZE_2):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
        acc_copy = acc
        acc_copy_0 = acc_copy
        x_tile = tl.load(x + (indices_0[:, None] * 1280 + indices_2[None, :] * 1), None)
        y_tile = tl.load(y + (indices_2[:, None] * 1 + indices_1[None, :] * 1280), None)
        mm = tl.dot(x_tile, y_tile)
        v_0 = mm.to(tl.float32)
        acc = acc_copy_0 + v_0
    v_2 = acc.to(tl.float16)
    tl.store(out + (indices_0[:, None] * 1280 + indices_1[None, :] * 1), v_2, None)

def fp8_gemm(x: torch.Tensor, y: torch.Tensor, *, _launcher=_default_launcher):
    """FP8 General Matrix Multiplication (GEMM).

    This kernel demonstrates FP8 computation in Helion.
    When lowered to Triton, the tl.dot operation will handle
    FP8 inputs natively and accumulate to FP32.

    Args:
        x: Input tensor of shape [m, k] in FP8 format
        y: Input tensor of shape [k, n] in FP8 format

    Returns:
        Output tensor of shape [m, n] in FP16 format
    """
    m, k = x.size()
    k2, n = y.size()
    assert k == k2, f'size mismatch {k} != {k2}'
    out = torch.empty([m, n], dtype=torch.float16, device=x.device)
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 32
    _launcher(_fp8_gemm_kernel, (triton.cdiv(1280, _BLOCK_SIZE_0) * triton.cdiv(1280, _BLOCK_SIZE_1),), x, y, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 10%|█         | 2/20 [00:10<01:22,  4.61s/it]WARNING:tritonbench.utils.env_utils:[tritonbench] Precision fp8 is handled by operator.
INFO:tritonbench.utils.triton_op:Took 0.08ms to get benchmark function for torch_fp8_gemm
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_fp8_gemm
INFO:tritonbench.utils.triton_op:Took 0.27ms to get benchmark function for helion_fp8_gemm
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _fp8_gemm_kernel(x, y, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    num_blocks_0 = tl.cdiv(1536, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
    for offset_2 in tl.range(0, 1536, _BLOCK_SIZE_2):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
        acc_copy = acc
        acc_copy_0 = acc_copy
        x_tile = tl.load(x + (indices_0[:, None] * 1536 + indices_2[None, :] * 1), None)
        y_tile = tl.load(y + (indices_2[:, None] * 1 + indices_1[None, :] * 1536), None)
        mm = tl.dot(x_tile, y_tile)
        v_0 = mm.to(tl.float32)
        acc = acc_copy_0 + v_0
    v_2 = acc.to(tl.float16)
    tl.store(out + (indices_0[:, None] * 1536 + indices_1[None, :] * 1), v_2, None)

def fp8_gemm(x: torch.Tensor, y: torch.Tensor, *, _launcher=_default_launcher):
    """FP8 General Matrix Multiplication (GEMM).

    This kernel demonstrates FP8 computation in Helion.
    When lowered to Triton, the tl.dot operation will handle
    FP8 inputs natively and accumulate to FP32.

    Args:
        x: Input tensor of shape [m, k] in FP8 format
        y: Input tensor of shape [k, n] in FP8 format

    Returns:
        Output tensor of shape [m, n] in FP16 format
    """
    m, k = x.size()
    k2, n = y.size()
    assert k == k2, f'size mismatch {k} != {k2}'
    out = torch.empty([m, n], dtype=torch.float16, device=x.device)
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 32
    _launcher(_fp8_gemm_kernel, (triton.cdiv(1536, _BLOCK_SIZE_0) * triton.cdiv(1536, _BLOCK_SIZE_1),), x, y, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 15%|█▌        | 3/20 [00:12<01:00,  3.55s/it]WARNING:tritonbench.utils.env_utils:[tritonbench] Precision fp8 is handled by operator.
INFO:tritonbench.utils.triton_op:Took 0.10ms to get benchmark function for torch_fp8_gemm
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_fp8_gemm
INFO:tritonbench.utils.triton_op:Took 0.27ms to get benchmark function for helion_fp8_gemm
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _fp8_gemm_kernel(x, y, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    num_blocks_0 = tl.cdiv(1792, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
    for offset_2 in tl.range(0, 1792, _BLOCK_SIZE_2):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
        acc_copy = acc
        acc_copy_0 = acc_copy
        x_tile = tl.load(x + (indices_0[:, None] * 1792 + indices_2[None, :] * 1), None)
        y_tile = tl.load(y + (indices_2[:, None] * 1 + indices_1[None, :] * 1792), None)
        mm = tl.dot(x_tile, y_tile)
        v_0 = mm.to(tl.float32)
        acc = acc_copy_0 + v_0
    v_2 = acc.to(tl.float16)
    tl.store(out + (indices_0[:, None] * 1792 + indices_1[None, :] * 1), v_2, None)

def fp8_gemm(x: torch.Tensor, y: torch.Tensor, *, _launcher=_default_launcher):
    """FP8 General Matrix Multiplication (GEMM).

    This kernel demonstrates FP8 computation in Helion.
    When lowered to Triton, the tl.dot operation will handle
    FP8 inputs natively and accumulate to FP32.

    Args:
        x: Input tensor of shape [m, k] in FP8 format
        y: Input tensor of shape [k, n] in FP8 format

    Returns:
        Output tensor of shape [m, n] in FP16 format
    """
    m, k = x.size()
    k2, n = y.size()
    assert k == k2, f'size mismatch {k} != {k2}'
    out = torch.empty([m, n], dtype=torch.float16, device=x.device)
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 32
    _launcher(_fp8_gemm_kernel, (triton.cdiv(1792, _BLOCK_SIZE_0) * triton.cdiv(1792, _BLOCK_SIZE_1),), x, y, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 20%|██        | 4/20 [00:14<00:49,  3.07s/it]WARNING:tritonbench.utils.env_utils:[tritonbench] Precision fp8 is handled by operator.
INFO:tritonbench.utils.triton_op:Took 0.09ms to get benchmark function for torch_fp8_gemm
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_fp8_gemm
INFO:tritonbench.utils.triton_op:Took 0.28ms to get benchmark function for helion_fp8_gemm
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _fp8_gemm_kernel(x, y, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    num_blocks_0 = tl.cdiv(2048, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
    for offset_2 in tl.range(0, 2048, _BLOCK_SIZE_2):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
        acc_copy = acc
        acc_copy_0 = acc_copy
        x_tile = tl.load(x + (indices_0[:, None] * 2048 + indices_2[None, :] * 1), None)
        y_tile = tl.load(y + (indices_2[:, None] * 1 + indices_1[None, :] * 2048), None)
        mm = tl.dot(x_tile, y_tile)
        v_0 = mm.to(tl.float32)
        acc = acc_copy_0 + v_0
    v_2 = acc.to(tl.float16)
    tl.store(out + (indices_0[:, None] * 2048 + indices_1[None, :] * 1), v_2, None)

def fp8_gemm(x: torch.Tensor, y: torch.Tensor, *, _launcher=_default_launcher):
    """FP8 General Matrix Multiplication (GEMM).

    This kernel demonstrates FP8 computation in Helion.
    When lowered to Triton, the tl.dot operation will handle
    FP8 inputs natively and accumulate to FP32.

    Args:
        x: Input tensor of shape [m, k] in FP8 format
        y: Input tensor of shape [k, n] in FP8 format

    Returns:
        Output tensor of shape [m, n] in FP16 format
    """
    m, k = x.size()
    k2, n = y.size()
    assert k == k2, f'size mismatch {k} != {k2}'
    out = torch.empty([m, n], dtype=torch.float16, device=x.device)
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 32
    _launcher(_fp8_gemm_kernel, (triton.cdiv(2048, _BLOCK_SIZE_0) * triton.cdiv(2048, _BLOCK_SIZE_1),), x, y, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 25%|██▌       | 5/20 [00:17<00:41,  2.79s/it]WARNING:tritonbench.utils.env_utils:[tritonbench] Precision fp8 is handled by operator.
INFO:tritonbench.utils.triton_op:Took 0.10ms to get benchmark function for torch_fp8_gemm
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_fp8_gemm
INFO:tritonbench.utils.triton_op:Took 0.27ms to get benchmark function for helion_fp8_gemm
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _fp8_gemm_kernel(x, y, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    num_blocks_0 = tl.cdiv(2560, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
    for offset_2 in tl.range(0, 2560, _BLOCK_SIZE_2):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
        acc_copy = acc
        acc_copy_0 = acc_copy
        x_tile = tl.load(x + (indices_0[:, None] * 2560 + indices_2[None, :] * 1), None)
        y_tile = tl.load(y + (indices_2[:, None] * 1 + indices_1[None, :] * 2560), None)
        mm = tl.dot(x_tile, y_tile)
        v_0 = mm.to(tl.float32)
        acc = acc_copy_0 + v_0
    v_2 = acc.to(tl.float16)
    tl.store(out + (indices_0[:, None] * 2560 + indices_1[None, :] * 1), v_2, None)

def fp8_gemm(x: torch.Tensor, y: torch.Tensor, *, _launcher=_default_launcher):
    """FP8 General Matrix Multiplication (GEMM).

    This kernel demonstrates FP8 computation in Helion.
    When lowered to Triton, the tl.dot operation will handle
    FP8 inputs natively and accumulate to FP32.

    Args:
        x: Input tensor of shape [m, k] in FP8 format
        y: Input tensor of shape [k, n] in FP8 format

    Returns:
        Output tensor of shape [m, n] in FP16 format
    """
    m, k = x.size()
    k2, n = y.size()
    assert k == k2, f'size mismatch {k} != {k2}'
    out = torch.empty([m, n], dtype=torch.float16, device=x.device)
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 32
    _launcher(_fp8_gemm_kernel, (triton.cdiv(2560, _BLOCK_SIZE_0) * triton.cdiv(2560, _BLOCK_SIZE_1),), x, y, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 30%|███       | 6/20 [00:19<00:36,  2.63s/it]WARNING:tritonbench.utils.env_utils:[tritonbench] Precision fp8 is handled by operator.
INFO:tritonbench.utils.triton_op:Took 0.08ms to get benchmark function for torch_fp8_gemm
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_fp8_gemm
INFO:tritonbench.utils.triton_op:Took 0.28ms to get benchmark function for helion_fp8_gemm
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _fp8_gemm_kernel(x, y, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    num_blocks_0 = tl.cdiv(3072, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
    for offset_2 in tl.range(0, 3072, _BLOCK_SIZE_2):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
        acc_copy = acc
        acc_copy_0 = acc_copy
        x_tile = tl.load(x + (indices_0[:, None] * 3072 + indices_2[None, :] * 1), None)
        y_tile = tl.load(y + (indices_2[:, None] * 1 + indices_1[None, :] * 3072), None)
        mm = tl.dot(x_tile, y_tile)
        v_0 = mm.to(tl.float32)
        acc = acc_copy_0 + v_0
    v_2 = acc.to(tl.float16)
    tl.store(out + (indices_0[:, None] * 3072 + indices_1[None, :] * 1), v_2, None)

def fp8_gemm(x: torch.Tensor, y: torch.Tensor, *, _launcher=_default_launcher):
    """FP8 General Matrix Multiplication (GEMM).

    This kernel demonstrates FP8 computation in Helion.
    When lowered to Triton, the tl.dot operation will handle
    FP8 inputs natively and accumulate to FP32.

    Args:
        x: Input tensor of shape [m, k] in FP8 format
        y: Input tensor of shape [k, n] in FP8 format

    Returns:
        Output tensor of shape [m, n] in FP16 format
    """
    m, k = x.size()
    k2, n = y.size()
    assert k == k2, f'size mismatch {k} != {k2}'
    out = torch.empty([m, n], dtype=torch.float16, device=x.device)
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 32
    _launcher(_fp8_gemm_kernel, (triton.cdiv(3072, _BLOCK_SIZE_0) * triton.cdiv(3072, _BLOCK_SIZE_1),), x, y, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 35%|███▌      | 7/20 [00:21<00:33,  2.56s/it]WARNING:tritonbench.utils.env_utils:[tritonbench] Precision fp8 is handled by operator.
INFO:tritonbench.utils.triton_op:Took 0.09ms to get benchmark function for torch_fp8_gemm
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_fp8_gemm
INFO:tritonbench.utils.triton_op:Took 0.27ms to get benchmark function for helion_fp8_gemm
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _fp8_gemm_kernel(x, y, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    num_blocks_0 = tl.cdiv(3584, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
    for offset_2 in tl.range(0, 3584, _BLOCK_SIZE_2):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
        acc_copy = acc
        acc_copy_0 = acc_copy
        x_tile = tl.load(x + (indices_0[:, None] * 3584 + indices_2[None, :] * 1), None)
        y_tile = tl.load(y + (indices_2[:, None] * 1 + indices_1[None, :] * 3584), None)
        mm = tl.dot(x_tile, y_tile)
        v_0 = mm.to(tl.float32)
        acc = acc_copy_0 + v_0
    v_2 = acc.to(tl.float16)
    tl.store(out + (indices_0[:, None] * 3584 + indices_1[None, :] * 1), v_2, None)

def fp8_gemm(x: torch.Tensor, y: torch.Tensor, *, _launcher=_default_launcher):
    """FP8 General Matrix Multiplication (GEMM).

    This kernel demonstrates FP8 computation in Helion.
    When lowered to Triton, the tl.dot operation will handle
    FP8 inputs natively and accumulate to FP32.

    Args:
        x: Input tensor of shape [m, k] in FP8 format
        y: Input tensor of shape [k, n] in FP8 format

    Returns:
        Output tensor of shape [m, n] in FP16 format
    """
    m, k = x.size()
    k2, n = y.size()
    assert k == k2, f'size mismatch {k} != {k2}'
    out = torch.empty([m, n], dtype=torch.float16, device=x.device)
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 32
    _launcher(_fp8_gemm_kernel, (triton.cdiv(3584, _BLOCK_SIZE_0) * triton.cdiv(3584, _BLOCK_SIZE_1),), x, y, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 40%|████      | 8/20 [00:24<00:30,  2.52s/it]WARNING:tritonbench.utils.env_utils:[tritonbench] Precision fp8 is handled by operator.
INFO:tritonbench.utils.triton_op:Took 0.08ms to get benchmark function for torch_fp8_gemm
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_fp8_gemm
INFO:tritonbench.utils.triton_op:Took 0.27ms to get benchmark function for helion_fp8_gemm
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _fp8_gemm_kernel(x, y, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    num_blocks_0 = tl.cdiv(4096, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
    for offset_2 in tl.range(0, 4096, _BLOCK_SIZE_2):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
        acc_copy = acc
        acc_copy_0 = acc_copy
        x_tile = tl.load(x + (indices_0[:, None] * 4096 + indices_2[None, :] * 1), None)
        y_tile = tl.load(y + (indices_2[:, None] * 1 + indices_1[None, :] * 4096), None)
        mm = tl.dot(x_tile, y_tile)
        v_0 = mm.to(tl.float32)
        acc = acc_copy_0 + v_0
    v_2 = acc.to(tl.float16)
    tl.store(out + (indices_0[:, None] * 4096 + indices_1[None, :] * 1), v_2, None)

def fp8_gemm(x: torch.Tensor, y: torch.Tensor, *, _launcher=_default_launcher):
    """FP8 General Matrix Multiplication (GEMM).

    This kernel demonstrates FP8 computation in Helion.
    When lowered to Triton, the tl.dot operation will handle
    FP8 inputs natively and accumulate to FP32.

    Args:
        x: Input tensor of shape [m, k] in FP8 format
        y: Input tensor of shape [k, n] in FP8 format

    Returns:
        Output tensor of shape [m, n] in FP16 format
    """
    m, k = x.size()
    k2, n = y.size()
    assert k == k2, f'size mismatch {k} != {k2}'
    out = torch.empty([m, n], dtype=torch.float16, device=x.device)
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 32
    _launcher(_fp8_gemm_kernel, (triton.cdiv(4096, _BLOCK_SIZE_0) * triton.cdiv(4096, _BLOCK_SIZE_1),), x, y, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 45%|████▌     | 9/20 [00:26<00:27,  2.51s/it]WARNING:tritonbench.utils.env_utils:[tritonbench] Precision fp8 is handled by operator.
INFO:tritonbench.utils.triton_op:Took 0.12ms to get benchmark function for torch_fp8_gemm
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_fp8_gemm
INFO:tritonbench.utils.triton_op:Took 0.29ms to get benchmark function for helion_fp8_gemm
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _fp8_gemm_kernel(x, y, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    num_blocks_0 = tl.cdiv(5120, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
    for offset_2 in tl.range(0, 5120, _BLOCK_SIZE_2):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
        acc_copy = acc
        acc_copy_0 = acc_copy
        x_tile = tl.load(x + (indices_0[:, None] * 5120 + indices_2[None, :] * 1), None)
        y_tile = tl.load(y + (indices_2[:, None] * 1 + indices_1[None, :] * 5120), None)
        mm = tl.dot(x_tile, y_tile)
        v_0 = mm.to(tl.float32)
        acc = acc_copy_0 + v_0
    v_2 = acc.to(tl.float16)
    tl.store(out + (indices_0[:, None] * 5120 + indices_1[None, :] * 1), v_2, None)

def fp8_gemm(x: torch.Tensor, y: torch.Tensor, *, _launcher=_default_launcher):
    """FP8 General Matrix Multiplication (GEMM).

    This kernel demonstrates FP8 computation in Helion.
    When lowered to Triton, the tl.dot operation will handle
    FP8 inputs natively and accumulate to FP32.

    Args:
        x: Input tensor of shape [m, k] in FP8 format
        y: Input tensor of shape [k, n] in FP8 format

    Returns:
        Output tensor of shape [m, n] in FP16 format
    """
    m, k = x.size()
    k2, n = y.size()
    assert k == k2, f'size mismatch {k} != {k2}'
    out = torch.empty([m, n], dtype=torch.float16, device=x.device)
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 32
    _launcher(_fp8_gemm_kernel, (triton.cdiv(5120, _BLOCK_SIZE_0) * triton.cdiv(5120, _BLOCK_SIZE_1),), x, y, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 50%|█████     | 10/20 [00:29<00:25,  2.53s/it]WARNING:tritonbench.utils.env_utils:[tritonbench] Precision fp8 is handled by operator.
INFO:tritonbench.utils.triton_op:Took 0.23ms to get benchmark function for torch_fp8_gemm
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_fp8_gemm
INFO:tritonbench.utils.triton_op:Took 0.29ms to get benchmark function for helion_fp8_gemm
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _fp8_gemm_kernel(x, y, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    num_blocks_0 = tl.cdiv(6144, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
    for offset_2 in tl.range(0, 6144, _BLOCK_SIZE_2):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
        acc_copy = acc
        acc_copy_0 = acc_copy
        x_tile = tl.load(x + (indices_0[:, None] * 6144 + indices_2[None, :] * 1), None)
        y_tile = tl.load(y + (indices_2[:, None] * 1 + indices_1[None, :] * 6144), None)
        mm = tl.dot(x_tile, y_tile)
        v_0 = mm.to(tl.float32)
        acc = acc_copy_0 + v_0
    v_2 = acc.to(tl.float16)
    tl.store(out + (indices_0[:, None] * 6144 + indices_1[None, :] * 1), v_2, None)

def fp8_gemm(x: torch.Tensor, y: torch.Tensor, *, _launcher=_default_launcher):
    """FP8 General Matrix Multiplication (GEMM).

    This kernel demonstrates FP8 computation in Helion.
    When lowered to Triton, the tl.dot operation will handle
    FP8 inputs natively and accumulate to FP32.

    Args:
        x: Input tensor of shape [m, k] in FP8 format
        y: Input tensor of shape [k, n] in FP8 format

    Returns:
        Output tensor of shape [m, n] in FP16 format
    """
    m, k = x.size()
    k2, n = y.size()
    assert k == k2, f'size mismatch {k} != {k2}'
    out = torch.empty([m, n], dtype=torch.float16, device=x.device)
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 32
    _launcher(_fp8_gemm_kernel, (triton.cdiv(6144, _BLOCK_SIZE_0) * triton.cdiv(6144, _BLOCK_SIZE_1),), x, y, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 55%|█████▌    | 11/20 [00:32<00:23,  2.58s/it]WARNING:tritonbench.utils.env_utils:[tritonbench] Precision fp8 is handled by operator.
INFO:tritonbench.utils.triton_op:Took 0.51ms to get benchmark function for torch_fp8_gemm
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_fp8_gemm
INFO:tritonbench.utils.triton_op:Took 0.28ms to get benchmark function for helion_fp8_gemm
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _fp8_gemm_kernel(x, y, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    num_blocks_0 = tl.cdiv(7168, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
    for offset_2 in tl.range(0, 7168, _BLOCK_SIZE_2):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
        acc_copy = acc
        acc_copy_0 = acc_copy
        x_tile = tl.load(x + (indices_0[:, None] * 7168 + indices_2[None, :] * 1), None)
        y_tile = tl.load(y + (indices_2[:, None] * 1 + indices_1[None, :] * 7168), None)
        mm = tl.dot(x_tile, y_tile)
        v_0 = mm.to(tl.float32)
        acc = acc_copy_0 + v_0
    v_2 = acc.to(tl.float16)
    tl.store(out + (indices_0[:, None] * 7168 + indices_1[None, :] * 1), v_2, None)

def fp8_gemm(x: torch.Tensor, y: torch.Tensor, *, _launcher=_default_launcher):
    """FP8 General Matrix Multiplication (GEMM).

    This kernel demonstrates FP8 computation in Helion.
    When lowered to Triton, the tl.dot operation will handle
    FP8 inputs natively and accumulate to FP32.

    Args:
        x: Input tensor of shape [m, k] in FP8 format
        y: Input tensor of shape [k, n] in FP8 format

    Returns:
        Output tensor of shape [m, n] in FP16 format
    """
    m, k = x.size()
    k2, n = y.size()
    assert k == k2, f'size mismatch {k} != {k2}'
    out = torch.empty([m, n], dtype=torch.float16, device=x.device)
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 32
    _launcher(_fp8_gemm_kernel, (triton.cdiv(7168, _BLOCK_SIZE_0) * triton.cdiv(7168, _BLOCK_SIZE_1),), x, y, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 60%|██████    | 12/20 [00:34<00:21,  2.64s/it]WARNING:tritonbench.utils.env_utils:[tritonbench] Precision fp8 is handled by operator.
INFO:tritonbench.utils.triton_op:Took 0.71ms to get benchmark function for torch_fp8_gemm
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_fp8_gemm
INFO:tritonbench.utils.triton_op:Took 0.30ms to get benchmark function for helion_fp8_gemm
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _fp8_gemm_kernel(x, y, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    num_blocks_0 = tl.cdiv(8192, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
    for offset_2 in tl.range(0, 8192, _BLOCK_SIZE_2):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
        acc_copy = acc
        acc_copy_0 = acc_copy
        x_tile = tl.load(x + (indices_0[:, None] * 8192 + indices_2[None, :] * 1), None)
        y_tile = tl.load(y + (indices_2[:, None] * 1 + indices_1[None, :] * 8192), None)
        mm = tl.dot(x_tile, y_tile)
        v_0 = mm.to(tl.float32)
        acc = acc_copy_0 + v_0
    v_2 = acc.to(tl.float16)
    tl.store(out + (indices_0[:, None] * 8192 + indices_1[None, :] * 1), v_2, None)

def fp8_gemm(x: torch.Tensor, y: torch.Tensor, *, _launcher=_default_launcher):
    """FP8 General Matrix Multiplication (GEMM).

    This kernel demonstrates FP8 computation in Helion.
    When lowered to Triton, the tl.dot operation will handle
    FP8 inputs natively and accumulate to FP32.

    Args:
        x: Input tensor of shape [m, k] in FP8 format
        y: Input tensor of shape [k, n] in FP8 format

    Returns:
        Output tensor of shape [m, n] in FP16 format
    """
    m, k = x.size()
    k2, n = y.size()
    assert k == k2, f'size mismatch {k} != {k2}'
    out = torch.empty([m, n], dtype=torch.float16, device=x.device)
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 32
    _launcher(_fp8_gemm_kernel, (triton.cdiv(8192, _BLOCK_SIZE_0) * triton.cdiv(8192, _BLOCK_SIZE_1),), x, y, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 65%|██████▌   | 13/20 [00:37<00:19,  2.73s/it]WARNING:tritonbench.utils.env_utils:[tritonbench] Precision fp8 is handled by operator.
INFO:tritonbench.utils.triton_op:Took 1.29ms to get benchmark function for torch_fp8_gemm
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_fp8_gemm
INFO:tritonbench.utils.triton_op:Took 0.30ms to get benchmark function for helion_fp8_gemm
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _fp8_gemm_kernel(x, y, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    num_blocks_0 = tl.cdiv(10240, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
    for offset_2 in tl.range(0, 10240, _BLOCK_SIZE_2):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
        acc_copy = acc
        acc_copy_0 = acc_copy
        x_tile = tl.load(x + (indices_0[:, None] * 10240 + indices_2[None, :] * 1), None)
        y_tile = tl.load(y + (indices_2[:, None] * 1 + indices_1[None, :] * 10240), None)
        mm = tl.dot(x_tile, y_tile)
        v_0 = mm.to(tl.float32)
        acc = acc_copy_0 + v_0
    v_2 = acc.to(tl.float16)
    tl.store(out + (indices_0[:, None] * 10240 + indices_1[None, :] * 1), v_2, None)

def fp8_gemm(x: torch.Tensor, y: torch.Tensor, *, _launcher=_default_launcher):
    """FP8 General Matrix Multiplication (GEMM).

    This kernel demonstrates FP8 computation in Helion.
    When lowered to Triton, the tl.dot operation will handle
    FP8 inputs natively and accumulate to FP32.

    Args:
        x: Input tensor of shape [m, k] in FP8 format
        y: Input tensor of shape [k, n] in FP8 format

    Returns:
        Output tensor of shape [m, n] in FP16 format
    """
    m, k = x.size()
    k2, n = y.size()
    assert k == k2, f'size mismatch {k} != {k2}'
    out = torch.empty([m, n], dtype=torch.float16, device=x.device)
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 32
    _launcher(_fp8_gemm_kernel, (triton.cdiv(10240, _BLOCK_SIZE_0) * triton.cdiv(10240, _BLOCK_SIZE_1),), x, y, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 70%|███████   | 14/20 [00:41<00:17,  2.90s/it]WARNING:tritonbench.utils.env_utils:[tritonbench] Precision fp8 is handled by operator.
INFO:tritonbench.utils.triton_op:Took 1.91ms to get benchmark function for torch_fp8_gemm
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_fp8_gemm
INFO:tritonbench.utils.triton_op:Took 0.30ms to get benchmark function for helion_fp8_gemm
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _fp8_gemm_kernel(x, y, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    num_blocks_0 = tl.cdiv(12288, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
    for offset_2 in tl.range(0, 12288, _BLOCK_SIZE_2):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
        acc_copy = acc
        acc_copy_0 = acc_copy
        x_tile = tl.load(x + (indices_0[:, None] * 12288 + indices_2[None, :] * 1), None)
        y_tile = tl.load(y + (indices_2[:, None] * 1 + indices_1[None, :] * 12288), None)
        mm = tl.dot(x_tile, y_tile)
        v_0 = mm.to(tl.float32)
        acc = acc_copy_0 + v_0
    v_2 = acc.to(tl.float16)
    tl.store(out + (indices_0[:, None] * 12288 + indices_1[None, :] * 1), v_2, None)

def fp8_gemm(x: torch.Tensor, y: torch.Tensor, *, _launcher=_default_launcher):
    """FP8 General Matrix Multiplication (GEMM).

    This kernel demonstrates FP8 computation in Helion.
    When lowered to Triton, the tl.dot operation will handle
    FP8 inputs natively and accumulate to FP32.

    Args:
        x: Input tensor of shape [m, k] in FP8 format
        y: Input tensor of shape [k, n] in FP8 format

    Returns:
        Output tensor of shape [m, n] in FP16 format
    """
    m, k = x.size()
    k2, n = y.size()
    assert k == k2, f'size mismatch {k} != {k2}'
    out = torch.empty([m, n], dtype=torch.float16, device=x.device)
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 32
    _launcher(_fp8_gemm_kernel, (triton.cdiv(12288, _BLOCK_SIZE_0) * triton.cdiv(12288, _BLOCK_SIZE_1),), x, y, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 75%|███████▌  | 15/20 [00:44<00:15,  3.19s/it]WARNING:tritonbench.utils.env_utils:[tritonbench] Precision fp8 is handled by operator.
INFO:tritonbench.utils.triton_op:Took 2.77ms to get benchmark function for torch_fp8_gemm
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_fp8_gemm
INFO:tritonbench.utils.triton_op:Took 0.29ms to get benchmark function for helion_fp8_gemm
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _fp8_gemm_kernel(x, y, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    num_blocks_0 = tl.cdiv(14336, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
    for offset_2 in tl.range(0, 14336, _BLOCK_SIZE_2):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
        acc_copy = acc
        acc_copy_0 = acc_copy
        x_tile = tl.load(x + (indices_0[:, None] * 14336 + indices_2[None, :] * 1), None)
        y_tile = tl.load(y + (indices_2[:, None] * 1 + indices_1[None, :] * 14336), None)
        mm = tl.dot(x_tile, y_tile)
        v_0 = mm.to(tl.float32)
        acc = acc_copy_0 + v_0
    v_2 = acc.to(tl.float16)
    tl.store(out + (indices_0[:, None] * 14336 + indices_1[None, :] * 1), v_2, None)

def fp8_gemm(x: torch.Tensor, y: torch.Tensor, *, _launcher=_default_launcher):
    """FP8 General Matrix Multiplication (GEMM).

    This kernel demonstrates FP8 computation in Helion.
    When lowered to Triton, the tl.dot operation will handle
    FP8 inputs natively and accumulate to FP32.

    Args:
        x: Input tensor of shape [m, k] in FP8 format
        y: Input tensor of shape [k, n] in FP8 format

    Returns:
        Output tensor of shape [m, n] in FP16 format
    """
    m, k = x.size()
    k2, n = y.size()
    assert k == k2, f'size mismatch {k} != {k2}'
    out = torch.empty([m, n], dtype=torch.float16, device=x.device)
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 32
    _launcher(_fp8_gemm_kernel, (triton.cdiv(14336, _BLOCK_SIZE_0) * triton.cdiv(14336, _BLOCK_SIZE_1),), x, y, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 80%|████████  | 16/20 [00:49<00:14,  3.70s/it]WARNING:tritonbench.utils.env_utils:[tritonbench] Precision fp8 is handled by operator.
INFO:tritonbench.utils.triton_op:Took 3.63ms to get benchmark function for torch_fp8_gemm
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_fp8_gemm
INFO:tritonbench.utils.triton_op:Took 0.31ms to get benchmark function for helion_fp8_gemm
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _fp8_gemm_kernel(x, y, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    num_blocks_0 = tl.cdiv(16384, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
    for offset_2 in tl.range(0, 16384, _BLOCK_SIZE_2):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
        acc_copy = acc
        acc_copy_0 = acc_copy
        x_tile = tl.load(x + (indices_0[:, None] * 16384 + indices_2[None, :] * 1), None)
        y_tile = tl.load(y + (indices_2[:, None] * 1 + indices_1[None, :] * 16384), None)
        mm = tl.dot(x_tile, y_tile)
        v_0 = mm.to(tl.float32)
        acc = acc_copy_0 + v_0
    v_2 = acc.to(tl.float16)
    tl.store(out + (indices_0[:, None] * 16384 + indices_1[None, :] * 1), v_2, None)

def fp8_gemm(x: torch.Tensor, y: torch.Tensor, *, _launcher=_default_launcher):
    """FP8 General Matrix Multiplication (GEMM).

    This kernel demonstrates FP8 computation in Helion.
    When lowered to Triton, the tl.dot operation will handle
    FP8 inputs natively and accumulate to FP32.

    Args:
        x: Input tensor of shape [m, k] in FP8 format
        y: Input tensor of shape [k, n] in FP8 format

    Returns:
        Output tensor of shape [m, n] in FP16 format
    """
    m, k = x.size()
    k2, n = y.size()
    assert k == k2, f'size mismatch {k} != {k2}'
    out = torch.empty([m, n], dtype=torch.float16, device=x.device)
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 32
    _launcher(_fp8_gemm_kernel, (triton.cdiv(16384, _BLOCK_SIZE_0) * triton.cdiv(16384, _BLOCK_SIZE_1),), x, y, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 85%|████████▌ | 17/20 [00:55<00:13,  4.39s/it]WARNING:tritonbench.utils.env_utils:[tritonbench] Precision fp8 is handled by operator.
INFO:tritonbench.utils.triton_op:Took 6.27ms to get benchmark function for torch_fp8_gemm
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_fp8_gemm
INFO:tritonbench.utils.triton_op:Took 0.30ms to get benchmark function for helion_fp8_gemm
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _fp8_gemm_kernel(x, y, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    num_blocks_0 = tl.cdiv(20480, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
    for offset_2 in tl.range(0, 20480, _BLOCK_SIZE_2):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
        acc_copy = acc
        acc_copy_0 = acc_copy
        x_tile = tl.load(x + (indices_0[:, None] * 20480 + indices_2[None, :] * 1), None)
        y_tile = tl.load(y + (indices_2[:, None] * 1 + indices_1[None, :] * 20480), None)
        mm = tl.dot(x_tile, y_tile)
        v_0 = mm.to(tl.float32)
        acc = acc_copy_0 + v_0
    v_2 = acc.to(tl.float16)
    tl.store(out + (indices_0[:, None] * 20480 + indices_1[None, :] * 1), v_2, None)

def fp8_gemm(x: torch.Tensor, y: torch.Tensor, *, _launcher=_default_launcher):
    """FP8 General Matrix Multiplication (GEMM).

    This kernel demonstrates FP8 computation in Helion.
    When lowered to Triton, the tl.dot operation will handle
    FP8 inputs natively and accumulate to FP32.

    Args:
        x: Input tensor of shape [m, k] in FP8 format
        y: Input tensor of shape [k, n] in FP8 format

    Returns:
        Output tensor of shape [m, n] in FP16 format
    """
    m, k = x.size()
    k2, n = y.size()
    assert k == k2, f'size mismatch {k} != {k2}'
    out = torch.empty([m, n], dtype=torch.float16, device=x.device)
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 32
    _launcher(_fp8_gemm_kernel, (triton.cdiv(20480, _BLOCK_SIZE_0) * triton.cdiv(20480, _BLOCK_SIZE_1),), x, y, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 90%|█████████ | 18/20 [01:05<00:12,  6.01s/it]WARNING:tritonbench.utils.env_utils:[tritonbench] Precision fp8 is handled by operator.
INFO:tritonbench.utils.triton_op:Took 8.91ms to get benchmark function for torch_fp8_gemm
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_fp8_gemm
INFO:tritonbench.utils.triton_op:Took 0.29ms to get benchmark function for helion_fp8_gemm
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _fp8_gemm_kernel(x, y, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    num_blocks_0 = tl.cdiv(24576, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
    for offset_2 in tl.range(0, 24576, _BLOCK_SIZE_2):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
        acc_copy = acc
        acc_copy_0 = acc_copy
        x_tile = tl.load(x + (indices_0[:, None] * 24576 + indices_2[None, :] * 1), None)
        y_tile = tl.load(y + (indices_2[:, None] * 1 + indices_1[None, :] * 24576), None)
        mm = tl.dot(x_tile, y_tile)
        v_0 = mm.to(tl.float32)
        acc = acc_copy_0 + v_0
    v_2 = acc.to(tl.float16)
    tl.store(out + (indices_0[:, None] * 24576 + indices_1[None, :] * 1), v_2, None)

def fp8_gemm(x: torch.Tensor, y: torch.Tensor, *, _launcher=_default_launcher):
    """FP8 General Matrix Multiplication (GEMM).

    This kernel demonstrates FP8 computation in Helion.
    When lowered to Triton, the tl.dot operation will handle
    FP8 inputs natively and accumulate to FP32.

    Args:
        x: Input tensor of shape [m, k] in FP8 format
        y: Input tensor of shape [k, n] in FP8 format

    Returns:
        Output tensor of shape [m, n] in FP16 format
    """
    m, k = x.size()
    k2, n = y.size()
    assert k == k2, f'size mismatch {k} != {k2}'
    out = torch.empty([m, n], dtype=torch.float16, device=x.device)
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 32
    _launcher(_fp8_gemm_kernel, (triton.cdiv(24576, _BLOCK_SIZE_0) * triton.cdiv(24576, _BLOCK_SIZE_1),), x, y, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

 95%|█████████▌| 19/20 [01:21<00:08,  8.94s/it]WARNING:tritonbench.utils.env_utils:[tritonbench] Precision fp8 is handled by operator.
INFO:tritonbench.utils.triton_op:Took 11.92ms to get benchmark function for torch_fp8_gemm
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_fp8_gemm
INFO:tritonbench.utils.triton_op:Took 0.29ms to get benchmark function for helion_fp8_gemm
INFO:helion.runtime.kernel:Output code: 
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _fp8_gemm_kernel(x, y, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    num_blocks_0 = tl.cdiv(28672, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
    for offset_2 in tl.range(0, 28672, _BLOCK_SIZE_2):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
        acc_copy = acc
        acc_copy_0 = acc_copy
        x_tile = tl.load(x + (indices_0[:, None] * 28672 + indices_2[None, :] * 1), None)
        y_tile = tl.load(y + (indices_2[:, None] * 1 + indices_1[None, :] * 28672), None)
        mm = tl.dot(x_tile, y_tile)
        v_0 = mm.to(tl.float32)
        acc = acc_copy_0 + v_0
    v_2 = acc.to(tl.float16)
    tl.store(out + (indices_0[:, None] * 28672 + indices_1[None, :] * 1), v_2, None)

def fp8_gemm(x: torch.Tensor, y: torch.Tensor, *, _launcher=_default_launcher):
    """FP8 General Matrix Multiplication (GEMM).

    This kernel demonstrates FP8 computation in Helion.
    When lowered to Triton, the tl.dot operation will handle
    FP8 inputs natively and accumulate to FP32.

    Args:
        x: Input tensor of shape [m, k] in FP8 format
        y: Input tensor of shape [k, n] in FP8 format

    Returns:
        Output tensor of shape [m, n] in FP16 format
    """
    m, k = x.size()
    k2, n = y.size()
    assert k == k2, f'size mismatch {k} != {k2}'
    out = torch.empty([m, n], dtype=torch.float16, device=x.device)
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 32
    _launcher(_fp8_gemm_kernel, (triton.cdiv(28672, _BLOCK_SIZE_0) * triton.cdiv(28672, _BLOCK_SIZE_1),), x, y, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

100%|██████████| 20/20 [01:45<00:00, 13.61s/it]
100%|██████████| 20/20 [01:45<00:00,  5.29s/it]

Benchmark Results:
                x_val    triton_fp8_gemm-speedup    helion_fp8_gemm-speedup
---------------------  -------------------------  -------------------------
   (1024, 1024, 1024)                   0.974277                  0.141127
   (1280, 1280, 1280)                   0.966292                  0.0911258
   (1536, 1536, 1536)                   0.78499                   0.0640199
   (1792, 1792, 1792)                   0.937729                  0.0545842
   (2048, 2048, 2048)                   0.97449                   0.0414737
   (2560, 2560, 2560)                   0.848616                  0.0368769
   (3072, 3072, 3072)                   0.72412                   0.0300494
   (3584, 3584, 3584)                   0.884867                  0.0280392
   (4096, 4096, 4096)                   0.925554                  0.029489
   (5120, 5120, 5120)                   0.821152                  0.0297191
   (6144, 6144, 6144)                   0.848241                  0.0282159
   (7168, 7168, 7168)                   0.83998                   0.0269086
   (8192, 8192, 8192)                   0.94507                   0.0310544
(10240, 10240, 10240)                   0.869965                  0.0288842
(12288, 12288, 12288)                   0.797854                  0.0251708
(14336, 14336, 14336)                   1.12469                   0.0345253
(16384, 16384, 16384)                   0.913596                  0.0289965
(20480, 20480, 20480)                   1.03218                   0.0312815
(24576, 24576, 24576)                   0.988939                  0.0287574
(28672, 28672, 28672)                   0.998241                  0.0281617
              average                   0.910042                  0.041923
Removed 7 outliers from 800 samples
Removed 7 outliers from 534 samples
Removed 2 outliers from 421 samples
Removed 3 outliers from 789 samples
Removed 5 outliers from 321 samples
Removed 10 outliers from 236 samples
Removed 3 outliers from 751 samples
Removed 27 outliers from 745 samples
Removed 2 outliers from 699 samples
Removed 13 outliers from 662 samples
Removed 20 outliers from 103 samples
Removed 7 outliers from 577 samples
Removed 12 outliers from 63 samples
Removed 1 outliers from 572 samples
Removed 1 outliers from 537 samples
Removed 89 outliers from 338 samples
Removed 3 outliers from 14 samples
Removed 1 outliers from 8 samples
Removed 57 outliers from 152 samples
Removed 6 outliers from 107 samples
Removed 2 outliers from 39 samples
Removed 2 outliers from 35 samples
Removed 3 outliers from 24 samples
